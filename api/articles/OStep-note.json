{"title":"OStep note","uid":"cc3ee0f3aa4d555f4b2c7e6d5e81b1c4","slug":"OStep-note","date":"2024-11-01T00:00:00.000Z","updated":"2024-12-01T16:28:18.479Z","comments":true,"path":"api/articles/OStep-note.json","keywords":null,"cover":[],"content":"<p><a href=\"https://itanken.github.io/ostep-chinese/\">操作系统导论（中文版） | ostep-chinese</a></p>\n<h1 id=\"Chapter-4-Processes\"><a href=\"#Chapter-4-Processes\" class=\"headerlink\" title=\"Chapter 4: Processes\"></a>Chapter 4: Processes</h1><p>OS provide the illusion of a nearly-endless supply of said CPUs by virtualizing the CPU, the OS can promote the illusion that many virtual CPUs exist when in fact there is only one physical CPU (or a few).</p>\n<p>This basic technique, known as <strong>time sharing</strong> of the CPU, allows users to run as many concurrent processes as they would like; the potential cost is performance, as each will run more slowly if the CPU(s) must be shared.</p>\n<p>In many operating systems, a common design paradigm is to separate high-level policies from their low-level mechanisms, which allows one easily to change policies without having to rethink the mechanism and is thus a form of modularity, a general software design principle.</p>\n<h2 id=\"4-1-Definition-of-a-Process\"><a href=\"#4-1-Definition-of-a-Process\" class=\"headerlink\" title=\"4.1 Definition of a Process\"></a>4.1 Definition of a Process</h2><ul>\n<li>A process is a program in execution.</li>\n<li>It serves as the foundation for resource management in an operating system.</li>\n</ul>\n<p><strong>component of machine state that comprises a process:</strong></p>\n<p><strong>Memory (Address Space)</strong></p>\n<ul>\n<li>Contains the instructions of the running program.</li>\n<li>Includes data that the program reads or writes during execution.</li>\n<li>Defines the range of memory a process can address.</li>\n</ul>\n<p><strong>Registers</strong></p>\n<ul>\n<li>Registers are used to execute instructions.</li>\n<li>Special registers include:<ul>\n<li><strong>Program Counter (PC)</strong>: Indicates the next instruction to execute.</li>\n<li><strong>Stack Pointer (SP)</strong> and <strong>Frame Pointer</strong>: Manage function parameters, local variables, and return addresses.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Persistent Storage (I/O Information)</strong></p>\n<ul>\n<li>Includes details about files and storage devices the process interacts with, such as open files.</li>\n</ul>\n<h2 id=\"4-2-Process-API\"><a href=\"#4-2-Process-API\" class=\"headerlink\" title=\"4.2 Process API\"></a>4.2 Process API</h2><p>The <strong>Process API</strong> defines the interface provided by the operating system to manage processes. These functionalities are critical for process lifecycle and control, and they exist in some form on all modern operating systems.</p>\n<p><strong>Key Components of the Process API</strong>:</p>\n<ol>\n<li><strong>Create</strong><ul>\n<li><strong>Function</strong>: Creates new processes.</li>\n<li>The OS is responsible for initializing a new process and associating it with the requested program.</li>\n</ul>\n</li>\n<li><strong>Destroy</strong><ul>\n<li><strong>Function</strong>: Forcefully terminates processes.</li>\n<li><strong>Usage</strong>:<ul>\n<li>Ends processes that have completed execution naturally.</li>\n<li>Halts runaway or misbehaving processes via user command </li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Wait</strong><ul>\n<li><strong>Function</strong>: Allows a process to wait until another process finishes execution.</li>\n<li><strong>Usage</strong>: Useful for synchronization between processes.</li>\n</ul>\n</li>\n<li><strong>Miscellaneous Control</strong><ul>\n<li><strong>Suspend and Resume</strong>: Temporarily stop and restart processes.</li>\n<li>Other controls, depending on the OS, may include priority changes or signaling.</li>\n</ul>\n</li>\n<li><strong>Status</strong><ul>\n<li><strong>Function</strong>: Retrieves process-related information.</li>\n<li>Examples:<ul>\n<li>How long the process has been running.</li>\n<li>Current process state (e.g., running, waiting, blocked).</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/img/image-20241121144729943.png\" alt=\"image-20241121144729943\"></p>\n<h2 id=\"4-3-Process-Creation\"><a href=\"#4-3-Process-Creation\" class=\"headerlink\" title=\"4.3 Process Creation\"></a>4.3 Process Creation</h2><p><strong>Steps of Process Creation:</strong></p>\n<ol>\n<li><strong>Loading Code and Static Data</strong><ul>\n<li>Programs reside on disk in an executable format.</li>\n<li>The OS loads the program’s code and static data (e.g., initialized variables) into the process’s address space in memory.<ul>\n<li><strong>Eager loading</strong>: Older systems load everything at once.</li>\n<li><strong>Lazy loading</strong>: Modern systems load code and data only as needed during execution (e.g., via paging).</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Allocating Runtime Stack</strong><ul>\n<li>The stack is used for:<ul>\n<li>Local variables.</li>\n<li>Function parameters.</li>\n<li>Return addresses.</li>\n</ul>\n</li>\n<li>The OS initializes the stack with arguments to <code>main()</code> (e.g., <code>argc</code> and <code>argv</code> in C programs).</li>\n</ul>\n</li>\n<li><strong>Allocating Heap Memory</strong><ul>\n<li>The <strong>heap</strong> is used for dynamically allocated data (e.g., via <code>malloc()</code> in C) and data structures such as linked lists, hash tables, trees.</li>\n<li>Initially small, it grows as the program requests more memory.</li>\n<li>The OS manages heap expansion during runtime.</li>\n</ul>\n</li>\n<li><strong>Setting Up Input/Output (I/O)</strong><ul>\n<li>In UNIX-like systems, every process starts with three default file descriptors:<ul>\n<li>Standard input (<code>stdin</code>).</li>\n<li>Standard output (<code>stdout</code>).</li>\n<li>Standard error (<code>stderr</code>).</li>\n</ul>\n</li>\n<li>These descriptors allow programs to read from the terminal and print output easily.</li>\n</ul>\n</li>\n<li><strong>Starting Execution</strong><ul>\n<li>After loading and initialization, the OS starts execution by transferring control of the CPU to the process.</li>\n<li>The process begins execution at its <strong>entry point</strong>, typically the <code>main()</code> function.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"4-4-Process-States\"><a href=\"#4-4-Process-States\" class=\"headerlink\" title=\"4.4 Process States\"></a>4.4 Process States</h2><p>Processes in an operating system exist in distinct <strong>states</strong>, which represent the progress of their execution and their interaction with system resources. At any given moment, a process can be in one of the following primary states:</p>\n<p><strong>1. Running</strong></p>\n<ul>\n<li>The process is actively executing instructions on the CPU.</li>\n<li>The OS scheduler allocates CPU time to the process, allowing it to progress.</li>\n</ul>\n<p><strong>2. Ready</strong></p>\n<ul>\n<li>The process is prepared to run but is not currently executing because the OS has assigned the CPU to another process.</li>\n<li>This state occurs due to scheduling decisions when multiple processes are competing for the CPU.</li>\n</ul>\n<p><strong>3. Blocked</strong></p>\n<ul>\n<li>The process is waiting for an external event, such as an I/O operation (e.g., reading from a disk or network).</li>\n<li>While blocked, the process cannot execute until the event it depends on is completed.</li>\n</ul>\n<p><strong>Process State Transitions:</strong></p>\n<p>The transitions between these states are governed by the OS based on scheduling and system events. The transitions are as follows:</p>\n<ul>\n<li><strong>Running → Ready:</strong><br>The process is <strong>descheduled</strong>, typically because the OS preempts it to allow another process to run.</li>\n<li><strong>Ready → Running:</strong><br>The process is <strong>scheduled</strong> by the OS, gaining control of the CPU.</li>\n<li><strong>Running → Blocked:</strong><br>The process initiates an operation (e.g., I/O request) that requires it to wait, becoming blocked.</li>\n<li><strong>Blocked → Ready:</strong><br>The event (e.g., I/O completion) occurs, making the process eligible to run again.</li>\n</ul>\n<p><img src=\"/img/image-20241121151020094.png\" alt=\"image-20241121151020094\"></p>\n<p><img src=\"/img/image-20241121151939527.png\" alt=\"image-20241121151939527\"></p>\n<p><img src=\"/img/image-20241121151949225.png\" alt=\"image-20241121151949225\"></p>\n<h2 id=\"4-5-Data-Structures\"><a href=\"#4-5-Data-Structures\" class=\"headerlink\" title=\"4.5 Data Structures\"></a>4.5 Data Structures</h2><p>Operating systems use specialized data structures to manage and track the state of processes. These structures are critical for enabling multitasking, handling process creation and termination, and performing context switches. Below are the key elements involved:</p>\n<p><strong>1. Process Control Block (PCB)</strong></p>\n<p>The <strong>Process Control Block (PCB)</strong> (or process descriptor) is a data structure used to store information about each process in the system. Each process has its own PCB, which contains essential details such as:</p>\n<ul>\n<li><strong>Process State:</strong><br>Tracks the current state (e.g., running, ready, blocked, zombie).</li>\n<li><strong>Process ID (PID):</strong><br>A unique identifier for the process.</li>\n<li><strong>Register Context:</strong><br>Stores the contents of CPU registers when the process is stopped. This includes:<ul>\n<li><strong>Instruction pointer (eip):</strong> Where the process will resume execution.</li>\n<li><strong>Stack pointer (esp):</strong> Points to the process stack.</li>\n<li>Other general-purpose registers (ebx, ecx, etc.).</li>\n</ul>\n</li>\n<li><strong>Memory Information:</strong><ul>\n<li>Pointer to the start of the process’s memory.</li>\n<li>Size of the memory allocated.</li>\n</ul>\n</li>\n<li><strong>Parent Process:</strong><br>A reference to the process’s creator (useful for hierarchical process management).</li>\n<li><strong>Open Files and Current Directory:</strong><br>Tracks resources the process is using, including open file descriptors and the working directory.</li>\n<li><strong>Kernel Stack Pointer:</strong><br>A kernel-mode stack for handling system calls and interrupts.</li>\n<li><strong>Trap Frame:</strong><br>Captures the state of the process during an interrupt.</li>\n</ul>\n<p><strong>2. Process List</strong></p>\n<p>The <strong>process list</strong> (or task list) is a system-wide structure that maintains references to all PCBs. It organizes processes based on their states:</p>\n<ul>\n<li><strong>Ready List:</strong> Processes ready to run.</li>\n<li><strong>Blocked List:</strong> Processes waiting for an event (e.g., I/O completion).</li>\n<li><strong>Running Process:</strong> Tracks the process currently using the CPU.</li>\n</ul>\n<p><strong>3. Additional States</strong></p>\n<p>While processes primarily cycle through <strong>running</strong>, <strong>ready</strong>, and <strong>blocked</strong>, additional states exist:</p>\n<ul>\n<li><strong>Unused:</strong> Represents a process slot that has not been assigned to any process.</li>\n<li><strong>Embryo:</strong> Represents a process that is being created but has not yet started execution.</li>\n<li><strong>Zombie:</strong> Indicates a process has finished execution but is waiting for its parent to retrieve its exit status using a system call like <code>wait()</code>. This ensures the OS cleans up the process data correctly.</li>\n</ul>\n<h1 id=\"Chapter-5-Process-API\"><a href=\"#Chapter-5-Process-API\" class=\"headerlink\" title=\"Chapter 5: Process API\"></a>Chapter 5: Process API</h1><h2 id=\"5-1-The-fork-System-Call\"><a href=\"#5-1-The-fork-System-Call\" class=\"headerlink\" title=\"5.1 The fork() System Call\"></a>5.1 The fork() System Call</h2><p>The <code>fork()</code> system call is one of the most essential yet unique operations in UNIX-based systems, allowing a process to create a nearly identical copy of itself, referred to as the child process.</p>\n<p><strong>Key Concepts of <code>fork()</code></strong></p>\n<ol>\n<li><strong>Process Duplication:</strong><br>After calling <code>fork()</code>, the operating system creates a new child process, duplicating the parent’s address space, registers, and process control block (PCB). Both processes execute the same code starting right after the <code>fork()</code> call.</li>\n<li><strong>Return Values:</strong><ul>\n<li>The <strong>parent</strong> process receives the PID (Process ID) of the child as the return value of <code>fork()</code>.</li>\n<li>The <strong>child</strong> process receives <code>0</code> as the return value.<br>These differing return values allow the program to distinguish between the parent and child processes and handle them accordingly.</li>\n</ul>\n</li>\n<li><strong>Address Space Isolation:</strong><br>The parent and child processes have <strong>independent address spaces</strong>, meaning changes in one process’s memory do not affect the other.</li>\n<li><strong>Non-Deterministic Scheduling:</strong><br>After <code>fork()</code>, either the parent or the child can execute first. This depends on the CPU scheduler, making the order of output non-deterministic.</li>\n</ol>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;stdlib.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> *argv[])</span> &#123;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;hello world (pid:%d)\\n&quot;</span>, (<span class=\"type\">int</span>) getpid());</span><br><span class=\"line\">    <span class=\"type\">int</span> rc = fork();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (rc &lt; <span class=\"number\">0</span>) &#123; <span class=\"comment\">// fork failed</span></span><br><span class=\"line\">        <span class=\"built_in\">fprintf</span>(<span class=\"built_in\">stderr</span>, <span class=\"string\">&quot;fork failed\\n&quot;</span>);</span><br><span class=\"line\">        <span class=\"built_in\">exit</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (rc == <span class=\"number\">0</span>) &#123; <span class=\"comment\">// child process</span></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;hello, I am child (pid:%d)\\n&quot;</span>, (<span class=\"type\">int</span>) getpid());</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">// parent process</span></span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;hello, I am parent of %d (pid:%d)\\n&quot;</span>, rc, (<span class=\"type\">int</span>) getpid());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Case 1:</p>\n<p>hello world (pid:29146)<br>hello, I am parent of 29147 (pid:29146)<br>hello, I am child (pid:29147)</p>\n<p>Case 2:</p>\n<p>hello world (pid:29146)<br>hello, I am child (pid:29147)<br>hello, I am parent of 29147 (pid:29146)</p>\n<p>The operating system’s scheduler determines the order in which parent and child processes run, and scheduling is nondeterministic. Both Case 1 and Case 2 are perfectly correct behaviors, and the order depends on the scheduler.<br>In multiprocess or multithreaded programming, such nondeterminism is common and is called a Race Condition. To deal with similar issues, the execution order of processes can be controlled through explicit synchronization, such as wait().</p>\n<h2 id=\"5-2-The-wait-System-Call\"><a href=\"#5-2-The-wait-System-Call\" class=\"headerlink\" title=\"5.2 The wait() System Call\"></a>5.2 The wait() System Call</h2><p>The <code>wait()</code> system call is used in Unix-like operating systems to make a parent process pause its execution until one of its child processes finishes. This is especially useful for ensuring that a parent doesn’t exit before its child, and it also allows the parent to retrieve information about the child’s exit status.</p>\n<p>With the <code>wait()</code> system call, we can be sure the child will always print first. If the child runs first, it prints before the parent. If the parent runs first, it immediately calls <code>wait()</code>, which makes it pause until the child finishes. Once the child exits, <code>wait()</code> returns, and then the parent prints its message.</p>\n<h2 id=\"5-3-The-exec-System-Call\"><a href=\"#5-3-The-exec-System-Call\" class=\"headerlink\" title=\"5.3 The exec() System Call\"></a>5.3 The <code>exec()</code> System Call</h2><p>The <code>exec()</code> system call is a crucial part of process creation in operating systems, particularly when you need to run a different program than the one currently executing. Here’s an overview of its purpose and behavior:</p>\n<ol>\n<li><strong>Purpose of <code>exec()</code></strong>:<ul>\n<li>The <code>exec()</code> system call allows a process to replace its own execution image with a different program. It is commonly used after a <code>fork()</code> to allow the child process to run a different program than the parent.</li>\n<li>Unlike <code>fork()</code>, which creates a new process, <code>exec()</code> replaces the current process with a new one. This means that the process ID (PID) and other resources like memory remain the same, but the program that is executing changes.</li>\n</ul>\n</li>\n<li><strong>How <code>exec()</code> Works</strong>:<ul>\n<li>The <code>exec()</code> function takes the name of an executable file and its arguments as parameters. It loads the new program into the current process’s memory space, replacing the previous program.</li>\n<li>The execution environment, including the stack and heap, is re-initialized, but the process itself remains the same (i.e., it does not create a new process).</li>\n<li>Once <code>exec()</code> is called, the current process is effectively transformed into the new program, and the old program ceases to exist in that process.</li>\n<li>The <code>exec()</code> call does not return if it is successful. If it fails, it returns an error.</li>\n</ul>\n</li>\n<li><strong>Example of <code>exec()</code> Usage</strong>:<ul>\n<li>In the example provided in the text, the child process calls <code>execvp()</code> to run the <code>wc</code> (word count) program on a file (<code>p3.c</code>).</li>\n<li>The code snippet looks like this:</li>\n</ul>\n</li>\n</ol>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">char</span> *myargs[<span class=\"number\">3</span>];</span><br><span class=\"line\">myargs[<span class=\"number\">0</span>] = strdup(<span class=\"string\">&quot;wc&quot;</span>);  <span class=\"comment\">// The program to run: &quot;wc&quot; (word count)</span></span><br><span class=\"line\">myargs[<span class=\"number\">1</span>] = strdup(<span class=\"string\">&quot;p3.c&quot;</span>);  <span class=\"comment\">// The file to count words in</span></span><br><span class=\"line\">myargs[<span class=\"number\">2</span>] = <span class=\"literal\">NULL</span>;  <span class=\"comment\">// Marks the end of the argument list</span></span><br><span class=\"line\">execvp(myargs[<span class=\"number\">0</span>], myargs);  <span class=\"comment\">// Replaces the current process with &quot;wc&quot;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>After <code>execvp()</code> is called, the child process runs the <code>wc</code> program, counting the words, lines, and bytes in the file <code>p3.c</code>. The output is displayed as:</p>\n<p>29 107 1030 p3.c</p>\n<h2 id=\"5-4-Why-fork-and-exec-Motivating-the-API\"><a href=\"#5-4-Why-fork-and-exec-Motivating-the-API\" class=\"headerlink\" title=\"5.4 Why fork() and exec()? Motivating the API\"></a>5.4 Why <code>fork()</code> and <code>exec()</code>? Motivating the API</h2><p>The separation of the <code>fork()</code> and <code>exec()</code> system calls might seem unusual, especially since they both deal with process creation. However, this design is fundamental to enabling flexible and powerful operations in UNIX-like systems, particularly for building tools like a shell.</p>\n<ol>\n<li><p><strong>Flexibility in the Shell</strong>:</p>\n<ul>\n<li>The key motivation for separating <code>fork()</code> and <code>exec()</code> is to allow the shell (or any process) to perform operations <strong>after the process is forked</strong> but <strong>before the new program is executed</strong>.</li>\n<li>This separation provides flexibility by allowing the shell to manipulate the environment (e.g., file descriptors, working directory, or other aspects) before executing the new program.</li>\n</ul>\n</li>\n<li><p><strong>How the Shell Works</strong>:</p>\n<ul>\n<li>The shell prompts the user for input (e.g., a command to execute) and processes that input.</li>\n<li>It calls <code>fork()</code> to create a new child process.</li>\n<li>The child process can then be modified (e.g., by redirecting input/output, setting environment variables) before calling <code>exec()</code> to run the desired program.</li>\n<li>Once the child process finishes, the parent waits for it to complete and then returns the prompt to the user.</li>\n</ul>\n</li>\n<li><p><strong>Example: Output Redirection</strong>:</p>\n<ul>\n<li><p>A common task in shells is redirecting output from a program to a file. The shell can accomplish this by closing the standard output (<code>STDOUT_FILENO</code>) and opening a file before calling <code>exec()</code>.</p>\n</li>\n<li><p>For example, in the following command:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">wc</span> p3.c &gt; newfile.txt</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ol>\n<p>The output of <code>wc</code> is redirected into <code>newfile.txt</code> instead of being printed to the terminal. This is done as follows:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">close(STDOUT_FILENO);  // Close the current standard output</span><br><span class=\"line\">open(<span class=\"string\">&quot;./newfile.txt&quot;</span>, O_CREAT | O_WRONLY | O_TRUNC, S_IRWXU);  // Open the file</span><br><span class=\"line\">execvp(myargs[0], myargs);  // Run the <span class=\"string\">&quot;wc&quot;</span> program</span><br></pre></td></tr></table></figure>\n<ol>\n<li><strong>Output of the Program</strong>:</li>\n</ol>\n<ul>\n<li>In the example program (<code>p4.c</code>), <code>fork()</code> creates a child process, and before calling <code>exec()</code> to run the <code>wc</code> program, the standard output is redirected to the file <code>p4.output</code>.</li>\n<li>When the program is run:</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./p4</span><br></pre></td></tr></table></figure>\n<p>The shell immediately returns the prompt (no output is shown), but the output of <code>wc</code> is written to the file <code>p4.output</code>. When the contents of the file are displayed:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cat</span> p4.output</span><br></pre></td></tr></table></figure>\n<ol>\n<li><strong>Pipes and Redirection</strong>:</li>\n</ol>\n<ul>\n<li>The same principle is used for <strong>pipes</strong>, where the output of one process is directed as input to another. This is done via the <code>pipe()</code> system call, which creates an in-memory buffer (pipe) between two processes. For example:</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">grep -o foo file | <span class=\"built_in\">wc</span> -l</span><br></pre></td></tr></table></figure>\n<p>Here, the output of <code>grep</code> (matching occurrences of “foo” in a file) is piped into <code>wc -l</code> (which counts the occurrences). The shell sets up the pipes and calls <code>fork()</code> and <code>exec()</code> for each process involved.</p>\n<ol>\n<li><strong>Why This Design Is Powerful</strong>:</li>\n</ol>\n<ul>\n<li>The <code>fork()</code>/<code>exec()</code> design allows processes to easily spawn new programs while still having control over the environment and input/output. This capability is essential for building complex tools like shells, which can run multiple programs, redirect output, chain commands, and more.</li>\n</ul>\n<h2 id=\"5-5-Process-Control-and-Users-in-UNIX-Systems\"><a href=\"#5-5-Process-Control-and-Users-in-UNIX-Systems\" class=\"headerlink\" title=\"5.5 Process Control and Users in UNIX Systems\"></a><strong>5.5 Process Control and Users in UNIX Systems</strong></h2><p>In UNIX-like operating systems, process control involves managing and interacting with running processes. The system provides various system calls to control processes, manage their execution, and handle signals.</p>\n<ol>\n<li><strong>Signals and Process Control</strong>:<ul>\n<li>The kill() system call allows processes to send signals to other processes. These signals can be used to interrupt, pause, or terminate processes.<ul>\n<li>For example, <code>SIGINT</code> (interrupt) is sent when a user presses <code>Ctrl-C</code> in a terminal, which typically causes a process to terminate.</li>\n<li><code>SIGTSTP</code> (stop) is sent when <code>Ctrl-Z</code> is pressed, pausing the process and allowing it to be resumed later.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Signal Handling</strong>:<ul>\n<li>Processes can catch certain signals by using the <code>signal()</code> system call. This allows them to handle signals in a custom way. For example, a process might catch <code>SIGTERM</code> (terminate) and perform cleanup tasks before exiting.</li>\n<li>Signals provide a mechanism for <strong>external events</strong> to interrupt or communicate with processes. The handling of signals is an important part of process control and interaction in UNIX systems.</li>\n</ul>\n</li>\n<li><strong>User Permissions</strong>:<ul>\n<li>Who can send signals to a process?<ul>\n<li>Generally, a user can only send signals to processes they own. This is important for system security and usability.</li>\n<li>If arbitrary users could send signals to processes owned by others (e.g., sending <code>SIGINT</code> to kill another user’s process), it would pose a significant security risk and could disrupt the system’s functionality.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>User Concept in UNIX</strong>:<ul>\n<li>UNIX systems are designed with the concept of <strong>users</strong> who have different access levels to resources. A <strong>user</strong> logs into the system, typically with a password, to access resources such as CPU, memory, and files.</li>\n<li>Once logged in, a user can create and control their own processes. For example, they can pause, resume, or kill the processes they own.</li>\n<li><strong>Resource Allocation</strong>: The operating system ensures fair and efficient distribution of system resources (like CPU and memory) among users and their processes, helping to maintain overall system stability and performance.</li>\n</ul>\n</li>\n<li><strong>Security and Process Control</strong>:<ul>\n<li>By limiting the ability to control processes to the user who owns them, the system ensures that each user’s actions are isolated from others. This helps maintain <strong>security</strong> and <strong>system integrity</strong>.</li>\n<li>The operating system manages which processes can communicate with each other and ensures that users cannot interfere with or disrupt each other’s processes.</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Chapter-6-Limited-Direct-Execution\"><a href=\"#Chapter-6-Limited-Direct-Execution\" class=\"headerlink\" title=\"Chapter 6:  Limited Direct Execution\"></a>Chapter 6:  Limited Direct Execution</h1><h2 id=\"6-1-Limited-Direct-Execution\"><a href=\"#6-1-Limited-Direct-Execution\" class=\"headerlink\" title=\"6.1 Limited Direct Execution\"></a>6.1 Limited Direct Execution</h2><p>The concept of <strong>limited direct execution</strong> is a technique that OS developers use to efficiently run programs on the CPU while still maintaining control over the system’s resources and processes. The technique involves running the user program as directly as possible on the CPU, but with constraints to ensure that the operating system can still manage and protect the system.</p>\n<p><strong>Basic Direct Execution Protocol</strong></p>\n<p>The process starts by creating an <strong>entry for the process</strong> in the process list. Then:</p>\n<ol>\n<li>The OS <strong>allocates memory</strong> for the program.</li>\n<li>The <strong>program code</strong> is loaded into memory (from the disk).</li>\n<li>The OS <strong>locates the entry point</strong> of the program (usually the <code>main()</code> function).</li>\n<li>The OS then <strong>jumps</strong> to that entry point and <strong>starts executing</strong> the program, running the user code directly on the CPU.</li>\n</ol>\n<p>This direct execution is shown in <strong>Figure 6.1</strong> in the book and is straightforward: the OS does basic setup (e.g., memory allocation, stack setup) and then hands control over to the user program to execute. Once the program finishes execution, it returns control to the OS, freeing memory and removing the process from the process list.</p>\n<p><img src=\"/img/image-20241121172412879.png\" alt=\"image-20241121172412879\"></p>\n<p><strong>Challenges with Direct Execution</strong></p>\n<p>While direct execution is efficient, it poses two key problems in the context of <strong>virtualizing the CPU</strong>:</p>\n<ol>\n<li><strong>Safety and Control</strong>: If the OS simply runs a program directly on the CPU, how can it prevent the program from performing unsafe actions, such as accessing restricted memory or executing harmful instructions?</li>\n<li><strong>Switching Between Processes</strong>: In a multitasking system, how can the OS ensure that it can <strong>stop</strong> one running process and switch to another to ensure <strong>time sharing</strong> between processes?</li>\n</ol>\n<p><strong>Why “Limited”?</strong></p>\n<p>The term <strong>limited</strong> in “limited direct execution” comes from the fact that the OS <strong>must impose limits</strong> to prevent the user program from violating system integrity or monopolizing CPU resources. Without these limitations, the OS would be at the mercy of user programs, and the system would fail to provide the <strong>virtualization</strong> and <strong>resource management</strong> required for multiple programs to run safely and efficiently.</p>\n<p>In essence, the operating system <strong>must control the execution</strong> of user programs, providing mechanisms for ensuring security, fairness, and process scheduling.</p>\n<h2 id=\"6-2-Problem-1-Restricted-Operations\"><a href=\"#6-2-Problem-1-Restricted-Operations\" class=\"headerlink\" title=\"6.2 Problem #1: Restricted Operations\"></a>6.2 Problem #1: Restricted Operations</h2><p>Direct execution offers the benefit of running user programs quickly since they execute directly on the CPU. However, it introduces a problem: what if the program attempts to perform operations that are restricted or privileged, such as input/output (I/O) operations, or requesting additional system resources like memory or CPU time?</p>\n<ol>\n<li><strong>The User Mode and Kernel Mode</strong></li>\n</ol>\n<p>To address these issues, most systems introduce two modes of operation: <strong>user mode</strong> and <strong>kernel mode</strong>.</p>\n<ul>\n<li><p><strong>User mode</strong>: When a program runs in user mode, it is restricted in what it can do. For example, it cannot directly perform I/O operations. If it tries to, the CPU raises an exception, and the operating system is called to handle the operation. If the program tries to perform a restricted operation, the OS can terminate it for attempting to do something unsafe.</p>\n</li>\n<li><p><strong>Kernel mode</strong>: In contrast, the operating system itself runs in kernel mode. In this mode, the OS has full access to all hardware resources and can perform privileged operations like issuing I/O requests, managing memory, and interacting with other processes.</p>\n</li>\n</ul>\n<ol>\n<li><strong>System Calls</strong></li>\n</ol>\n<p>To allow user programs to request privileged operations, the OS provides <strong>system calls</strong>, which enable programs to ask the OS to perform actions like reading from a disk or allocating memory.</p>\n<p>System calls are triggered by a special <strong>trap instruction</strong>. When a program needs to perform a system call, it executes the trap instruction, raising the privilege level and switching the CPU to kernel mode. The kernel then performs the requested operation and, once finished, uses a <strong>return-from-trap</strong> instruction to switch back to user mode and return control to the program.</p>\n<p>This mechanism ensures that the OS controls access to privileged operations, protecting the system from malicious or faulty programs.</p>\n<ol>\n<li><strong>Trap Table</strong></li>\n</ol>\n<p>The OS sets up a <strong>trap table</strong> during boot, which tells the CPU which code to run for events like system calls or interrupts (e.g., from a keyboard or disk). When a system call is made, the CPU looks up the handler in the trap table and jumps to the appropriate code in kernel mode.</p>\n<p>Programs don’t directly specify where to jump in the kernel. Instead, they pass a system-call number, which the OS uses to find the correct code to execute. This prevents user programs from accessing arbitrary parts of the kernel, enhancing security.</p>\n<p>The trap table is set up during boot and remains in place until the system is rebooted.</p>\n<ol>\n<li><strong>System Call Security</strong></li>\n</ol>\n<p>Even though system calls are carefully protected by the trap mechanism, there are still security considerations. For instance, the OS must ensure that the arguments passed to system calls are valid. If a user program passes a bad memory address to a system call (e.g., a pointer that points to kernel memory), the OS must reject the call to avoid security vulnerabilities. In secure systems, the OS must check all user inputs carefully to prevent malicious attacks.</p>\n<p>In summary, the key idea in <strong>limited direct execution</strong> is that while user programs execute directly on the CPU for efficiency, the operating system controls access to restricted operations via the user mode/kernel mode distinction and system calls. The use of system calls, traps, and a trap table ensures that the OS can perform privileged operations on behalf of user programs without compromising security or control.</p>\n<p><strong>Limited Direct Execution Protocol</strong></p>\n<p> illustrates the flow of events during the lifecycle of a program in a system using the <strong>Limited Direct Execution (LDE)</strong> protocol. This approach balances the need for performance (letting programs run directly on hardware in user mode) and control (using the OS for privileged operations in kernel mode). </p>\n<p><img src=\"/img/image-20241121203101603.png\" alt=\"image-20241121203101603\"></p>\n<ol>\n<li><strong>OS @ Boot (Kernel Mode)</strong></li>\n</ol>\n<p>This section describes what the OS does when the system boots:</p>\n<ul>\n<li><strong>Initialize trap table</strong>: The OS sets up the trap table, mapping events (e.g., system calls or interrupts) to their corresponding handlers.</li>\n<li><strong>Remember syscall handler address</strong>: The trap table includes the address of the system-call handler, so the CPU knows where to jump during a system call.</li>\n</ul>\n<ol>\n<li><strong>OS @ Run (Kernel Mode)</strong></li>\n</ol>\n<p>When a program starts running, the OS sets up the necessary environment:</p>\n<ul>\n<li><strong>Create entry for process list</strong>: The OS tracks the process in a data structure.</li>\n<li><strong>Allocate memory for program</strong>: The OS allocates memory for the program to run.</li>\n<li><strong>Load program into memory</strong>: The program’s code and data are loaded into RAM.</li>\n<li><strong>Setup user stack with <code>argv</code></strong>: The OS initializes the user stack with arguments passed to the program (e.g., <code>argc</code> and <code>argv</code> in <code>main()</code>).</li>\n<li><strong>Fill kernel stack with reg/PC</strong>: Registers and program counter (PC) are saved in the kernel stack to allow later context switches.</li>\n<li><strong><code>return-from-trap</code></strong>: The CPU switches to user mode.</li>\n<li><strong>Restore regs (from kernel stack)</strong>: The CPU restores registers for the user program.</li>\n<li><strong>Move to user mode</strong>: Execution switches from kernel to user mode.</li>\n<li><strong>Jump to <code>main</code></strong>: The program begins execution at its <code>main()</code> function.</li>\n</ul>\n<ol>\n<li><strong>Run Program (User Mode)</strong></li>\n</ol>\n<p>The program runs directly on the CPU until it needs a privileged operation:</p>\n<ul>\n<li><strong>Call system call</strong>: The program requests a system call (e.g., file I/O or memory allocation).</li>\n<li><strong>Trap into OS</strong>: The trap instruction causes a switch from user mode to kernel mode.</li>\n<li><strong>Save regs (to kernel stack)</strong>: CPU registers are saved in the kernel stack.</li>\n<li><strong>Move to kernel mode</strong>: Privileges are elevated to allow execution of the system-call handler.</li>\n<li><strong>Jump to trap handler</strong>: The OS runs the system-call handler to process the request.</li>\n</ul>\n<ol>\n<li><strong>Handle Trap (Kernel Mode)</strong></li>\n</ol>\n<p>The OS handles the system call:</p>\n<ul>\n<li><strong>Do work of syscall</strong>: The OS performs the requested operation.</li>\n<li><strong><code>return-from-trap</code></strong>: The system call completes, and control returns to the program.</li>\n<li><strong>Restore regs (from kernel stack)</strong>: CPU registers are restored.</li>\n<li><strong>Move to user mode</strong>: Execution switches back to user mode.</li>\n<li><strong>Jump to PC after trap</strong>: The program resumes execution right after the system-call instruction.</li>\n</ul>\n<ol>\n<li><strong>Return from <code>main()</code> (Exit)</strong></li>\n</ol>\n<p>When the program finishes, it makes an exit system call:</p>\n<ul>\n<li><strong>Trap (via <code>exit()</code>)</strong>: The program signals the OS it is done by invoking <code>exit()</code>.</li>\n<li><strong>Free memory of process</strong>: The OS deallocates memory used by the process.</li>\n<li><strong>Remove from process list</strong>: The OS updates its data structures to mark the process as terminated.</li>\n</ul>\n<h2 id=\"6-3-Problem-2-Switching-Between-Processes\"><a href=\"#6-3-Problem-2-Switching-Between-Processes\" class=\"headerlink\" title=\"6.3 Problem #2: Switching Between Processes\"></a>6.3 Problem #2: Switching Between Processes</h2><p>Switching between processes is challenging because the OS cannot act while it is not running on the CPU. This creates the problem of how the OS can regain control of the CPU to switch tasks.</p>\n<p><strong>A Cooperative Approach: Waiting for System Calls</strong></p>\n<ul>\n<li>In older systems like early Macintosh or Xerox Alto, processes were expected to voluntarily yield control by:<ul>\n<li><strong>Making system calls</strong> (e.g., file operations or process creation).</li>\n<li><strong>Generating traps</strong> through illegal operations (e.g., dividing by zero or accessing invalid memory).</li>\n<li><strong>Explicitly yielding</strong> via a specific system call.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Drawbacks:</strong></p>\n<ul>\n<li>If a process enters an infinite loop without making system calls or errors, the OS cannot regain control. The only solution in such cases is to reboot the machine.</li>\n</ul>\n<p><strong>A Non-Cooperative Approach: Using a Timer Interrupt</strong></p>\n<ul>\n<li><p>Key Idea:</p>\n<p> A hardware timer generates interrupts periodically, allowing the OS to regain control.</p>\n<ul>\n<li>When the timer triggers, the running process is interrupted.</li>\n<li>The hardware saves the process state and transfers control to an OS interrupt handler.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Advantages:</strong></p>\n<ul>\n<li>The OS can preempt uncooperative processes.</li>\n<li>It ensures fairness and prevents rogue processes from monopolizing the CPU.</li>\n</ul>\n<p><strong>Context Switching: Saving and Restoring Process State</strong></p>\n<ul>\n<li><strong>Decision Making:</strong> After regaining control, the OS decides whether to resume the current process or switch to another, based on scheduling policies.</li>\n<li>Steps in Context Switching:<ol>\n<li>Save the state (registers, program counter, kernel stack pointer) of the current process.</li>\n<li>Restore the state of the next process.</li>\n<li>Use the restored state to resume execution of the next process.</li>\n</ol>\n</li>\n</ul>\n<p><strong>Details:</strong></p>\n<ul>\n<li>During an interrupt, the hardware implicitly saves user registers on the kernel stack.</li>\n<li>When switching processes, the OS explicitly saves kernel registers into the process structure of the current process and restores the next process’s state.</li>\n</ul>\n<p><strong>Significance of Timer Interrupts</strong></p>\n<ul>\n<li>Timer interrupts are essential to prevent uncooperative behavior, allowing the OS to maintain system control.</li>\n<li><strong>Rebooting:</strong> While sometimes necessary (e.g., in infinite loops), rebooting ensures system reliability by resetting to a stable state and reclaiming leaked resources.</li>\n</ul>\n<p><img src=\"/img/image-20241121212633764.png\" alt=\"image-20241121212633764\"></p>\n<h1 id=\"Chapter-7-Introduction-of-Scheduling\"><a href=\"#Chapter-7-Introduction-of-Scheduling\" class=\"headerlink\" title=\"Chapter 7:  Introduction of Scheduling\"></a>Chapter 7:  Introduction of Scheduling</h1><h2 id=\"7-1-Workload-Assumptions\"><a href=\"#7-1-Workload-Assumptions\" class=\"headerlink\" title=\"7.1 Workload Assumptions\"></a>7.1 Workload Assumptions</h2><p>The following are the assumptions made about processes (or jobs) running in the system:</p>\n<ol>\n<li><p><strong>Uniform Job Duration:</strong><br>All jobs have the same runtime, simplifying comparisons and fairness among jobs.</p>\n</li>\n<li><p><strong>Simultaneous Arrival:</strong><br>All jobs arrive at the same time, eliminating complexities related to arrival time differences.</p>\n</li>\n<li><p><strong>No Preemption (Run to Completion):</strong><br>Once a job starts, it runs to completion without interruptions.</p>\n</li>\n<li><p><strong>CPU-Only Jobs:</strong><br>Jobs exclusively use the CPU, without performing any I/O operations.</p>\n</li>\n<li><p><strong>Known Job Runtimes:</strong><br>The runtime of each job is known beforehand, granting the scheduler perfect knowledge of job behavior.</p>\n</li>\n</ol>\n<p><strong>Unrealistic Assumptions</strong></p>\n<ul>\n<li><p><strong>Known Job Runtimes:</strong><br>This assumption is especially unrealistic, as no scheduler in real life has omniscient knowledge of how long a job will take. In practice, this information must be inferred or estimated.</p>\n</li>\n<li><p><strong>CPU-Only Jobs:</strong><br>Real-world jobs often involve both CPU processing and I/O operations, introducing variability in their behavior and scheduling requirements.</p>\n</li>\n<li><p><strong>Simultaneous Arrival:</strong><br>Jobs typically arrive at different times in real systems, leading to challenges in dynamically managing the workload.</p>\n</li>\n</ul>\n<p><strong>Purpose of These Assumptions</strong></p>\n<ul>\n<li>These assumptions provide an idealized and highly controlled environment for studying scheduling policies.</li>\n<li>By starting with such a simplified model, we can analyze scheduling algorithms without being distracted by the complexities of real-world systems.</li>\n<li>Later, these assumptions will be relaxed to explore more nuanced and practical scheduling policies.</li>\n</ul>\n<h2 id=\"7-2-Scheduling-Metrics\"><a href=\"#7-2-Scheduling-Metrics\" class=\"headerlink\" title=\"7.2 Scheduling Metrics\"></a>7.2 Scheduling Metrics</h2><p>Scheduling policies need <strong>metrics</strong> to evaluate their effectiveness and to compare different approaches. This section introduces <strong>turnaround time</strong> as the primary metric for performance evaluation, with a brief discussion of the trade-offs between performance and fairness.</p>\n<p><strong>Primary Metric: Turnaround Time</strong></p>\n<p>The <strong>turnaround time</strong> measures how long a job takes to complete after it enters the system. Mathematically, it is defined as:</p>\n<script type=\"math/tex; mode=display\">\nT_{\\text{turnaround}} = T_{\\text{completion}} - T_{\\text{arrival}}</script><p><strong>Current Simplification</strong></p>\n<p>Given the assumption that all jobs arrive simultaneously</p>\n<script type=\"math/tex; mode=display\">\n(T_{\\text{arrival}} = 0)</script><p>, turnaround time reduces to:</p>\n<script type=\"math/tex; mode=display\">\nT_{\\text{turnaround}} = T_{\\text{completion}}</script><p>This simplification allows us to focus solely on how completion times vary under different scheduling policies.</p>\n<h2 id=\"7-3-First-In-First-Out-FIFO\"><a href=\"#7-3-First-In-First-Out-FIFO\" class=\"headerlink\" title=\"7.3 First In, First Out (FIFO)\"></a>7.3 First In, First Out (FIFO)</h2><p>First In, First Out (FIFO), also called First Come, First Served (FCFS), processes jobs in the order of their arrival. Its simplicity makes it straightforward to implement, but this can lead to inefficiencies, especially when job durations vary significantly.</p>\n<p><img src=\"/img/image-20241122094140153.png\" alt=\"image-20241122094140153\"></p>\n<p><img src=\"/img/image-20241122094216800.png\" alt=\"image-20241122094216800\"></p>\n<p>The <strong>convoy effect</strong> is evident here: shorter jobs B and C are delayed significantly because the longer job AAA dominates CPU time. This highlights the inefficiency of FIFO when job lengths vary.</p>\n<h2 id=\"7-4-Shortest-Job-First-SJF\"><a href=\"#7-4-Shortest-Job-First-SJF\" class=\"headerlink\" title=\"7.4 Shortest Job First (SJF)\"></a>7.4 Shortest Job First (SJF)</h2><p>Shortest Job First (SJF) scheduling, a technique inspired by operations research, prioritizes jobs based on their runtime, with shorter jobs running before longer ones. This approach significantly reduces average turnaround time compared to FIFO, especially for workloads with varying job durations.</p>\n<p><img src=\"/img/image-20241122101711084.png\" alt=\"image-20241122101711084\"></p>\n<p><img src=\"/img/image-20241122103458276.png\" alt=\"image-20241122103458276\"></p>\n<h2 id=\"7-5-Shortest-Time-to-Completion-First-STCF\"><a href=\"#7-5-Shortest-Time-to-Completion-First-STCF\" class=\"headerlink\" title=\"7.5 Shortest Time-to-Completion First (STCF)\"></a>7.5 Shortest Time-to-Completion First (STCF)</h2><p>Shortest Time-to-Completion First (STCF), also known as <strong>Preemptive Shortest Job First (PSJF)</strong>, is a scheduling algorithm that addresses the limitations of SJF by introducing <strong>preemption</strong>. STCF dynamically evaluates the job queue and schedules the job with the <strong>shortest remaining time</strong>. If a shorter job arrives during the execution of another, the scheduler preempts the running job and switches to the new one.</p>\n<p><img src=\"/img/image-20241122104841331.png\" alt=\"image-20241122104841331\"></p>\n<h2 id=\"7-6-A-New-Metric-Response-Time\"><a href=\"#7-6-A-New-Metric-Response-Time\" class=\"headerlink\" title=\"7.6 A New Metric: Response Time\"></a>7.6 A New Metric: Response Time</h2><p>Response time measures the <strong>time from when a job arrives in the system to the first time it is scheduled</strong>. It is crucial for assessing <strong>interactivity</strong> in systems where user inputs expect prompt feedback.</p>\n<p>The formula for <strong>response time</strong> is:</p>\n<script type=\"math/tex; mode=display\">\nT_{\\text{response}} = T_{\\text{firstrun}} - T_{\\text{arrival}}</script><p><strong>Importance of Response Time</strong></p>\n<ul>\n<li>In early <strong>batch systems</strong>, turnaround time was the primary metric, and scheduling like STCF worked well for optimizing it.</li>\n<li>However, with the advent of <strong>time-sharing systems</strong>, users began interacting directly with computers. Quick responses became vital for user satisfaction.</li>\n<li>Poor response time can frustrate users, particularly in interactive systems where a delay can be perceived as a system failure.</li>\n</ul>\n<h2 id=\"7-7-Round-Robin\"><a href=\"#7-7-Round-Robin\" class=\"headerlink\" title=\"7.7 Round Robin\"></a>7.7 Round Robin</h2><p>Round Robin (RR) scheduling is a <strong>fair, time-sharing scheduling algorithm</strong> designed to optimize <strong>response time</strong>. It cycles through jobs, giving each a fixed <strong>time slice</strong> (or scheduling quantum) to run before switching to the next job. The process repeats until all jobs are completed.</p>\n<p><strong>Key Features</strong></p>\n<ol>\n<li><strong>Time Slices</strong>:<ul>\n<li>Jobs are preempted after a specific time slice.</li>\n<li>The time slice must be a multiple of the <strong>timer interrupt period</strong> (e.g., 10ms, 20ms, etc.).</li>\n</ul>\n</li>\n<li><strong>Fairness</strong>:<ul>\n<li>RR ensures all jobs receive CPU time in a cyclic manner.</li>\n</ul>\n</li>\n<li><strong>Time-Slicing Trade-Off</strong>:<ul>\n<li><strong>Short time slices</strong> improve response time but increase the cost of context switching.</li>\n<li><strong>Long time slices</strong> reduce the overhead of context switching but degrade system responsiveness.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Execution Order</strong>:</p>\n<ul>\n<li>A, B, and C are interleaved, and each gets a short turn to execute: ABC ABC ABC ABC ABC ……</li>\n</ul>\n<p><img src=\"/img/image-20241122112049651.png\" alt=\"image-20241122112049651\"></p>\n<p><img src=\"/img/image-20241122112137512.png\" alt=\"image-20241122112137512\"></p>\n<h2 id=\"7-8-Incorporating-I-O\"><a href=\"#7-8-Incorporating-I-O\" class=\"headerlink\" title=\"7.8 Incorporating I/O\"></a>7.8 Incorporating I/O</h2><p>Relaxing <strong>Assumption 4</strong> acknowledges that most programs perform I/O operations, which means they alternate between CPU and I/O usage. The scheduler must handle jobs that are <strong>blocked</strong> (waiting for I/O to complete) differently from jobs actively using the CPU</p>\n<p><strong>Key Concepts</strong></p>\n<ol>\n<li><strong>Blocking I/O</strong>:<ul>\n<li>When a job performs I/O, it cannot use the CPU and is moved to a <strong>blocked state</strong>.</li>\n<li>While blocked, the scheduler can assign the CPU to another job.</li>\n</ul>\n</li>\n<li><strong>Handling I/O Completion</strong>:<ul>\n<li>When I/O completes, an <strong>interrupt</strong> is triggered.</li>\n<li>The job is moved from the <strong>blocked state</strong> to the <strong>ready state</strong>.</li>\n<li>The scheduler decides whether to immediately run this job or continue with the current job.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example Scenario</strong></p>\n<ul>\n<li><strong>Jobs</strong>:<ul>\n<li>A: Runs for 10ms, then performs I/O for 10ms, repeating this cycle 5 times (total CPU time = 50ms).</li>\n<li>B: CPU-intensive job that requires 50ms of uninterrupted CPU time.</li>\n</ul>\n</li>\n<li><strong>Scheduler</strong>: Shortest Time-to-Completion First (STCF).</li>\n</ul>\n<p><strong>Naive Scheduling</strong>:</p>\n<ul>\n<li>Run A until completion, then run B.</li>\n<li>This results in poor resource utilization:<ul>\n<li>The CPU is idle while A performs I/O.</li>\n<li>Total completion time: 100ms for A + 50ms for B = <strong>150ms</strong>.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Improved Scheduling (Overlapping I/O)</strong>:</p>\n<ul>\n<li>Treat each 10ms CPU burst of A as an independent “sub-job.”</li>\n<li>Alternate between A and B, ensuring better CPU and disk utilization.</li>\n</ul>\n<p><strong>Metrics</strong>:</p>\n<ul>\n<li>Total completion time: <strong>100ms</strong> (overlapping reduces idle time).</li>\n<li>A performs I/O while B uses the CPU, maximizing utilization.</li>\n</ul>\n<p><img src=\"/img/image-20241122203556937.png\" alt=\"image-20241122203556937\"></p>\n<h1 id=\"Chapter-8-The-Multi-Level-Feedback-Queue-in-Scheduling\"><a href=\"#Chapter-8-The-Multi-Level-Feedback-Queue-in-Scheduling\" class=\"headerlink\" title=\"Chapter 8: The Multi-Level Feedback Queue in Scheduling\"></a>Chapter 8: The Multi-Level Feedback Queue in Scheduling</h1><h2 id=\"8-1-MLFQ-Basic-Rules\"><a href=\"#8-1-MLFQ-Basic-Rules\" class=\"headerlink\" title=\"8.1 MLFQ: Basic Rules\"></a>8.1 MLFQ: Basic Rules</h2><p>The Multi-Level Feedback Queue (MLFQ) scheduling algorithm is designed to dynamically adjust job priorities based on their observed behavior. This approach ensures responsiveness for interactive jobs while still allowing long-running, CPU-bound tasks to execute eventually.</p>\n<p><strong>Key Concepts</strong></p>\n<ol>\n<li><p><strong>Queues and Priorities</strong>:</p>\n<ul>\n<li>MLFQ consists of multiple queues, each representing a different <strong>priority level</strong>.</li>\n<li>Jobs in higher-priority queues run before jobs in lower-priority queues.</li>\n<li>Within the same queue, jobs are scheduled using <strong>round-robin (RR)</strong>.</li>\n</ul>\n</li>\n<li><p><strong>Rules of Scheduling</strong>:</p>\n<ul>\n<li><strong>Rule 1</strong>: If Priority(A)&gt;Priority(B), job A runs (and B does not).</li>\n<li><strong>Rule 2</strong>: If Priority(A)=Priority(B), A and B share the CPU in round-robin fashion.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Dynamic Priority Adjustment</strong></p>\n<p>MLFQ <strong>learns job behavior</strong> and adjusts priorities dynamically:</p>\n<ol>\n<li><strong>Interactive Jobs</strong>:<ul>\n<li>Jobs that frequently relinquish the CPU (e.g., waiting for keyboard input) are assumed to be interactive.</li>\n<li>These jobs are kept at <strong>higher priority</strong>, ensuring responsiveness.</li>\n</ul>\n</li>\n<li><strong>CPU-Bound Jobs</strong>:<ul>\n<li>Jobs that use the CPU intensively without I/O are considered CPU-bound.</li>\n<li>Their priority is <strong>lowered over time</strong>, allowing interactive jobs to run first.</li>\n</ul>\n</li>\n</ol>\n<p>This dynamic adjustment helps MLFQ balance system responsiveness with fairness.</p>\n<h2 id=\"8-2-Attempt-1-How-To-Change-Priority\"><a href=\"#8-2-Attempt-1-How-To-Change-Priority\" class=\"headerlink\" title=\"8.2 Attempt #1: How To Change Priority\"></a>8.2 Attempt #1: How To Change Priority</h2><p><strong>Priority Adjustment Rules:</strong></p>\n<ul>\n<li><strong>Rule 3</strong>: A new job starts at the <strong>highest priority</strong> (top queue).</li>\n<li><strong>Rule 4a</strong>: If a job uses its <strong>entire time slice</strong>, its priority is <strong>reduced</strong> (moved to a lower queue).</li>\n<li><strong>Rule 4b</strong>: If a job <strong>relinquishes the CPU</strong> before the time slice ends, it <strong>stays at the same priority level</strong>.</li>\n</ul>\n<p><strong>Examples:</strong></p>\n<ol>\n<li><p><strong>Single Long-Running Job</strong>:</p>\n<ul>\n<li>Starts at <strong>highest priority</strong> (Q2).</li>\n<li>Uses the entire time slice (10 ms), so it is <strong>demoted</strong> to Q1.</li>\n<li>Eventually moves to the <strong>lowest priority queue</strong> (Q0) and remains there.</li>\n<li><strong>Observation</strong>: The MLFQ steadily lowers the priority of long-running jobs.</li>\n</ul>\n<p><img src=\"/img/image-20241124101300041.png\" alt=\"image-20241124101300041\"></p>\n</li>\n<li><p><strong>Interactive vs. CPU-Intensive Job</strong>:</p>\n<ul>\n<li>Job A (long-running, CPU-bound) runs at Q0.</li>\n<li>Job B (short, interactive) arrives and starts at Q2.</li>\n<li>Since B completes quickly, it <strong>finishes in high-priority queues</strong> without being demoted.</li>\n<li><strong>Observation</strong>: MLFQ prioritizes short jobs initially, mimicking Shortest Job First (SJF).</li>\n</ul>\n<p><img src=\"/img/image-20241124102410793.png\" alt=\"image-20241124102410793\"></p>\n</li>\n<li><p><strong>I/O-Bound Job</strong>:</p>\n<ul>\n<li>Interactive Job B uses CPU briefly (e.g., 1 ms) before performing I/O.</li>\n<li>MLFQ keeps B in <strong>high priority</strong> (Q2) as it relinquishes the CPU early.</li>\n<li><strong>Observation</strong>: MLFQ accommodates interactive jobs efficiently, ensuring they get CPU time promptly.</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/img/image-20241124102745375.png\" alt=\"image-20241124102745375\"></p>\n<p><strong>Problems with the Current MLFQ Design:</strong></p>\n<ol>\n<li><strong>Starvation</strong>:<ul>\n<li>A <strong>flood of interactive jobs</strong> can monopolize the CPU, leaving long-running jobs <strong>starved</strong> at lower priorities.</li>\n</ul>\n</li>\n<li><strong>Gaming the Scheduler</strong>:<ul>\n<li>A clever user could manipulate priority by:<ul>\n<li>Relinquishing the CPU just before the time slice ends.</li>\n<li>Staying in higher queues, monopolizing CPU time.</li>\n</ul>\n</li>\n<li><strong>Example</strong>: Running for 99% of the time slice before issuing an I/O request.</li>\n</ul>\n</li>\n<li><strong>Behavior Change</strong>:<ul>\n<li>A job that transitions from being CPU-bound to interactive cannot <strong>regain higher priority</strong> dynamically.</li>\n</ul>\n</li>\n<li><strong>Security Concerns</strong>:<ul>\n<li>In shared environments (e.g., data centers), poor scheduler design may allow one user to <strong>exploit the system</strong>, harming others.</li>\n<li>Scheduling is thus a <strong>security-critical component</strong>.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"8-3-Attempt-2-The-Priority-Boost\"><a href=\"#8-3-Attempt-2-The-Priority-Boost\" class=\"headerlink\" title=\"8.3 Attempt #2: The Priority Boost\"></a>8.3 Attempt #2: The Priority Boost</h2><p><strong>Key Concepts:</strong></p>\n<ol>\n<li><strong>Priority Boost (Rule 5)</strong>:<ul>\n<li>To address <strong>starvation</strong> and better handle <strong>dynamic job behavior</strong>, periodically boost <strong>all jobs</strong> to the <strong>topmost priority queue</strong>.</li>\n<li>This ensures that long-running jobs eventually receive CPU time, while jobs transitioning from CPU-bound to interactive are treated fairly.</li>\n</ul>\n</li>\n<li><strong>Implementation</strong>:<ul>\n<li>After a specific time period <strong>S</strong>, move <strong>all jobs</strong> in the system to the <strong>highest-priority queue</strong> (Q2 in the example).</li>\n</ul>\n</li>\n</ol>\n<p><strong>Effects of Priority Boost:</strong></p>\n<ol>\n<li><strong>Prevention of Starvation</strong>:<ul>\n<li>Without a boost, long-running jobs in lower-priority queues may never get CPU time if interactive jobs dominate.</li>\n<li>With a periodic boost, these jobs run in <strong>round-robin fashion</strong> in the top queue, ensuring they get some service.</li>\n</ul>\n</li>\n<li><strong>Dynamic Job Behavior</strong>:<ul>\n<li>If a previously CPU-bound job becomes interactive, the <strong>priority boost</strong> allows the scheduler to <strong>treat it correctly</strong>.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example:</strong></p>\n<ul>\n<li>Scenario:<ul>\n<li>A long-running job competes with two short interactive jobs.</li>\n</ul>\n</li>\n<li>Without Boost:<ul>\n<li>The long-running job is <strong>starved</strong> in the lowest-priority queue as interactive jobs dominate the CPU.</li>\n</ul>\n</li>\n<li>With Boost:<ul>\n<li>Every 50 ms (in the example), all jobs are moved to the top queue.</li>\n<li>The long-running job periodically receives CPU time.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/img/image-20241124144023608.png\" alt=\"image-20241124144023608\"></p>\n<h2 id=\"8-4-Attempt-3-Better-Accounting\"><a href=\"#8-4-Attempt-3-Better-Accounting\" class=\"headerlink\" title=\"8.4 Attempt #3: Better Accounting\"></a>8.4 Attempt #3: Better Accounting</h2><p><strong>Problem Addressed</strong>:</p>\n<ul>\n<li>The previous rules (including priority boost) improved fairness but still had inefficiencies:<ol>\n<li>Some jobs could manipulate the system by becoming temporarily interactive to regain high priority.</li>\n<li>The scheduler lacked precise tracking of CPU usage across jobs, leading to potential unfairness.</li>\n</ol>\n</li>\n</ul>\n<p><strong>Proposed Solution - Better Accounting</strong>:</p>\n<p>The idea is to improve <strong>accounting mechanisms</strong> to ensure:</p>\n<ol>\n<li>Accurate tracking of <strong>CPU usage</strong>.</li>\n<li>Prevention of <strong>gaming the system</strong> by processes.</li>\n</ol>\n<p><img src=\"/img/image-20241124154249183.png\" alt=\"image-20241124154249183\"></p>\n<h2 id=\"8-5-Tuning-MLFQ-And-Other-Issues\"><a href=\"#8-5-Tuning-MLFQ-And-Other-Issues\" class=\"headerlink\" title=\"8.5 Tuning MLFQ And Other Issues\"></a>8.5 Tuning MLFQ And Other Issues</h2><p><strong>Challenges in Tuning MLFQ</strong>:</p>\n<p>The Multi-Level Feedback Queue (MLFQ) scheduler requires careful parameter tuning to achieve an optimal balance between fairness and efficiency. However, finding the right configuration is non-trivial, leading to challenges like:</p>\n<ol>\n<li><strong>Number of Queues</strong>:<ul>\n<li>Too few queues: Insufficient differentiation between job types.</li>\n<li>Too many queues: Increased complexity and overhead.</li>\n</ul>\n</li>\n<li><strong>Time Slice per Queue</strong>:<ul>\n<li>High-priority queues (interactive jobs): Require <strong>shorter time slices</strong> (e.g., 10 ms) for responsiveness.</li>\n<li>Low-priority queues (CPU-bound jobs): Benefit from <strong>longer time slices</strong> (e.g., 100+ ms) to reduce context switching overhead.</li>\n</ul>\n</li>\n<li><strong>Priority Boost Frequency</strong>:<ul>\n<li><strong>Frequent boosts</strong>: Minimize starvation but may disrupt fairness.</li>\n<li><strong>Infrequent boosts</strong>: Allow starvation, especially for long-running CPU-bound jobs.</li>\n</ul>\n</li>\n<li><strong>Dynamic Behavior</strong>:<ul>\n<li>Workloads vary widely, making fixed parameters suboptimal in many cases.</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/img/image-20241124155658032.png\" alt=\"image-20241124155658032\"></p>\n<p><strong>Avoiding “Voo-Doo Constants”</strong> (Ousterhout’s Law):</p>\n<ul>\n<li>A term coined by John Ousterhout, it warns against arbitrary constants (e.g., fixed time slice lengths or boost intervals).</li>\n<li>These constants are often left unmodified, relying on defaults that may not suit every workload.</li>\n</ul>\n<p><strong>Potential Solutions:</strong></p>\n<ol>\n<li><p><strong>Learning-Based Tuning</strong>:</p>\n<ul>\n<li>Systems can adapt by monitoring workloads and adjusting parameters dynamically.</li>\n<li>However, implementing such learning systems is complex.</li>\n</ul>\n</li>\n<li><p><strong>Configuration Files</strong>:</p>\n<ul>\n<li><p>Allow experienced administrators to fine-tune parameters.</p>\n</li>\n<li><p>These often remain unused unless performance issues arise.</p>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example: Solaris Time-Sharing Scheduler</strong>:</p>\n<ul>\n<li>Table-Based Configuration:<ul>\n<li>Solaris MLFQ uses configurable tables to define:<ul>\n<li>Priority adjustments over time.</li>\n<li>Time slice lengths (e.g., 20 ms for high-priority queues, hundreds of ms for low-priority queues).</li>\n<li>Priority boost intervals (~1 second).</li>\n</ul>\n</li>\n<li>Default: <strong>60 queues</strong> with gradually increasing time slices.</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Chapter-9-Proportional-Share\"><a href=\"#Chapter-9-Proportional-Share\" class=\"headerlink\" title=\"Chapter 9: Proportional Share\"></a>Chapter 9: Proportional Share</h1><h2 id=\"9-1-Basic-Concept-Tickets-Represent-Your-Share\"><a href=\"#9-1-Basic-Concept-Tickets-Represent-Your-Share\" class=\"headerlink\" title=\"9.1 Basic Concept: Tickets Represent Your Share\"></a>9.1 Basic Concept: Tickets Represent Your Share</h2><p><strong>Core Concept: Tickets</strong></p>\n<ul>\n<li><strong>Definition</strong>: Tickets represent the share of a resource (e.g., CPU) allocated to a process or user.</li>\n<li>Proportional Representation: The fraction of tickets a process holds corresponds to the percentage of the resource it should receive.<ul>\n<li>Example:<ul>\n<li>Process A has 75 tickets, and Process B has 25 tickets.</li>\n<li>Total tickets = 100.</li>\n<li>Process A should receive <strong>75%</strong> of the CPU, and Process B <strong>25%</strong>.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Mechanism</strong>:</p>\n<ol>\n<li><strong>Lottery System</strong>:<ul>\n<li>At every scheduling event (e.g., time slice), a lottery is held.</li>\n<li>A random number (ticket) is drawn from 0 to the total number of tickets - 1.</li>\n<li>The process holding the winning ticket gets to run.</li>\n</ul>\n</li>\n<li><strong>Example</strong>:<ul>\n<li>Process A: Tickets 0–74.</li>\n<li>Process B: Tickets 75–99.</li>\n<li>Winning ticket determines the selected process:<ul>\n<li>Ticket 63 → A.</li>\n<li>Ticket 85 → B.</li>\n</ul>\n</li>\n<li>Over many time slices, the CPU usage approaches the desired proportions <strong>probabilistically</strong>.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Advantages of Lottery Scheduling</strong>:</p>\n<ol>\n<li><p><strong>Randomness as a Robust Mechanism</strong>:</p>\n<ul>\n<li><p>Avoids Corner Cases:</p>\n<p>Random approaches are less prone to worst-case scenarios compared to deterministic algorithms.</p>\n<ul>\n<li>Example: Random replacement avoids LRU’s worst-case performance in some cyclic workloads.</li>\n</ul>\n</li>\n<li><p>Minimal State Tracking:</p>\n<ul>\n<li>Traditional scheduling requires tracking per-process CPU usage.</li>\n<li>Lottery scheduling only needs to manage ticket counts.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Simplicity and Speed</strong>:</p>\n<ul>\n<li>Minimal bookkeeping makes it lightweight and efficient.</li>\n<li>Generating a random number is computationally inexpensive.</li>\n</ul>\n</li>\n<li><p><strong>Fairness over Time</strong>:</p>\n<ul>\n<li>While randomness may cause deviations in the short term, fairness is achieved <strong>over time</strong> through probabilistic correctness.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Drawbacks</strong>:</p>\n<ul>\n<li>Short-Term Inaccuracy:<ul>\n<li>Randomness introduces variability. For example, a process may receive less than its share of the CPU in a small time window.</li>\n<li>Example: Process B should receive 25% but only gets 20% over 20 time slices.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/img/image-20241124160821269.png\" alt=\"image-20241124160821269\"></p>\n<p><strong>Applications of Tickets</strong>:</p>\n<ol>\n<li><strong>Proportional Resource Allocation</strong>:<ul>\n<li>Beyond CPU scheduling, tickets can be used in other contexts:<ul>\n<li><strong>Memory Allocation</strong> in virtualized systems.</li>\n<li><strong>I/O Scheduling</strong> to divide bandwidth among processes.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Real-World Use Case</strong>:<ul>\n<li>Waldspurger applied tickets in virtual memory management for hypervisors to control guest operating systems’ memory shares.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"9-2-Ticket-Mechanisms\"><a href=\"#9-2-Ticket-Mechanisms\" class=\"headerlink\" title=\"9.2 Ticket Mechanisms\"></a>9.2 Ticket Mechanisms</h2><p><strong>1. Ticket Currency</strong>:</p>\n<ul>\n<li><strong>Purpose</strong>: Allows users to manage tickets within their own namespace or “currency” while the system converts them to a global value.</li>\n<li>How It Works:<ul>\n<li>Each user allocates tickets in their local “currency.”</li>\n<li>The system normalizes these tickets to the global scale.</li>\n<li>This ensures fairness when multiple users share the same resource.</li>\n</ul>\n</li>\n<li>Example:<ul>\n<li>User A:<ul>\n<li>Total tickets: 100.</li>\n<li>Allocates <strong>500 tickets each</strong> to two jobs (A1 and A2) in local currency.</li>\n<li>Converted to <strong>50 tickets each</strong> in global currency.</li>\n</ul>\n</li>\n<li>User B:<ul>\n<li>Total tickets: 100.</li>\n<li>Allocates <strong>10 tickets</strong> to one job (B1) in local currency.</li>\n<li>Converted to <strong>100 tickets</strong> in global currency.</li>\n</ul>\n</li>\n<li>Global lottery:<ul>\n<li>Total tickets: 200.</li>\n<li>Jobs compete proportionally based on their global ticket allocation.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Ticket Transfer</strong>:</p>\n<ul>\n<li><strong>Purpose</strong>: Temporarily transfers tickets between processes to reflect changing priorities or dependencies.</li>\n<li>Use Case:<ul>\n<li>Common in client/server systems.</li>\n<li>Example:<ul>\n<li>A <strong>client process</strong> requests work from a <strong>server process</strong>.</li>\n<li>The client transfers its tickets to the server to prioritize the server’s performance.</li>\n<li>After the task, the server transfers the tickets back to the client.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Ticket Inflation</strong>:</p>\n<ul>\n<li><strong>Purpose</strong>: Temporarily increases or decreases the number of tickets a process holds to adapt to specific needs.</li>\n<li>Constraints:<ul>\n<li>Useful in <strong>trusted environments</strong> where processes cooperate.</li>\n<li>Not suitable for competitive scenarios, as greedy processes could exploit this mechanism.</li>\n</ul>\n</li>\n<li>Example:<ul>\n<li>A process anticipates heavy CPU usage and inflates its ticket count to signal its need for more resources.</li>\n<li>After completing the task, it deflates its tickets back to the original count.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"9-3-Implementation\"><a href=\"#9-3-Implementation\" class=\"headerlink\" title=\"9.3 Implementation\"></a>9.3 Implementation</h2><p><strong>Key Elements of Implementation</strong>:</p>\n<p>Lottery scheduling is notable for its simplicity and elegance. The key components needed for implementation include:</p>\n<ol>\n<li><strong>Random Number Generator</strong>:<ul>\n<li>Used to pick a winning ticket number from the range <code>[0, total_tickets-1]</code>.</li>\n</ul>\n</li>\n<li><strong>Process List</strong>:<ul>\n<li>A data structure (e.g., a linked list) to track all processes and their ticket allocations.</li>\n</ul>\n</li>\n<li><strong>Total Ticket Count</strong>:<ul>\n<li>The sum of tickets across all processes, used as the range for random number selection.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Process Example</strong>:</p>\n<ul>\n<li><strong>Processes and Tickets</strong>:<ul>\n<li>A: 100 tickets.</li>\n<li>B: 50 tickets.</li>\n<li>C: 250 tickets.</li>\n<li><strong>Total tickets</strong>: 400.</li>\n</ul>\n</li>\n<li><strong>Winning Ticket Selection</strong>:<ul>\n<li>Randomly generate a number between <code>0</code> and <code>399</code> (e.g., <code>300</code>).</li>\n</ul>\n</li>\n<li><strong>Finding the Winner</strong>:<ul>\n<li>Traverse the process list, summing ticket counts with a counter.</li>\n<li>Steps:<ul>\n<li>Start with <code>counter = 0</code>.</li>\n<li>Add A’s tickets: <code>counter = 100</code> (less than 300, continue).</li>\n<li>Add B’s tickets: <code>counter = 150</code> (still less than 300, continue).</li>\n<li>Add C’s tickets: <code>counter = 400</code> (greater than 300, C is the winner).</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> counter = <span class=\"number\">0</span>;                          <span class=\"comment\">// Tracks the sum of tickets</span></span><br><span class=\"line\"><span class=\"type\">int</span> winner = getrandom(<span class=\"number\">0</span>, totaltickets);  <span class=\"comment\">// Randomly pick the winning ticket</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">node_t</span> *current = head;                   <span class=\"comment\">// Start at the head of the process list</span></span><br><span class=\"line\"><span class=\"keyword\">while</span> (current) &#123;</span><br><span class=\"line\">    counter += current-&gt;tickets;          <span class=\"comment\">// Add tickets of current process</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (counter &gt; winner)                 <span class=\"comment\">// Check if the current process wins</span></span><br><span class=\"line\">        <span class=\"keyword\">break</span>;                            <span class=\"comment\">// Stop when the winner is found</span></span><br><span class=\"line\">    current = current-&gt;next;              <span class=\"comment\">// Move to the next process</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// &#x27;current&#x27; points to the winning process; schedule it.</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h2 id=\"9-4-An-Example\"><a href=\"#9-4-An-Example\" class=\"headerlink\" title=\"9.4 An Example\"></a>9.4 An Example</h2><p><strong>Experiment Setup</strong>:</p>\n<ol>\n<li><strong>Scenario</strong>:<ul>\n<li>Two jobs, each with:<ul>\n<li><strong>100 tickets</strong>.</li>\n<li><strong>Identical run times</strong>, denoted as R, which varies during the experiment.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Objective</strong>:<ul>\n<li>Both jobs should ideally finish at the same time.</li>\n<li>Measure the fairness of lottery scheduling, which uses randomness and does not guarantee deterministic behavior.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Fairness Metric</strong>:</p>\n<ul>\n<li><p><strong>Definition</strong>:</p>\n<ul>\n<li><script type=\"math/tex; mode=display\">\nF = \\frac{\\text{Completion time of the first job}}{\\text{Completion time of the second job}}</script></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>F≈1: Perfect fairness (both jobs finish simultaneously).</p>\n</li>\n<li><p>F&lt;1: First job finishes significantly earlier than the second job.</p>\n</li>\n</ul>\n<ul>\n<li><p><strong>Example</strong>:</p>\n<ul>\n<li><p>If R=10:</p>\n<ul>\n<li><p>First job finishes at t=10, second at t=20.</p>\n</li>\n<li><script type=\"math/tex; mode=display\">\nF = \\frac{10}{20} = 0.5</script></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Observations from Figure 9.2</strong>:</p>\n<ol>\n<li><strong>Short Job Runs</strong>:<ul>\n<li>For small R, fairness (F) tends to be low.</li>\n<li>Reason:<ul>\n<li>With fewer time slices, the randomness in lottery scheduling results in higher variability in job completion times.</li>\n<li>There isn’t enough time for the probabilistic fairness to even out.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Longer Job Runs</strong>:<ul>\n<li>As R increases, F approaches 1 (better fairness).</li>\n<li>Reason:<ul>\n<li>Over many time slices, the inherent randomness averages out, and the two jobs receive their fair share of CPU time proportional to their ticket allocation.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/img/image-20241124163544275.png\" alt=\"image-20241124163544275\"></p>\n<h2 id=\"9-5-How-To-Assign-Tickets\"><a href=\"#9-5-How-To-Assign-Tickets\" class=\"headerlink\" title=\"9.5 How To Assign Tickets?\"></a>9.5 How To Assign Tickets?</h2><p>This problem is a tough one, because of course how the system behaves is strongly dependent on how tickets are allocated. Thus, given a set of jobs, the “ticket-assignment problem” remains open</p>\n<h2 id=\"9-6-Stride-Scheduling\"><a href=\"#9-6-Stride-Scheduling\" class=\"headerlink\" title=\"9.6 Stride Scheduling\"></a>9.6 Stride Scheduling</h2><p><strong>Stride Scheduling</strong></p>\n<ol>\n<li><p><strong>Overview</strong>:</p>\n<ul>\n<li>Deterministic alternative to lottery scheduling.</li>\n<li>Ensures <strong>exact proportional resource allocation</strong> based on ticket values.</li>\n</ul>\n</li>\n<li><p><strong>Key Concepts</strong>:</p>\n<ul>\n<li><p>Stride:</p>\n<ul>\n<li><p>Inversely proportional to the number of tickets a job holds.</p>\n</li>\n<li><p>Calculated as:</p>\n<script type=\"math/tex; mode=display\">\n\\text{Stride} = \\frac{\\text{Large Number}}{\\text{Number of Tickets}}</script><p>Example: With 10,000 as the large number:</p>\n<ul>\n<li>Job A (100 tickets): <script type=\"math/tex; mode=display\">\nStride = \\frac{10,000}{100} =100</script></li>\n</ul>\n</li>\n<li><p>Job B (50 tickets): </p>\n<script type=\"math/tex; mode=display\">\n  Stride = \\frac{10,000}{50} = 200.</script><ul>\n<li>Job C (250 tickets): <script type=\"math/tex; mode=display\">\nStride = \\frac{10,000}{250} = 40.</script>Pass Value:</li>\n</ul>\n</li>\n<li><p>Tracks cumulative progress of each job.</p>\n</li>\n<li>Incremented by the job’s stride after every time slice.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Scheduling Algorithm</strong>:</p>\n<ul>\n<li><p>Select the job with the <strong>lowest pass value</strong>.</p>\n</li>\n<li><p>After the job runs, update its pass value: </p>\n<script type=\"math/tex; mode=display\">\n\\text{New Pass Value} = \\text{Old Pass Value} + \\text{Stride}</script></li>\n</ul>\n</li>\n</ol>\n<ol>\n<li><strong>Example Execution (Figure 9.3)</strong>:</li>\n</ol>\n<p><img src=\"/img/image-20241124164407758.png\" alt=\"image-20241124164407758\"></p>\n<p>Proportions achieved:</p>\n<ul>\n<li>C (250 tickets): Runs 5 times.</li>\n<li>A (100 tickets): Runs 2 times.</li>\n<li>B (50 tickets): Runs 1 time.</li>\n<li>Reflects ticket allocation exactly.</li>\n</ul>\n<p><img src=\"/img/屏幕截图 2024-11-24 164813.png\" alt=\"屏幕截图 2024-11-24 164813\"></p>\n<h1 id=\"Chapter-10-Multiprocessor-Scheduling-Advanced\"><a href=\"#Chapter-10-Multiprocessor-Scheduling-Advanced\" class=\"headerlink\" title=\"Chapter 10: Multiprocessor Scheduling (Advanced)\"></a>Chapter 10: Multiprocessor Scheduling (Advanced)</h1><p><strong>Single vs. Multi-CPU Systems</strong></p>\n<ul>\n<li><strong>Single CPU System:</strong><ul>\n<li>Uses <strong>hardware caches</strong> to speed up memory access.</li>\n<li>Cache Hierarchy:<ul>\n<li>Small, fast cache memory holds copies of frequently accessed data.</li>\n<li>Large, slower main memory stores all data.</li>\n</ul>\n</li>\n<li><strong>Temporal Locality:</strong> Frequently accessed data is likely to be accessed again soon.</li>\n<li><strong>Spatial Locality:</strong> Access to an address makes nearby addresses more likely to be accessed.</li>\n</ul>\n</li>\n<li><strong>Multi-CPU System:</strong><ul>\n<li>Shares a <strong>single main memory</strong> between multiple CPUs.</li>\n<li>Each CPU has its own private cache.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/img/image-20241124174209427.png\" alt=\"image-20241124174209427\"></p>\n<p><strong>Cache Behavior in Multiprocessor Systems</strong></p>\n<ul>\n<li><p>When a CPU modifies data, changes may remain in its private cache without immediately updating main memory.</p>\n</li>\n<li><p>Example Problem:</p>\n<ol>\n<li><strong>CPU 1</strong> fetches a value <code>D</code> from memory (address <code>A</code>) and caches it.</li>\n<li>CPU 1 modifies <code>D</code> to <code>D&#39;</code> in its cache.</li>\n<li>The program is moved to <strong>CPU 2</strong>, which fetches data from main memory.</li>\n<li>CPU 2 receives the old value <code>D</code> (not <code>D&#39;</code>).</li>\n</ol>\n<p>This mismatch is called the <strong>Cache Coherence Problem</strong>.</p>\n</li>\n</ul>\n<p><strong>Cache Coherence: Challenges and Solutions</strong></p>\n<ul>\n<li><strong>Problem:</strong> How to ensure all CPUs see a consistent view of shared memory.</li>\n<li>Hardware Solution:<ul>\n<li>Bus Snooping:<ul>\n<li>Each cache monitors memory updates on the bus connecting caches and memory.</li>\n<li>If a cache sees a change to a memory address it holds:<ul>\n<li><strong>Invalidate</strong> the stale copy.</li>\n<li>Or <strong>Update</strong> the cache with the new value.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Complexities arise with <strong>write-back caches</strong>, which delay updates to main memory.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"10-2-Don’t-Forget-Synchronization\"><a href=\"#10-2-Don’t-Forget-Synchronization\" class=\"headerlink\" title=\"10.2 Don’t Forget Synchronization\"></a>10.2 Don’t Forget Synchronization</h2><p><strong>Cache Coherence and Synchronization</strong></p>\n<ul>\n<li><p><strong>Cache coherence</strong> ensures that all processors have a consistent view of memory.</p>\n</li>\n<li><p>However, </p>\n<p>coherence alone is insufficient</p>\n<p> for managing </p>\n<p>shared data</p>\n<p> correctly.</p>\n<ul>\n<li>Programs and the OS must still implement <strong>synchronization mechanisms</strong> when accessing or modifying shared data.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Key Concept: Mutual Exclusion</strong></p>\n<ul>\n<li><strong>Why Needed:</strong> Prevent <strong>race conditions</strong> where multiple threads or CPUs simultaneously modify shared data, causing inconsistencies.</li>\n<li><strong>Example Problem:</strong><br>Removing an element from a shared linked list without synchronization:</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">List_Pop</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    Node_t *tmp = head; <span class=\"comment\">// store current head</span></span><br><span class=\"line\">    <span class=\"type\">int</span> value = head-&gt;value; <span class=\"comment\">// get value</span></span><br><span class=\"line\">    head = head-&gt;next; <span class=\"comment\">// move head pointer</span></span><br><span class=\"line\">    <span class=\"built_in\">free</span>(tmp); <span class=\"comment\">// free old head</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> value;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>If two threads execute this code at the same time:</p>\n<ul>\n<li>Both read the same <code>head</code> into their local <code>tmp</code>.</li>\n<li>Both try to free the same memory, causing a <strong>double free</strong>.</li>\n<li>Both return the same value, causing logical errors.</li>\n</ul>\n<p><strong>Solution: Locks</strong></p>\n<ul>\n<li>Use <strong>mutual exclusion (locks)</strong> to ensure only one thread executes the critical section at a time.</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">pthread_mutex_t</span> m;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">List_Pop</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    lock(&amp;m);         <span class=\"comment\">// Acquire lock</span></span><br><span class=\"line\">    Node_t *tmp = head;</span><br><span class=\"line\">    <span class=\"type\">int</span> value = head-&gt;value;</span><br><span class=\"line\">    head = head-&gt;next;</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(tmp);</span><br><span class=\"line\">    unlock(&amp;m);       <span class=\"comment\">// Release lock</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> value;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"10-3-One-Final-Issue-Cache-Affinity\"><a href=\"#10-3-One-Final-Issue-Cache-Affinity\" class=\"headerlink\" title=\"10.3 One Final Issue: Cache Affinity\"></a>10.3 One Final Issue: Cache Affinity</h2><p>A process, when run on a particular CPU, builds up a fair bit of state in the caches (and TLBs) of the CPU. The next time the process runs, it is often advantageous to run it on the same CPU, as it will run faster if some of its state is already present in the caches on that CPU. If, instead, one runs a process on a different CPU each time, the performance of the process will be worse, as it will have to reload the state each time it runs (note it will run correctly on a different CPU thanks to the cache coherence protocols of the hardware).</p>\n<h2 id=\"10-4-Single-Queue-Scheduling\"><a href=\"#10-4-Single-Queue-Scheduling\" class=\"headerlink\" title=\"10.4 Single-Queue Scheduling\"></a>10.4 Single-Queue Scheduling</h2><p><strong>Overview of SQMS</strong></p>\n<ul>\n<li><strong>Definition:</strong> SQMS uses a single queue to store all jobs that need scheduling, regardless of the number of CPUs.</li>\n<li>Advantages:<ul>\n<li><strong>Simplicity:</strong> Easily adapted from single-CPU scheduling algorithms.</li>\n<li><strong>Unified Policy:</strong> Centralized decision-making allows for consistent job selection.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Key Challenges of SQMS</strong></p>\n<ol>\n<li><strong>Scalability Issues</strong></li>\n</ol>\n<ul>\n<li><p>Locks and Contention:</p>\n<ul>\n<li><p>A single queue requires <strong>locking mechanisms</strong> to ensure proper synchronization when multiple CPUs access the queue simultaneously.</p>\n</li>\n<li><p>Performance Degradation:</p>\n<p> As the number of CPUs increases, contention for the queue’s lock grows, leading to:</p>\n<ul>\n<li><strong>Lock overhead.</strong></li>\n<li>CPUs spending more time waiting for the lock rather than executing jobs.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><strong>Cache Affinity</strong></li>\n</ol>\n<ul>\n<li><p>Problem Description:</p>\n<ul>\n<li>Jobs frequently <strong>migrate between CPUs</strong> when the next available job is picked from the global queue.</li>\n<li>This job movement undermines <strong>cache affinity</strong>, forcing jobs to reload state into the cache each time they run on a new CPU.</li>\n</ul>\n</li>\n<li><p>Example of Poor Affinity:</p>\n<p><img src=\"/img/image-20241124185533242.png\" alt=\"image-20241124185533242\"></p>\n</li>\n</ul>\n<p><strong>Affinity Mechanisms in SQMS</strong></p>\n<ul>\n<li><p><strong>Preserving Affinity:</strong></p>\n<ul>\n<li><p>Implement mechanisms to increase the likelihood that jobs continue to run on the <strong>same CPU</strong>.</p>\n</li>\n<li><p>Example:</p>\n<ul>\n<li>Jobs A through D stay on CPUs 0 through 3, respectively.</li>\n<li>Job E migrates across CPUs as needed for <strong>load balancing</strong>.</li>\n</ul>\n<p><img src=\"/img/image-20241124185733446.png\" alt=\"image-20241124185733446\"></p>\n</li>\n</ul>\n</li>\n<li><p><strong>Affinity Fairness:</strong></p>\n<ul>\n<li>Rotate job migrations to ensure fairness across jobs.</li>\n<li>For example:<ul>\n<li>Job E migrates first.</li>\n<li>In subsequent cycles, a different job might be chosen to migrate.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Complexity:</strong> These mechanisms increase the complexity of the scheduler and may not fully eliminate performance concerns.</p>\n</li>\n</ul>\n<h2 id=\"10-5-Multi-Queue-Scheduling\"><a href=\"#10-5-Multi-Queue-Scheduling\" class=\"headerlink\" title=\"10.5 Multi-Queue Scheduling\"></a>10.5 Multi-Queue Scheduling</h2><p><strong>Overview of MQMS</strong></p>\n<ul>\n<li><strong>Definition:</strong> In MQMS, each CPU is assigned its own scheduling queue. Jobs are distributed across these queues using heuristics, such as random assignment or balancing the number of jobs per queue.</li>\n<li>Advantages:<ul>\n<li><strong>Scalability:</strong> Reduces contention since queues operate independently, avoiding the bottlenecks of a single shared queue.</li>\n<li><strong>Cache Affinity:</strong> Jobs typically stay on the same CPU, preserving cached data and improving performance.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Key Challenges of MQMS</strong></p>\n<ol>\n<li><strong>Load Imbalance</strong></li>\n</ol>\n<ul>\n<li>Description:<ul>\n<li>Some CPUs may finish their jobs earlier than others, leading to uneven CPU utilization.</li>\n<li>For example:<ul>\n<li>Queue 0: Jobs A, C</li>\n<li>Queue 1: Jobs B, D</li>\n<li>If C finishes, CPU 0 may have only A left while CPU 1 still alternates between B and D.</li>\n</ul>\n</li>\n<li>If A finishes, CPU 0 becomes idle while CPU 1 continues to run jobs, resulting in <strong>underutilization</strong> of available CPUs.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/img/image-20241124191428949.png\" alt=\"image-20241124191428949\"></p>\n<p><img src=\"/img/image-20241124191655377.png\" alt=\"image-20241124191655377\"></p>\n<p><strong>Solution: Job Migration</strong></p>\n<ul>\n<li><p>Migration Mechanism:</p>\n<p> Jobs are moved between queues to balance the workload across CPUs.</p>\n<ul>\n<li><p>Simple Example:</p>\n<ul>\n<li>Queue 0: Empty</li>\n<li>Queue 1: Jobs B, D</li>\n<li>Migrate B or D to Queue 0, ensuring both CPUs remain active.</li>\n</ul>\n</li>\n<li><p>Complex Example:</p>\n<ul>\n<li><p>Queue 0: Job A</p>\n</li>\n<li><p>Queue 1: Jobs B, D</p>\n</li>\n<li><p>Alternating migration might be needed to balance load:</p>\n<ul>\n<li>Move B to Queue 0 for a few time slices, allowing D to run alone on CPU 1.</li>\n<li>Then migrate B back, allowing A to share CPU 0 with D.</li>\n</ul>\n<p><img src=\"/img/image-20241124191733265.png\" alt=\"image-20241124191733265\"></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><strong>Work-Stealing Technique</strong></li>\n</ol>\n<ul>\n<li><strong>Definition:</strong> A queue low on jobs (source queue) “steals” jobs from a more loaded queue (target queue).</li>\n<li><strong>Benefits:</strong> Balances the load dynamically without requiring centralized scheduling.</li>\n<li>Challenges:<ul>\n<li><strong>Overhead:</strong> Frequent checks for load imbalances increase system overhead, potentially impacting performance.</li>\n<li><strong>Infrequent Checks:</strong> If checks are too rare, queues might become heavily imbalanced, negating the benefits of migration.</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Chapter-13-Address-Spaces\"><a href=\"#Chapter-13-Address-Spaces\" class=\"headerlink\" title=\"Chapter 13: Address Spaces\"></a>Chapter 13: Address Spaces</h1><h2 id=\"13-2-Multiprogramming-and-Time-Sharing\"><a href=\"#13-2-Multiprogramming-and-Time-Sharing\" class=\"headerlink\" title=\"13.2 Multiprogramming and Time Sharing\"></a>13.2 Multiprogramming and Time Sharing</h2><p><strong>Transition to Time Sharing</strong></p>\n<ul>\n<li><p><strong>Problem with Multiprogramming:</strong> </p>\n<p>While multiprogramming increased CPU efficiency, it was not interactive enough for users. Program debugging was long and inefficient in batch processing, and real-time feedback was not possible.</p>\n</li>\n<li><p>Demand for Interactivity:</p>\n<ul>\n<li>Programmers and users wanted quicker responses and the ability to interact with running programs.</li>\n<li>This led to the concept of <strong>time sharing</strong>, where multiple users could interact with a machine concurrently.</li>\n</ul>\n</li>\n<li><p>Time Sharing Mechanism:</p>\n<ul>\n<li>A simple approach would be to run a process for a short period, save its state (including all memory) to disk, and load another process’s state to run next.</li>\n<li><strong>Challenge:</strong> Saving and restoring all memory content to/from disk was slow and inefficient, particularly as memory sizes grew.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Efficient Time Sharing with Memory Management</strong></p>\n<ul>\n<li>Improved Time Sharing:<ul>\n<li>Instead of saving and restoring entire memory contents, the OS keeps processes in memory and switches between them more efficiently.</li>\n<li>Each process is given a portion of physical memory, enabling concurrent execution and reducing the time overhead associated with disk I/O.</li>\n</ul>\n</li>\n<li>Example Scenario (Figure 13.2):<ul>\n<li>Assume a 512KB physical memory, divided among processes A, B, and C.</li>\n<li>The OS runs one process (e.g., A) while the others (B and C) are waiting in the ready queue.</li>\n<li>The CPU switches between processes, enabling multiple tasks to execute and improving system responsiveness.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/img/image-20241128092455682.png\" alt=\"image-20241128092455682\"></p>\n<p><strong>Protection and Isolation</strong></p>\n<ul>\n<li>New Challenges with Time Sharing:<ul>\n<li><strong>Memory Protection:</strong> Multiple processes sharing the same memory space raise concerns about security and stability.</li>\n<li><strong>Goal:</strong> Prevent one process from accessing or modifying another process’s memory.</li>\n</ul>\n</li>\n<li>Importance of Isolation:<ul>\n<li>Ensures that processes cannot interfere with each other, which is crucial for system integrity and security.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"13-3-The-Address-Space\"><a href=\"#13-3-The-Address-Space\" class=\"headerlink\" title=\"13.3 The Address Space\"></a>13.3 The Address Space</h2><p><strong>Address Space Abstraction</strong></p>\n<ul>\n<li><strong>Definition:</strong> The address space is the running program’s view of memory as provided by the operating system (OS). This abstraction helps simplify the interaction between programs and the physical memory hardware.</li>\n<li>Components of Address Space:<ul>\n<li><strong>Code Segment:</strong> Contains the executable instructions of the program.</li>\n<li><strong>Stack:</strong> Used for tracking function calls, local variables, and passing parameters and return values.</li>\n<li><strong>Heap:</strong> Used for dynamically allocated memory, like data from <code>malloc()</code> in C or <code>new</code> in C++/Java.</li>\n<li><strong>Other Elements:</strong> Includes static variables and other memory structures, but for simplicity, focus is often on code, stack, and heap.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Example of Address Space (Figure 13.3)</strong></p>\n<ul>\n<li>16KB Address Space Example:<ul>\n<li><strong>Code Segment:</strong> Occupies the first 1KB (top of the address space); it’s static and does not grow.</li>\n<li><strong>Heap:</strong> Starts at 1KB and grows downward (allocations made via <code>malloc()</code>).</li>\n<li><strong>Stack:</strong> Starts at 16KB and grows upward (due to function calls and local data).</li>\n</ul>\n</li>\n<li><strong>Growth Direction:</strong> The stack and heap grow toward each other, enabling dynamic expansion without interfering. This arrangement helps with efficient use of available space.</li>\n<li><img src=\"/img/image-20241128092722943.png\" alt=\"image-20241128092722943\"></li>\n</ul>\n<p><strong>Virtualization of Memory</strong></p>\n<ul>\n<li><strong>Concept:</strong> Memory virtualization is the OS’s ability to present each process with the illusion of a private, large address space, even when only a portion of the physical memory is used.</li>\n<li>Physical vs. Virtual Memory:<ul>\n<li><strong>Virtual Address:</strong> The address used by a running program.</li>\n<li><strong>Physical Address:</strong> The actual location in physical memory where the process is loaded.</li>\n</ul>\n</li>\n<li><strong>OS and Hardware Support:</strong> When a program attempts to access a virtual address, the OS, with assistance from hardware (e.g., memory management unit), maps it to the corresponding physical address.</li>\n<li><strong>Why It Matters:</strong> Virtual memory allows multiple processes to share the same physical memory without interfering with each other. This abstraction is critical for process isolation, security, and efficient memory management.</li>\n</ul>\n<h2 id=\"13-4-Goals\"><a href=\"#13-4-Goals\" class=\"headerlink\" title=\"13.4 Goals\"></a>13.4 Goals</h2><p><strong>Key Goals of Virtual Memory (VM) Systems</strong></p>\n<p>The OS plays a crucial role in virtualizing memory to create a seamless and efficient experience for programs. To achieve this, there are three primary goals:</p>\n<ol>\n<li><strong>Transparency</strong><ul>\n<li><strong>Definition:</strong> The OS should make the virtualization of memory invisible to running programs. Programs should operate as if they have access to their own dedicated physical memory, unaware that the memory they use is actually shared and virtualized.</li>\n<li><strong>Importance:</strong> This ensures that programs can function without needing to manage or be aware of the underlying memory virtualization processes. This is critical for maintaining the simplicity of program design and operation.</li>\n</ul>\n</li>\n<li><strong>Efficiency</strong><ul>\n<li><strong>Definition:</strong> The OS must strive to virtualize memory efficiently, both in terms of time (speed of operations) and space (amount of memory used for managing the virtualization).</li>\n<li><strong>Time Efficiency:</strong> The virtualization mechanism should not introduce significant delays or make programs run slower. Hardware support, such as Translation Lookaside Buffers (TLBs), is often leveraged to speed up memory address translation.</li>\n<li><strong>Space Efficiency:</strong> The OS must minimize the memory overhead used to manage the virtual memory system (e.g., data structures that map virtual to physical memory).</li>\n</ul>\n</li>\n<li><strong>Protection</strong><ul>\n<li><strong>Definition:</strong> The OS should ensure that processes are isolated from each other and from the OS itself. When a process performs memory operations (like load, store, or instruction fetch), it should not be able to access or interfere with the memory of other processes or the OS.</li>\n<li><strong>Purpose:</strong> This isolation prevents one process from corrupting or compromising another, enhancing the system’s security and reliability. It also protects the OS from faulty or malicious processes.</li>\n<li><strong>Principle of Isolation:</strong> Isolation ensures that if one process fails, it does not affect other processes or the OS. Some systems implement even stricter isolation, such as microkernels, which can further improve reliability by limiting interactions within the OS.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Transparency Clarification</strong></p>\n<ul>\n<li><strong>Common Misunderstanding:</strong> Transparency in this context means making the virtualization process invisible, not keeping information hidden. A transparent system is one that runs in the background without being noticed by applications, not one that is open or exposed.</li>\n<li><strong>Implication:</strong> The OS should handle the mapping between virtual and physical addresses seamlessly, so applications don’t need to be aware of or interact with this process.</li>\n</ul>\n<p><strong>Implications of Virtual Memory Goals</strong></p>\n<ul>\n<li><strong>Isolation and Security:</strong> Proper isolation enables a system where each process runs in its own space, preventing issues where one process could tamper with another. This is essential for system stability and for protecting sensitive data from potential threats.</li>\n<li><strong>Efficiency Considerations:</strong> The OS must balance how to manage memory effectively, using algorithms and hardware to optimize space and speed. The goal is to avoid bottlenecks and ensure responsive performance.</li>\n</ul>\n<h1 id=\"Chapter-14-Memory-API\"><a href=\"#Chapter-14-Memory-API\" class=\"headerlink\" title=\"Chapter 14: Memory API\"></a>Chapter 14: Memory API</h1><h2 id=\"14-1-Types-of-Memory\"><a href=\"#14-1-Types-of-Memory\" class=\"headerlink\" title=\"14.1 Types of Memory\"></a>14.1 Types of Memory</h2><p><strong>Types of Memory Allocation in C</strong></p>\n<ol>\n<li><p><strong>Stack Memory (Automatic Memory)</strong></p>\n<ul>\n<li><p><strong>Definition:</strong> Memory that is allocated and deallocated automatically by the compiler.</p>\n</li>\n<li><p><strong>Management:</strong> The compiler takes care of stack memory management. When a function is called, space is allocated on the stack for local variables. When the function returns, this memory is deallocated automatically.</p>\n</li>\n<li><p><strong>Declaration Example:</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">func</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> x; <span class=\"comment\">// Memory for integer x is allocated on the stack.</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Characteristics:</p>\n<ul>\n<li><strong>Short-lived:</strong> The memory only persists as long as the function is running.</li>\n<li><strong>Scope:</strong> Limited to the function’s scope.</li>\n<li><strong>Usage:</strong> Ideal for variables that don’t need to persist beyond the function call.</li>\n</ul>\n</li>\n<li><p><strong>Limitation:</strong> If you need data to persist after the function ends, storing it on the stack is not sufficient.</p>\n</li>\n</ul>\n</li>\n</ol>\n<ol>\n<li><p><strong>Heap Memory (Dynamic Memory)</strong></p>\n<ul>\n<li><p><strong>Definition:</strong> Memory that is explicitly allocated and deallocated by the programmer at runtime.</p>\n</li>\n<li><p><strong>Management:</strong> The programmer is responsible for handling allocations and deallocations, which can lead to potential issues such as memory leaks or dangling pointers if not done correctly.</p>\n</li>\n<li><p><strong>Allocation Example:</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">func</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> *x = (<span class=\"type\">int</span> *) <span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>)); <span class=\"comment\">// Allocates an integer on the heap.</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Characteristics:</strong></p>\n<ul>\n<li><strong>Long-lived:</strong> The memory persists until it is explicitly deallocated using <code>free()</code>.</li>\n<li><strong>Explicit Control:</strong> The programmer must manage the memory lifecycle.</li>\n<li><strong>Usage:</strong> Suitable for data that needs to exist beyond the function call or for variable-sized data structures (e.g., arrays).</li>\n</ul>\n<p><strong>Challenges:</strong></p>\n<ul>\n<li><strong>Memory Leaks:</strong> Occur when memory is allocated but not properly deallocated.</li>\n<li><strong>Dangling Pointers:</strong> Occur when memory is deallocated but pointers still reference it.</li>\n<li><strong>Debugging Complexity:</strong> Errors related to heap memory are often harder to find and fix.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>How Memory Allocation Works</strong></p>\n<ul>\n<li>Stack Allocation:<ul>\n<li><strong>Implicit Management:</strong> Declaring <code>int x;</code> in a function automatically allocates space for <code>x</code> on the stack.</li>\n<li><strong>Lifetime:</strong> The memory is automatically reclaimed when the function exits.</li>\n</ul>\n</li>\n<li>Heap Allocation:<ul>\n<li><strong>Explicit Management:</strong> The <code>malloc()</code> function allocates memory from the heap and returns a pointer to it.</li>\n<li><strong>Storage:</strong> The pointer returned is stored on the stack, while the actual memory allocated resides on the heap.</li>\n<li><strong>Deallocation:</strong> The programmer must use <code>free()</code> to release the memory and avoid leaks.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"14-2-The-malloc-Call\"><a href=\"#14-2-The-malloc-Call\" class=\"headerlink\" title=\"14.2 The malloc() Call\"></a>14.2 The malloc() Call</h2><p><strong>Overview of <code>malloc()</code></strong></p>\n<ul>\n<li><p><strong>Function Purpose:</strong> Allocates a specified number of bytes on the heap.</p>\n</li>\n<li><p><strong>Return Value:</strong> Returns a <code>void</code> pointer to the allocated memory or <code>NULL</code> if the allocation fails.</p>\n</li>\n<li><p><strong>Header File:</strong> To use <code>malloc()</code>, include <code>#include &lt;stdlib.h&gt;</code>, although the library is linked by default in C programs.</p>\n</li>\n<li><p>Prototype:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> *<span class=\"title function_\">malloc</span><span class=\"params\">(<span class=\"type\">size_t</span> size)</span>;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>Key Points about <code>malloc()</code></strong></p>\n<ul>\n<li><p><strong>Parameter:</strong> Takes a single argument of type <code>size_t</code>, representing the number of bytes to allocate.</p>\n</li>\n<li><p>Usage:</p>\n<ul>\n<li>It is common practice to use the <code>sizeof()</code> operator to determine the amount of memory needed for data types or structures.</li>\n<li><strong>Example for a single <code>double</code>:</strong></li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">double</span> *d = (<span class=\"type\">double</span> *) <span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(<span class=\"type\">double</span>));</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>Understanding <code>sizeof()</code></strong></p>\n<ul>\n<li><p><strong>Compile-Time Operator:</strong> <code>sizeof()</code> evaluates at compile time to determine the size of a type or variable in bytes.</p>\n</li>\n<li><p><strong>Example Use Cases:</strong></p>\n<ul>\n<li><p><strong>Allocating space for an array of integers:</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> *x = <span class=\"built_in\">malloc</span>(<span class=\"number\">10</span> * <span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));  <span class=\"comment\">// Allocates space for 10 integers.</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p><strong>Checking the size of a pointer (not the memory allocated):</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d\\n&quot;</span>, <span class=\"keyword\">sizeof</span>(x));  <span class=\"comment\">// Outputs the size of the pointer (e.g., 4 or 8 bytes, depending on the architecture).</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<ul>\n<li><p><strong>Common Pitfall:</strong></p>\n<ul>\n<li>Using <code>sizeof()</code> on a pointer type (e.g., <code>int *x</code>) returns the size of the pointer itself, not the allocated space.</li>\n<li><strong>Correcting this:</strong> When you need the size of an allocated array or object, use <code>sizeof()</code> on the array or type, not on the pointer.</li>\n</ul>\n</li>\n<li><p><strong>Static vs. Dynamic Memory Allocation:</strong></p>\n<ul>\n<li><p><strong>Static Memory (e.g., <code>int x[10];</code>)</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d\\n&quot;</span>, <span class=\"keyword\">sizeof</span>(x));  <span class=\"comment\">// Outputs the total size of the array (e.g., 40 bytes for 10 integers on a 32-bit system).</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Special Considerations for Strings</strong></p>\n<ul>\n<li><p><strong>Allocation for Strings:</strong></p>\n<ul>\n<li><p>When allocating memory for a string, remember to include space for the null terminator:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">char</span> *str = (<span class=\"type\">char</span> *) <span class=\"built_in\">malloc</span>(<span class=\"built_in\">strlen</span>(s) + <span class=\"number\">1</span>);</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Why Not <code>sizeof()</code>?</strong> </p>\n<p>Using <code>sizeof()</code> on a string declared as <code>char *str</code> only gives the size of the pointer, not the length of the string it points to.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Pointer Casting and <code>malloc()</code></strong></p>\n<ul>\n<li><p><strong>Return Type:</strong> <code>malloc()</code> returns a <code>void *</code>, which is a generic pointer type.</p>\n</li>\n<li><p>Casting:</p>\n<ul>\n<li><p><strong>Why Cast?</strong> Casting the result of <code>malloc()</code> (e.g., <code>double *d = (double *) malloc(sizeof(double));</code>) is not required for functionality but helps with code readability and ensures type safety, especially in C++.</p>\n</li>\n<li><p><strong>Caution:</strong> Casting does not affect the correctness or behavior of the program.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Tips for Using <code>malloc()</code></strong></p>\n<ul>\n<li><p><strong>Always Check for <code>NULL</code>:</strong> Verify that <code>malloc()</code> succeeded before using the allocated memory:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> *arr = (<span class=\"type\">int</span> *) <span class=\"built_in\">malloc</span>(<span class=\"number\">10</span> * <span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));</span><br><span class=\"line\"><span class=\"keyword\">if</span> (arr == <span class=\"literal\">NULL</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Handle memory allocation failure</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Freeing Memory:</strong> Always use <code>free()</code> to release memory when it is no longer needed to avoid memory leaks:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">free</span>(arr);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"14-3-The-free-Call\"><a href=\"#14-3-The-free-Call\" class=\"headerlink\" title=\"14.3 The free() Call\"></a>14.3 The free() Call</h2><p>To free heap memory that is no longer in use, programmers simply call <strong>free():</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> *x = <span class=\"built_in\">malloc</span>(<span class=\"number\">10</span> * <span class=\"keyword\">sizeof</span>(<span class=\"type\">int</span>));</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"built_in\">free</span>(x);</span><br></pre></td></tr></table></figure>\n<h2 id=\"14-4-Common-Errors\"><a href=\"#14-4-Common-Errors\" class=\"headerlink\" title=\"14.4 Common Errors\"></a>14.4 Common Errors</h2><ol>\n<li><strong>Forgetting to Allocate Memory</strong></li>\n</ol>\n<ul>\n<li><p><strong>Error</strong>: Attempting to use <code>strcpy()</code> on an uninitialized pointer.</p>\n</li>\n<li><p><strong>Issue</strong>: Results in a segmentation fault, as there is no allocated space to copy data into.</p>\n</li>\n<li><p><strong>Solution</strong>: Always allocate memory before using a pointer, e.g., <code>char *dst = (char *) malloc(strlen(src) + 1);</code>.</p>\n</li>\n</ul>\n<ol>\n<li><strong>Not Allocating Enough Memory (Buffer Overflow)</strong></li>\n</ol>\n<ul>\n<li><p><strong>Error</strong>: Allocating too little space for a string or array.</p>\n</li>\n<li><p><strong>Issue</strong>: Leads to writing past the allocated space, causing undefined behavior, potential crashes, or security vulnerabilities.</p>\n</li>\n<li><p><strong>Example</strong>: <code>char *dst = (char *) malloc(strlen(src));</code> is insufficient for copying <code>src</code>, which includes a null terminator.</p>\n</li>\n<li><p><strong>Solution</strong>: Use <code>malloc(strlen(src) + 1)</code> to ensure enough space is allocated.</p>\n</li>\n</ul>\n<ol>\n<li><strong>Forgetting to Initialize Allocated Memory</strong></li>\n</ol>\n<ul>\n<li><p><strong>Error</strong>: Allocating memory without setting its contents.</p>\n</li>\n<li><p><strong>Issue</strong>: Reading uninitialized memory may yield random values, leading to unpredictable behavior.</p>\n</li>\n<li><p><strong>Solution</strong>: Initialize memory after allocation, using functions like <code>memset()</code> or explicit value assignment.</p>\n</li>\n</ul>\n<ol>\n<li><strong>Forgetting to Free Memory (Memory Leaks)</strong></li>\n</ol>\n<ul>\n<li><p><strong>Error</strong>: Failing to call <code>free()</code> after memory is no longer needed.</p>\n</li>\n<li><p><strong>Issue</strong>: Leads to memory leaks, which accumulate over time and can cause applications to run out of memory.</p>\n</li>\n<li><p><strong>Solution</strong>: Always <code>free()</code> memory when it is no longer needed, even in short-lived programs.</p>\n</li>\n</ul>\n<ol>\n<li><strong>Freeing Memory Before It’s Done (Dangling Pointers)</strong></li>\n</ol>\n<ul>\n<li><p><strong>Error</strong>: Calling <code>free()</code> and then using the same pointer.</p>\n</li>\n<li><p><strong>Issue</strong>: Causes crashes or unexpected behavior if the pointer is dereferenced after being freed.</p>\n</li>\n<li><p><strong>Solution</strong>: Set the pointer to <code>NULL</code> after <code>free()</code> to prevent accidental use.</p>\n</li>\n</ul>\n<ol>\n<li><strong>Freeing Memory Multiple Times (Double Free)</strong></li>\n</ol>\n<ul>\n<li><p><strong>Error</strong>: Calling <code>free()</code> on the same pointer more than once.</p>\n</li>\n<li><p><strong>Issue</strong>: Leads to undefined behavior, potential crashes, or data corruption.</p>\n</li>\n<li><p><strong>Solution</strong>: Avoid freeing the same memory block multiple times.</p>\n</li>\n</ul>\n<p><strong>Best Practices for Memory Management</strong></p>\n<ul>\n<li><p><strong>Verify Allocations</strong>: Always check if <code>malloc()</code> returns <code>NULL</code> to ensure memory was successfully allocated.</p>\n</li>\n<li><p><strong>Use <code>sizeof()</code> Properly</strong>: For correct memory allocation, use <code>sizeof()</code> to calculate the size of the type.</p>\n</li>\n<li><p><strong>Freeing Memory</strong>: Ensure memory is only freed once and set the pointer to <code>NULL</code> post-free.</p>\n</li>\n<li><p><strong>Short-Lived Programs</strong>: Memory leaks are less impactful, as the OS reclaims all memory when the process ends. However, developing good memory habits is essential for long-running programs, such as servers, where memory leaks can lead to crashes.</p>\n</li>\n</ul>\n<p><strong>The Importance of <code>free()</code> and Garbage Collection</strong></p>\n<p>In contrast to languages with automatic garbage collection (e.g., Java or Python), C requires explicit management of memory. Improper use of <code>malloc()</code> and <code>free()</code> can lead to subtle, hard-to-find bugs.</p>\n<h1 id=\"Chapter-15-Address-Translation\"><a href=\"#Chapter-15-Address-Translation\" class=\"headerlink\" title=\"Chapter 15: Address Translation\"></a>Chapter 15: Address Translation</h1><h2 id=\"15-1-Assumptions\"><a href=\"#15-1-Assumptions\" class=\"headerlink\" title=\"15.1 Assumptions\"></a>15.1 Assumptions</h2><ol>\n<li><strong>Contiguous Allocation</strong></li>\n</ol>\n<ul>\n<li><strong>Assumption</strong>: The user’s address space is allocated contiguously in physical memory.</li>\n<li><strong>Implication</strong>: This simplification allows the system to work without needing complex mapping between virtual and physical addresses.</li>\n</ul>\n<ol>\n<li><strong>Small Address Space</strong></li>\n</ol>\n<ul>\n<li><strong>Assumption</strong>: The size of the address space is less than the physical memory available.</li>\n<li><strong>Implication</strong>: This constraint makes memory management simpler because there is no need to handle cases where the address space exceeds the physical memory capacity.</li>\n</ul>\n<ol>\n<li><strong>Uniform Address Space Size</strong></li>\n</ol>\n<ul>\n<li><strong>Assumption</strong>: Each address space is the same size.</li>\n<li><strong>Implication</strong>: This simplifies memory management, as the system does not need to handle different sizes of address spaces, making allocation and mapping easier to manage.</li>\n</ul>\n<h2 id=\"15-2-An-Example\"><a href=\"#15-2-An-Example\" class=\"headerlink\" title=\"15.2 An Example\"></a>15.2 An Example</h2><p>The example begins with a simple C code snippet that initializes a variable, increments it, and stores it back into memory:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">func</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> x = <span class=\"number\">3000</span>; <span class=\"comment\">// initial value</span></span><br><span class=\"line\">    x = x + <span class=\"number\">3</span>;   <span class=\"comment\">// line of code we are interested in</span></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>Assembly Code Representation</strong></p>\n<p>The compiler translates this code into assembly instructions, as shown in the x86 assembly snippet:</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">128</span>: movl <span class=\"number\">0</span><span class=\"built_in\">x0</span>(%ebx), %eax ; load <span class=\"number\">0</span>+ebx into eax</span><br><span class=\"line\"><span class=\"number\">132</span>: addl $<span class=\"number\">0</span>x03, %eax      ; add <span class=\"number\">3</span> <span class=\"selector-tag\">to</span> eax</span><br><span class=\"line\"><span class=\"number\">135</span>: movl %eax, <span class=\"number\">0</span><span class=\"built_in\">x0</span>(%ebx)  ; store eax back <span class=\"selector-tag\">to</span> memory</span><br></pre></td></tr></table></figure>\n<p><strong>Memory Accesses in the Process</strong></p>\n<p>From the process’s perspective, the memory accesses during this code execution are:</p>\n<ol>\n<li><p>Fetch the instruction at address 128.</p>\n</li>\n<li><p>Execute the instruction, which loads the value at address 15 KB (variable <code>x</code>).</p>\n</li>\n<li><p>Fetch the instruction at address 132.</p>\n</li>\n<li><p>Execute the instruction (no memory reference; it only modifies <code>eax</code>).</p>\n</li>\n<li><p>Fetch the instruction at address 135.</p>\n</li>\n<li><p>Execute the instruction, which stores the updated value at address 15 KB.</p>\n</li>\n</ol>\n<p><strong>Address Space and Physical Memory</strong></p>\n<ul>\n<li><p><strong>Process’s Address Space</strong>: The process sees its own address space as starting from 0 and extending to a maximum of 16 KB. All memory references are assumed to be within this range.</p>\n</li>\n<li><p><strong>Physical Memory Layout</strong>: The OS can place the process’s address space anywhere in physical memory, not necessarily starting at address 0. The section discusses how the process is relocated in physical memory without the process being aware of this relocation.</p>\n</li>\n<li><p>Figure 15.2</p>\n<p> illustrates how physical memory might look after relocating the process. In the example:</p>\n<ul>\n<li><p>The OS occupies the first slot of physical memory.</p>\n</li>\n<li><p>The process is placed starting at physical address 32 KB.</p>\n</li>\n<li><p>Other slots in memory (e.g., 16 KB–32 KB and 48 KB–64 KB) are free.</p>\n<p><img src=\"../img/image-20241128203050707.png\" alt=\"image-20241128203050707\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Address Translation Mechanism</strong></p>\n<p>The main challenge here is how the OS can manage the translation of virtual addresses (used by the process) to physical addresses (actual locations in RAM). This translation must be transparent to the process to ensure that it can continue to execute as if it were working within its original virtual address space starting at 0.</p>\n<p><strong>Key Concept: Interposition</strong></p>\n<ul>\n<li><strong>Definition</strong>: Interposition is a powerful technique where an intermediary (in this case, the OS) steps in to modify or translate operations.</li>\n<li><strong>Application to Virtual Memory</strong>: The hardware interposes during each memory access made by the process, translating each virtual address to a physical address where the data is stored.</li>\n<li><strong>Benefit</strong>: This translation is transparent to the process; it operates as if it has direct access to its original address space, even though the actual data might reside at different physical locations.</li>\n</ul>\n<h2 id=\"15-3-Dynamic-Hardware-based-Relocation\"><a href=\"#15-3-Dynamic-Hardware-based-Relocation\" class=\"headerlink\" title=\"15.3 Dynamic (Hardware-based) Relocation\"></a>15.3 Dynamic (Hardware-based) Relocation</h2><p><strong>Key Concept: Base and Bounds</strong></p>\n<ul>\n<li><strong>Dynamic Relocation</strong> (or <strong>Base and Bounds</strong>) uses hardware registers to translate <strong>virtual addresses</strong> (used by programs) into <strong>physical addresses</strong> (actual memory locations).</li>\n<li>Each CPU requires:<ol>\n<li><strong>Base Register</strong>: Contains the starting physical address where the process is loaded.</li>\n<li><strong>Bounds (Limit) Register</strong>: Defines the size of the address space, ensuring the process accesses memory within its range.</li>\n</ol>\n</li>\n</ul>\n<p><strong>Address Translation Process</strong></p>\n<ol>\n<li><p><strong>Base Register</strong>: When a virtual address is generated by a process, it is added to the base register to calculate the <strong>physical address</strong>: </p>\n<script type=\"math/tex; mode=display\">\n\\text{Physical Address} = \\text{Virtual Address} + \\text{Base}</script></li>\n<li><p><strong>Bounds Register</strong>: Before translation, the CPU ensures that the virtual address is within the range defined by the bounds. If not:</p>\n<ul>\n<li>An <strong>exception</strong> is raised, and the process is terminated.</li>\n</ul>\n</li>\n<li><p><strong>These mechanisms provide:</strong></p>\n<ul>\n<li><strong>Relocation</strong>: Processes can run anywhere in physical memory.</li>\n<li><strong>Protection</strong>: Prevents processes from accessing memory outside their allocated space.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example of Address Translation via base-and-bounds</strong></p>\n<ol>\n<li>Process loaded at 16 KB in physical memory, with a virtual address space of 4 KB:<ul>\n<li>Virtual address <code>0</code> → Physical address <code>16 KB</code></li>\n<li>Virtual address <code>1 KB</code> → Physical address <code>17 KB</code></li>\n<li>Virtual address <code>3 KB</code> → Physical address <code>19 KB</code></li>\n<li>Virtual address <code>4.4 KB</code> → <strong>Fault</strong> (out of bounds)</li>\n</ul>\n</li>\n</ol>\n<p><strong>Advantages of Base and Bounds</strong></p>\n<ol>\n<li><strong>Transparency</strong>: Programs can operate as if starting at address <code>0</code>, even when relocated in physical memory.</li>\n<li><strong>Protection</strong>: Ensures processes only access their allocated memory.</li>\n<li><strong>Flexibility</strong>: The OS can move processes to different memory locations dynamically.</li>\n</ol>\n<p><strong>Role of the Memory Management Unit (MMU)</strong></p>\n<ul>\n<li>The <strong>MMU</strong> (part of the CPU) handles address translation using base and bounds registers.</li>\n<li>Future memory management techniques extend the MMU’s functionality.</li>\n</ul>\n<p><strong>Notes on Implementation</strong></p>\n<ul>\n<li><p>Bounds Register</p>\n<p> can operate in two modes:</p>\n<ol>\n<li>Holds the <strong>size</strong> of the address space (check before adding base).</li>\n<li>Holds the <strong>end physical address</strong> (check after adding base).</li>\n</ol>\n</li>\n<li><p>Both modes are logically equivalent; the first is used for simplicity.</p>\n</li>\n</ul>\n<h2 id=\"15-4-Hardware-Support-A-Summary\"><a href=\"#15-4-Hardware-Support-A-Summary\" class=\"headerlink\" title=\"15.4 Hardware Support: A Summary\"></a>15.4 Hardware Support: A Summary</h2><p><strong>Key Hardware Requirements</strong></p>\n<p>To implement dynamic relocation effectively, hardware must support the following:</p>\n<ol>\n<li><p><strong>CPU Modes</strong>:</p>\n<ul>\n<li><strong>User Mode</strong>: Applications run with restricted privileges to prevent unauthorized operations.</li>\n<li><strong>Kernel (Privileged) Mode</strong>: The OS runs with full machine access.</li>\n<li>A <strong>mode bit</strong> in the processor indicates the current mode, switching during system calls, interrupts, or exceptions.</li>\n</ul>\n</li>\n<li><p><strong>Base and Bounds Registers</strong>:</p>\n<ul>\n<li><p><strong>Base Register</strong>: Stores the starting physical address of a process’s memory.</p>\n</li>\n<li><p><strong>Bounds Register</strong>: Specifies the size (or end address) of the process’s memory.</p>\n</li>\n<li><p>Both registers are part of the </p>\n<p>Memory Management Unit (MMU)</p>\n<p> and ensure:</p>\n<ul>\n<li>Address translation: Virtual address + Base → Physical address.</li>\n<li>Address validation: Ensures memory references stay within bounds.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Privileged Instructions</strong>:</p>\n<ul>\n<li>The OS must control the base and bounds registers using privileged instructions.</li>\n<li>These instructions prevent user processes from modifying these registers and disrupting memory isolation.</li>\n</ul>\n</li>\n<li><p><strong>Exception Handling</strong>:</p>\n<ul>\n<li>The CPU must raise exceptions when:<ul>\n<li>A user process accesses an invalid address (out of bounds).</li>\n<li>A user process attempts to execute privileged instructions in user mode.</li>\n</ul>\n</li>\n<li>The OS must handle exceptions via <strong>exception handlers</strong>, such as terminating offending processes or logging violations.</li>\n</ul>\n</li>\n<li><p><strong>Free Memory Management</strong>:</p>\n<ul>\n<li>The OS uses data structures like a <strong>free list</strong> to track unused memory regions, allowing efficient allocation to processes.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"15-5-Operating-System-Issues\"><a href=\"#15-5-Operating-System-Issues\" class=\"headerlink\" title=\"15.5 Operating System Issues\"></a>15.5 Operating System Issues</h2><p><strong>Key Responsibilities of the OS</strong></p>\n<ol>\n<li><p><strong>Memory Management</strong>:</p>\n<ul>\n<li>Process Creation:<ul>\n<li>When a process is created, the OS allocates memory for its address space by:<ul>\n<li>Searching a <strong>free list</strong> (a structure tracking unused memory regions).</li>\n<li>Marking the selected memory region as used.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Process Termination:<ul>\n<li>Upon process termination, the OS:<ul>\n<li>Reclaims memory allocated to the process.</li>\n<li>Returns the memory to the free list for reuse.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Base and Bounds Register Management</strong>:</p>\n<ul>\n<li><p>During </p>\n<p>context switches:</p>\n<ul>\n<li>The OS saves the current process’s base and bounds register values to its <strong>Process Control Block (PCB)</strong>.</li>\n<li>Loads the base and bounds register values for the next process.</li>\n</ul>\n</li>\n<li><p>Relocating Processes:</p>\n<ul>\n<li>If needed, the OS can move a process’s memory:<ul>\n<li>Deschedules the process.</li>\n<li>Copies its address space to a new location.</li>\n<li>Updates the saved base register in the PCB.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Exception Handling</strong>:</p>\n<ul>\n<li>The OS defines and installs <strong>exception handlers</strong> (e.g., illegal memory access handler) during system boot.</li>\n<li>Handles exceptions raised by the CPU, such as:<ul>\n<li>Out-of-bounds memory access → OS terminates the process.</li>\n<li>Illegal instruction execution → OS terminates the process.</li>\n</ul>\n</li>\n<li>Protects the system by responding firmly to misbehaving processes.</li>\n</ul>\n</li>\n</ol>\n<p><strong>OS Actions During System Boot</strong></p>\n<ul>\n<li>Initializes key data structures and hardware:<ul>\n<li><strong>Trap Table</strong>: Defines addresses of handlers (e.g., for system calls, timers, exceptions).</li>\n<li><strong>Timer</strong>: Sets up interrupt timers to ensure regular context switching.</li>\n<li><strong>Process Table</strong>: Tracks running and ready processes.</li>\n<li><strong>Free List</strong>: Manages available memory regions.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Hardware-OS Interaction Timeline (Figures 15.5 and 15.6)</strong></p>\n<ol>\n<li><strong>At Boot Time</strong>:<ul>\n<li>The OS sets up hardware and initializes data structures (e.g., free list, trap table).</li>\n</ul>\n</li>\n<li><strong>During Process Execution</strong>:<ul>\n<li>The OS sets base and bounds registers, then lets the process execute directly.</li>\n<li>Minimal OS intervention is needed unless:<ul>\n<li>A <strong>timer interrupt</strong> occurs → triggers a context switch.</li>\n<li>The process misbehaves (e.g., accessing invalid memory) → the OS handles the exception.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"../img/image-20241128222300433.png\" alt=\"image-20241128222300433\"></p>\n<p><img src=\"../img/image-20241128222333829.png\" alt=\"image-20241128222333829\"></p>\n<h1 id=\"Chapter-16-Segmentation\"><a href=\"#Chapter-16-Segmentation\" class=\"headerlink\" title=\"Chapter 16: Segmentation\"></a>Chapter 16: Segmentation</h1><h2 id=\"16-1-Segmentation-Generalized-Base-Bounds\"><a href=\"#16-1-Segmentation-Generalized-Base-Bounds\" class=\"headerlink\" title=\"16.1 Segmentation: Generalized Base/Bounds\"></a>16.1 Segmentation: Generalized Base/Bounds</h2><p><strong>Key Concepts in Segmentation</strong></p>\n<ol>\n<li><strong>Segments</strong>:<ul>\n<li>A <strong>segment</strong> is a contiguous portion of the address space with a specific length and purpose.</li>\n<li>Common logical segments:<ul>\n<li><strong>Code</strong>: Contains executable instructions.</li>\n<li><strong>Heap</strong>: Used for dynamically allocated memory.</li>\n<li><strong>Stack</strong>: Manages function calls and local variables.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Advantages of Segmentation</strong>:<ul>\n<li>Each segment can be placed independently in physical memory.</li>\n<li>Avoids allocating space for unused parts of virtual memory.</li>\n<li>Efficient for sparse address spaces (e.g., processes with large virtual address ranges but minimal actual usage).</li>\n</ul>\n</li>\n</ol>\n<p><strong>How Segmentation Works</strong></p>\n<ol>\n<li><p><strong>Memory Layout</strong>:</p>\n<ul>\n<li><p>Figure 16.1: Virtual memory is divided into logical segments (e.g., Code, Heap, Stack).</p>\n<p><img src=\"../img/image-20241128223355650.png\" alt=\"image-20241128223355650\"></p>\n</li>\n<li><p>Figure 16.2: These segments are mapped into physical memory independently.</p>\n<p><img src=\"../img/image-20241128223406928.png\" alt=\"image-20241128223406928\"></p>\n</li>\n</ul>\n</li>\n<li><p><strong>MMU (Memory Management Unit)</strong>:</p>\n<ul>\n<li><p>Maintains a set of <strong>base</strong> and <strong>bounds</strong> registers for each segment.</p>\n</li>\n<li><p>Example from Figure 16.3:</p>\n<ul>\n<li><p>Code segment: Base = 32KB, Size = 2KB</p>\n</li>\n<li><p>Heap segment: Base = 34KB, Size = 3KB</p>\n</li>\n<li><p>Stack segment: Base = 28KB, Size = 2KB</p>\n<p><img src=\"../img/image-20241128223429181.png\" alt=\"image-20241128223429181\"></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Address Translation</strong>:</p>\n<ul>\n<li>Virtual addresses are interpreted as <strong>segment + offset</strong>.</li>\n<li>Hardware translates the virtual address to a physical address by:<ol>\n<li>Adding the segment’s base value to the offset.</li>\n<li>Checking if the resulting address falls within the segment’s bounds.</li>\n</ol>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example Address Translations</strong></p>\n<ol>\n<li><p><strong>Code Segment</strong>:</p>\n<ul>\n<li>Virtual address = 100 (falls in the Code segment, as per Figure 16.1).</li>\n<li>Base (Code) = 32KB.</li>\n<li>Physical address = 100+32KB=32,868.</li>\n</ul>\n</li>\n<li><p><strong>Heap Segment</strong>:</p>\n<ul>\n<li>Virtual address = 4200.</li>\n<li>Base (Heap) = 34KB.</li>\n<li>Offset = 4200−4096=104 (Heap starts at 4KB in virtual memory).</li>\n<li>Physical address = 34KB+104=34,920.</li>\n</ul>\n</li>\n<li><p><strong>Invalid Access</strong>:</p>\n<ul>\n<li><p>Virtual address = 7KB (outside the heap’s range of 4KB–7KB).</p>\n</li>\n<li><p>Hardware detects the out-of-bounds access and raises a <strong>segmentation fault</strong>.</p>\n</li>\n<li><p>The OS handles the fault, typically terminating the offending process.</p>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"16-2-Which-Segment-Are-We-Referring-To\"><a href=\"#16-2-Which-Segment-Are-We-Referring-To\" class=\"headerlink\" title=\"16.2 Which Segment Are We Referring To?\"></a>16.2 Which Segment Are We Referring To?</h2><p><strong>Explicit Approach: Using Address Bits</strong></p>\n<ol>\n<li><p><strong>Segment Identification</strong>:</p>\n<ul>\n<li><p>The <strong>top bits</strong> of the virtual address specify the segment.</p>\n</li>\n<li><p>The <strong>remaining bits</strong> represent the offset within the segment.</p>\n</li>\n<li><p>Example (14-bit virtual address):</p>\n<ul>\n<li><p><strong>Bits 13-12</strong>: Segment identifier.</p>\n</li>\n<li><p><strong>Bits 11-0</strong>: Offset.</p>\n<p><img src=\"../img/image-20241128225521462.png\" alt=\"image-20241128225521462\"></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Example Translation</strong>:</p>\n<ul>\n<li><p>Virtual address: 4200</p>\n<ul>\n<li><p><strong>Segment Identifier</strong>: Top 2 bits (010101) → Heap.</p>\n</li>\n<li><p><strong>Offset</strong>: Bottom 12 bits (0000_0110_1000) → 104</p>\n<p><img src=\"../img/image-20241128225652531.png\" alt=\"image-20241128225652531\"></p>\n</li>\n</ul>\n</li>\n<li><p>Translation steps:</p>\n<ol>\n<li>Determine the segment (Heap).</li>\n<li>Use the segment’s base register (34KB) and bounds (3KB).</li>\n<li>Check bounds: Offset 104&lt;3KB→ Valid.</li>\n<li>Physical address: 34KB+104=34,920.</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p><strong>Hardware Logic (Simplified)</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Segment = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SEG_SHIFT;</span><br><span class=\"line\">Offset = VirtualAddress &amp; OFFSET_MASK;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> (Offset &gt;= Bounds[Segment]) &#123;</span><br><span class=\"line\">    RaiseException(PROTECTION_FAULT);</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    PhysAddr = Base[Segment] + Offset;</span><br><span class=\"line\">    Register = AccessMemory(PhysAddr);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p><strong>SEG_MASK</strong>: 0x3000(extract top 2 bits).</p>\n</li>\n<li><p><strong>SEG_SHIFT</strong>: 12 (shift to isolate the segment).</p>\n</li>\n<li><p><strong>OFFSET_MASK</strong>: 0xFFF(extract lower 12 bits).</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Limitations</strong>:</p>\n<ul>\n<li><p><strong>Unused Segment</strong>: Using two bits for three segments leaves one unused.</p>\n</li>\n<li><p><strong>Segment Size Limit</strong>: Each segment is limited to 4KB (16KB/4).</p>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Implicit Approach: Inferring the Segment</strong></p>\n<ol>\n<li><strong>Segment Determination</strong>:<ul>\n<li><strong>Code segment</strong>: Addresses generated from the <strong>program counter</strong>.</li>\n<li><strong>Stack segment</strong>: Addresses based on the <strong>stack pointer</strong>.</li>\n<li><strong>Heap segment</strong>: Any other addresses.</li>\n</ul>\n</li>\n<li><strong>Advantages</strong>:<ul>\n<li>Avoids reserving bits for segment identification.</li>\n<li>Supports dynamic segment sizes.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"16-3-What-About-The-Stack\"><a href=\"#16-3-What-About-The-Stack\" class=\"headerlink\" title=\"16.3 What About The Stack?\"></a>16.3 What About The Stack?</h2><p><strong>Key Adjustments for Backward-Growing Stacks</strong></p>\n<ol>\n<li><p><strong>Growth Direction</strong>:</p>\n<ul>\n<li><p>Most segments (e.g., code, heap) grow <strong>forward</strong> (towards higher addresses).</p>\n</li>\n<li><p>The stack grows <strong>backward</strong> (towards lower addresses).</p>\n</li>\n<li><p>To accommodate this, the hardware includes an additional </p>\n<p>growth direction flag:</p>\n<ul>\n<li><code>1</code>: Grows forward.</li>\n<li><code>0</code>: Grows backward.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Segment Register Example</strong>:</p>\n<ul>\n<li>Each segment tracks its base, size, and growth direction:</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"../img/image-20241128230431340.png\" alt=\"image-20241128230431340\"></p>\n<p><strong>Stack Translation Example</strong></p>\n<ul>\n<li><p><strong>Virtual Address</strong>: 15KB = 0x3C00(binary: 11 1100 0000 0000).</p>\n<ul>\n<li><strong>Segment Identifier</strong>: Top two bits (11) → Stack.</li>\n<li><strong>Offset</strong>: 3KB</li>\n</ul>\n</li>\n<li><p><strong>Translation Steps</strong>:</p>\n<ol>\n<li><p>Identify the segment (Stack) and its <strong>base</strong> (28KB).</p>\n</li>\n<li><p>Convert the offset for a </p>\n<p>backward-growing segment:</p>\n<ul>\n<li>Subtract the <strong>max segment size</strong> (4KB) from the offset (3KB).</li>\n<li>3KB−4KB=−1KB</li>\n</ul>\n</li>\n<li><p>Calculate the physical address:</p>\n<ul>\n<li>Add the <strong>negative offset</strong> (−1KB) to the base (28KB).</li>\n<li>28KB−1KB=27KB.</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"16-5-Fine-grained-vs-Coarse-grained-Segmentation\"><a href=\"#16-5-Fine-grained-vs-Coarse-grained-Segmentation\" class=\"headerlink\" title=\"16.5 Fine-grained vs. Coarse-grained Segmentation\"></a>16.5 Fine-grained vs. Coarse-grained Segmentation</h2><p><strong>Coarse-Grained Segmentation</strong></p>\n<ul>\n<li><strong>Definition</strong>: Divides the virtual address space into a <strong>few large segments</strong> (e.g., code, heap, stack).</li>\n<li>Examples:<ul>\n<li>Systems we’ve discussed so far (e.g., VAX/VMS).</li>\n<li>Address space split into just three primary segments.</li>\n</ul>\n</li>\n<li>Advantages:<ul>\n<li>Simple implementation with limited hardware requirements.</li>\n<li>Easy to understand and manage.</li>\n</ul>\n</li>\n<li>Disadvantages:<ul>\n<li>Less flexible.</li>\n<li>Large segments may waste memory if their allocated space isn’t fully utilized.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Fine-Grained Segmentation</strong></p>\n<ul>\n<li><strong>Definition</strong>: Breaks the address space into <strong>many smaller segments</strong>, offering more flexibility.</li>\n<li>Historical Examples:<ul>\n<li><strong>Multics</strong>: Used fine-grained segmentation to improve flexibility and memory efficiency.</li>\n<li><strong>Burroughs B5000</strong>: Supported thousands of segments, expecting compilers to split code and data into these smaller chunks.</li>\n</ul>\n</li>\n<li>Implementation Requirements:<ul>\n<li>A <strong>segment table</strong> stored in memory to manage the many segments.</li>\n<li>Hardware capable of handling large numbers of segments and translating addresses efficiently.</li>\n</ul>\n</li>\n<li>Advantages:<ul>\n<li>Enables more efficient memory use by identifying and utilizing only active segments.</li>\n<li>Provides flexibility for applications to organize code and data into logically independent segments.</li>\n</ul>\n</li>\n<li>Disadvantages:<ul>\n<li>Increased complexity in hardware and OS support.</li>\n<li>More overhead in managing segment tables.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"16-6-OS-Support\"><a href=\"#16-6-OS-Support\" class=\"headerlink\" title=\"16.6 OS Support\"></a>16.6 OS Support</h2><p><strong>1. Context Switching</strong></p>\n<ul>\n<li>Requirement: Segment registers must be saved and restored during a context switch.<ul>\n<li>Each process has its own virtual address space.</li>\n<li>The OS ensures the correct segment register values are loaded when a process is scheduled to run.</li>\n</ul>\n</li>\n<li><strong>Why It Matters</strong>: Mismanaged segment registers could result in incorrect memory access, violating process isolation.</li>\n</ul>\n<p><strong>2. Growing (or Shrinking) Segments</strong></p>\n<ul>\n<li>Dynamic Memory Requests:<ul>\n<li>When a program requests additional memory (e.g., via <code>malloc()</code>), the heap may need to expand.</li>\n<li>If the heap segment cannot accommodate the request, the OS must allocate more space.</li>\n<li>Example: The traditional UNIX <code>sbrk()</code> system call grows the heap and updates the segment size register.</li>\n</ul>\n</li>\n<li>Possible OS Responses:<ul>\n<li><strong>Grant the Request</strong>: Update the segment size and allocate the required memory.</li>\n<li>Reject the Request:<ul>\n<li>If there is insufficient physical memory.</li>\n<li>If the process has already exceeded its allowed memory quota.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Key Insight</strong>: The OS balances allocation requests while preventing overuse of physical memory.</li>\n</ul>\n<p><strong>3. Managing Free Space</strong></p>\n<ul>\n<li><p>Challenge: Physical memory fragmentation.</p>\n<ul>\n<li><p>Segments of varying sizes leave gaps in physical memory after allocation and deallocation.</p>\n</li>\n<li><p>This <strong>external fragmentation</strong> makes it difficult to find contiguous memory for new or growing segments.</p>\n<p><img src=\"../img/image-20241128232423285.png\" alt=\"image-20241128232423285\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Solutions for Fragmentation</strong>:</p>\n<ol>\n<li><strong>Memory Compaction</strong>:<ul>\n<li>Rearranges segments in physical memory to consolidate free space into a single contiguous block.</li>\n<li>Steps:<ul>\n<li>Stop processes.</li>\n<li>Copy segment data to new locations.</li>\n<li>Update segment registers to reflect the new physical addresses.</li>\n</ul>\n</li>\n<li>Drawbacks:<ul>\n<li>Time-consuming and processor-intensive.</li>\n<li>May trigger additional rearrangement if segments grow.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Free-List Management Algorithms</strong>:<ul>\n<li>Maintain a list of free memory blocks to manage allocations more efficiently.</li>\n<li>Algorithms:<ul>\n<li><strong>Best-Fit</strong>: Selects the smallest block that satisfies the request.</li>\n<li><strong>Worst-Fit</strong>: Selects the largest available block.</li>\n<li><strong>First-Fit</strong>: Allocates the first block that meets the size requirement.</li>\n<li><strong>Buddy Algorithm</strong>: Divides memory into power-of-two-sized blocks, making splitting and merging efficient.</li>\n</ul>\n</li>\n<li><strong>Limitations</strong>: External fragmentation remains unavoidable, though good algorithms aim to minimize it.</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Chapter-17-Free-Space-Management\"><a href=\"#Chapter-17-Free-Space-Management\" class=\"headerlink\" title=\"Chapter 17: Free-Space Management\"></a>Chapter 17: Free-Space Management</h1><h2 id=\"17-1-Assumptions\"><a href=\"#17-1-Assumptions\" class=\"headerlink\" title=\"17.1 Assumptions\"></a>17.1 Assumptions</h2><p><strong>1. Memory Allocation Interface</strong></p>\n<ul>\n<li>Functions:<ul>\n<li><code>malloc(size_t size)</code>:<ul>\n<li>Takes a <code>size</code> parameter (number of bytes requested).</li>\n<li>Returns a pointer (<code>void*</code>) to the allocated region of memory.</li>\n</ul>\n</li>\n<li><code>free(void* ptr)</code>:<ul>\n<li>Takes a pointer and frees the associated memory chunk.</li>\n<li><strong>Key Limitation</strong>: The size of the freed chunk is not provided; the allocator must infer it.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Implication</strong>: Allocators must internally track the size of each allocated memory chunk to handle <code>free()</code> correctly.</li>\n</ul>\n<p><strong>2. Heap and Free List</strong></p>\n<ul>\n<li>Heap:<ul>\n<li>The managed memory region where allocation and deallocation occur.</li>\n</ul>\n</li>\n<li>Free List:<ul>\n<li>A data structure used to track available (free) memory chunks.</li>\n<li><strong>Flexible Structure</strong>: While called a “list,” any structure that efficiently manages free space (e.g., trees, arrays) can be used.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Focus on Fragmentation</strong></p>\n<ul>\n<li>External Fragmentation:<ul>\n<li>The primary concern in allocator design.</li>\n<li>Occurs when free memory exists but is fragmented into non-contiguous chunks, preventing large allocations.</li>\n</ul>\n</li>\n<li>Internal Fragmentation:<ul>\n<li>Occurs when allocated chunks are larger than requested, leaving unused space within the chunk.</li>\n<li><strong>Assumption</strong>: This chapter focuses on <strong>external fragmentation</strong> for simplicity and because it poses more interesting challenges.</li>\n</ul>\n</li>\n</ul>\n<p><strong>4. No Memory Relocation</strong></p>\n<ul>\n<li>Once memory is allocated to a client:<ul>\n<li>It <strong>cannot</strong> be moved until explicitly freed.</li>\n<li>This lack of relocation precludes <strong>compaction</strong>, a common method to reduce fragmentation in OS-level memory management.</li>\n</ul>\n</li>\n<li>Why It Matters:<ul>\n<li>Fragmentation accumulates over time, as memory cannot be reshuffled to consolidate free space.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. Contiguous Region Management</strong></p>\n<ul>\n<li><p>Allocators manage a single, contiguous region of memory.</p>\n</li>\n<li><p>Growth of the Heap:</p>\n<ul>\n<li>While allocators might request additional memory from the OS (e.g., via <code>sbrk()</code>), the assumption here is a <strong>fixed-size heap</strong> for simplicity.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"17-2-Low-level-Mechanisms\"><a href=\"#17-2-Low-level-Mechanisms\" class=\"headerlink\" title=\"17.2 Low-level Mechanisms\"></a>17.2 Low-level Mechanisms</h2><p><strong>Splitting and Coalescing</strong></p>\n<ol>\n<li><p><strong>Free List</strong>: A data structure that tracks free space in the heap. For example:</p>\n<ul>\n<li><p>Heap: </p>\n<p><img src=\"../img/image-20241128234250689.png\" alt=\"image-20241128234250689\"></p>\n</li>\n<li><p>Free List:</p>\n<p><img src=\"../img/image-20241128234301856.png\" alt=\"image-20241128234301856\"></p>\n</li>\n</ul>\n</li>\n<li><p><strong>Splitting</strong>: When allocating memory smaller than a free chunk:</p>\n<ul>\n<li><p>Example: Request 1 byte from a 10-byte free chunk at address 20.</p>\n</li>\n<li><p>Result:</p>\n<p><img src=\"../img/image-20241128234431661.png\" alt=\"image-20241128234431661\"></p>\n</li>\n</ul>\n</li>\n<li><p><strong>Coalescing</strong>: Combines adjacent free chunks into one:</p>\n<ul>\n<li><p>Example: Freeing address 10 in <code>[free][used][free]</code>.</p>\n</li>\n<li><p>Before: Three chunks (0–9, 10–19, 20–29).</p>\n<p><img src=\"../img/image-20241128234844396.png\" alt=\"image-20241128234844396\"></p>\n</li>\n<li><p>After Coalescing: One chunk (0–29).</p>\n<p><img src=\"../img/image-20241128234850907.png\" alt=\"image-20241128234850907\" style=\"zoom: 80%;\" /></p>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Tracking Allocated Region Size</strong></p>\n<ul>\n<li><p><strong>Header Information</strong>: Each allocated block contains a header before the user’s data.</p>\n<ul>\n<li>Example Header:</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> size;</span><br><span class=\"line\">    <span class=\"type\">int</span> magic;</span><br><span class=\"line\">&#125; <span class=\"type\">header_t</span>;</span><br></pre></td></tr></table></figure>\n<p>The <code>size</code> field indicates the allocated memory size, and <code>magic</code> is for integrity checks.</p>\n<p><img src=\"../img/image-20241128235950068.png\" alt=\"image-20241128235950068\"></p>\n</li>\n<li><p><strong>Allocation with Header</strong>:</p>\n<ul>\n<li><p>Memory layout:</p>\n<ul>\n<li>Header: Contains size (20) and magic number.</li>\n<li>Allocated Space: User’s 20 bytes.</li>\n</ul>\n</li>\n<li><p>Free space searched for 28 bytes (20 + header size).</p>\n<p><img src=\"../img/image-20241128235911632.png\" alt=\"image-20241128235911632\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Embedding a Free List</strong></p>\n<ul>\n<li><p>The free list is embedded directly in the heap.</p>\n</li>\n<li><p>Each free chunk includes metadata:</p>\n<p>A node in the free list is defined to contain:</p>\n<ul>\n<li><code>size</code>: the size of the chunk of free memory.</li>\n<li><code>next</code>: a pointer to the next node in the list.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Code Structure</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">node_t</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span>              size;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">node_t</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">&#125; <span class=\"type\">node_t</span>;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>Initialization:</p>\n<ul>\n<li><p>Start with a single chunk covering the entire heap:</p>\n<ul>\n<li>Example: A 4096-byte heap minus header size.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Initializing the Free List</strong></p>\n<ol>\n<li><p><strong>Memory Allocation</strong>: The heap is typically initialized with a system call such as <code>mmap()</code> to acquire a chunk of memory.</p>\n</li>\n<li><p><strong>Single Free Node</strong>: Initially, the free list contains one entry that spans the entire heap size (minus the space needed for the header).</p>\n<ul>\n<li><p>Code Example:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">node_t</span> *head = mmap(<span class=\"literal\">NULL</span>, <span class=\"number\">4096</span>, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, <span class=\"number\">-1</span>, <span class=\"number\">0</span>);</span><br><span class=\"line\">head-&gt;size = <span class=\"number\">4096</span> - <span class=\"keyword\">sizeof</span>(<span class=\"type\">node_t</span>);  <span class=\"comment\">// Adjust size for the node header.</span></span><br><span class=\"line\">head-&gt;next = <span class=\"literal\">NULL</span>;  <span class=\"comment\">// No other nodes in the list yet.</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Memory Layout</strong>:</p>\n<ul>\n<li>At head, we have a node_t structure with:<ul>\n<li><code>size</code>: 4088 bytes (4096 minus the size of <code>node_t</code>).</li>\n<li><code>next</code>: <code>NULL</code> (indicating no subsequent free chunks).</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"../img/image-20241129081251137.png\" alt=\"image-20241129081251137\"></p>\n<p><strong>Handling Allocations</strong></p>\n<ol>\n<li><p><strong>Request for Memory</strong>: When a program requests, say, 100 bytes, the library searches for a free chunk that can satisfy this request.</p>\n</li>\n<li><p>Splitting the Chunk:</p>\n<ul>\n<li>The chosen chunk (4088 bytes) is split into:<ul>\n<li><strong>Allocated</strong>: 108 bytes (100 bytes + header size).</li>\n<li><strong>Remaining Free Chunk</strong>: 3980 bytes.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Memory Layout Post-Allocation:</p>\n<ul>\n<li><strong>Header</strong>: 8 bytes (size and magic number).</li>\n<li><strong>Allocated Space</strong>: 100 bytes.</li>\n<li><strong>Updated Free List</strong>: Now points to the 3980-byte chunk.</li>\n</ul>\n<p><img src=\"../img/image-20241129081400699.png\" alt=\"image-20241129081400699\"></p>\n</li>\n</ol>\n<p><strong>Further Allocations and Fragmentation</strong></p>\n<ul>\n<li><p>Multiple Allocations:</p>\n<ul>\n<li><p>Allocating additional 100-byte chunks leads to more headers and smaller free chunks.</p>\n</li>\n<li><p>The heap becomes fragmented, with free nodes potentially spread out.</p>\n</li>\n<li><p>Example (Figure 17.5):</p>\n<ul>\n<li><p>Three 100-byte allocations, each with its header.</p>\n</li>\n<li><p>Free space: One large chunk (3764 bytes).</p>\n<p><img src=\"../img/image-20241129081544115.png\" alt=\"image-20241129081544115\"></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Memory Deallocation and Fragmentation</strong></p>\n<ul>\n<li><p><strong>Freeing a Chunk</strong>:</p>\n<ul>\n<li><p>When memory is freed (e.g., <code>free(16500)</code>), the library determines the chunk size and adds it back to the free list.</p>\n</li>\n<li><p>Visualization After Freeing</p>\n<p> (Figure 17.6):</p>\n<ul>\n<li><p>The free list now points to a 3764-byte chunk and possibly other smaller chunks.</p>\n<p><img src=\"../img/image-20241129082454147.png\" alt=\"image-20241129082454147\"></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Uncoalesced Free List</strong>:</p>\n<ul>\n<li><p>After freeing chunks, the list may contain non-contiguous free chunks, leading to fragmentation (Figure 17.7).</p>\n<p><img src=\"../img/image-20241129082519037.png\" alt=\"image-20241129082519037\"></p>\n</li>\n<li><p><strong>Problem</strong>: Although the memory is technically available, it appears fragmented and cannot be used as a single large block.</p>\n</li>\n<li><p><strong>Coalescing</strong>: To solve fragmentation, adjacent free chunks must be merged to form larger contiguous free blocks.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"17-3-Basic-Strategies\"><a href=\"#17-3-Basic-Strategies\" class=\"headerlink\" title=\"17.3 Basic Strategies\"></a>17.3 Basic Strategies</h2><p>This section discusses various strategies for managing free space in memory allocators, highlighting the trade-offs between speed and fragmentation reduction. Here’s an overview of the key strategies:</p>\n<p> <strong>1. Best Fit</strong></p>\n<ul>\n<li><strong>Description</strong>: Searches the entire free list to find the smallest block that fits the requested size.</li>\n<li>Advantages:<ul>\n<li>Reduces internal fragmentation by minimizing wasted space within allocated blocks.</li>\n</ul>\n</li>\n<li>Disadvantages:<ul>\n<li>Requires a full scan of the free list, leading to high performance costs.</li>\n<li>Often results in many small, unusable free chunks, increasing external fragmentation.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Worst Fit</strong></p>\n<ul>\n<li><strong>Description</strong>: Selects the largest free block to satisfy the allocation request.</li>\n<li>Advantages:<ul>\n<li>Tries to avoid creating many small chunks, leaving larger contiguous free spaces.</li>\n</ul>\n</li>\n<li>Disadvantages:<ul>\n<li>Requires a full scan of the free list, leading to high overhead.</li>\n<li>Studies show it performs poorly, often resulting in excessive fragmentation.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. First Fit</strong></p>\n<ul>\n<li><strong>Description</strong>: Starts from the beginning of the free list and allocates the first block that is large enough.</li>\n<li>Advantages:<ul>\n<li>Faster than exhaustive strategies (like Best Fit or Worst Fit) as it stops searching as soon as it finds a suitable block.</li>\n</ul>\n</li>\n<li>Disadvantages:<ul>\n<li>Can “pollute” the beginning of the free list with small leftover chunks.</li>\n</ul>\n</li>\n<li><strong>Optimization</strong>: Using <strong>address-based ordering</strong> of the free list simplifies coalescing and reduces fragmentation.</li>\n</ul>\n<p><strong>4. Next Fit</strong></p>\n<ul>\n<li><strong>Description</strong>: Similar to First Fit, but instead of starting at the beginning of the list for each allocation, it resumes from where the previous search ended.</li>\n<li>Advantages:<ul>\n<li>Spreads searches more uniformly across the free list, reducing small leftover chunks at the start.</li>\n</ul>\n</li>\n<li>Disadvantages:<ul>\n<li>Similar to First Fit in terms of search cost and fragmentation tendencies.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Examples</strong></p>\n<ul>\n<li><strong>Initial Free List</strong>: 10 → 30 → 20 → NULL (block sizes in the list)</li>\n<li>Allocation Request: 15 bytes<ul>\n<li><strong>Best Fit</strong>: Chooses block 20 (smallest fit) → Remaining list: 10 → 30 → 5 → NULL</li>\n<li><strong>Worst Fit</strong>: Chooses block 30 (largest fit) → Remaining list: 10 → 15 → 20 → NULL</li>\n<li><strong>First Fit</strong>: Also chooses block 30 in this example, as it’s the first block large enough → Remaining list: 10 → 15 → 20 → NULL</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"17-4-Other-Approaches\"><a href=\"#17-4-Other-Approaches\" class=\"headerlink\" title=\"17.4 Other Approaches\"></a>17.4 Other Approaches</h2><p><strong>1. Segregated Lists</strong></p>\n<ul>\n<li><strong>Description</strong>: Maintains separate free lists for different object sizes. Frequently requested sizes are handled by dedicated lists, while other sizes use a general-purpose allocator.</li>\n<li><strong>Advantages</strong>:<ul>\n<li>Reduces fragmentation by dedicating memory pools to specific sizes.</li>\n<li>Improves allocation and deallocation speed for common sizes due to the reduced need for complex searches.</li>\n</ul>\n</li>\n<li><strong>Challenges</strong>:<ul>\n<li>Balancing memory allocation between specialized pools and the general allocator.</li>\n<li>Managing space when specialized pools run low or are underutilized.</li>\n</ul>\n</li>\n<li><strong>Example</strong>: The <strong>slab allocator</strong> in the Solaris kernel, created by Jeff Bonwick:<ul>\n<li>Allocates memory in slabs (multiples of page size).</li>\n<li>Keeps freed objects in an initialized state to reduce initialization and destruction costs.</li>\n<li>Reclaims slabs with all zeroed reference counts when memory is needed elsewhere.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Buddy Allocation</strong></p>\n<ul>\n<li><p><strong>Description</strong>: A hierarchical allocation system based on powers of two. Free memory is split recursively into halves until a block of the required size is found.</p>\n</li>\n<li><p><strong>Key Operations</strong>:</p>\n<ul>\n<li><p><strong>Allocation</strong>: Divides memory until the smallest power-of-two block that can accommodate the request is found.</p>\n</li>\n<li><p><strong>Deallocation</strong>: Checks if the “buddy” (block of the same size) is free and coalesces recursively up the tree.</p>\n<p><img src=\"../img/image-20241129102717526.png\" alt=\"image-20241129102717526\"></p>\n</li>\n</ul>\n</li>\n<li><p><strong>Advantages</strong>:</p>\n<ul>\n<li>Simplifies coalescing, as the buddy of a block can be determined by flipping a specific bit in the address.</li>\n<li>Efficient for block merging and splitting.</li>\n</ul>\n</li>\n<li><p><strong>Disadvantages</strong>:</p>\n<ul>\n<li>Can lead to <strong>internal fragmentation</strong>, as memory is allocated only in power-of-two block sizes.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Other Advanced Techniques</strong></p>\n<ul>\n<li><strong>Scaling Challenges</strong>:<ul>\n<li>Basic list-based allocators struggle to scale, especially in systems with high allocation demands or multiple processors.</li>\n<li>To address this, advanced allocators use more complex data structures, such as:<ul>\n<li><strong>Balanced binary trees</strong></li>\n<li><strong>Splay trees</strong></li>\n<li><strong>Partially-ordered trees</strong></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Concurrency-Friendly Allocators</strong>:<ul>\n<li>Modern systems with multi-threaded workloads require scalable allocators designed for multiprocessor environments.</li>\n<li>Examples:<ul>\n<li>Berger et al. (2000): A scalable allocator for multi-threaded systems.</li>\n<li>Evans (2006): Focuses on performance in parallel environments.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"Chapter-18-Introduction-of-Paging\"><a href=\"#Chapter-18-Introduction-of-Paging\" class=\"headerlink\" title=\"Chapter 18: Introduction of Paging\"></a>Chapter 18: Introduction of Paging</h1><h2 id=\"18-1-A-Simple-Example-And-Overview\"><a href=\"#18-1-A-Simple-Example-And-Overview\" class=\"headerlink\" title=\"18.1 A Simple Example And Overview\"></a>18.1 A Simple Example And Overview</h2><p><strong>Address Space &amp; Pages:</strong></p>\n<ul>\n<li><p>Virtual Address Space: Divided into pages. Each page has a fixed size, making memory management straightforward.</p>\n<ul>\n<li><p>Example: A tiny 64-byte address space has 4 pages, each 16 bytes.</p>\n<p><img src=\"../img/image-20241129110345549.png\" alt=\"image-20241129110345549\"></p>\n</li>\n</ul>\n</li>\n<li><p>Physical Memory: Divided into page frames. Each frame can hold one page.</p>\n<ul>\n<li>Example: A 128-byte physical memory has 8 frames.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Mapping Virtual to Physical Memory</strong></p>\n<p>Virtual addresses are mapped to physical memory through a <strong>page table</strong>.</p>\n<ul>\n<li><p>Page Table: A data structure maintained by the operating system to track the mapping of virtual pages to physical frames.</p>\n<ul>\n<li><p>Example</p>\n<p>: In this scenario:</p>\n<ul>\n<li><p>Virtual page 0 → Physical frame 3</p>\n</li>\n<li><p>Virtual page 1 → Physical frame 7</p>\n</li>\n<li><p>Virtual page 2 → Physical frame 5</p>\n</li>\n<li><p>Virtual page 3 → Physical frame 2</p>\n<p><img src=\"../img/image-20241129110602621.png\" alt=\"image-20241129110602621\"></p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Address Translation Process</strong></p>\n<p><strong>Problem</strong>: How to translate a virtual address VA=21 into a physical address?</p>\n<p><strong>(1) Break Down the Virtual Address</strong></p>\n<ul>\n<li><p>Virtual address structure:</p>\n<ul>\n<li>Address space is 64 bytes → Requires 6 bits (2^6 = 64).</li>\n<li>Page size is 16 bytes → Offset within a page requires 4 bits.</li>\n<li>Top 2 bits are the <strong>Virtual Page Number (VPN)</strong>; bottom 4 bits are the <strong>offset</strong>.</li>\n</ul>\n</li>\n<li><p>Virtual address VA = 21 in binary: 010101:</p>\n<ul>\n<li><p><strong>VPN</strong>: 010101 (Virtual page 1).</p>\n</li>\n<li><p><strong>Offset</strong>: 010101 (5th byte within the page).</p>\n<p><img src=\"../img/image-20241129111122050.png\" alt=\"image-20241129111122050\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>(2) Look Up the Page Table</strong></p>\n<ul>\n<li>From the page table, find the physical frame for VPN 01:<ul>\n<li>Virtual page 1 → Physical frame 7 (111).</li>\n</ul>\n</li>\n</ul>\n<p><strong>(3) Construct the Physical Address</strong></p>\n<ul>\n<li><p>Replace the virtual page number with the physical frame number:</p>\n<ul>\n<li>Virtual address: 010101 (VPN 01, Offset 0101).</li>\n<li>Physical address: 1110101(PFN 111, Offset 0101).</li>\n</ul>\n</li>\n<li><p><strong>Resulting Physical Address</strong>: 1110101(2)=117(10)</p>\n<p><img src=\"../img/image-20241129111150278.png\" alt=\"image-20241129111150278\"></p>\n</li>\n</ul>\n<p><strong>(4) Access Memory</strong></p>\n<ul>\n<li>The system accesses data at physical address 117, completing the memory operation.</li>\n</ul>\n<h2 id=\"18-2-Where-Are-Page-Tables-Stored\"><a href=\"#18-2-Where-Are-Page-Tables-Stored\" class=\"headerlink\" title=\"18.2 Where Are Page Tables Stored\"></a>18.2 Where Are Page Tables Stored</h2><p><strong>1. The Size Problem of Page Tables</strong></p>\n<p>Page tables can grow to enormous sizes, especially in systems with large address spaces. Let’s break it down:</p>\n<ul>\n<li><p><strong>32-bit Address Space Example</strong>:</p>\n<ul>\n<li>Address space: 2^32bytes.</li>\n<li>Page size: 4 KB (2^12).</li>\n<li>Virtual Address Structure:<ul>\n<li>20-bit <strong>VPN</strong> (Virtual Page Number).</li>\n<li>12-bit <strong>Offset</strong>.</li>\n</ul>\n</li>\n<li><strong>Total Pages</strong>: 2^20</li>\n</ul>\n</li>\n<li><p><strong>Memory Requirement</strong>:</p>\n<ul>\n<li><p>Assume each <strong>Page Table Entry (PTE)</strong> takes 4 bytes.</p>\n</li>\n<li><p>Total memory for one page table: </p>\n<script type=\"math/tex; mode=display\">\n2^{20} \\times 4 \\, \\text{bytes} = 4 \\, \\text{MB}</script></li>\n<li><p>For 100 processes: </p>\n<script type=\"math/tex; mode=display\">\n100 \\times 4 \\, \\text{MB} = 400 \\, \\text{MB}</script></li>\n<li><p><strong>Modern Challenge</strong>: Even with gigabytes of memory, dedicating hundreds of MB to page tables is inefficient.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Why Not Store Page Tables in On-Chip Memory?</strong></p>\n<ul>\n<li><strong>Limited On-Chip Space</strong>: Hardware like the <strong>Memory Management Unit (MMU)</strong> does not have enough space for full page tables.</li>\n<li>Instead, page tables for each process are stored in physical memory.<ul>\n<li>Later, we’ll explore how <strong>virtualization of OS memory</strong> allows page tables themselves to be paged or swapped to disk.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"18-3-What’s-Actually-In-The-Page-Table\"><a href=\"#18-3-What’s-Actually-In-The-Page-Table\" class=\"headerlink\" title=\"18.3 What’s Actually In The Page Table\"></a>18.3 What’s Actually In The Page Table</h2><p><strong>1. Overview of Page Table Structure</strong></p>\n<p>The page table is a <strong>data structure</strong> used by the operating system to map virtual addresses to physical addresses. Specifically, it translates <strong>Virtual Page Numbers (VPNs)</strong> to <strong>Physical Frame Numbers (PFNs)</strong>.</p>\n<ul>\n<li>Simple Implementation:<ul>\n<li>A <strong>linear page table</strong> is the most straightforward form.</li>\n<li>It is implemented as an array, where:<ul>\n<li>The <strong>VPN</strong> serves as the index.</li>\n<li>The corresponding <strong>Page Table Entry (PTE)</strong> stores the mapping to the physical frame.</li>\n</ul>\n</li>\n<li>This design is simple but inefficient for large address spaces, as it requires significant memory.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Key Components of a Page Table Entry (PTE)</strong></p>\n<p>Each PTE contains several important bits to manage memory effectively. Let’s break them down:</p>\n<p><strong>a. Valid Bit</strong></p>\n<ul>\n<li><strong>Purpose</strong>: Indicates whether the page’s translation is valid.</li>\n<li>Usage:<ul>\n<li>Pages that the process is not allowed to access (e.g., unallocated pages in a sparse address space) are marked invalid.</li>\n<li>If an invalid page is accessed, the CPU triggers a <strong>trap</strong> to the OS.</li>\n<li>Example:<ul>\n<li>At program startup, code and heap occupy one part of the address space, and the stack another.</li>\n<li>Unused memory in between is marked <strong>invalid</strong> to save physical memory.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>b. Protection Bits</strong></p>\n<ul>\n<li>Define how the page can be accessed:<ul>\n<li><strong>Read</strong>: Indicates if the page can be read.</li>\n<li><strong>Write</strong>: Determines if the page can be modified.</li>\n<li><strong>Execute</strong>: Specifies if instructions can be executed from the page.</li>\n</ul>\n</li>\n<li><strong>Trap on Violation</strong>: If the process attempts an operation not allowed by the protection bits, a trap to the OS occurs.</li>\n</ul>\n<p><strong>c. Present Bit</strong></p>\n<ul>\n<li>Indicates whether the page is currently in <strong>physical memory</strong> or has been swapped to disk.</li>\n<li>If not present:<ul>\n<li>A <strong>page fault</strong> occurs.</li>\n<li>The OS retrieves the page from disk and loads it into physical memory.</li>\n</ul>\n</li>\n</ul>\n<p><strong>d. Dirty Bit</strong></p>\n<ul>\n<li>Tracks whether the page has been <strong>modified</strong> since it was loaded into memory.</li>\n<li><strong>Purpose</strong>: Helps the OS determine whether it needs to write the page back to disk during eviction.</li>\n</ul>\n<p><strong>e. Reference/Accessed Bit</strong></p>\n<ul>\n<li>Tracks whether the page has been accessed recently.</li>\n<li><strong>Usage</strong>: Critical for page replacement algorithms to decide which pages to keep in memory.</li>\n</ul>\n<p><strong>f. Caching Bits</strong></p>\n<ul>\n<li><strong>Hardware-Specific</strong>: Control how the hardware caches the page.</li>\n</ul>\n<p><strong>3. Example: x86 Page Table Entry</strong></p>\n<p><img src=\"../img/image-20241129140316128.png\" alt=\"image-20241129140316128\"></p>\n<ul>\n<li><strong>P (Present)</strong>: Indicates if the page is in memory.</li>\n<li><strong>R/W (Read/Write)</strong>: Determines if writes are allowed.</li>\n<li><strong>U/S (User/Supervisor)</strong>: Controls access based on privilege level.</li>\n<li><strong>A (Accessed)</strong>: Indicates if the page has been accessed.</li>\n<li><strong>D (Dirty)</strong>: Tracks if the page has been modified.</li>\n<li><strong>PFN (Page Frame Number)</strong>: Points to the actual location in physical memory.</li>\n<li><strong>PWT (Page Write-Through)</strong>: Determines write-through caching policy.</li>\n<li><strong>PCD (Page Cache Disable)</strong>: Disables caching for the page.</li>\n<li><strong>PAT (Page Attribute Table)</strong>: Additional control for caching.</li>\n<li><strong>G (Global)</strong>: Prevents the TLB from being flushed on context switches.</li>\n</ul>\n<h2 id=\"18-4-Paging-Also-Too-Slow\"><a href=\"#18-4-Paging-Also-Too-Slow\" class=\"headerlink\" title=\"18.4 Paging: Also Too Slow\"></a><strong>18.4 Paging: Also Too Slow</strong></h2><p><strong>1. The Problem with Page Tables in Memory</strong></p>\n<p>Using page tables introduces two key challenges:</p>\n<ol>\n<li><p><strong>Space Overhead</strong>:<br> Page tables can be very large, especially for processes with sparse address spaces.</p>\n</li>\n<li><p>Performance Overhead:</p>\n<p>Accessing memory becomes slower because each memory reference now requires </p>\n<p>two memory accesses:</p>\n<ul>\n<li>One to retrieve the <strong>Page Table Entry (PTE)</strong>.</li>\n<li>One to fetch the actual data.</li>\n</ul>\n</li>\n</ol>\n<p><strong>2. Example Walkthrough of a Memory Access</strong></p>\n<p>Consider the instruction:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">movl 21, %eax</span><br></pre></td></tr></table></figure>\n<p>The process requires data from <strong>virtual address 21</strong>, which must be translated to the corresponding <strong>physical address</strong> before fetching the data.</p>\n<p><strong>Step-by-Step Translation Process</strong></p>\n<ol>\n<li><p><strong>Extract the Virtual Page Number (VPN):</strong></p>\n<ul>\n<li><p>Use a <strong>VPN_MASK</strong> to isolate the VPN bits from the virtual address.</p>\n</li>\n<li><p>Shift these bits right by SHIFT to form the integer VPN:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p><strong>Locate the Page Table Entry (PTE):</strong></p>\n<ul>\n<li><p>Use the <strong>Page Table Base Register (PTBR)</strong> to find the physical address of the page table.</p>\n</li>\n<li><p>Add the VPN (multiplied by the size of each PTE) to the PTBR to locate the specific PTE:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PTEAddr = PTBR + (VPN * sizeof(PTE))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p><strong>Fetch the PTE from Memory:</strong></p>\n<ul>\n<li><p>Retrieve the PTE by accessing memory at PTEAddr:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PTE = AccessMemory(PTEAddr)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p><strong>Validate the Access:</strong></p>\n<ul>\n<li><p>Check the Valid bit of the PTE:</p>\n<ul>\n<li>If <code>False</code>, trigger a <strong>segmentation fault</strong>.</li>\n</ul>\n</li>\n<li><p>Check the </p>\n<p>protection bits:</p>\n<ul>\n<li>If access is not allowed, trigger a <strong>protection fault</strong>.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Form the Physical Address (PhysAddr):</strong></p>\n<ul>\n<li><p>Extract the offset from the virtual address:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">offset = VirtualAddress &amp; OFFSET_MASK</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Concatenate the </p>\n<p>Page Frame Number (PFN) from the PTE with the offset:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offset</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p><strong>Fetch the Desired Data:</strong></p>\n<ul>\n<li><p>Access memory at PhysAddr to retrieve the data and store it in the register:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Register = AccessMemory(PhysAddr)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Summary of Translation Protocol</strong></p>\n<p>The process is illustrated in <strong>Figure 18.6</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">1</span> <span class=\"comment\">// Extract the VPN from the virtual address</span></span><br><span class=\"line\"><span class=\"number\">2</span> VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT</span><br><span class=\"line\"><span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">4</span> <span class=\"comment\">// Form the address of the page-table entry (PTE)</span></span><br><span class=\"line\"><span class=\"number\">5</span> PTEAddr = PTBR + (VPN * <span class=\"keyword\">sizeof</span>(PTE))</span><br><span class=\"line\"><span class=\"number\">6</span></span><br><span class=\"line\"><span class=\"number\">7</span> <span class=\"comment\">// Fetch the PTE</span></span><br><span class=\"line\"><span class=\"number\">8</span> PTE = AccessMemory(PTEAddr)</span><br><span class=\"line\"><span class=\"number\">9</span></span><br><span class=\"line\"><span class=\"number\">10</span> <span class=\"comment\">// Check if process can access the page</span></span><br><span class=\"line\"><span class=\"number\">11</span> <span class=\"keyword\">if</span> (PTE.Valid == False)</span><br><span class=\"line\"><span class=\"number\">12</span>     RaiseException(SEGMENTATION_FAULT)</span><br><span class=\"line\"><span class=\"number\">13</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (CanAccess(PTE.ProtectBits) == False)</span><br><span class=\"line\"><span class=\"number\">14</span>     RaiseException(PROTECTION_FAULT)</span><br><span class=\"line\"><span class=\"number\">15</span> <span class=\"keyword\">else</span></span><br><span class=\"line\"><span class=\"number\">16</span>     <span class=\"comment\">// Access is OK: form physical address and fetch it</span></span><br><span class=\"line\"><span class=\"number\">17</span>     offset = VirtualAddress &amp; OFFSET_MASK</span><br><span class=\"line\"><span class=\"number\">18</span>     PhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offset</span><br><span class=\"line\"><span class=\"number\">19</span>     Register = AccessMemory(PhysAddr)</span><br></pre></td></tr></table></figure>\n<p><strong>3. Performance Issue: Double Memory Access</strong></p>\n<ul>\n<li>Two memory references are required per instruction:<ol>\n<li>Fetch the PTE.</li>\n<li>Fetch the data using the translated physical address.</li>\n</ol>\n</li>\n<li>This effectively <strong>doubles memory access time</strong>, leading to significant performance degradation.</li>\n</ul>\n<h2 id=\"18-5-A-Memory-Trace\"><a href=\"#18-5-A-Memory-Trace\" class=\"headerlink\" title=\"18.5 A Memory Trace\"></a>18.5 A Memory Trace</h2><p><strong>1. Program and Context</strong></p>\n<ul>\n<li><p>The program initializes an integer array:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"built_in\">array</span>[<span class=\"number\">1000</span>];</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">1000</span>; i++)</span><br><span class=\"line\">    <span class=\"built_in\">array</span>[i] = <span class=\"number\">0</span>;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Each iteration of the loop sets one array element to <code>0</code>.</p>\n</li>\n</ul>\n<p><strong>2. Assembly Instructions</strong></p>\n<p>Disassembling the program yields the following assembly code:</p>\n<ol>\n<li><code>movl $0x0, (%edi, %eax, 4)</code><ul>\n<li>Writes <code>0</code> into the memory location calculated as <code>(%edi + %eax * 4)</code>.</li>\n<li><code>%edi</code>: Base address of the array.</li>\n<li><code>%eax</code>: Current array index.</li>\n</ul>\n</li>\n<li><code>incl %eax</code>: Increments <code>%eax</code> to point to the next array element.</li>\n<li><code>cmpl $0x03e8, %eax</code>: Compares <code>%eax</code> with <code>1000</code> (end condition).</li>\n<li><code>jne 0x1024</code>: Jumps back to the first instruction if <code>%eax</code> is not yet <code>1000</code>.</li>\n</ol>\n<p>Each loop iteration processes one array element, requiring several memory operations.</p>\n<p><strong>3. Paging Mechanism</strong></p>\n<p><strong>Assumptions:</strong></p>\n<ol>\n<li><strong>Virtual Address Space:</strong> 64KB (unrealistically small for clarity).</li>\n<li><strong>Page Size:</strong> 1KB.</li>\n<li><strong>Page Table Location:</strong> Physical address 1024 (1KB).</li>\n<li><strong>Mappings:</strong><ul>\n<li><strong>Instructions:</strong> Virtual page 1 (VPN 1) maps to physical frame 4 (PFN 4).</li>\n<li><strong>Array:</strong> Virtual pages 39–42 (VPN 39–42) map to physical frames 7–10 (PFN 7–10).</li>\n</ul>\n</li>\n</ol>\n<p><strong>Page Table Format:</strong></p>\n<ul>\n<li>A linear page table (array-based) is used.</li>\n<li>Virtual addresses are translated by looking up their respective entries in the page table</li>\n</ul>\n<p><strong>4. Memory Access Breakdown</strong></p>\n<p>Each memory access involves two steps:</p>\n<ol>\n<li><strong>Page Table Lookup:</strong> Translate the virtual address to a physical address.</li>\n<li><strong>Data Access:</strong> Use the physical address to fetch the instruction or data.</li>\n</ol>\n<p><strong>Example for One Loop Iteration:</strong></p>\n<ol>\n<li><p>Fetch the <code>movl</code> instruction:</p>\n<ul>\n<li>Translate virtual address (VA) for the instruction using the page table.</li>\n<li>Fetch the instruction from physical memory.</li>\n</ul>\n</li>\n<li><p>Fetch the array element address:</p>\n<ul>\n<li>Translate the VA of the array element (e.g., 40000 for the first element).</li>\n<li>Fetch the page table entry for VPN 39.</li>\n<li>Fetch the physical address (7232 for the first element).</li>\n</ul>\n</li>\n<li><p>Write <code>0</code> to the physical address (array data).</p>\n</li>\n<li><p>Fetch and execute the incl, cmpl, and jne</p>\n<p>instructions:</p>\n<ul>\n<li>Each requires translating their respective VAs to PAs.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Total Memory Accesses Per Iteration:</strong></p>\n<ul>\n<li><strong>Instruction fetches:</strong> 8 (page table lookup <em>4 + data access </em>4).</li>\n<li><strong>Explicit update:</strong> 2(page table lookup + data write).</li>\n<li><strong>Total:</strong> 10 memory accesses.</li>\n</ul>\n<p><strong>5. Visualization (Figure 18.7)</strong></p>\n<ul>\n<li><strong>X-axis:</strong> Memory access sequence across five iterations.</li>\n<li>Y-axis:<ul>\n<li><strong>Black:</strong> Virtual and physical addresses of instructions.</li>\n<li><strong>Dark gray:</strong> Virtual and physical addresses of array data.</li>\n<li><strong>Light gray:</strong> Physical addresses for page table lookups.</li>\n</ul>\n</li>\n</ul>\n<p>The graph shows:</p>\n<ol>\n<li>Instruction fetches are repeated every iteration.</li>\n<li>Array accesses shift to new virtual pages (VPN 39–42) as the loop progresses.</li>\n</ol>\n<p><img src=\"../img/image-20241129150105341.png\" alt=\"image-20241129150105341\"></p>\n<h1 id=\"Chapter-19-Paging-Faster-Translations-TLBs\"><a href=\"#Chapter-19-Paging-Faster-Translations-TLBs\" class=\"headerlink\" title=\"Chapter 19: Paging: Faster Translations (TLBs)\"></a>Chapter 19: Paging: Faster Translations (TLBs)</h1><h2 id=\"19-1-TLB-Basic-Algorithm\"><a href=\"#19-1-TLB-Basic-Algorithm\" class=\"headerlink\" title=\"19.1 TLB Basic Algorithm\"></a>19.1 TLB Basic Algorithm</h2><p><strong>1. Virtual Address Translation Begins</strong></p>\n<ul>\n<li>The CPU generates a <strong>virtual address (VA)</strong>.</li>\n<li>The virtual address is divided into:<ul>\n<li><strong>Virtual Page Number (VPN)</strong>: Determines which virtual page is being accessed.</li>\n<li><strong>Offset</strong>: Specifies the exact byte within the page.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Check the TLB</strong></p>\n<ul>\n<li>The hardware checks if the translation for the VPN exists in the TLB:<ul>\n<li>TLB Hit:<ul>\n<li>The desired translation is found in the TLB.</li>\n<li>The hardware retrieves the <strong>Page Frame Number (PFN)</strong> from the TLB entry.</li>\n<li>The physical address (PA) is constructed by combining the PFN with the original offset.</li>\n<li>The memory is accessed, completing the operation quickly.</li>\n</ul>\n</li>\n<li>TLB Miss:<ul>\n<li>The desired translation is not in the TLB.</li>\n<li>The hardware must now access the page table to find the correct translation.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Handling a TLB Miss</strong></p>\n<ul>\n<li>The hardware performs these additional steps:<ol>\n<li>Accesses the <strong>page table</strong> in physical memory using the VPN to find the corresponding PFN.</li>\n<li>Performs <strong>protection checks</strong> to ensure the process has the appropriate access rights (e.g., read/write).</li>\n<li>Updates the TLB with the new translation (VPN → PFN), so future accesses to this page are faster.</li>\n<li>Retries the memory access, which now succeeds because the TLB holds the required translation.</li>\n</ol>\n</li>\n</ul>\n<p><strong>4. Costs of a TLB Miss</strong></p>\n<ul>\n<li>A <strong>TLB hit</strong> is fast because the TLB is close to the CPU and optimized for speed.</li>\n<li>A TLB miss incurs a high cost because:<ol>\n<li>The page table must be accessed, which requires at least one additional memory access (and possibly more if multi-level page tables are used).</li>\n<li>Updating the TLB takes time.</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"19-2-Example-Accessing-an-Array-with-TLB\"><a href=\"#19-2-Example-Accessing-an-Array-with-TLB\" class=\"headerlink\" title=\"19.2 Example: Accessing an Array with TLB\"></a><strong>19.2 Example: Accessing an Array with TLB</strong></h2><p><strong>System Setup</strong></p>\n<ol>\n<li><strong>Virtual Address Space</strong>: 8 bits (256 possible addresses).</li>\n<li><strong>Page Size:</strong> 16 bytes.<ul>\n<li>VPN: 4 bits (16 virtual pages).</li>\n<li>Offset: 4 bits (16 bytes per page).</li>\n</ul>\n</li>\n<li><strong>Array:</strong> 10 integers, each 4 bytes, starting at virtual address 100.<ul>\n<li>Total size: 10×4 = 40 bytes.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Array Placement in Virtual Memory</strong></p>\n<p>The array starts at address <code>100</code> (binary: <code>01100100</code>). Breaking this into VPN and offset:</p>\n<ul>\n<li>VPN = 0110 (Page 6)</li>\n<li>Offset = 0100 (Byte 4 in the page)</li>\n</ul>\n<p><strong>Array Layout</strong>:</p>\n<p><img src=\"/img/image-20241129152242794.png\" alt=\"image-20241129152242794\"></p>\n<p><strong>Access Pattern in C</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> sum = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">10</span>; i++) &#123;</span><br><span class=\"line\">    sum += a[i];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>Detailed TLB Walkthrough</strong></p>\n<p><strong>Initial Condition</strong>: TLB is empty at the start.</p>\n<p><strong>Access 1: <code>a[0]</code></strong></p>\n<ul>\n<li><strong>Virtual Address</strong>: 100 → VPN = 06, Offset = 04.</li>\n<li><strong>TLB Lookup</strong>: Miss.</li>\n<li><strong>Action</strong>: Hardware fetches the physical frame number (PFN) for VPN = 06 from the page table and updates the TLB.</li>\n<li><strong>Cost</strong>: High (page table lookup).</li>\n</ul>\n<p><strong>Access 2: <code>a[1]</code></strong></p>\n<ul>\n<li><strong>Virtual Address</strong>: 104 → VPN = 06, Offset = 08.</li>\n<li><strong>TLB Lookup</strong>: Hit.</li>\n<li><strong>Action</strong>: Address translation is done using the cached TLB entry for VPN = 06.</li>\n<li><strong>Cost</strong>: Low (TLB hit).</li>\n</ul>\n<p><strong>Access 3: <code>a[2]</code></strong></p>\n<ul>\n<li><strong>Virtual Address</strong>: 108 → VPN = 06, Offset = 12.</li>\n<li><strong>TLB Lookup</strong>: Hit.</li>\n<li><strong>Cost</strong>: Low.</li>\n</ul>\n<p><strong>Access 4: <code>a[3]</code></strong></p>\n<ul>\n<li><strong>Virtual Address</strong>: 112 → VPN = 07, Offset = 00.</li>\n<li><strong>TLB Lookup</strong>: Miss.</li>\n<li><strong>Action</strong>: Fetch the translation for VPN = 07 from the page table and update the TLB.</li>\n<li><strong>Cost</strong>: High.</li>\n</ul>\n<p><strong>Access 5-7: <code>a[4]</code>, <code>a[5]</code>, <code>a[6]</code></strong></p>\n<ul>\n<li>All these accesses are within VPN = 07.</li>\n<li><strong>TLB Lookup</strong>: Hits for all.</li>\n<li><strong>Cost</strong>: Low.</li>\n</ul>\n<p><strong>Access 8: <code>a[7]</code></strong></p>\n<ul>\n<li><strong>Virtual Address</strong>: 128 → VPN = 08, Offset = 00.</li>\n<li><strong>TLB Lookup</strong>: Miss.</li>\n<li><strong>Action</strong>: Fetch the translation for VPN = 08 and update the TLB.</li>\n<li><strong>Cost</strong>: High.</li>\n</ul>\n<p><strong>Access 9-10: <code>a[8]</code>, <code>a[9]</code></strong></p>\n<ul>\n<li>Both accesses are within VPN = 08.</li>\n<li><strong>TLB Lookup</strong>: Hits for both.</li>\n<li><strong>Cost</strong>: Low.</li>\n</ul>\n<p><strong>TLB Activity Summary</strong></p>\n<ul>\n<li><p>Access Pattern: <strong>Miss, Hit, Hit, Miss, Hit, Hit, Hit, Miss, Hit, Hit.</strong></p>\n</li>\n<li><p><strong>Hit Rate</strong>:</p>\n<script type=\"math/tex; mode=display\">\n\\text{Hit Rate} = \\frac{\\text{Hits}}{\\text{Total Accesses}} = \\frac{7}{10} = 70\\%.</script></li>\n</ul>\n<h2 id=\"19-3-Who-Handles-the-TLB-Miss\"><a href=\"#19-3-Who-Handles-the-TLB-Miss\" class=\"headerlink\" title=\"19.3 Who Handles the TLB Miss?\"></a><strong>19.3 Who Handles the TLB Miss?</strong></h2><p><strong>1. Hardware-Managed TLBs</strong></p>\n<ul>\n<li><p><strong>Description</strong>:</p>\n<ul>\n<li>In hardware-managed TLBs, the hardware itself resolves the TLB miss without any direct involvement from the operating system.</li>\n<li>The hardware must have detailed knowledge of the <strong>page table’s location and structure</strong> to perform this task.</li>\n</ul>\n</li>\n<li><p><strong>Process</strong>:</p>\n<ol>\n<li>The hardware detects a TLB miss during address translation.</li>\n<li>It automatically <strong>walks the page table</strong>, retrieving the physical frame number (PFN) associated with the requested virtual page.</li>\n<li>The retrieved translation is <strong>inserted into the TLB</strong>.</li>\n<li>The instruction that caused the miss is retried, now resulting in a TLB hit.</li>\n</ol>\n</li>\n<li><p><strong>Key Features</strong>:</p>\n<ul>\n<li>Requires a <strong>page-table base register</strong> (e.g., CR3 register in Intel x86) to locate the page table.</li>\n<li>The hardware assumes a fixed, predefined format for the page table (e.g., multi-level page tables in x86).</li>\n<li>The entire process is handled without invoking the OS.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Software-Managed TLBs</strong></p>\n<ul>\n<li><p><strong>Description</strong>:</p>\n<ul>\n<li>Modern architectures like <strong>MIPS R10k</strong> or <strong>Sun SPARC v9</strong> handle TLB misses using the operating system.</li>\n<li>The hardware raises a <strong>trap or exception</strong> on a TLB miss, transferring control to the operating system’s <strong>TLB miss handler</strong>.</li>\n</ul>\n</li>\n<li><p><strong>Process</strong>:</p>\n<ol>\n<li><p>On a TLB miss, the hardware raises an exception and enters <strong>kernel mode</strong>.</p>\n</li>\n<li><p>Control is passed to the OS, which executes a TLB miss handler</p>\n<p> This handler:</p>\n<ul>\n<li>Locates the translation in the page table.</li>\n<li>Updates the TLB with the correct virtual-to-physical mapping using privileged instructions.</li>\n</ul>\n</li>\n<li><p>The OS <strong>returns from the trap</strong>, resuming execution at the <strong>instruction that caused the TLB miss</strong>, ensuring it now hits in the TLB.</p>\n</li>\n</ol>\n</li>\n<li><p><strong>Key Features</strong>:</p>\n<ul>\n<li>Greater <strong>flexibility</strong>: The OS can implement any page table structure or data format it prefers.</li>\n<li>The TLB miss handler is carefully managed to avoid <strong>infinite TLB misses</strong>, often by keeping handler code in unmapped physical memory or using reserved TLB entries.</li>\n</ul>\n</li>\n<li><p><strong>Example</strong>:<br> <strong>RISC processors</strong> (e.g., MIPS or SPARC) use software-managed TLBs to simplify hardware design and allow OS control over memory management.</p>\n</li>\n</ul>\n<p><strong>Comparison: Hardware vs. Software Management</strong></p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th><strong>Aspect</strong></th>\n<th><strong>Hardware-Managed TLBs</strong></th>\n<th><strong>Software-Managed TLBs</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Complexity</strong></td>\n<td>More complex hardware design.</td>\n<td>Simpler hardware, relies on OS.</td>\n</tr>\n<tr>\n<td><strong>Flexibility</strong></td>\n<td>Fixed page table structure.</td>\n<td>OS decides the page table structure.</td>\n</tr>\n<tr>\n<td><strong>Performance</strong></td>\n<td>Faster on a miss due to direct hardware handling.</td>\n<td>Slightly slower as it involves an exception.</td>\n</tr>\n<tr>\n<td><strong>Examples</strong></td>\n<td>Intel x86 (older), CISC architectures.</td>\n<td>MIPS, SPARC, RISC architectures.</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h2 id=\"19-4-TLB-Contents-What’s-In-There\"><a href=\"#19-4-TLB-Contents-What’s-In-There\" class=\"headerlink\" title=\"19.4 TLB Contents: What’s In There?\"></a>19.4 TLB Contents: What’s In There?</h2><p><strong>1. TLB Entry Structure</strong></p>\n<p>A typical TLB entry contains a few key components:</p>\n<ul>\n<li><strong>VPN (Virtual Page Number)</strong>: This is the <strong>virtual page number</strong>, which is the part of the virtual address that is translated into a physical address. It’s the index into the page table.</li>\n<li><strong>PFN (Physical Frame Number)</strong>: The <strong>physical frame number</strong> is the corresponding physical address for the page, which is what the memory management unit (MMU) uses to access the actual memory location.</li>\n<li><strong>Other Bits</strong>: These additional bits control and define the behavior of the entry. These can include:<ul>\n<li><strong>Valid Bit</strong>: This bit indicates whether the entry contains a valid translation. If the valid bit is <strong>0</strong>, it means the entry is not valid, and a TLB miss occurs. If it’s <strong>1</strong>, the translation is valid and ready to use.</li>\n<li><strong>Protection Bits</strong>: These determine the types of access allowed for the page, similar to the protection flags in the page table. For instance:<ul>\n<li>Read/Write (RW): Indicates whether the page can be read from or written to.</li>\n<li>Read/Execute (RX): Specifies if the page can be executed as code.</li>\n<li>Read-Only (RO): Some pages are marked read-only, preventing write operations.</li>\n</ul>\n</li>\n<li><strong>Address-Space Identifier (ASID)</strong>: Some TLBs include an ASID to distinguish between different address spaces, useful for multi-tasking environments. This ensures that one process’s TLB entries don’t accidentally mix with another’s.</li>\n<li><strong>Dirty Bit</strong>: This bit tracks whether the page has been modified (i.e., written to) since it was last loaded into memory. This helps the operating system determine if the page needs to be written back to disk (for paging or swapping) when it’s evicted from memory.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Example of TLB Entry</strong></p>\n<p>A simple <strong>TLB entry</strong> might look like this:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th><strong>VPN</strong></th>\n<th><strong>PFN</strong></th>\n<th><strong>Valid Bit</strong></th>\n<th><strong>Protection Bits</strong></th>\n<th><strong>ASID</strong></th>\n<th><strong>Dirty Bit</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0x12345</td>\n<td>0x6789A</td>\n<td>1</td>\n<td>Read/Write</td>\n<td>0x01</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n</div>\n<ul>\n<li><strong>VPN</strong>: The virtual page number (0x12345).</li>\n<li><strong>PFN</strong>: The physical frame number (0x6789A).</li>\n<li><strong>Valid Bit</strong>: Indicates that this entry is valid (1).</li>\n<li><strong>Protection Bits</strong>: This page can be read and written to.</li>\n<li><strong>ASID</strong>: Belongs to address space 0x01.</li>\n<li><strong>Dirty Bit</strong>: This page hasn’t been modified (0).</li>\n</ul>\n<h2 id=\"19-5-TLB-Issue-Context-Switches\"><a href=\"#19-5-TLB-Issue-Context-Switches\" class=\"headerlink\" title=\"19.5 TLB Issue: Context Switches\"></a>19.5 TLB Issue: Context Switches</h2><p><strong>The Problem with Context Switching</strong></p>\n<p>Consider the following scenario:</p>\n<ul>\n<li><strong>Process P1</strong> is running and has its 10th virtual page (VPN 10) mapped to physical frame 100.</li>\n<li><strong>Process P2</strong> is about to be run, and it has its 10th virtual page (VPN 10) mapped to physical frame 170.</li>\n</ul>\n<p>The TLB could have entries like:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th><strong>VPN</strong></th>\n<th><strong>PFN</strong></th>\n<th><strong>Valid</strong></th>\n<th><strong>Protection</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10</td>\n<td>100</td>\n<td>1</td>\n<td>rwx</td>\n</tr>\n<tr>\n<td>10</td>\n<td>170</td>\n<td>1</td>\n<td>rwx</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Here, the TLB is ambiguous: it contains entries for VPN 10 that map to different physical frames (100 and 170). When the hardware searches for a translation, it cannot determine which entry belongs to P1 or P2. This can lead to incorrect translations and errors when the TLB is used for address lookup.</p>\n<p><strong>Approaches to Managing TLB on Context Switch</strong></p>\n<p><strong>1. Flushing the TLB</strong></p>\n<ul>\n<li><p><strong>Concept</strong>: Clear all entries in the TLB upon context switches.</p>\n</li>\n<li><p>Mechanism:</p>\n<ul>\n<li>This can be achieved using a privileged hardware instruction in a software-managed system or automatically by changing the <strong>page-table base register (PTBR)</strong> in a hardware-managed TLB.</li>\n<li>The flush operation sets all valid bits to 0, invalidating all entries.</li>\n</ul>\n</li>\n<li><p>Pros:</p>\n<ul>\n<li>Ensures that there are no stale or incorrect translations in the TLB for the new process.</li>\n</ul>\n</li>\n<li><p>Cons:</p>\n<ul>\n<li><p>Introduces performance overhead, as the new process will incur <strong>TLB misses</strong> when accessing data and code pages for the first time.</p>\n</li>\n<li><p>If context switches are frequent, the performance cost can be significant due to the repeated TLB misses.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Using Address Space Identifiers (ASIDs)</strong></p>\n<ul>\n<li><strong>Concept</strong>: Add an <strong>ASID</strong> field to TLB entries to differentiate between translations for different processes.</li>\n<li><strong>Mechanism:</strong><ul>\n<li>Each TLB entry includes an <strong>ASID</strong> (a process identifier, typically smaller in size than a PID, e.g., 8 bits).</li>\n<li>The TLB can store entries for multiple processes simultaneously, and the ASID helps the hardware distinguish which translations belong to the currently running process.</li>\n</ul>\n</li>\n<li><strong>Example</strong>:</li>\n</ul>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th><strong>VPN</strong></th>\n<th><strong>PFN</strong></th>\n<th><strong>Valid</strong></th>\n<th><strong>Protection</strong></th>\n<th><strong>ASID</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10</td>\n<td>100</td>\n<td>1</td>\n<td>rwx</td>\n<td>1</td>\n</tr>\n<tr>\n<td>10</td>\n<td>170</td>\n<td>1</td>\n<td>rwx</td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n</div>\n<ul>\n<li>Pros:<ul>\n<li>Reduces the need to flush the TLB on context switches, as translations for other processes are ignored based on the ASID.</li>\n</ul>\n</li>\n<li>Cons:<ul>\n<li>The OS must update a privileged register with the current process’s ASID during a context switch.</li>\n<li>The TLB needs additional logic to filter entries based on the ASID, which can add complexity.</li>\n</ul>\n</li>\n</ul>\n<p><strong>TLB Sharing Across Processes</strong></p>\n<p>ASIDs also enable <strong>TLB sharing</strong> in cases where multiple processes share a physical page:</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th><strong>VPN</strong></th>\n<th><strong>PFN</strong></th>\n<th><strong>Valid</strong></th>\n<th><strong>Protection</strong></th>\n<th><strong>ASID</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10</td>\n<td>101</td>\n<td>1</td>\n<td>r-x</td>\n<td>1</td>\n</tr>\n<tr>\n<td>50</td>\n<td>101</td>\n<td>1</td>\n<td>r-x</td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n</div>\n<ul>\n<li><strong>Example</strong>: Process 1 (P1) and Process 2 (P2) share physical page 101, but map it to different virtual pages (VPN 10 and 50).</li>\n<li><strong>Benefits:</strong><ul>\n<li><strong>Code sharing</strong> between processes, such as shared libraries, reduces memory overhead.</li>\n<li>The TLB can hold separate entries for each process’s mapping, differentiated by the ASID.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"19-6-TLB-Replacement-Policy\"><a href=\"#19-6-TLB-Replacement-Policy\" class=\"headerlink\" title=\"19.6 TLB Replacement Policy\"></a><strong>19.6 TLB Replacement Policy</strong></h2><p><strong>Common TLB Replacement Policies</strong></p>\n<p><strong>1. Least Recently Used (LRU)</strong></p>\n<ul>\n<li><strong>Concept</strong>: The least recently used (LRU) entry is evicted when a new entry needs to be installed.</li>\n<li><strong>Rationale</strong>: This policy assumes that entries that have not been accessed recently are less likely to be used in the near future, leveraging the concept of <strong>temporal locality</strong> in memory access.</li>\n<li><strong>Limitations</strong>: Can perform poorly in certain scenarios, such as when a program loops over <code>n + 1</code> pages with a TLB of size <code>n</code>, resulting in a miss for every access (known as the <strong>“thrashing” problem</strong>). In such cases, LRU is suboptimal.</li>\n</ul>\n<p><strong>2. Random Replacement Policy</strong></p>\n<ul>\n<li><strong>Concept</strong>: A TLB entry is evicted at random when a new entry needs to be installed.</li>\n<li><strong>Rationale</strong>: This approach is simple and avoids some edge cases where more sophisticated policies, like LRU, might behave poorly.</li>\n<li><strong>Use Case</strong>: Random replacement is particularly effective when the TLB size is small relative to the working set of the program.</li>\n</ul>\n<p><strong>3. Other Policies</strong></p>\n<p>While LRU and Random are among the most common, other replacement strategies are also possible:</p>\n<ul>\n<li><strong>First-In, First-Out (FIFO)</strong>: Evicts the oldest entry in the TLB. It’s simpler than LRU but may not be as effective in optimizing for cache hits.</li>\n<li><strong>Least Frequently Used (LFU)</strong>: Evicts the entry that has been accessed the fewest times. This policy can be complex to implement due to the need for tracking access counts.</li>\n<li><strong>Clock Policy</strong>: A practical approximation of LRU, using a circular buffer and a “clock hand” to check and evict entries based on their reference bits. If an entry’s reference bit is 1, it’s cleared and the hand moves to the next entry. If it’s 0, the entry is evicted.</li>\n</ul>\n<h2 id=\"19-7-Real-TLB-Entry-The-MIPS-R4000\"><a href=\"#19-7-Real-TLB-Entry-The-MIPS-R4000\" class=\"headerlink\" title=\"19.7 Real TLB Entry: The MIPS R4000\"></a><strong>19.7 Real TLB Entry: The MIPS R4000</strong></h2><p>The <strong>MIPS R4000</strong> is a modern system that uses <strong>software-managed TLBs</strong>. This section discusses the structure of a TLB entry in the MIPS R4000 and the associated concepts.</p>\n<p><strong>Structure of a MIPS TLB Entry</strong>:</p>\n<ul>\n<li><strong>Address Space</strong>: The MIPS R4000 supports a 32-bit address space with 4KB pages. The virtual address is split into:<ul>\n<li><strong>20-bit Virtual Page Number (VPN)</strong>.</li>\n<li><strong>12-bit Offset</strong> (used within the page).</li>\n</ul>\n</li>\n<li><strong>VPN Size</strong>: While 20 bits are expected for the VPN in a general 32-bit virtual address, only <strong>19 bits</strong> are used for user addresses. This is because half of the 32-bit address space is reserved for the kernel, reducing the VPN size to 19 bits.</li>\n<li><strong>Physical Frame Number (PFN)</strong>: The PFN has up to <strong>24 bits</strong>, supporting up to <strong>64 GB of physical memory</strong> (with 4KB pages).</li>\n</ul>\n<p><img src=\"/img/image-20241129155940896.png\" alt=\"image-20241129155940896\"></p>\n<p><strong>TLB Management in MIPS R4000</strong>:</p>\n<ul>\n<li><strong>Software-Managed TLB</strong>: Unlike hardware-managed TLBs, the MIPS R4000 requires software to manage TLB entries. The OS must explicitly update and maintain the TLB contents.</li>\n<li>Instructions for Managing the TLB:<ul>\n<li><strong>TLBP</strong>: Probes the TLB to check if a particular translation exists.</li>\n<li><strong>TLBR</strong>: Reads a TLB entry’s contents into registers.</li>\n<li><strong>TLBWI</strong>: Writes to a specific TLB entry.</li>\n<li><strong>TLBWR</strong>: Replaces a random TLB entry.</li>\n</ul>\n</li>\n<li><strong>Privileged Instructions</strong>: These TLB management instructions are privileged and should only be used by the OS. Allowing a user process to modify the TLB could lead to severe security issues, such as executing arbitrary code or disrupting system operations.</li>\n</ul>\n<h1 id=\"Chapter-20-Paging-Smaller-Tables\"><a href=\"#Chapter-20-Paging-Smaller-Tables\" class=\"headerlink\" title=\"Chapter 20: Paging: Smaller Tables\"></a>Chapter 20: Paging: Smaller Tables</h1><h2 id=\"20-1-Simple-Solution-Bigger-Pages\"><a href=\"#20-1-Simple-Solution-Bigger-Pages\" class=\"headerlink\" title=\"20.1 Simple Solution: Bigger Pages\"></a><strong>20.1 Simple Solution: Bigger Pages</strong></h2><p>One way to reduce the size of the page table is by using <strong>larger page sizes</strong>. This approach helps minimize the number of entries needed in the page table, thus reducing its total size. Let’s consider the following example for a 32-bit address space:</p>\n<ul>\n<li>Example with 16KB Pages:<ul>\n<li>The address space would be divided into a <strong>18-bit Virtual Page Number (VPN)</strong> and a <strong>14-bit offset</strong>.</li>\n<li>If each Page Table Entry (PTE) takes 4 bytes, the linear page table would have 2182^{18} entries.</li>\n<li>This results in a total page table size of <strong>1MB</strong>, which is a <strong>factor of four smaller</strong> than a table with 4KB pages (which would have 2202^{20} entries for the same 32-bit address space).</li>\n</ul>\n</li>\n</ul>\n<p><strong>Challenges with Larger Pages</strong>:</p>\n<ul>\n<li><p><strong>Internal Fragmentation</strong>: One significant drawback of using larger pages is <strong>internal fragmentation</strong>, where space within a page is wasted if applications do not fully use the entire page. This inefficiency happens because the allocated pages may only be partially filled, leading to unused memory within those pages.</p>\n</li>\n<li><p><strong>Memory Utilization</strong>: Applications may need to allocate pages but end up using only a small portion of each. This can lead to the memory filling up with many partially used large pages, reducing overall memory efficiency.</p>\n</li>\n</ul>\n<h2 id=\"20-2-Hybrid-Approach-Paging-and-Segments\"><a href=\"#20-2-Hybrid-Approach-Paging-and-Segments\" class=\"headerlink\" title=\"20.2 Hybrid Approach: Paging and Segments\"></a><strong>20.2 Hybrid Approach: Paging and Segments</strong></h2><p><strong>Concept of Hybrid Paging and Segmentation</strong>:</p>\n<p>A <strong>hybrid solution</strong> involves using <strong>multiple page tables</strong>—one for each logical segment of the address space (e.g., code, heap, stack). Instead of having a single large page table for all segments, there are separate tables for each segment, reducing wasted space.</p>\n<p><strong>Example of a 32-bit Virtual Address Space</strong>:</p>\n<ol>\n<li><p><strong>Segmentation</strong>: A 32-bit address is split into:</p>\n<ul>\n<li><p><strong>Top bits</strong>: Used to identify the segment (e.g., <code>01</code> for code, <code>10</code> for heap, <code>11</code> for stack).</p>\n</li>\n<li><p><strong>VPN (Virtual Page Number)</strong>: Identifies the page within the segment.</p>\n</li>\n<li><p><strong>Offset</strong>: The offset within the page.</p>\n<p><img src=\"/img/image-20241129170646344.png\" alt=\"image-20241129170646344\"></p>\n</li>\n</ul>\n</li>\n<li><p><strong>Base and Bounds Registers</strong>:</p>\n<ul>\n<li>Each segment has a <strong>base register</strong> pointing to the physical address of its page table.</li>\n<li>A <strong>bounds register</strong> indicates the number of valid pages in the segment.</li>\n</ul>\n</li>\n<li><p><strong>Address Translation</strong>:</p>\n<ul>\n<li>When a memory access occurs and a TLB miss happens, the hardware uses the segment bits to select the appropriate base and bounds register for the segment.</li>\n<li>The hardware combines the segment’s base address with the VPN to find the page table entry (PTE).</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SN = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SN_SHIFT</span><br><span class=\"line\">VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; VPN_SHIFT</span><br><span class=\"line\">AddressOfPTE = Base[SN] + (VPN * sizeof(PTE))</span><br></pre></td></tr></table></figure>\n<p>This process resembles the linear page table lookup but uses multiple segment base registers.</p>\n</li>\n</ol>\n<p><strong>Benefits</strong>:</p>\n<ul>\n<li><strong>Memory Efficiency</strong>: Reduces memory waste by only allocating page tables for used segments.</li>\n<li><strong>Smaller Page Tables</strong>: Unused pages between segments do not consume space in a page table.</li>\n</ul>\n<p><strong>Challenges</strong>:</p>\n<ol>\n<li><strong>Limited Flexibility</strong>: Segmentation assumes a certain usage pattern. For instance, a large but sparsely used heap can still lead to waste within the segment’s page table.</li>\n<li><strong>External Fragmentation</strong>: Page tables can be of variable size (multiples of PTEs), making it harder to find contiguous space in physical memory to store them.</li>\n</ol>\n<h2 id=\"20-3-Multi-level-Page-Tables\"><a href=\"#20-3-Multi-level-Page-Tables\" class=\"headerlink\" title=\"20.3 Multi-level Page Tables\"></a>20.3 Multi-level Page Tables</h2><p><strong>Concept Overview</strong></p>\n<p>A multi-level page table divides the page table into smaller, page-sized units. If an entire page of page table entries (PTEs) is invalid (i.e., unused), it is not allocated in memory. To manage these pages, a <strong>page directory</strong> is introduced. The page directory tracks whether a page of the page table exists in memory and points to it if valid.</p>\n<p><strong>Page Directory</strong>: Contains entries (Page Directory Entries, PDEs) that indicate the validity and address of the corresponding pages of the page table. Each PDE minimally has:</p>\n<ul>\n<li>A <strong>valid bit</strong> indicating if the page table it points to has valid entries.</li>\n<li>A <strong>page frame number (PFN)</strong>, which points to the physical location of the page table in memory.</li>\n</ul>\n<p>When a PDE is marked as valid, it means that at least one entry on the referenced page of the page table is valid.</p>\n<p><img src=\"/img/image-20241129171439790.png\" alt=\"image-20241129171439790\"></p>\n<p><strong>Structure and Benefits</strong></p>\n<ul>\n<li><strong>Space Efficiency</strong>: Multi-level tables allocate space proportional to the actual address space in use, avoiding memory allocation for unused regions.</li>\n<li><strong>Memory Management</strong>: Each level of the table can be managed within a single memory page, simplifying allocation and growth.</li>\n<li><strong>Flexibility</strong>: Page table pages can be scattered in physical memory, unlike a linear page table that requires contiguous memory.</li>\n</ul>\n<p>However, multi-level tables involve more complex lookups:</p>\n<ul>\n<li><strong>TLB Miss</strong>: Accessing a virtual address results in two memory loads (one for the PDE and one for the PTE), compared to one load in a linear page table. This represents a trade-off between space savings and lookup time.</li>\n</ul>\n<p><strong>Example of Multi-Level Page Table</strong></p>\n<p>Consider a 16KB address space with 64-byte pages, resulting in a 14-bit virtual address space (8 bits for the VPN and 6 bits for the offset). A linear page table for this space would need 256 entries, each 4 bytes in size, totaling 1KB. This table can be divided into 16 pages, each holding 16 PTEs.</p>\n<p><img src=\"/img/image-20241129171716962.png\" alt=\"image-20241129171716962\"></p>\n<p><strong>Page Directory</strong>:</p>\n<ul>\n<li>For the given example, we divide the page table into 16 pages and create a page directory with 16 entries, each pointing to a page of the page table.</li>\n</ul>\n<p><strong>Indexing</strong>:</p>\n<p><img src=\"/img/image-20241129171807548.png\" alt=\"image-20241129171807548\"></p>\n<ul>\n<li><p>The top 4 bits of the VPN are used to index into the page directory (PDIndex). The address of the PDE is calculated as:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PDEAddr = PageDirBase + (PDIndex * sizeof(PDE))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><img src=\"/img/image-20241129171923014.png\" alt=\"image-20241129171923014\"></p>\n<ul>\n<li><p>If the PDE is valid, the PFN points to the page table page. The remaining bits of the VPN are used to index within that page to get the PTE:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PTEAddr = (PDE.PFN &lt;&lt; SHIFT) + (PTIndex * sizeof(PTE))</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><strong>Page Tables</strong>:</p>\n<ul>\n<li>Each valid entry in the page directory points to a page of the page table. This page table contains entries that map specific VPNs to PFNs.</li>\n<li>In the example, there are two page tables:<ul>\n<li><strong>Page Table at PFN 100</strong>: Contains entries for VPNs 0 to 15. Entries for VPNs 0 and 1 are valid (code segment), and VPNs 4 and 5 are also valid (heap segment). The remaining entries are marked as invalid.</li>\n<li><strong>Page Table at PFN 101</strong>: Contains entries for the last 16 VPNs. Only VPNs 254 and 255 are valid (stack segment), while the other entries are invalid.</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/img/image-20241129172537835.png\" alt=\"image-20241129172537835\"></p>\n<p><strong>Example Address Translation</strong>:</p>\n<ul>\n<li>To translate a virtual address like 0x3F80 (binary 1111 1111 1000 0000), we:<ul>\n<li>Extract the PDIndex (<code>1111</code>) to access the PDE in the page directory.</li>\n<li>Use the PFN from the PDE to locate the page table page at <code>PFN 101</code>.</li>\n<li>Extract the PTIndex (<code>1110</code>) to index into the page table and find the correct PTE.</li>\n</ul>\n</li>\n</ul>\n<p><strong>The Need for More Than Two Levels</strong></p>\n<p>The goal of a multi-level page table is to keep each piece of the table within a single page, but as the address space grows, the page directory can become too large to fit into one page.</p>\n<p>Given:</p>\n<ul>\n<li>Page size: 512 bytes</li>\n<li>PTE size: 4 bytes</li>\n</ul>\n<p>You can fit 128 PTEs on one page (512 / 4 = 128). The VPN needs 7 bits (log₂128) to index these entries within one page.</p>\n<p>With a 30-bit address, the VPN’s higher-order bits point to the page directory index. In this case, there are 14 bits left to address the page directory. This results in a page directory with 2^14 entries, which is too large to fit in one page.</p>\n<p><img src=\"/img/image-20241129173126734.png\" alt=\"image-20241129173126734\"></p>\n<p>To handle this, the concept of a <em>multi-level page table</em> is introduced:</p>\n<ul>\n<li><p>The page directory itself is divided into multiple pages.</p>\n</li>\n<li><p>A higher-level page directory is added to point to these sub-pages of the main directory.</p>\n</li>\n</ul>\n<p><strong>Virtual Address Breakdown</strong></p>\n<p>The virtual address is split as follows:</p>\n<ul>\n<li><p><strong>PD Index 0</strong>: Top bits used to index into the top-level page directory.</p>\n</li>\n<li><p><strong>PD Index 1</strong>: Next set of bits used to index into the second-level page directory.</p>\n</li>\n<li><p><strong>Page Table Index</strong>: The remaining bits to index into the page table.</p>\n<p><img src=\"/img/image-20241129173137959.png\" alt=\"image-20241129173137959\"></p>\n</li>\n</ul>\n<p><strong>Step-by-Step Translation</strong></p>\n<p>When translating a virtual address:</p>\n<ol>\n<li><strong>Check the TLB</strong>: If there’s a TLB hit, the physical address can be formed directly without accessing the page table.</li>\n<li><strong>TLB Miss:</strong><ul>\n<li>Use the top bits (PD Index 0) to look up the first-level page directory and obtain the physical address (PDBR).</li>\n<li>Use the second set of bits (PD Index 1) to look up the second-level page directory.</li>\n<li>Combine the physical frame number from the first-level directory with the second-level index to access the page table.</li>\n<li>Finally, use the page table index to find the PTE and obtain the physical frame number.</li>\n<li>If any lookup fails or the PTE is invalid, a segmentation fault is raised.</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Chapter-21-Beyond-Physical-Memory-Mechanisms\"><a href=\"#Chapter-21-Beyond-Physical-Memory-Mechanisms\" class=\"headerlink\" title=\"Chapter 21: Beyond Physical Memory: Mechanisms\"></a>Chapter 21: Beyond Physical Memory: Mechanisms</h1><h2 id=\"21-1-Swap-Space\"><a href=\"#21-1-Swap-Space\" class=\"headerlink\" title=\"21.1 Swap Space\"></a>21.1 Swap Space</h2><p><strong>What is Swap Space?</strong></p>\n<p>Swap space is a reserved area on the disk used for temporarily holding pages of memory that are swapped out of RAM. It allows the system to maintain more memory pages in use than the size of the physical memory.</p>\n<ul>\n<li><strong>Purpose</strong>: To extend the apparent size of the system’s memory.</li>\n<li><strong>Mechanism</strong>: Pages from RAM are written to swap space (swapped out), freeing up physical memory for other uses. Pages can later be read back into memory (swapped in) when needed.</li>\n</ul>\n<p><strong>Characteristics of Swap Space</strong></p>\n<ol>\n<li>Disk Address Tracking:<ul>\n<li>The OS must keep track of the disk location for every swapped-out page.</li>\n</ul>\n</li>\n<li>Size:<ul>\n<li>Determines the maximum number of memory pages that can be swapped out.</li>\n<li>For simplicity, assume swap space is very large in examples.</li>\n</ul>\n</li>\n<li>Swapping Unit:<ul>\n<li>Pages are swapped in and out in fixed-size units (page-sized).</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example from Figure 21.1</strong></p>\n<ul>\n<li><p>Physical Memory:</p>\n<ul>\n<li>Four physical frames (PFN 0–3), each holding one page.</li>\n<li>Active processes share this memory, with some of their pages swapped out to disk.</li>\n</ul>\n</li>\n<li><p>Swap Space:</p>\n<ul>\n<li><p>Contains eight blocks, holding swapped-out pages for processes.</p>\n</li>\n<li><p>One block is free for new swap operations.</p>\n<p><img src=\"/img/image-20241129222549052.png\" alt=\"image-20241129222549052\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<p>Processes in this example:</p>\n<ul>\n<li><p>Proc 0:</p>\n<ul>\n<li>VPN 0 is in physical memory.</li>\n<li>VPN 1 and VPN 2 are in swap space (Block 0 and Block 1).</li>\n</ul>\n</li>\n<li><p>Proc 1:</p>\n<ul>\n<li>VPN 2 and VPN 3 are in physical memory.</li>\n<li>VPN 0 and VPN 1 are in swap space (Block 3 and Block 4).</li>\n</ul>\n</li>\n<li><p>Proc 2:</p>\n<ul>\n<li>VPN 0 is in physical memory.</li>\n<li>VPN 1 is in swap space (Block 6).</li>\n</ul>\n</li>\n<li><p>Proc 3:</p>\n<ul>\n<li>All pages are in swap space, meaning the process is not currently running.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Key Insights</strong></p>\n<ul>\n<li>Simulating Larger Memory:<ul>\n<li>Swap space enables the system to pretend it has more memory than available physical RAM, supporting more active processes or larger applications.</li>\n</ul>\n</li>\n<li>Freeing Memory:<ul>\n<li>By swapping out infrequently used pages, the system can prioritize memory for active processes.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Code Pages and Swap Space</strong></p>\n<ul>\n<li><p>Code Pages:</p>\n<ul>\n<li>Pages containing program instructions (like binaries) are initially stored on disk.</li>\n<li>These are loaded into memory when the program executes.</li>\n</ul>\n</li>\n<li><p>Reclaiming Memory:</p>\n<ul>\n<li>Code pages can be safely evicted from RAM to make space for other needs, as they can always be reloaded from the binary on disk. These pages typically don’t occupy swap space.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"21-2-The-Present-Bit\"><a href=\"#21-2-The-Present-Bit\" class=\"headerlink\" title=\"21.2 The Present Bit\"></a>21.2 The Present Bit</h2><p><strong>Present Bit: Adding Disk Swapping Support</strong></p>\n<p>The <strong>present bit</strong> is a field in each <strong>Page Table Entry (PTE)</strong> used to indicate whether a page is currently in physical memory or stored on disk.</p>\n<ul>\n<li>Present Bit Values:<ul>\n<li><strong>1</strong>: The page is in physical memory.</li>\n<li><strong>0</strong>: The page is not in memory but resides on disk.</li>\n</ul>\n</li>\n</ul>\n<p>This mechanism allows the system to support <strong>swapping</strong>, where pages not in use are moved to disk to free up physical memory.</p>\n<p><strong>Memory Reference Process Recap</strong></p>\n<ol>\n<li><strong>Virtual Memory Reference</strong>:<ul>\n<li>A process generates a <strong>virtual address</strong> for an instruction or data access.</li>\n<li>The system must translate this virtual address into a <strong>physical address</strong> to access the data.</li>\n</ul>\n</li>\n<li><strong>Translation Lookaside Buffer (TLB)</strong>:<ul>\n<li>The TLB caches mappings from Virtual Page Numbers (VPNs) to Physical Frame Numbers (PFNs) for faster translation.</li>\n<li>A <strong>TLB hit</strong> avoids accessing the page table in memory, speeding up the process.</li>\n</ul>\n</li>\n<li><strong>Page Table Lookup</strong> (if TLB miss):<ul>\n<li>The hardware locates the <strong>page table</strong> using the page table base register.</li>\n<li>It uses the VPN to index into the table and retrieve the corresponding PTE.</li>\n</ul>\n</li>\n<li><strong>Valid and Present Pages</strong>:<ul>\n<li>If the PTE is valid and the present bit is <strong>1</strong>, the page is in memory.</li>\n<li>The PFN is extracted, the TLB is updated, and the memory access is retried, resulting in a TLB hit.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"21-3-The-Page-Fault\"><a href=\"#21-3-The-Page-Fault\" class=\"headerlink\" title=\"21.3 The Page Fault\"></a><strong>21.3 The Page Fault</strong></h2><p><strong>Page Fault Handling</strong></p>\n<p>A <strong>page fault</strong> occurs when a process accesses a page that is not present in physical memory. The OS handles page faults in the following steps:</p>\n<ol>\n<li><strong>Trigger the Page-Fault Handler</strong>:<ul>\n<li>When a page fault occurs, the <strong>page-fault handler</strong> (a part of the OS) is invoked to determine the next steps.</li>\n</ul>\n</li>\n<li><strong>Locate the Missing Page</strong>:<ul>\n<li>The page that caused the fault may have been swapped out to disk.</li>\n<li>The OS checks the <strong>page table entry (PTE)</strong> for information about the page’s location on disk. This disk address can be stored in the bits of the PTE that would typically store the page frame number (PFN).</li>\n</ul>\n</li>\n<li><strong>Fetch the Page from Disk</strong>:<ul>\n<li>The OS issues a request to the disk to fetch the page back into physical memory.</li>\n</ul>\n</li>\n<li><strong>Update the Page Table</strong>:<ul>\n<li>After the disk I/O completes, the OS updates the page table:<ul>\n<li>Marks the page as <strong>present</strong>.</li>\n<li>Updates the <strong>PFN</strong> field with the page’s new physical memory location.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Retry the Instruction</strong>:<ul>\n<li>The OS retries the instruction that caused the page fault:<ul>\n<li>The retry may generate a <strong>TLB miss</strong>, which will be serviced by updating the TLB with the new translation.</li>\n<li>The final retry will succeed, fetching the desired data or instruction from memory.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Why Does the OS Handle Page Faults?</strong></p>\n<p>Although hardware often takes the lead in managing the TLB, it defers page fault handling to the OS for two key reasons:</p>\n<ol>\n<li><strong>Disk I/O Latency</strong>:<ul>\n<li>Disk operations are <strong>slow</strong>. Even if the OS executes many instructions to service the page fault, the overhead is negligible compared to the time it takes to complete a disk I/O.</li>\n</ul>\n</li>\n<li><strong>Hardware Simplicity</strong>:<ul>\n<li>Handling page faults requires knowledge of the <strong>swap space</strong>, issuing I/O operations, and managing disk access. Offloading this complexity to the OS simplifies hardware design.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Process State During a Page Fault</strong></p>\n<ul>\n<li>While the OS services the page fault, the process that caused the fault enters the <strong>blocked state</strong>.</li>\n<li>The OS schedules other <strong>ready processes</strong> to run during this time, maximizing the CPU’s utilization. This overlapping of I/O and computation is a key advantage of multiprogrammed systems.</li>\n</ul>\n<p><strong>Terminology Clarification</strong></p>\n<ul>\n<li><strong>Page Faults vs. Illegal Memory Access</strong>:<ul>\n<li>A <strong>page fault</strong> occurs during a valid memory access when the requested page is not in physical memory.</li>\n<li>Illegal memory access refers to attempts to access memory outside the allocated virtual address space, leading to a segmentation fault or similar error.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"21-4-What-Happens-If-Memory-Is-Full\"><a href=\"#21-4-What-Happens-If-Memory-Is-Full\" class=\"headerlink\" title=\"21.4 What Happens If Memory Is Full?\"></a><strong>21.4 What Happens If Memory Is Full?</strong></h2><ul>\n<li>Key Problem:<ul>\n<li>When all physical memory is in use, there is no free space to load a page from swap space during a page fault.</li>\n</ul>\n</li>\n<li>Solution:<ul>\n<li>The operating system uses a <strong>page-replacement policy</strong> to decide which page(s) to evict from memory to make room for the new page.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Page-Replacement Policy</strong></p>\n<ul>\n<li>Definition:<ul>\n<li>A strategy used by the OS to select a page to remove from physical memory when space is needed.</li>\n</ul>\n</li>\n<li>Significance:<ul>\n<li>Choosing the wrong page can severely degrade performance, as frequent disk accesses (caused by poor decisions) are orders of magnitude slower than memory accesses.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Performance Implications</strong></p>\n<ul>\n<li>Good Policy:<ul>\n<li>Reduces the number of page faults and improves overall program performance.</li>\n</ul>\n</li>\n<li>Bad Policy:<ul>\n<li>Causes thrashing, where a program spends more time swapping pages in and out than executing, leading to disk-like speeds (10,000–100,000 times slower than memory).</li>\n</ul>\n</li>\n</ul>\n<p><strong>Page-Fault Control Flow Algorithm</strong></p>\n<p>The control flow when a process accesses memory involves handling <strong>TLB hits, TLB misses</strong>, and <strong>page faults</strong>. Below is a simplified explanation of <strong>Figure 21.2</strong>:</p>\n<ol>\n<li><p><strong>TLB Lookup</strong>:</p>\n<ul>\n<li><p>The system checks the Translation Lookaside Buffer (TLB) for a match.</p>\n</li>\n<li><p>If the Virtual Page Number (VPN) is found (</p>\n<p>TLB hit ):</p>\n<ul>\n<li>Validate the access permissions. If valid, compute the physical address and access memory.</li>\n<li>If permissions are invalid, raise a <strong>protection fault</strong>.</li>\n</ul>\n</li>\n<li><p>If the VPN is not found (<strong>TLB miss</strong>), proceed to the page table.</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Page Table Lookup</strong>:</p>\n<ul>\n<li>Compute the <strong>Page Table Entry (PTE)</strong> address.</li>\n<li>Check the PTE validity:<ul>\n<li><strong>Invalid PTE</strong>: Raise a <strong>segmentation fault</strong> (invalid memory access).</li>\n<li>Valid PTE:<ul>\n<li>Check access permissions; if invalid, raise a <strong>protection fault</strong>.</li>\n<li>If the page is present in memory, update the TLB and retry the instruction.</li>\n<li>If the page is not present, raise a <strong>page fault</strong>.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Page Fault Handling</strong>:</p>\n<ul>\n<li>The OS takes over to handle the page fault:<ol>\n<li>Use the <strong>page-replacement policy</strong> if memory is full.</li>\n<li>Load the required page from swap space to physical memory.</li>\n<li>Update the PTE and TLB to reflect the new page’s presence.</li>\n</ol>\n</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/img/image-20241129224926678.png\" alt=\"image-20241129224926678\"></p>\n<h2 id=\"21-5-Page-Fault-Control-Flow\"><a href=\"#21-5-Page-Fault-Control-Flow\" class=\"headerlink\" title=\"21.5 Page Fault Control Flow\"></a><strong>21.5 Page Fault Control Flow</strong></h2><p><strong>Hardware Control Flow</strong></p>\n<p>The hardware handles memory access up to the point of detecting a <strong>TLB miss</strong>. The control flow splits into three possible cases:</p>\n<p><img src=\"/img/image-20241129224926678.png\" alt=\"image-20241129224926678\"></p>\n<ol>\n<li><strong>Valid and Present Page</strong>:<ul>\n<li>The TLB miss handler checks the page table.</li>\n<li>If the page is valid and present (lines 18–21):<ul>\n<li>The <strong>PFN</strong> is retrieved from the <strong>PTE</strong>.</li>\n<li>The TLB is updated, and the instruction is retried.</li>\n<li>This retry results in a TLB hit, and memory access succeeds.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Valid but Not Present Page</strong>:<ul>\n<li>If the page is valid but not present (lines 22–23):<ul>\n<li>A <strong>page fault</strong> is triggered, and control is handed to the OS page-fault handler.</li>\n<li>The OS resolves the issue by loading the page from disk into memory.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Invalid Page</strong>:<ul>\n<li>If the page is invalid (lines 13–14):<ul>\n<li>The hardware traps this access as an error.</li>\n<li>The OS trap handler likely terminates the process, as this typically indicates a bug.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Software Control Flow</strong></p>\n<p>When a <strong>page fault</strong> occurs, the OS handles it as follows (Figure 21.3):</p>\n<p><img src=\"/img/image-20241129225408834.png\" alt=\"image-20241129225408834\"></p>\n<ol>\n<li><strong>Find a Free Physical Page</strong>:<ul>\n<li>The OS searches for an available physical frame for the faulted page.</li>\n<li>If no free frame exists, the <strong>page replacement algorithm</strong> runs, evicting a page from memory to free up space.</li>\n</ul>\n</li>\n<li><strong>Fetch Page from Disk</strong>:<ul>\n<li>The OS issues a disk I/O request to load the page from the <strong>swap space</strong> into memory.</li>\n<li>The process is blocked during this operation.</li>\n</ul>\n</li>\n<li><strong>Update the Page Table</strong>:<ul>\n<li>After the I/O completes, the OS updates the page table:<ul>\n<li>Marks the page as <strong>present</strong>.</li>\n<li>Sets the <strong>PFN</strong> field to indicate the page’s new location in memory.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Retry the Instruction</strong>:<ul>\n<li>The OS retries the instruction that caused the fault:<ul>\n<li>The retry will result in a TLB miss, as the translation is not yet cached in the TLB.</li>\n<li>On another retry, the TLB hit occurs, and the hardware completes the memory access.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Chapter-22-Beyond-Physical-Memory-Policies\"><a href=\"#Chapter-22-Beyond-Physical-Memory-Policies\" class=\"headerlink\" title=\"Chapter 22:  Beyond Physical Memory: Policies\"></a>Chapter 22:  Beyond Physical Memory: Policies</h1><h2 id=\"22-1-Cache-Management\"><a href=\"#22-1-Cache-Management\" class=\"headerlink\" title=\"22.1 Cache Management\"></a><strong>22.1 Cache Management</strong></h2><p><strong>Key Concepts</strong></p>\n<ol>\n<li><p><strong>Main Memory as a Cache</strong>:</p>\n<ul>\n<li>Main memory holds only a subset of all virtual memory pages.</li>\n<li>Disk acts as the backing store for pages not in memory.</li>\n<li>Minimizing disk accesses (cache misses) is critical because disk access is orders of magnitude slower than memory access.</li>\n</ul>\n</li>\n<li><p><strong>AMAT(Average Memory Access Time ) Calculation</strong>:</p>\n<ul>\n<li><p><strong>AMAT Formula</strong>:</p>\n<script type=\"math/tex; mode=display\">\nAMAT = T_M + (P_{Miss} \\cdot T_D)</script><p>where:</p>\n<ul>\n<li>TM: Memory access time (e.g., 100 nanoseconds).</li>\n<li>PMiss: Probability of a cache miss.</li>\n<li>TD: Disk access time (e.g., 10 milliseconds).</li>\n</ul>\n</li>\n<li><p>A high hit rate significantly reduces AMAT, as misses incur the costly overhead of disk I/O.</p>\n</li>\n</ul>\n</li>\n<li><p><strong>Hit and Miss Rates</strong>:</p>\n<ul>\n<li><strong>Hit Rate</strong> (PHit): Fraction of memory accesses found in main memory.</li>\n<li><strong>Miss Rate</strong> (PMiss): Fraction of accesses requiring a disk fetch.</li>\n<li>PHit+PMiss= 1.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example Scenario</strong></p>\n<ul>\n<li><p><strong>System Configuration</strong>:</p>\n<ul>\n<li>Address space: <strong>4 KB</strong>.</li>\n<li>Page size: <strong>256 bytes</strong>.</li>\n<li>VPN: <strong>4 bits</strong> (16 virtual pages).</li>\n<li>Offset: <strong>8 bits</strong>.</li>\n</ul>\n</li>\n<li><p><strong>Memory References</strong>:</p>\n<ul>\n<li>Sequence: 0x000,0x100,0x200,0x300,0x400,0x500,0x600,0x700,0x800,0x900</li>\n<li>Pages in memory: All except <strong>virtual page 3</strong>.</li>\n<li>Behavior:<ul>\n<li>Hits: 9 times (pages in memory).</li>\n<li>Misses: 1 time (page 3).</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p><strong>Hit and Miss Rates</strong>:</p>\n<ul>\n<li>Hit rate: 90%</li>\n<li>Miss rate: 10%</li>\n</ul>\n</li>\n<li><p><strong>AMAT Calculation</strong>:</p>\n<ul>\n<li><p>Given:</p>\n<ul>\n<li>TM=100 ns</li>\n<li>TD=10 ms</li>\n<li>PMiss=0.1</li>\n</ul>\n</li>\n<li><script type=\"math/tex; mode=display\">\nAMAT = 100 \\, \\text{ns} + (0.1 \\cdot 10 \\, \\text{ms}) = 1.0001</script></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>If PMiss=0.00(99.9% hit rate):</p>\n<ul>\n<li><script type=\"math/tex; mode=display\">\nAMAT = 100 \\, \\text{ns} + (0.001 \\cdot 10 \\, \\text{ms}) = 10.1 \\, \\mu \\text{s}.</script></li>\n</ul>\n</li>\n</ul>\n<p><strong>Performance Implications</strong></p>\n<ol>\n<li><strong>Disk Cost Dominates AMAT</strong>:<ul>\n<li>The high cost of disk access makes even small miss rates dramatically increase AMAT.</li>\n<li>For example, a 0.1%miss rate results in AMAT ∼100×lower than a 10% miss rate.</li>\n</ul>\n</li>\n<li><strong>Importance of Replacement Policies</strong>:<ul>\n<li>Optimizing hit rates with intelligent policies is essential for performance.</li>\n<li>Poor replacement policies can result in frequent misses, throttling system performance to disk speeds.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"22-2-The-Optimal-Replacement-Policy\"><a href=\"#22-2-The-Optimal-Replacement-Policy\" class=\"headerlink\" title=\"22.2 The Optimal Replacement Policy\"></a><strong>22.2 The Optimal Replacement Policy</strong></h2><p><strong>Key Concepts</strong></p>\n<ol>\n<li><strong>Optimal Replacement Policy</strong>:<ul>\n<li>Replace the page that will <strong>not be accessed for the longest time</strong> in the future.</li>\n<li>Minimizes cache misses, making it the best possible policy in terms of performance.</li>\n</ul>\n</li>\n<li><strong>Usefulness of Optimal as a Benchmark</strong>:<ul>\n<li>Helps in evaluating new policies by providing a reference point.</li>\n<li>Example: If a policy achieves an 80% hit rate and the optimal policy achieves 82%, the new policy is close to the ideal.</li>\n</ul>\n</li>\n<li><strong>Intuition</strong>:<ul>\n<li>Pages accessed sooner are more important to keep.</li>\n<li>Evicting the page needed furthest in the future minimizes future misses.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example Walkthrough</strong></p>\n<ul>\n<li><strong>Access Sequence</strong>: 0,1,2,0,1,3,0,3,1,2,1</li>\n<li><strong>Cache Size</strong>: 3 pages.</li>\n</ul>\n<p><strong>Trace Analysis</strong>:</p>\n<p><img src=\"/img/image-20241129231048020.png\" alt=\"image-20241129231048020\"></p>\n<p><strong>Performance Metrics</strong>:</p>\n<ul>\n<li><strong>Hit Rate</strong>: <script type=\"math/tex; mode=display\">\n{Hit Rate} = \\frac{\\text{Hits}}{\\text{Hits} + \\text{Misses}} = \\frac{6}{6 + 5} = 54.5\\%</script></li>\n</ul>\n<ul>\n<li><p>Hit Rate Excluding Compulsory Misses:</p>\n<ul>\n<li><p>Compulsory Misses: First access to pages 0,1,2,3</p>\n</li>\n<li><p>Effective Hit Rate: </p>\n<script type=\"math/tex; mode=display\">\n\\frac{6}{6 + 1} = 85.7\\%</script><p>.</p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Types of Cache Misses</strong></p>\n<ol>\n<li><strong>Compulsory Misses</strong>:<ul>\n<li>Occur on the <strong>first reference</strong> to a page.</li>\n<li>Example: Misses for pages 0,1,20, 1, 2 in the empty cache.</li>\n</ul>\n</li>\n<li><strong>Capacity Misses</strong>:<ul>\n<li>Occur when the cache is <strong>full</strong>, and a page must be evicted.</li>\n<li>Example: Miss for page 33 when replacing page 22.</li>\n</ul>\n</li>\n<li><strong>Conflict Misses</strong>:<ul>\n<li>Relevant only in hardware caches with restrictions on placement.</li>\n<li>Not applicable to fully-associative OS page caches.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"22-3-FIFO-First-In-First-Out\"><a href=\"#22-3-FIFO-First-In-First-Out\" class=\"headerlink\" title=\"22.3: FIFO (First-In, First-Out)\"></a>22.3: FIFO (First-In, First-Out)</h2><p><strong>Example Trace</strong><br> Consider the following sequence of virtual page references:<br> <code>0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1</code><br> Assuming a cache size of 3, FIFO behaves as follows:</p>\n<p><img src=\"/img/image-20241129232128717.png\" alt=\"image-20241129232128717\"></p>\n<p><strong>Hit Rate</strong></p>\n<ul>\n<li><p>Total Hits: 4</p>\n</li>\n<li><p>Total Misses: 7</p>\n</li>\n<li><p><strong>Hit Rate</strong>: </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\text{Hits}}{\\text{Hits + Misses}} = \\frac{4}{11} \\approx 36.4\\%</script></li>\n</ul>\n<ul>\n<li><strong>Hit Rate (Excluding Compulsory Misses)</strong>: <script type=\"math/tex; mode=display\">\n\\frac{\\text{Hits}}{\\text{Hits + Non-Compulsory Misses}} = \\frac{4}{7} \\approx 57.1\\%</script></li>\n</ul>\n<h2 id=\"22-4-Random-Replacement\"><a href=\"#22-4-Random-Replacement\" class=\"headerlink\" title=\"22.4: Random Replacement\"></a>22.4: Random Replacement</h2><p><strong>Example Trace</strong><br> Using the same reference sequence as before:<br> <code>0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1</code><br> And assuming a cache size of 3, the behavior of Random Replacement might look like this (depending on the random eviction decisions):</p>\n<p><img src=\"/img/image-20241129232728241.png\" alt=\"image-20241129232728241\"></p>\n<p><strong>Performance</strong></p>\n<ul>\n<li><strong>Total Hits</strong>: 4</li>\n<li><strong>Total Misses</strong>: 7</li>\n<li><strong>Hit Rate</strong>: 411≈36.4%\\frac{4}{11} \\approx 36.4\\%</li>\n</ul>\n<p><strong>Observation</strong>: Random achieves similar results to FIFO in this specific trial but could perform better or worse depending on the random eviction choices.</p>\n<p><strong>Statistical Analysis</strong><br> In a study of Random’s behavior over 10,000 trials (using the same reference sequence), the following outcomes were observed:</p>\n<ul>\n<li><strong>Number of Hits</strong>: Ranges from 2 to 6 hits per trial.</li>\n<li><strong>Best Case</strong>: 6 hits (equivalent to the optimal policy), occurring in about <strong>40% of trials</strong>.</li>\n<li><strong>Worst Case</strong>: 2 hits or fewer, occurring in the <strong>lower tail of trials</strong>.</li>\n<li><strong>Distribution</strong>: Random’s performance is highly variable, with a mean hit rate slightly better than FIFO but significantly worse than optimal or LRU.</li>\n</ul>\n<p><img src=\"/img/image-20241129232759705.png\" alt=\"image-20241129232759705\"></p>\n<h2 id=\"22-5-The-LRU-Policy\"><a href=\"#22-5-The-LRU-Policy\" class=\"headerlink\" title=\"22.5: The LRU Policy\"></a>22.5: The LRU Policy</h2><p><strong>Historical Basis</strong></p>\n<ul>\n<li><strong>Temporal Locality</strong>: LRU relies on the observation that programs often revisit recently accessed pages, demonstrating <strong>temporal locality</strong>.</li>\n<li>Spatial vs. Temporal Locality:<ul>\n<li><strong>Spatial locality</strong> refers to accessing nearby pages (e.g., an array).</li>\n<li><strong>Temporal locality</strong> focuses on the recurrence of accessing the same page.</li>\n</ul>\n</li>\n</ul>\n<p><strong>How LRU Works</strong><br> LRU maintains an ordered list or queue of pages in memory, where the most recently accessed page is at the front and the least recently accessed one is at the back. When a new page needs to be loaded and the cache is full, the page at the back of the list (the least recently used) is evicted.</p>\n<p><strong>Example of LRU Trace</strong><br> Given the reference stream <code>0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1</code> and a cache size of 3, LRU’s decision-making can be shown as:</p>\n<p><img src=\"/img/image-20241129233010274.png\" alt=\"image-20241129233010274\"></p>\n<p><strong>Performance</strong></p>\n<ul>\n<li><strong>Results</strong>: LRU can perform similarly to the <strong>optimal</strong> policy, achieving high hit rates.</li>\n<li><strong>Correct Decisions</strong>: LRU correctly evicts pages that will not be accessed soon, which increases the likelihood of hits in subsequent accesses.</li>\n</ul>\n<p><strong>Why LRU Outperforms FIFO and Random</strong></p>\n<ul>\n<li><strong>FIFO and Random</strong>: These policies can evict pages that are still useful (e.g., FIFO evicting the first-in page regardless of usage).</li>\n<li><strong>LRU</strong>: By keeping track of recency, it avoids such mistakes and maintains a better hit rate. In the example above, LRU’s decisions are informed by the history of accesses, leading to fewer misses.</li>\n</ul>\n<h2 id=\"22-6-Workload-Examples-and-Policy-Performance\"><a href=\"#22-6-Workload-Examples-and-Policy-Performance\" class=\"headerlink\" title=\"22.6: Workload Examples and Policy Performance\"></a>22.6: Workload Examples and Policy Performance</h2><ol>\n<li><strong>No-Locality Workload</strong></li>\n</ol>\n<ul>\n<li><p><strong>Description</strong>: In this workload, 100 unique pages are accessed in a random manner out of 10,000 total accesses. This kind of workload has no specific pattern, so each access is independent.</p>\n<p><img src=\"/img/image-20241129233538845.png\" alt=\"image-20241129233538845\"></p>\n</li>\n</ul>\n<ol>\n<li><strong>80-20 Workload</strong></li>\n</ol>\n<ul>\n<li><p><strong>Description</strong>: This workload exhibits <strong>locality</strong> where 80% of the accesses are made to 20% of the pages (the “hot” pages), while 20% of the accesses are made to the remaining 80% of pages (the “cold” pages).</p>\n<p><img src=\"/img/image-20241129233630337.png\" alt=\"image-20241129233630337\"></p>\n</li>\n</ul>\n<ol>\n<li><strong>Looping-Sequential Workload</strong></li>\n</ol>\n<ul>\n<li><p><strong>Description</strong>: This workload involves 50 pages accessed sequentially from 0 to 49 and then loops back to 0, continuing for 10,000 total accesses.</p>\n<p><img src=\"/img/image-20241129233651978.png\" alt=\"image-20241129233651978\"></p>\n</li>\n</ul>\n<h2 id=\"22-7-Implementing-Historical-Algorithms\"><a href=\"#22-7-Implementing-Historical-Algorithms\" class=\"headerlink\" title=\"22.7: Implementing Historical Algorithms\"></a>22.7: Implementing Historical Algorithms</h2><p><strong>Challenges of Implementing LRU</strong></p>\n<ul>\n<li><strong>Data Structure Management</strong>: To implement LRU perfectly, each time a page is accessed, the system must update a data structure to move that page to the front, representing the <strong>most recently used (MRU)</strong> side. This requires frequent updates on each memory access, which can be computationally expensive.</li>\n<li><strong>Comparison with FIFO</strong>: Unlike LRU, <strong>FIFO (First-In, First-Out)</strong> only requires updates when a new page is added to the end of the list or when an existing page is evicted from the beginning. This is simpler and less costly to manage than LRU’s continuous repositioning of pages.</li>\n<li><strong>Accounting Costs</strong>: The constant accounting for LRU can significantly impact system performance, especially if the method for tracking access history is not optimized.</li>\n</ul>\n<p><strong>Hardware Support for LRU</strong></p>\n<p>One approach to optimize the implementation of LRU is to leverage <strong>hardware support</strong>:</p>\n<ul>\n<li><strong>Time Stamps</strong>: Hardware could update a time field in memory whenever a page is accessed. These time fields could be part of a per-process page table or stored in a separate array, with one entry per physical page.</li>\n<li><strong>Page Replacement</strong>: When the OS needs to evict a page, it would scan the time fields to identify the page that has not been accessed for the longest time.</li>\n</ul>\n<p><strong>Issue with Large-Scale Systems</strong>:</p>\n<ul>\n<li><strong>Scalability Problems</strong>: The solution of scanning an array of time fields to find the least-recently-used page becomes impractical as the number of pages increases. For instance, on a system with <strong>4GB of memory</strong> divided into <strong>4KB pages</strong>, there would be <strong>1 million pages</strong>. Scanning such a large array to find the LRU page would be prohibitively slow, even on modern CPUs.</li>\n</ul>\n<h2 id=\"22-8-Approximating-LRU\"><a href=\"#22-8-Approximating-LRU\" class=\"headerlink\" title=\"22.8: Approximating LRU\"></a>22.8: Approximating LRU</h2><p><strong>The Use Bit Concept</strong></p>\n<ul>\n<li><p><strong>Definition</strong>: The use bit is a simple binary flag associated with each page. When a page is accessed (read or written), the hardware sets the use bit to <code>1</code>. The hardware does not reset this bit to <code>0</code>; instead, this is managed by the <strong>operating system (OS)</strong>.</p>\n</li>\n<li><p><strong>Purpose</strong>: The use bit is used by the OS to approximate LRU behavior, helping the system identify pages that have not been accessed recently.</p>\n</li>\n</ul>\n<p><strong>The Clock Algorithm</strong></p>\n<p>A common approach to approximate LRU is the <strong>Clock Algorithm</strong>, which is inspired by the idea of a clock with a hand that points to a specific page. The algorithm works as follows:</p>\n<ol>\n<li><p><strong>Circular Arrangement</strong>: All pages in memory are arranged in a circular list, and the “clock hand” points to a page.</p>\n</li>\n<li><p><strong>Replacement Decision:</strong></p>\n<ul>\n<li>The OS checks the use bit of the page pointed to by the clock hand.</li>\n<li>If the use bit is <code>1</code>, the page has been accessed recently. The OS resets the use bit to <code>0</code> and moves the clock hand to the next page.</li>\n<li>If the use bit is <code>0</code>, the page has not been accessed recently and is chosen for replacement.</li>\n</ul>\n</li>\n<li><p><strong>Cycle</strong>: If all pages have their use bit set to <code>1</code> during the scan, the OS clears all use bits and starts the scan again, ensuring no page is unfairly excluded from the search.</p>\n</li>\n</ol>\n<p><strong>Advantages of the Clock Algorithm</strong></p>\n<ul>\n<li><strong>Efficient Scanning</strong>: The clock algorithm avoids repeatedly scanning the entire memory for the least-recently-used page, making it more efficient than a full scan.</li>\n<li><strong>Simplicity</strong>: The algorithm is straightforward to implement and operates with minimal overhead.</li>\n</ul>\n<h2 id=\"22-9-Considering-Dirty-Pages\"><a href=\"#22-9-Considering-Dirty-Pages\" class=\"headerlink\" title=\"22.9: Considering Dirty Pages\"></a>22.9: Considering Dirty Pages</h2><p><strong>Why Consider Dirty Pages?</strong></p>\n<ul>\n<li><p><strong>Performance Impact</strong>: Evicting a <strong>dirty page</strong> requires writing its contents back to disk, which incurs significant I/O overhead and impacts system performance.</p>\n</li>\n<li><p><strong>Clean Pages</strong>: In contrast, evicting a <strong>clean page</strong> is more efficient because no data needs to be written to disk; the physical frame can simply be reused.</p>\n</li>\n</ul>\n<p><strong>The Modified (Dirty) Bit</strong></p>\n<ul>\n<li><p><strong>Definition</strong>: A <strong>modified bit</strong> (or <strong>dirty bit</strong>) is a hardware-supported flag that indicates whether a page has been modified since it was brought into memory.</p>\n</li>\n<li><p><strong>Setting the Bit</strong>: This bit is set whenever a page is written to. When the page is read-only or has not been modified, the bit remains clear (indicating that the page is clean).</p>\n</li>\n</ul>\n<p><strong>Incorporating Dirty Page Considerations in the Clock Algorithm</strong></p>\n<ul>\n<li>The clock algorithm can be enhanced to consider the status of the dirty bit:<ol>\n<li><strong>Initial Check</strong>: When the clock hand points to a page for potential eviction, the algorithm first checks if the page is both <strong>unused</strong> (use bit = 0) and <strong>clean</strong> (dirty bit = 0). If such a page is found, it is evicted, as it requires no I/O overhead.</li>\n<li><strong>Fallback</strong>: If no clean, unused pages are found, the algorithm then looks for pages that are <strong>unused but dirty</strong> (dirty bit = 1). While this page must be written back to disk, it is still a candidate for eviction if no clean pages are available.</li>\n<li><strong>Repeat</strong>: If the algorithm cannot find a suitable page to evict, it may scan through the entire memory set (clearing use bits as needed) until an appropriate page is found.</li>\n</ol>\n</li>\n</ul>\n<h1 id=\"Chapter-26-Introduction-of-Concurrency\"><a href=\"#Chapter-26-Introduction-of-Concurrency\" class=\"headerlink\" title=\"Chapter 26: Introduction of Concurrency\"></a>Chapter 26: Introduction of Concurrency</h1><h2 id=\"26-1-Why-Use-Threads\"><a href=\"#26-1-Why-Use-Threads\" class=\"headerlink\" title=\"26.1: Why Use Threads?\"></a>26.1: Why Use Threads?</h2><p><strong>1. Parallelism</strong></p>\n<ul>\n<li><strong>Definition</strong>: Parallelism leverages multiple processors (or cores) to perform different parts of a task simultaneously.</li>\n<li><strong>Example</strong>: Consider adding two large arrays or incrementing each element of an array. A single-threaded program will process each element sequentially, one by one.</li>\n<li><strong>With Threads</strong>: On a multi-processor system, the workload can be divided among threads, each handling a portion of the data. For instance, four threads could each process 25% of the array, utilizing all available CPUs and speeding up execution.</li>\n<li><strong>Parallelization</strong>: Transforming a single-threaded program into one that utilizes multiple CPUs through threads is a common technique to improve performance on modern hardware.</li>\n</ul>\n<p><strong>2. Responsiveness and Avoiding I/O Blocking</strong></p>\n<ul>\n<li>Challenge with I/O: Programs often need to wait for slow I/O operations</li>\n<li>Solution with Threads:<ul>\n<li>Instead of making the entire program wait, threads allow the program to continue performing other tasks. For example:<ul>\n<li>One thread can wait for I/O completion.</li>\n<li>Another thread can utilize the CPU to perform computations or issue additional I/O requests.</li>\n</ul>\n</li>\n<li>This overlap of I/O and computation enhances the program’s overall responsiveness and efficiency.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Threads vs. Processes</strong></p>\n<ul>\n<li>Threads:<ul>\n<li>Share the same address space, making it easy to share data.</li>\n<li>Suitable for tasks that require shared memory or frequent data exchange between threads.</li>\n<li>Lightweight compared to processes, with less overhead for creation and context switching.</li>\n</ul>\n</li>\n<li>Processes:<ul>\n<li>Operate in separate address spaces, offering stronger isolation.</li>\n<li>Ideal for logically separate tasks where minimal data sharing is needed.</li>\n<li>Heavier than threads due to separate memory and context.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"26-2-An-Example-Thread-Creation\"><a href=\"#26-2-An-Example-Thread-Creation\" class=\"headerlink\" title=\"26.2: An Example: Thread Creation\"></a>26.2: An Example: Thread Creation</h2><p><strong>Code Overview</strong></p>\n<p>The program demonstrates creating two threads, each running a function <code>mythread()</code> that prints a character string passed as an argument.</p>\n<p><strong>Key Code Snippets</strong></p>\n<ol>\n<li><p><strong>Thread Function (<code>mythread</code>)</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> *<span class=\"title function_\">mythread</span><span class=\"params\">(<span class=\"type\">void</span> *arg)</span> &#123;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%s\\n&quot;</span>, (<span class=\"type\">char</span> *) arg);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>The function takes a generic pointer <code>arg</code> and prints the string it points to.</li>\n<li>It returns <code>NULL</code> after completing.</li>\n</ul>\n</li>\n<li><p><strong>Main Function</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> *argv[])</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">pthread_t</span> p1, p2;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;main: begin\\n&quot;</span>);</span><br><span class=\"line\">    Pthread_create(&amp;p1, <span class=\"literal\">NULL</span>, mythread, <span class=\"string\">&quot;A&quot;</span>);</span><br><span class=\"line\">    Pthread_create(&amp;p2, <span class=\"literal\">NULL</span>, mythread, <span class=\"string\">&quot;B&quot;</span>);</span><br><span class=\"line\">    Pthread_join(p1, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    Pthread_join(p2, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;main: end\\n&quot;</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p><strong><code>pthread_t</code></strong>: Represents a thread identifier.</p>\n</li>\n<li><p><code>Pthread_create</code>: Creates a new thread. The arguments are:</p>\n<ul>\n<li>Pointer to the thread identifier (<code>p1</code> or <code>p2</code>).</li>\n<li>Attributes (set to <code>NULL</code> for default attributes).</li>\n<li>Function pointer (<code>mythread</code>) to run in the thread.</li>\n<li>Argument passed to the thread function (<code>&quot;A&quot;</code> or <code>&quot;B&quot;</code>).</li>\n</ul>\n</li>\n<li><p><strong><code>Pthread_join</code></strong>: Waits for the thread to finish. The main thread waits for <code>p1</code> and <code>p2</code> to complete before continuing.</p>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Execution Ordering</strong></p>\n<ul>\n<li><p>The thread scheduler determines the execution order, which can vary across runs. Possible orderings:</p>\n<ol>\n<li><p>Sequential (Figure 26.3):</p>\n<ul>\n<li>Main thread runs first, creating Thread 1 and then Thread 2.</li>\n<li>Thread 1 runs and prints “A”.</li>\n<li>Thread 2 runs and prints “B”.</li>\n<li>The main thread resumes and prints “main: end”.</li>\n</ul>\n</li>\n<li><p>Thread 1 First (Figure 26.4):</p>\n<ul>\n<li><p>Main thread creates Thread 1, which starts running immediately and prints “A”.</p>\n</li>\n<li><p>The main thread then creates Thread 2, which runs and prints “B”.</p>\n</li>\n<li><p>Main thread resumes, printing “main: end”.</p>\n<p><img src=\"/img/image-20241201105509018.png\" alt=\"image-20241201105509018\"></p>\n</li>\n</ul>\n</li>\n<li><p>Thread 2 First (Figure 26.5):</p>\n<ul>\n<li><p>Main thread creates both threads.</p>\n</li>\n<li><p>Thread 2 runs first and prints “B”.</p>\n</li>\n<li><p>Thread 1 runs later and prints “A”.</p>\n</li>\n<li><p>Main thread resumes and prints “main: end”.</p>\n<p><img src=\"/img/image-20241201105421001.png\" alt=\"image-20241201105421001\"></p>\n</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"26-3-Why-It-Gets-Worse-Shared-Data\"><a href=\"#26-3-Why-It-Gets-Worse-Shared-Data\" class=\"headerlink\" title=\"26.3: Why It Gets Worse - Shared Data\"></a>26.3: Why It Gets Worse - Shared Data</h2><p><strong>Code Overview</strong></p>\n<p>The code in Figure 26.6 demonstrates a global shared variable, <code>counter</code>, being updated by two threads concurrently. Each thread attempts to increment <code>counter</code> 10 million times.</p>\n<p><strong>Key Aspects</strong></p>\n<ol>\n<li><p><strong>Global Shared Variable</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">static</span> <span class=\"keyword\">volatile</span> <span class=\"type\">int</span> counter = <span class=\"number\">0</span>;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>static</code>: Limits the scope of <code>counter</code> to the current file.</li>\n<li><code>volatile</code>: Prevents compiler optimizations on <code>counter</code>, ensuring direct memory access each time.</li>\n</ul>\n</li>\n<li><p><strong>Thread Function</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> *<span class=\"title function_\">mythread</span><span class=\"params\">(<span class=\"type\">void</span> *arg)</span> &#123;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%s: begin\\n&quot;</span>, (<span class=\"type\">char</span> *) arg);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">1e7</span>; i++) &#123;</span><br><span class=\"line\">        counter = counter + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%s: done\\n&quot;</span>, (<span class=\"type\">char</span> *) arg);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Each thread adds <code>1</code> to <code>counter</code> in a loop.</li>\n<li>The desired final value of <code>counter</code> is <strong>20,000,000</strong>.</li>\n</ul>\n</li>\n<li><p><strong>Main Function</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> *argv[])</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">pthread_t</span> p1, p2;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;main: begin (counter = %d)\\n&quot;</span>, counter);</span><br><span class=\"line\">    Pthread_create(&amp;p1, <span class=\"literal\">NULL</span>, mythread, <span class=\"string\">&quot;A&quot;</span>);</span><br><span class=\"line\">    Pthread_create(&amp;p2, <span class=\"literal\">NULL</span>, mythread, <span class=\"string\">&quot;B&quot;</span>);</span><br><span class=\"line\">    Pthread_join(p1, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    Pthread_join(p2, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;main: done with both (counter = %d)\\n&quot;</span>, counter);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Two threads are created, each running <code>mythread</code>.</li>\n<li>The main thread waits for both threads to finish using <code>pthread_join</code>.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Expected vs. Actual Output</strong></p>\n<ol>\n<li><p><strong>Expected Output</strong>:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">main: begin (counter = 0)</span><br><span class=\"line\">A: begin</span><br><span class=\"line\">B: begin</span><br><span class=\"line\">A: done</span><br><span class=\"line\">B: done</span><br><span class=\"line\">main: done with both (counter = 20000000)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Actual Output</strong> (Examples):</p>\n<ul>\n<li><code>counter = 19345221</code></li>\n<li><code>counter = 19221041</code></li>\n</ul>\n</li>\n</ol>\n<p><strong>Why It Happens</strong></p>\n<p>The discrepancy occurs due to <strong>race conditions</strong> when <code>counter</code> is incremented by multiple threads simultaneously. Let’s break it down:</p>\n<ol>\n<li><p><strong>Increment Operation</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">counter = counter + <span class=\"number\">1</span>;</span><br></pre></td></tr></table></figure>\n<p>This statement is not atomic; it involves:</p>\n<ul>\n<li><strong>Read</strong>: Load the current value of <code>counter</code> into a register.</li>\n<li><strong>Modify</strong>: Increment the value in the register.</li>\n<li><strong>Write</strong>: Store the incremented value back into <code>counter</code>.</li>\n</ul>\n</li>\n<li><p><strong>Interleaved Execution</strong>:</p>\n<ul>\n<li>When threads execute concurrently, their instructions can interleave. For example:<ul>\n<li>Thread 1 reads <code>counter = 0</code>.</li>\n<li>Thread 2 reads <code>counter = 0</code>.</li>\n<li>Thread 1 writes <code>counter = 1</code>.</li>\n<li>Thread 2 writes <code>counter = 1</code>. (Overwrites Thread 1’s increment.)</li>\n</ul>\n</li>\n<li>The final value is <code>1</code> instead of <code>2</code>.</li>\n</ul>\n</li>\n<li><p><strong>Non-Deterministic Behavior</strong>:</p>\n<ul>\n<li>The exact interleaving depends on the OS scheduler, leading to different results each time the program runs.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"26-4-The-Heart-of-the-Problem-Uncontrolled-Scheduling\"><a href=\"#26-4-The-Heart-of-the-Problem-Uncontrolled-Scheduling\" class=\"headerlink\" title=\"26.4 The Heart of the Problem: Uncontrolled Scheduling\"></a>26.4 The Heart of the Problem: Uncontrolled Scheduling</h2><p>To understand why the issue occurs, we must examine the sequence of instructions generated by the compiler for the code that updates the shared variable <code>counter</code>. For instance, in x86 assembly, adding 1 to <code>counter</code> might translate to:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mov 0x8049a1c, %eax   ; Load the value of counter into register eax</span><br><span class=\"line\">add $0x1, %eax        ; Increment the value in register eax</span><br><span class=\"line\">mov %eax, 0x8049a1c   ; Store the updated value back into memory</span><br></pre></td></tr></table></figure>\n<p>Here, we assume <code>counter</code> is stored at memory address <code>0x8049a1c</code>. This sequence illustrates how updating <code>counter</code> requires three instructions: fetching the current value into a register, modifying it, and writing it back to memory.</p>\n<p><strong>Race Condition Example:</strong></p>\n<p>Imagine two threads, Thread 1 and Thread 2, concurrently executing this code to increment <code>counter</code>, initially set to <code>50</code>.</p>\n<ol>\n<li><strong>Thread 1 begins execution:</strong><ul>\n<li>It executes <code>mov</code>, loading <code>counter</code>‘s value (<code>50</code>) into its private register <code>eax</code>. Now, <code>eax = 50</code>.</li>\n<li>It then increments <code>eax</code> to <code>51</code>.</li>\n</ul>\n</li>\n<li><strong>A context switch occurs:</strong><ul>\n<li>The OS interrupts Thread 1 before it stores the updated value back into <code>counter</code>. The state of Thread 1 (including its <code>eax</code> register) is saved.</li>\n</ul>\n</li>\n<li><strong>Thread 2 starts execution:</strong><ul>\n<li>It executes the same <code>mov</code> instruction, fetching the still-unchanged value of <code>counter</code> (<code>50</code>) into its register. Now, <code>eax = 50</code> for Thread 2.</li>\n<li>Thread 2 increments <code>eax</code> to <code>51</code> and completes the third <code>mov</code> instruction, writing the updated value (<code>51</code>) back to <code>counter</code>.</li>\n</ul>\n</li>\n<li><strong>Another context switch occurs:</strong><ul>\n<li>Thread 1 resumes execution. It finishes its previously interrupted sequence by executing the final <code>mov</code> instruction, writing its <code>eax</code> value (<code>51</code>) back to <code>counter</code>.</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/img/image-20241201112623944.png\" alt=\"image-20241201112623944\"></p>\n<p><strong>Critical Section and Mutual Exclusion:</strong></p>\n<p>The problematic code segment is an example of a <strong>critical section</strong>, where multiple threads access a shared resource (like <code>counter</code>). When executed without proper synchronization, this can lead to race conditions.</p>\n<p>To address this, we need <strong>mutual exclusion</strong>, ensuring that only one thread can execute the critical section at a time. Synchronization primitives (e.g., locks or atomic operations) can achieve this.</p>\n<p><strong>Atomic Operations:</strong></p>\n<p>A fundamental solution to race conditions is the use of <strong>atomic operations</strong>, which ensure that a series of actions either occur entirely or not at all, without interruption. This principle underpins synchronization in concurrent code, as well as broader applications like file systems and database transactions.</p>\n<h2 id=\"26-5-The-Wish-for-Atomicity\"><a href=\"#26-5-The-Wish-for-Atomicity\" class=\"headerlink\" title=\"26.5 The Wish for Atomicity\"></a>26.5 The Wish for Atomicity</h2><p>One potential solution to the race condition problem is to have more advanced instructions that can perform all necessary operations in a single, uninterrupted step. This would eliminate the risk of an unexpected interrupt occurring mid-operation. For example, consider a hypothetical instruction:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">memory-add 0x8049a1c, $0x1</span><br></pre></td></tr></table></figure>\n<p>Assuming this instruction atomically adds a value to a specified memory location, the hardware guarantees that it completes without being interrupted mid-operation. This guarantee means that when an interrupt occurs, the instruction has either not started at all or has completed fully, with no intermediate state. This type of atomic behavior ensures the consistency and correctness of the operation.</p>\n<p><strong>Atomicity Explained:</strong></p>\n<p>In this context, <strong>atomicity</strong> means performing an operation as a single, indivisible unit. This is often summarized as “all or none”—either the entire operation is completed, or none of it is. The goal is to execute the three-step code sequence below atomically:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mov 0x8049a1c, %eax</span><br><span class=\"line\">add $0x1, %eax</span><br><span class=\"line\">mov %eax, 0x8049a1c</span><br></pre></td></tr></table></figure>\n<p>If we had a single instruction that could do this entire sequence atomically, we could simply use that instruction to ensure consistency. However, in most cases, such a specialized instruction is not feasible or practical. For instance, in complex data structures like a concurrent B-tree, we wouldn’t want hardware to have a dedicated instruction for an “atomic update of a B-tree.”</p>\n<p><strong>Practical Solution:</strong></p>\n<p>Instead, hardware supports a limited set of atomic operations that serve as building blocks for <strong>synchronization primitives</strong>. By leveraging these primitives in combination with assistance from the operating system, we can develop multi-threaded code that accesses critical sections safely and produces the correct results, even under the complexity of concurrent execution.</p>\n<h1 id=\"Chapter-27-Thread-API\"><a href=\"#Chapter-27-Thread-API\" class=\"headerlink\" title=\"Chapter 27: Thread API\"></a>Chapter 27: Thread API</h1><h2 id=\"27-1-Thread-Creation\"><a href=\"#27-1-Thread-Creation\" class=\"headerlink\" title=\"27.1 Thread Creation\"></a>27.1 Thread Creation</h2><p>Creating new threads is the foundational step for writing multi-threaded programs. In POSIX-compliant systems, this is done using the <code>pthread_create()</code> function, which provides an interface for thread creation. The function is defined as follows:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;pthread.h&gt;</span></span></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">pthread_create</span><span class=\"params\">(<span class=\"type\">pthread_t</span> *thread, <span class=\"type\">const</span> <span class=\"type\">pthread_attr_t</span> *attr,</span></span><br><span class=\"line\"><span class=\"params\">                   <span class=\"type\">void</span> *(*start_routine)(<span class=\"type\">void</span>*), <span class=\"type\">void</span> *arg)</span>;</span><br></pre></td></tr></table></figure>\n<p>The function takes four arguments:</p>\n<ol>\n<li><p><strong><code>thread</code></strong>: This is a pointer to a <code>pthread_t</code> structure, which is used to interact with the thread. You pass this structure to <code>pthread_create()</code> to initialize it.</p>\n</li>\n<li><p><strong><code>attr</code></strong>: This argument specifies any attributes the thread might have, such as stack size or scheduling priority. Attributes are initialized using <code>pthread_attr_init()</code>. However, most programs will use the default settings, so you can pass <code>NULL</code> if you don’t need custom attributes.</p>\n</li>\n<li><p><strong><code>start_routine</code></strong>: This is the function that the thread will start running. It is declared as a function pointer with the signature <code>void *(*start_routine)(void*)</code>. This indicates that the function:</p>\n<ul>\n<li>Takes a single argument of type <code>void *</code>.</li>\n<li>Returns a value of type <code>void *</code>.</li>\n</ul>\n<p>If the thread function needed to take an integer as an argument, the declaration would change to:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">pthread_create</span><span class=\"params\">(..., <span class=\"type\">void</span> *(*start_routine)(<span class=\"type\">int</span>), <span class=\"type\">int</span> arg)</span>;</span><br></pre></td></tr></table></figure>\n<p>If the function returned an integer instead, the declaration would be:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">pthread_create</span><span class=\"params\">(..., <span class=\"type\">int</span> (*start_routine)(<span class=\"type\">void</span> *), <span class=\"type\">void</span> *arg)</span>;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong><code>arg</code></strong>: This is the argument passed to the <code>start_routine</code> function when the thread begins execution. The use of <code>void *</code> for this argument and the return type allows for flexibility, enabling you to pass and return any type of data.</p>\n</li>\n</ol>\n<h2 id=\"27-2-Thread-Completion\"><a href=\"#27-2-Thread-Completion\" class=\"headerlink\" title=\"27.2 Thread Completion\"></a>27.2 Thread Completion</h2><p>Once a thread is created, you may need to wait for it to complete before proceeding. This is done using the <code>pthread_join()</code> function, which allows one thread to wait for another thread to finish its execution.</p>\n<p><strong>Function Signature:</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">pthread_join</span><span class=\"params\">(<span class=\"type\">pthread_t</span> thread, <span class=\"type\">void</span> **value_ptr)</span>;</span><br></pre></td></tr></table></figure>\n<p><strong>Parameters:</strong></p>\n<ul>\n<li><strong><code>thread</code></strong>: The thread identifier to wait for, which is initialized by <code>pthread_create()</code> when you pass a pointer to it.</li>\n<li><strong><code>value_ptr</code></strong>: A pointer to where the thread’s return value should be stored. Since the return value can be of any type, this parameter is a pointer to <code>void</code>. The <code>pthread_join()</code> function modifies the value at <code>value_ptr</code>, so it needs to be a pointer, not a direct value.</li>\n</ul>\n<p><strong>Example of Waiting for a Thread</strong></p>\n<p>In the example below (Figure 27.1), a thread is created, passed arguments packed into a <code>myarg_t</code> structure, and then <code>pthread_join()</code> is called to wait for its completion. The thread returns values, which are stored in a <code>myret_t</code> structure. The main thread can then access these returned values after <code>pthread_join()</code> returns.</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span> <span class=\"type\">int</span> a; <span class=\"type\">int</span> b; &#125; <span class=\"type\">myarg_t</span>;</span><br><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span> <span class=\"type\">int</span> x; <span class=\"type\">int</span> y; &#125; <span class=\"type\">myret_t</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> *<span class=\"title function_\">mythread</span><span class=\"params\">(<span class=\"type\">void</span> *arg)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">myret_t</span> *rvals = <span class=\"built_in\">malloc</span>(<span class=\"keyword\">sizeof</span>(<span class=\"type\">myret_t</span>));</span><br><span class=\"line\">    rvals-&gt;x = <span class=\"number\">1</span>;</span><br><span class=\"line\">    rvals-&gt;y = <span class=\"number\">2</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"type\">void</span> *) rvals;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> *argv[])</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">pthread_t</span> p;</span><br><span class=\"line\">    <span class=\"type\">myret_t</span> *rvals;</span><br><span class=\"line\">    <span class=\"type\">myarg_t</span> args = &#123; <span class=\"number\">10</span>, <span class=\"number\">20</span> &#125;;</span><br><span class=\"line\">    pthread_create(&amp;p, <span class=\"literal\">NULL</span>, mythread, &amp;args);</span><br><span class=\"line\">    pthread_join(p, (<span class=\"type\">void</span> **) &amp;rvals);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;returned %d %d\\n&quot;</span>, rvals-&gt;x, rvals-&gt;y);</span><br><span class=\"line\">    <span class=\"built_in\">free</span>(rvals);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>Important Points:</strong></p>\n<ol>\n<li><p><strong>Argument and Return Simplification</strong>: If a thread does not need complex arguments or return values, you can pass <code>NULL</code> when creating the thread or in <code>pthread_join()</code> if you don’t need the return value.</p>\n</li>\n<li><p><strong>Avoid Stack Allocation</strong>: Never return a pointer to a local variable allocated on the thread’s stack. Once the function returns, the stack memory is deallocated, leading to undefined behavior. Example of what <em>not</em> to do:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> *<span class=\"title function_\">mythread</span><span class=\"params\">(<span class=\"type\">void</span> *arg)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">myarg_t</span> *args = (<span class=\"type\">myarg_t</span> *) arg;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%d %d\\n&quot;</span>, args-&gt;a, args-&gt;b);</span><br><span class=\"line\">    <span class=\"type\">myret_t</span> oops; <span class=\"comment\">// Allocated on stack: BAD!</span></span><br><span class=\"line\">    oops.x = <span class=\"number\">1</span>;</span><br><span class=\"line\">    oops.y = <span class=\"number\">2</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"type\">void</span> *) &amp;oops; <span class=\"comment\">// Dangerous: points to deallocated memory</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p><strong>Simpler Argument Passing</strong>: If a thread only needs to pass a simple value, you don’t have to package it into a structure. Example (Figure 27.3):</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> *<span class=\"title function_\">mythread</span><span class=\"params\">(<span class=\"type\">void</span> *arg)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">long</span> <span class=\"type\">long</span> <span class=\"type\">int</span> value = (<span class=\"type\">long</span> <span class=\"type\">long</span> <span class=\"type\">int</span>) arg;</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%lld\\n&quot;</span>, value);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"type\">void</span> *) (value + <span class=\"number\">1</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">main</span><span class=\"params\">(<span class=\"type\">int</span> argc, <span class=\"type\">char</span> *argv[])</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">pthread_t</span> p;</span><br><span class=\"line\">    <span class=\"type\">long</span> <span class=\"type\">long</span> <span class=\"type\">int</span> rvalue;</span><br><span class=\"line\">    pthread_create(&amp;p, <span class=\"literal\">NULL</span>, mythread, (<span class=\"type\">void</span> *) <span class=\"number\">100</span>);</span><br><span class=\"line\">    pthread_join(p, (<span class=\"type\">void</span> **) &amp;rvalue);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;returned %lld\\n&quot;</span>, rvalue);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p><strong>When to Use <code>pthread_join()</code>:</strong></p>\n<ul>\n<li><p>Not all multi-threaded programs use <code>pthread_join()</code>. For example, a web server with worker threads might not need it, as the main thread handles request acceptance and passes tasks to the worker threads indefinitely.</p>\n</li>\n<li><p>However, in programs that create threads to perform specific tasks in parallel, <code>pthread_join()</code> is essential to ensure that all work is completed before exiting or moving to the next stage of computation.</p>\n</li>\n</ul>\n<h2 id=\"27-3-Locks\"><a href=\"#27-3-Locks\" class=\"headerlink\" title=\"27.3 Locks\"></a>27.3 Locks</h2><p><strong>Basic Lock Operations</strong></p>\n<ol>\n<li><p>Locking and Unlocking:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">pthread_mutex_lock</span><span class=\"params\">(<span class=\"type\">pthread_mutex_t</span> *mutex)</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">pthread_mutex_unlock</span><span class=\"params\">(<span class=\"type\">pthread_mutex_t</span> *mutex)</span>;</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>pthread_mutex_lock()</code> attempts to acquire the lock. If it’s already locked by another thread, the calling thread waits until it is released.</li>\n<li><code>pthread_mutex_unlock()</code> releases the lock, allowing other threads to acquire it.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Proper Initialization</strong></p>\n<ul>\n<li><p><strong>Static Initialization</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">pthread_mutex_t</span> lock = PTHREAD_MUTEX_INITIALIZER;</span><br></pre></td></tr></table></figure>\n<p>This method sets up the lock to default values, making it ready for use.</p>\n</li>\n<li><p><strong>Dynamic Initialization</strong>:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> rc = pthread_mutex_init(&amp;lock, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">assert(rc == <span class=\"number\">0</span>); <span class=\"comment\">// Check for successful initialization</span></span><br></pre></td></tr></table></figure>\n<p>Here, <code>pthread_mutex_init()</code> allows runtime initialization, with <code>NULL</code> as the second argument to use default attributes.</p>\n</li>\n<li><p><strong>Destruction</strong>: When done using a lock, ensure you call <code>pthread_mutex_destroy(&amp;lock)</code> to clean up resources.</p>\n</li>\n</ul>\n<p><strong>Error Handling</strong></p>\n<ul>\n<li><p>Error Code Checking: Always check the return codes from <code>pthread_mutex_lock()</code> and <code>pthread_mutex_unlock()</code></p>\n<p> to ensure they succeed:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">Pthread_mutex_lock</span><span class=\"params\">(<span class=\"type\">pthread_mutex_t</span> *mutex)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> rc = pthread_mutex_lock(mutex);</span><br><span class=\"line\">    assert(rc == <span class=\"number\">0</span>); <span class=\"comment\">// Ensure successful lock</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>This approach ensures that your code doesn’t proceed if there is an issue with lock acquisition.</p>\n</li>\n</ul>\n<p><strong>Advanced Lock Functions</strong></p>\n<ul>\n<li><strong><code>pthread_mutex_trylock()</code></strong>: Tries to acquire the lock without waiting. Returns immediately if the lock is already held.</li>\n<li><strong><code>pthread_mutex_timedlock()</code></strong>: Tries to acquire the lock within a specified time. Returns after the timeout or if the lock is acquired.</li>\n</ul>\n<p>These advanced functions help avoid indefinite blocking, which is essential for handling deadlocks or specific use cases where you need non-blocking behavior.</p>\n<h2 id=\"27-4-Condition-Variables\"><a href=\"#27-4-Condition-Variables\" class=\"headerlink\" title=\"27.4 Condition Variables\"></a>27.4 Condition Variables</h2><p><strong>Basic Concepts</strong></p>\n<ul>\n<li><strong>Condition Variables</strong>: Used to signal between threads, allowing one thread to wait for another to notify it of an event.</li>\n<li><strong>Associated Lock</strong>: Condition variables require a mutex to prevent race conditions when checking or updating shared variables.</li>\n</ul>\n<p><strong>Key Routines</strong></p>\n<ol>\n<li><p><strong><code>pthread_cond_wait()</code></strong>:</p>\n<ul>\n<li><p>Suspends the calling thread and releases the lock while it waits.</p>\n</li>\n<li><p>Re-acquires the lock when it wakes up.</p>\n</li>\n<li><p>Typical usage:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">pthread_mutex_t</span> lock = PTHREAD_MUTEX_INITIALIZER;</span><br><span class=\"line\"><span class=\"type\">pthread_cond_t</span> cond = PTHREAD_COND_INITIALIZER;</span><br><span class=\"line\"></span><br><span class=\"line\">Pthread_mutex_lock(&amp;lock);</span><br><span class=\"line\"><span class=\"keyword\">while</span> (ready == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    Pthread_cond_wait(&amp;cond, &amp;lock);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">Pthread_mutex_unlock(&amp;lock);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p><strong><code>pthread_cond_signal()</code></strong>:</p>\n<ul>\n<li><p>Wakes up one waiting thread.</p>\n</li>\n<li><p>The lock should be held when signaling to avoid race conditions.</p>\n</li>\n<li><p>Example:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Pthread_mutex_lock(&amp;lock);</span><br><span class=\"line\">ready = <span class=\"number\">1</span>;</span><br><span class=\"line\">Pthread_cond_signal(&amp;cond);</span><br><span class=\"line\">Pthread_mutex_unlock(&amp;lock);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Important Points</strong></p>\n<ul>\n<li><p><strong>Lock Holding</strong>: <code>pthread_cond_wait()</code> releases the lock while putting the thread to sleep, which allows other threads to acquire the lock and signal the condition. When the waiting thread is woken, it re-acquires the lock before continuing.</p>\n</li>\n<li><p><strong>While Loop for Safety</strong>: The condition should be checked inside a <code>while</code> loop, not an <code>if</code> statement. This is because some implementations of <code>pthread_cond_wait()</code> might spuriously wake up a thread without a change in the condition. Using a <code>while</code> loop ensures the condition is re-checked before proceeding.</p>\n</li>\n<li><p><strong>Performance Considerations</strong>: Simple flags (e.g., using busy-waiting loops) can be tempting but are inefficient and error-prone:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (ready == <span class=\"number\">0</span>) ; <span class=\"comment\">// spin (bad practice)</span></span><br></pre></td></tr></table></figure>\n<p>This approach wastes CPU cycles and increases the risk of synchronization bugs.</p>\n</li>\n</ul>\n<h1 id=\"Chapter-28-Lock\"><a href=\"#Chapter-28-Lock\" class=\"headerlink\" title=\"Chapter 28: Lock\"></a>Chapter 28: Lock</h1><h2 id=\"28-1-Locks-The-Basic-Idea\"><a href=\"#28-1-Locks-The-Basic-Idea\" class=\"headerlink\" title=\"28.1 Locks: The Basic Idea\"></a>28.1 Locks: The Basic Idea</h2><p><strong>What is a Lock?</strong></p>\n<ul>\n<li><strong>Definition</strong>: A lock is a synchronization primitive that controls access to shared resources by multiple threads.</li>\n<li><strong>State</strong>: A lock has two states:<ul>\n<li><strong>Available (Unlocked/Free)</strong>: No thread holds the lock; it can be acquired by any thread.</li>\n<li><strong>Acquired (Locked/Held)</strong>: A thread holds the lock, preventing other threads from acquiring it until it is released.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Basic Lock Operations</strong></p>\n<ul>\n<li><strong>Lock Operation (<code>lock(&amp;mutex)</code>)</strong>: This routine tries to acquire the lock. If no thread currently holds it, the calling thread becomes the owner and can enter the critical section.</li>\n<li><strong>Unlock Operation (<code>unlock(&amp;mutex)</code>)</strong>: This routine releases the lock, making it available for other threads to acquire. If there are threads waiting to acquire the lock, one will be selected (based on scheduling policies) to proceed.</li>\n</ul>\n<p><strong>Code Example</strong></p>\n<p>Consider a simple critical section updating a shared variable:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">lock_t</span> mutex; <span class=\"comment\">// Declare a globally-allocated lock variable</span></span><br><span class=\"line\">...</span><br><span class=\"line\">lock(&amp;mutex);       <span class=\"comment\">// Try to acquire the lock</span></span><br><span class=\"line\">balance = balance + <span class=\"number\">1</span>; <span class=\"comment\">// Critical section</span></span><br><span class=\"line\">unlock(&amp;mutex);     <span class=\"comment\">// Release the lock</span></span><br></pre></td></tr></table></figure>\n<p><strong>How Locks Work:</strong></p>\n<ul>\n<li>Lock Acquisition:<ul>\n<li>When <code>lock()</code> is called, if the lock is free, the thread acquires it and becomes the “owner”.</li>\n<li>If the lock is already acquired by another thread, the calling thread waits until it becomes free.</li>\n</ul>\n</li>\n<li>Lock Release:<ul>\n<li>When <code>unlock()</code> is called by the owner, the lock is released, and it transitions to the available state.</li>\n<li>If other threads are waiting, one of them will acquire the lock and proceed with its critical section.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Benefits of Using Locks</strong></p>\n<ul>\n<li><strong>Controlled Access</strong>: Locks prevent multiple threads from simultaneously executing a critical section, ensuring data consistency and avoiding race conditions.</li>\n<li><strong>Thread Safety</strong>: Ensures that only one thread at a time can execute a specific section of code, protecting shared resources from concurrent modifications.</li>\n<li><strong>Scheduling Control</strong>: While the OS controls thread scheduling, using locks allows the programmer to manage the execution order within critical sections, giving more predictable behavior in multi-threaded programs.</li>\n</ul>\n<h2 id=\"28-2-Pthread-Locks\"><a href=\"#28-2-Pthread-Locks\" class=\"headerlink\" title=\"28.2 Pthread Locks\"></a>28.2 Pthread Locks</h2><p>The POSIX library uses the term “mutex” for a lock, which stands for <em>mutual exclusion</em>. This is crucial for ensuring that only one thread accesses a critical section at a time, preventing other threads from entering until the current thread has completed its operation. The code snippet provided demonstrates how to use a POSIX mutex:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">1</span> <span class=\"type\">pthread_mutex_t</span> lock = PTHREAD_MUTEX_INITIALIZER;</span><br><span class=\"line\"><span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"number\">3</span> Pthread_mutex_lock(&amp;lock); <span class=\"comment\">// wrapper; exits on failure</span></span><br><span class=\"line\"><span class=\"number\">4</span> balance = balance + <span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"number\">5</span> Pthread_mutex_unlock(&amp;lock);</span><br></pre></td></tr></table></figure>\n<p>In this code, <code>lock</code> is passed to <code>Pthread_mutex_lock</code> and <code>Pthread_mutex_unlock</code>, allowing different variables to be protected by different locks. This strategy, known as <em>fine-grained locking</em>, enhances concurrency compared to using a single large lock for all critical sections, which is termed <em>coarse-grained locking</em>. The fine-grained approach allows more threads to execute locked code simultaneously, improving efficiency.</p>\n<h2 id=\"28-3-Building-A-Lock\"><a href=\"#28-3-Building-A-Lock\" class=\"headerlink\" title=\"28.3 Building A Lock\"></a>28.3 Building A Lock</h2><p>Understanding how to build a lock goes beyond just using them; it involves knowing the hardware and OS support needed. Efficient locks ensure mutual exclusion with minimal overhead and may possess additional desirable properties. Building an effective lock requires hardware support—using specific CPU instructions—and OS-level assistance to create a comprehensive locking library. While the detailed implementation of these hardware primitives is a topic for computer architecture courses, understanding their use is essential for building synchronization primitives like locks.</p>\n<h2 id=\"28-4-Evaluating-Locks\"><a href=\"#28-4-Evaluating-Locks\" class=\"headerlink\" title=\"28.4 Evaluating Locks\"></a>28.4 Evaluating Locks</h2><p>Before creating locks, it’s important to define evaluation criteria. A lock’s primary function is mutual exclusion—ensuring that only one thread can enter a critical section at a time. The lock should be assessed for fairness, which means ensuring that each thread contending for the lock has a fair chance to acquire it, preventing starvation (where a thread never gets access). Performance is another key criterion, focusing on the overhead introduced by using the lock. Performance should be measured in different scenarios:</p>\n<ul>\n<li><strong>No contention</strong>: What overhead does the lock introduce when only one thread is active?</li>\n<li><strong>Single CPU with contention</strong>: How does the lock behave when multiple threads compete on a single CPU?</li>\n<li><strong>Multiple CPUs with contention</strong>: How does the lock perform when multiple threads on different CPUs try to access it?</li>\n</ul>\n<p>Evaluating these cases provides insight into the performance impact of various locking techniques.</p>\n<h2 id=\"28-5-Controlling-Interrupts\"><a href=\"#28-5-Controlling-Interrupts\" class=\"headerlink\" title=\"28.5 Controlling Interrupts\"></a>28.5 Controlling Interrupts</h2><p>One early method for providing mutual exclusion was disabling interrupts in a single-processor system:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">1</span> <span class=\"type\">void</span> <span class=\"title function_\">lock</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\"><span class=\"number\">2</span> DisableInterrupts();</span><br><span class=\"line\"><span class=\"number\">3</span> &#125;</span><br><span class=\"line\"><span class=\"number\">4</span> <span class=\"type\">void</span> <span class=\"title function_\">unlock</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\"><span class=\"number\">5</span> EnableInterrupts();</span><br><span class=\"line\"><span class=\"number\">6</span> &#125;</span><br></pre></td></tr></table></figure>\n<p>Disabling interrupts ensures that code within a critical section runs without interruption, making it effectively atomic. Once the critical section is completed, interrupts are re-enabled, resuming normal program operation.</p>\n<p><strong>Advantages</strong>:</p>\n<ul>\n<li><strong>Simplicity</strong>: This method is straightforward and ensures code executes without interference.</li>\n</ul>\n<p><strong>Drawbacks</strong>:</p>\n<ul>\n<li><strong>Privileged Operation</strong>: Allowing threads to disable interrupts introduces trust issues. Malicious or poorly designed programs could monopolize the processor by calling <code>lock()</code> at the start of execution or enter an infinite loop, effectively halting the system.</li>\n<li><strong>Multiprocessor Limitation</strong>: In multi-CPU systems, disabling interrupts on one CPU does not prevent threads on other CPUs from running, making this method ineffective.</li>\n<li><strong>Lost Interrupts</strong>: Extended interruption masking can lead to missed events, such as the completion of I/O operations, causing the OS to lose track of essential events.</li>\n<li><strong>Performance Issues</strong>: Operations to mask and unmask interrupts are slower than normal CPU instructions.</li>\n</ul>\n<p>Due to these limitations, disabling interrupts is used sparingly, often within the operating system itself to ensure atomic operations for critical data structures. This is safe because the OS can trust its own code to perform privileged operations correctly.</p>\n<p><strong>Figure 28.1: First Attempt: A Simple Flag</strong></p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">1</span> <span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">lock_t</span> &#123;</span> <span class=\"type\">int</span> flag; &#125; <span class=\"type\">lock_t</span>;</span><br><span class=\"line\"><span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"number\">3</span> <span class=\"type\">void</span> <span class=\"title function_\">init</span><span class=\"params\">(<span class=\"type\">lock_t</span> *mutex)</span> &#123;</span><br><span class=\"line\"><span class=\"number\">4</span> <span class=\"comment\">// 0 -&gt; lock is available, 1 -&gt; held</span></span><br><span class=\"line\"><span class=\"number\">5</span> mutex-&gt;flag = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"number\">6</span> &#125;</span><br><span class=\"line\"><span class=\"number\">7</span></span><br><span class=\"line\"><span class=\"number\">8</span> <span class=\"type\">void</span> <span class=\"title function_\">lock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *mutex)</span> &#123;</span><br><span class=\"line\"><span class=\"number\">9</span> <span class=\"keyword\">while</span> (mutex-&gt;flag == <span class=\"number\">1</span>) <span class=\"comment\">// TEST the flag</span></span><br><span class=\"line\"><span class=\"number\">10</span> ; <span class=\"comment\">// spin-wait (do nothing)</span></span><br><span class=\"line\"><span class=\"number\">11</span> mutex-&gt;flag = <span class=\"number\">1</span>; <span class=\"comment\">// now SET it!</span></span><br><span class=\"line\"><span class=\"number\">12</span> &#125;</span><br><span class=\"line\"><span class=\"number\">13</span></span><br><span class=\"line\"><span class=\"number\">14</span> <span class=\"type\">void</span> <span class=\"title function_\">unlock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *mutex)</span> &#123;</span><br><span class=\"line\"><span class=\"number\">15</span> mutex-&gt;flag = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"number\">16</span> &#125;</span><br></pre></td></tr></table></figure>\n<p>This simple lock implementation uses a <code>flag</code> to indicate whether the lock is available (<code>0</code>) or held (<code>1</code>). The <code>lock()</code> function continuously checks the flag in a spin-wait loop until it is set to <code>0</code>, then sets it to <code>1</code> to claim the lock. The <code>unlock()</code> function resets the flag to <code>0</code>, releasing the lock.</p>\n<p><strong>Limitations</strong>:</p>\n<ul>\n<li>This basic implementation can lead to busy-waiting, which wastes CPU resources.</li>\n<li>This method only works for single-processor systems; in multi-CPU systems, it won’t prevent other CPUs from accessing the critical section.</li>\n</ul>\n<h2 id=\"28-6-A-Failed-Attempt-Just-Using-Loads-Stores\"><a href=\"#28-6-A-Failed-Attempt-Just-Using-Loads-Stores\" class=\"headerlink\" title=\"28.6 A Failed Attempt: Just Using Loads/Stores\"></a><strong>28.6 A Failed Attempt: Just Using Loads/Stores</strong></h2><ul>\n<li><strong>Objective</strong>: Build a lock using a single variable (<code>flag</code>) accessed via normal loads and stores.</li>\n<li>Approach:<ul>\n<li>A <code>flag</code> variable indicates lock possession: <code>0</code> means the lock is free; <code>1</code> means the lock is held.</li>\n<li>The <code>lock()</code> function checks if the <code>flag</code> is <code>0</code>. If true, it sets the <code>flag</code> to <code>1</code> (acquiring the lock). Otherwise, it waits in a loop until the lock is released.</li>\n<li>The <code>unlock()</code> function resets the <code>flag</code> to <code>0</code>, releasing the lock.</li>\n</ul>\n</li>\n<li>Issues:<ul>\n<li>Correctness Problem: The approach fails to ensure mutual exclusion.<ul>\n<li>Example: Two threads could simultaneously read <code>flag = 0</code>, and both set it to <code>1</code>, leading both to enter the critical section.</li>\n</ul>\n</li>\n<li><strong>Performance Problem</strong>: Spin-waiting wastes CPU cycles, especially on uniprocessors, as the waiting thread cannot proceed until a context switch occurs.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"28-7-Building-Working-Spin-Locks-with-Test-And-Set\"><a href=\"#28-7-Building-Working-Spin-Locks-with-Test-And-Set\" class=\"headerlink\" title=\"28.7 Building Working Spin Locks with Test-And-Set\"></a><strong>28.7 Building Working Spin Locks with Test-And-Set</strong></h2><ul>\n<li><p>Hardware Support: Early systems introduced instructions like <code>TestAndSet</code> to solve locking issues.</p>\n<ul>\n<li><p><strong>Definition</strong>: <code>TestAndSet</code> performs two actions atomically: retrieves the current value at a memory location and updates it to a new value.</p>\n</li>\n<li><p>Example C code:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">TestAndSet</span><span class=\"params\">(<span class=\"type\">int</span> *old_ptr, <span class=\"type\">int</span> new)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> old = *old_ptr; <span class=\"comment\">// Fetch the old value</span></span><br><span class=\"line\">    *old_ptr = new;    <span class=\"comment\">// Set the new value</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> old;        <span class=\"comment\">// Return the old value</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p>Spin Lock Implementation:</p>\n<ul>\n<li><p>Structure:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">lock_t</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> flag;</span><br><span class=\"line\">&#125; <span class=\"type\">lock_t</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">init</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    lock-&gt;flag = <span class=\"number\">0</span>; <span class=\"comment\">// 0: available, 1: held</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">lock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (TestAndSet(&amp;lock-&gt;flag, <span class=\"number\">1</span>) == <span class=\"number\">1</span>)</span><br><span class=\"line\">        ; <span class=\"comment\">// Spin-wait</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">unlock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    lock-&gt;flag = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Mechanism:</p>\n<ul>\n<li>A thread calls <code>lock()</code>. If the <code>flag</code> is <code>0</code>, <code>TestAndSet</code> returns <code>0</code> and sets <code>flag = 1</code>, acquiring the lock.</li>\n<li>If the lock is already held (<code>flag = 1</code>), <code>TestAndSet</code> repeatedly returns <code>1</code>, causing the thread to spin until the lock is released.</li>\n<li>Once the critical section is completed, <code>unlock()</code> sets <code>flag = 0</code>.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"28-8-Evaluating-Spin-Locks\"><a href=\"#28-8-Evaluating-Spin-Locks\" class=\"headerlink\" title=\"28.8 Evaluating Spin Locks\"></a><strong>28.8 Evaluating Spin Locks</strong></h2><ul>\n<li><strong>Correctness</strong>: Spin locks ensure mutual exclusion; only one thread can enter the critical section.</li>\n<li>Fairness:<ul>\n<li>Spin locks provide no guarantees of fairness. A thread may spin indefinitely under contention, leading to starvation.</li>\n</ul>\n</li>\n<li>Performance:<ul>\n<li><strong>Single CPU</strong>: Inefficient. If the thread holding the lock is preempted, others spin needlessly.</li>\n<li><strong>Multiple CPUs</strong>: More effective, especially if the number of threads matches the number of CPUs.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"28-9-Compare-And-Swap\"><a href=\"#28-9-Compare-And-Swap\" class=\"headerlink\" title=\"28.9 Compare-And-Swap\"></a><strong>28.9 Compare-And-Swap</strong></h2><ul>\n<li><p>Definition:</p>\n<ul>\n<li><p>Compares a memory location’s current value with an expected value. If they match, updates the memory with a new value; otherwise, no action is taken.</p>\n</li>\n<li><p>Example C code:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">CompareAndSwap</span><span class=\"params\">(<span class=\"type\">int</span> *ptr, <span class=\"type\">int</span> expected, <span class=\"type\">int</span> new)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> original = *ptr;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (original == expected)</span><br><span class=\"line\">        *ptr = new;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> original;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n<li><p>Usage in Locks:</p>\n<ul>\n<li><p>Replace <code>TestAndSet</code> with <code>CompareAndSwap</code></p>\n<p> in the spin-lock implementation:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">lock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (CompareAndSwap(&amp;lock-&gt;flag, <span class=\"number\">0</span>, <span class=\"number\">1</span>) == <span class=\"number\">1</span>)</span><br><span class=\"line\">        ; <span class=\"comment\">// Spin</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Ensures mutual exclusion by atomically swapping the flag value only when it matches the expected value (<code>0</code>).</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"28-10-Load-Linked-and-Store-Conditional\"><a href=\"#28-10-Load-Linked-and-Store-Conditional\" class=\"headerlink\" title=\"28.10: Load-Linked and Store-Conditional\"></a>28.10: Load-Linked and Store-Conditional</h2><p>Some computer architectures, like MIPS, offer <strong>load-linked (LL)</strong> and <strong>store-conditional (SC)</strong> instructions to implement synchronization primitives such as locks. These instructions work as follows:</p>\n<ul>\n<li><strong>Load-Linked (LL):</strong> Loads a value from memory into a register.</li>\n<li><strong>Store-Conditional (SC):</strong> Stores a new value to the same memory address only if no other thread has modified it since the LL operation. It returns <code>1</code> on success and <code>0</code> on failure.</li>\n</ul>\n<p><strong>Example: Building a Lock with LL/SC</strong><br>Using LL/SC, locks can be implemented by spinning until the flag indicating lock availability becomes <code>0</code>. The thread then attempts to set the flag to <code>1</code> atomically:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">lock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (LoadLinked(&amp;lock-&gt;flag) == <span class=\"number\">1</span>); <span class=\"comment\">// Spin until flag is 0</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (StoreConditional(&amp;lock-&gt;flag, <span class=\"number\">1</span>) == <span class=\"number\">1</span>) <span class=\"keyword\">return</span>; <span class=\"comment\">// Acquire lock</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">unlock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    lock-&gt;flag = <span class=\"number\">0</span>; <span class=\"comment\">// Release lock</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>Optimized Implementation:</strong><br> A concise version uses short-circuit boolean conditionals:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">lock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (LoadLinked(&amp;lock-&gt;flag) || !StoreConditional(&amp;lock-&gt;flag, <span class=\"number\">1</span>));</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>This ensures only one thread can successfully update the flag to <code>1</code>.</p>\n<h2 id=\"28-11-Fetch-and-Add\"><a href=\"#28-11-Fetch-and-Add\" class=\"headerlink\" title=\"28.11: Fetch-and-Add\"></a>28.11: Fetch-and-Add</h2><p>The <strong>Fetch-and-Add (FAA)</strong> instruction atomically increments a value at a memory address while returning its old value. This can be leveraged to implement ticket-based locks that ensure fairness.</p>\n<p><strong>Example: Ticket Locks</strong><br> A ticket lock uses a <code>ticket</code> and <code>turn</code> variable:</p>\n<ol>\n<li>A thread fetches its ticket number (<code>myturn</code>) using FAA.</li>\n<li>It waits until <code>myturn</code> equals <code>turn</code> to enter the critical section.</li>\n<li>On unlock, <code>turn</code> is incremented to allow the next thread to proceed.</li>\n</ol>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">lock_t</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> ticket;</span><br><span class=\"line\">    <span class=\"type\">int</span> turn;</span><br><span class=\"line\">&#125; <span class=\"type\">lock_t</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">lock_init</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    lock-&gt;ticket = <span class=\"number\">0</span>;</span><br><span class=\"line\">    lock-&gt;turn = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">lock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> myturn = FetchAndAdd(&amp;lock-&gt;ticket);</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (lock-&gt;turn != myturn); <span class=\"comment\">// Spin until turn matches</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">unlock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *lock)</span> &#123;</span><br><span class=\"line\">    lock-&gt;turn++;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"28-12-Too-Much-Spinning\"><a href=\"#28-12-Too-Much-Spinning\" class=\"headerlink\" title=\"28.12: Too Much Spinning\"></a>28.12: Too Much Spinning</h2><p><strong>Issue with Spinning</strong><br> Spinning wastes CPU cycles, especially in single-processor systems where the lock-holding thread must finish before other threads can proceed. This inefficiency escalates with more threads contending for a lock.</p>\n<h2 id=\"28-13-Yielding-to-Reduce-Spinning\"><a href=\"#28-13-Yielding-to-Reduce-Spinning\" class=\"headerlink\" title=\"28.13: Yielding to Reduce Spinning\"></a>28.13: Yielding to Reduce Spinning</h2><p><strong>Yielding</strong><br> A thread finding a lock held can yield the CPU to allow another thread to run. This reduces wasted CPU time:</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">lock</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (TestAndSet(&amp;flag, <span class=\"number\">1</span>) == <span class=\"number\">1</span>)</span><br><span class=\"line\">        yield(); <span class=\"comment\">// Give up CPU</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">unlock</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    flag = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>While better than spinning, yielding can still waste time with excessive context switches and doesn’t prevent starvation.</p>\n<h2 id=\"28-14-Sleeping-Instead-of-Spinning\"><a href=\"#28-14-Sleeping-Instead-of-Spinning\" class=\"headerlink\" title=\"28.14: Sleeping Instead of Spinning\"></a>28.14: Sleeping Instead of Spinning</h2><p><strong>Using Queues</strong><br> A better approach involves <strong>sleeping</strong> waiting threads and waking them when the lock is available. This ensures efficient CPU usage and prevents starvation.</p>\n<p><strong>Example: Queue-Based Locks</strong><br> This approach uses:</p>\n<ul>\n<li>A <strong>guard lock</strong> to manage access to critical lock variables.</li>\n<li>A <strong>queue</strong> to track waiting threads.</li>\n<li>OS primitives like <code>park()</code> (to put a thread to sleep) and <code>unpark()</code> (to wake a thread).</li>\n</ul>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">lock_t</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> flag;</span><br><span class=\"line\">    <span class=\"type\">int</span> guard;</span><br><span class=\"line\">    <span class=\"type\">queue_t</span> *q;</span><br><span class=\"line\">&#125; <span class=\"type\">lock_t</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">lock_init</span><span class=\"params\">(<span class=\"type\">lock_t</span> *m)</span> &#123;</span><br><span class=\"line\">    m-&gt;flag = <span class=\"number\">0</span>;</span><br><span class=\"line\">    m-&gt;guard = <span class=\"number\">0</span>;</span><br><span class=\"line\">    queue_init(m-&gt;q);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">lock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *m)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (TestAndSet(&amp;m-&gt;guard, <span class=\"number\">1</span>) == <span class=\"number\">1</span>); <span class=\"comment\">// Acquire guard lock</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (m-&gt;flag == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        m-&gt;flag = <span class=\"number\">1</span>; <span class=\"comment\">// Acquire lock</span></span><br><span class=\"line\">        m-&gt;guard = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        queue_add(m-&gt;q, gettid());</span><br><span class=\"line\">        m-&gt;guard = <span class=\"number\">0</span>;</span><br><span class=\"line\">        park(); <span class=\"comment\">// Sleep</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">unlock</span><span class=\"params\">(<span class=\"type\">lock_t</span> *m)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (TestAndSet(&amp;m-&gt;guard, <span class=\"number\">1</span>) == <span class=\"number\">1</span>); <span class=\"comment\">// Acquire guard lock</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (queue_empty(m-&gt;q)) &#123;</span><br><span class=\"line\">        m-&gt;flag = <span class=\"number\">0</span>; <span class=\"comment\">// Release lock</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        unpark(queue_remove(m-&gt;q)); <span class=\"comment\">// Wake next thread</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    m-&gt;guard = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>Advantages</strong></p>\n<ul>\n<li>No unnecessary CPU usage during contention.</li>\n<li>Fairness ensured by servicing threads in queue order.</li>\n<li>Prevents starvation and optimizes system responsiveness.</li>\n</ul>\n<h1 id=\"Chapter-29：Lock-based-Concurrent-Data-Structures\"><a href=\"#Chapter-29：Lock-based-Concurrent-Data-Structures\" class=\"headerlink\" title=\"Chapter 29：Lock-based Concurrent Data Structures\"></a>Chapter 29：Lock-based Concurrent Data Structures</h1><h2 id=\"29-1-Concurrent-Counters\"><a href=\"#29-1-Concurrent-Counters\" class=\"headerlink\" title=\"29.1 Concurrent Counters\"></a>29.1 Concurrent Counters</h2><p><strong>1. Non-Synchronized Counter (Figure 29.1)</strong></p>\n<p>The simplest implementation of a counter lacks thread safety. If multiple threads access and modify the counter concurrently, <strong>race conditions</strong> may occur, leading to incorrect results.</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">counter_t</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> value;</span><br><span class=\"line\">&#125; <span class=\"type\">counter_t</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">init</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c)</span> &#123; c-&gt;value = <span class=\"number\">0</span>; &#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">increment</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c)</span> &#123; c-&gt;value++; &#125;</span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">decrement</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c)</span> &#123; c-&gt;value--; &#125;</span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">get</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c)</span> &#123; <span class=\"keyword\">return</span> c-&gt;value; &#125;</span><br></pre></td></tr></table></figure>\n<p><strong>2. Lock-Based Counter (Figure 29.2)</strong></p>\n<p>To make the counter thread-safe, a <strong>mutex lock</strong> is added. Each method locks the counter during an operation, ensuring that only one thread can modify the counter at a time.</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">counter_t</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> value;</span><br><span class=\"line\">    <span class=\"type\">pthread_mutex_t</span> lock;</span><br><span class=\"line\">&#125; <span class=\"type\">counter_t</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">init</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c)</span> &#123;</span><br><span class=\"line\">    c-&gt;value = <span class=\"number\">0</span>;</span><br><span class=\"line\">    pthread_mutex_init(&amp;c-&gt;lock, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">increment</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c)</span> &#123;</span><br><span class=\"line\">    pthread_mutex_lock(&amp;c-&gt;lock);</span><br><span class=\"line\">    c-&gt;value++;</span><br><span class=\"line\">    pthread_mutex_unlock(&amp;c-&gt;lock);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">decrement</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c)</span> &#123;</span><br><span class=\"line\">    pthread_mutex_lock(&amp;c-&gt;lock);</span><br><span class=\"line\">    c-&gt;value--;</span><br><span class=\"line\">    pthread_mutex_unlock(&amp;c-&gt;lock);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">get</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c)</span> &#123;</span><br><span class=\"line\">    pthread_mutex_lock(&amp;c-&gt;lock);</span><br><span class=\"line\">    <span class=\"type\">int</span> rc = c-&gt;value;</span><br><span class=\"line\">    pthread_mutex_unlock(&amp;c-&gt;lock);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> rc;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>Trade-Off</strong>:<br> While this implementation ensures correctness, performance suffers as the number of threads increases. The mutex introduces contention: threads must wait for others to release the lock.</p>\n<p><strong>Example Walkthrough (Figure 29.3)</strong></p>\n<ul>\n<li><p><strong>Setup</strong>: A system with 4 CPUs (<code>L1</code> to <code>L4</code> are local counters) and a global counter (<code>G</code>). Threshold <code>S</code> is set to 5.</p>\n</li>\n<li><p>Process:</p>\n<ol>\n<li>At <strong>Time 1</strong>, <code>L3</code> is incremented.</li>\n<li>At <strong>Time 2</strong>, <code>L1</code> and <code>L3</code> are incremented again.</li>\n<li>At <strong>Time 6</strong>, <code>L1</code> reaches the threshold (<code>5</code>), its value is transferred to <code>G</code>, and <code>L1</code> is reset to <code>0</code>.</li>\n<li>At <strong>Time 7</strong>, <code>L4</code> reaches the threshold, transfers its value to <code>G</code>, and resets.</li>\n</ol>\n<p><img src=\"/img/image-20241201235619901.png\" alt=\"image-20241201235619901\"></p>\n</li>\n</ul>\n<p><strong>3. Approximate Counters (Figure 29.4)</strong></p>\n<p>To address scalability issues, an <strong>approximate counter</strong> is introduced. Instead of using a single shared counter, <strong>local counters</strong> are maintained for each CPU/core, and updates to the global counter occur periodically.</p>\n<p><strong>Key Components:</strong></p>\n<ol>\n<li><strong>Local Counters</strong>: Each CPU has its own counter, which threads running on that CPU increment. This reduces contention since threads update different counters.</li>\n<li><strong>Global Counter</strong>: Periodically updated by transferring the values of local counters.</li>\n<li><strong>Threshold (S)</strong>: Determines when the local counter value is transferred to the global counter. A higher threshold improves performance but reduces accuracy.</li>\n</ol>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> __<span class=\"title\">counter_t</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> global;                     <span class=\"comment\">// global count</span></span><br><span class=\"line\">    <span class=\"type\">pthread_mutex_t</span> glock;          <span class=\"comment\">// global lock</span></span><br><span class=\"line\">    <span class=\"type\">int</span> local[NUMCPUS];             <span class=\"comment\">// per-CPU local counters</span></span><br><span class=\"line\">    <span class=\"type\">pthread_mutex_t</span> llock[NUMCPUS]; <span class=\"comment\">// locks for local counters</span></span><br><span class=\"line\">    <span class=\"type\">int</span> threshold;                  <span class=\"comment\">// threshold for global update</span></span><br><span class=\"line\">&#125; <span class=\"type\">counter_t</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">init</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c, <span class=\"type\">int</span> threshold)</span> &#123;</span><br><span class=\"line\">    c-&gt;threshold = threshold;</span><br><span class=\"line\">    c-&gt;global = <span class=\"number\">0</span>;</span><br><span class=\"line\">    pthread_mutex_init(&amp;c-&gt;glock, <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; NUMCPUS; i++) &#123;</span><br><span class=\"line\">        c-&gt;local[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        pthread_mutex_init(&amp;c-&gt;llock[i], <span class=\"literal\">NULL</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> <span class=\"title function_\">update</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c, <span class=\"type\">int</span> threadID, <span class=\"type\">int</span> amt)</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> cpu = threadID % NUMCPUS;  <span class=\"comment\">// map thread to CPU</span></span><br><span class=\"line\">    pthread_mutex_lock(&amp;c-&gt;llock[cpu]);</span><br><span class=\"line\">    c-&gt;local[cpu] += amt;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (c-&gt;local[cpu] &gt;= c-&gt;threshold) &#123;</span><br><span class=\"line\">        pthread_mutex_lock(&amp;c-&gt;glock);</span><br><span class=\"line\">        c-&gt;global += c-&gt;local[cpu];</span><br><span class=\"line\">        pthread_mutex_unlock(&amp;c-&gt;glock);</span><br><span class=\"line\">        c-&gt;local[cpu] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    pthread_mutex_unlock(&amp;c-&gt;llock[cpu]);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">get</span><span class=\"params\">(<span class=\"type\">counter_t</span> *c)</span> &#123;</span><br><span class=\"line\">    pthread_mutex_lock(&amp;c-&gt;glock);</span><br><span class=\"line\">    <span class=\"type\">int</span> val = c-&gt;global;</span><br><span class=\"line\">    pthread_mutex_unlock(&amp;c-&gt;glock);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> val; <span class=\"comment\">// approximate value</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>Performance Analysis</strong></p>\n<ol>\n<li><strong>Precise Counter (Lock-Based)</strong>:<ul>\n<li>Scales poorly with increasing threads.</li>\n<li>Even two threads lead to a massive slowdown due to lock contention.</li>\n</ul>\n</li>\n<li><strong>Approximate Counter</strong>:<ul>\n<li>Reduces contention by using local counters and periodic updates.</li>\n<li>Threshold SS determines performance and accuracy:<ul>\n<li>Lower SS: Higher accuracy, lower performance.</li>\n<li>Higher SS: Lower accuracy, higher performance.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p><strong>Example Trace (Threshold S=5S = 5):</strong></p>\n<ul>\n<li>Local counters increment until they reach 5.</li>\n<li>Once the threshold is hit, the local counter transfers its value to the global counter.</li>\n</ul>\n<p><img src=\"/img/image-20241201235715993.png\" alt=\"image-20241201235715993\"></p>\n<h2 id=\"29-2-Concurrent-Linked-Lists\"><a href=\"#29-2-Concurrent-Linked-Lists\" class=\"headerlink\" title=\"29.2 Concurrent Linked Lists\"></a>29.2 Concurrent Linked Lists</h2><ol>\n<li><strong>Basic Concurrent Linked List Design</strong></li>\n</ol>\n<ul>\n<li><p>The <strong>initial implementation</strong> (Figure 29.7) locks the entire list during critical sections like insertion and lookup.</p>\n<p><img src=\"/img/image-20241202000216558.png\" alt=\"image-20241202000216558\"></p>\n</li>\n<li><p>Exceptional paths (e.g., <code>malloc()</code> failure) complicate this design since the lock must be released before returning, increasing the likelihood of bugs.</p>\n</li>\n<li><p>A redesign (Figure 29.8) optimizes this by:</p>\n<ul>\n<li><p>Locking only the shared critical section during an insert.</p>\n</li>\n<li><p>Using a single return path for lookup to reduce error-prone unlock scenarios.</p>\n<p><img src=\"/img/image-20241202000247996.png\" alt=\"image-20241202000247996\"></p>\n</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><strong>Locking Strategies</strong></li>\n</ol>\n<ul>\n<li>The redesigned <code>List_Insert</code> defers locking until it reaches the critical section, assuming <code>malloc()</code> is thread-safe.</li>\n<li><code>List_Lookup</code> minimizes lock and unlock points by consolidating logic.</li>\n</ul>\n<ol>\n<li><strong>Scaling with Hand-Over-Hand Locking</strong></li>\n</ol>\n<ul>\n<li><p>Concept:</p>\n<p> Instead of a single lock for the list, assign a lock per node. As the traversal proceeds:</p>\n<ul>\n<li>Acquire the lock for the next node.</li>\n<li>Release the lock for the current node.</li>\n</ul>\n</li>\n<li><p>Challenges:</p>\n<ul>\n<li>High locking overhead during traversal.</li>\n<li>In practice, single-lock approaches often outperform due to simplicity and reduced overhead.</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><strong>Practical Considerations</strong></li>\n</ol>\n<ul>\n<li>Reducing complexity in control flow (e.g., common exit points) is critical in concurrent programming.</li>\n<li>Hand-over-hand locking might be better suited for edge cases or hybrid designs where locks are distributed strategically (e.g., every few nodes).</li>\n</ul>\n<h2 id=\"29-3-Concurrent-Queues\"><a href=\"#29-3-Concurrent-Queues\" class=\"headerlink\" title=\"29.3 Concurrent Queues\"></a>29.3 Concurrent Queues</h2><p><strong>Key Components</strong></p>\n<ol>\n<li><strong>Data Structure</strong></li>\n</ol>\n<ul>\n<li>Node (<code>node_t</code>):<ul>\n<li>Each node contains a value (<code>int value</code>) and a pointer to the next node (<code>node_t *next</code>).</li>\n</ul>\n</li>\n<li>Queue (<code>queue_t</code>):<ul>\n<li>Includes pointers to the head and tail nodes (<code>node_t *head, *tail</code>).</li>\n<li>Two separate locks:<ul>\n<li><code>head_lock</code> for dequeue operations.</li>\n<li><code>tail_lock</code> for enqueue operations.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><strong>Initialization (<code>Queue_Init</code>)</strong></li>\n</ol>\n<ul>\n<li>A <strong>dummy node</strong> is allocated during initialization, ensuring the head and tail always have a valid node to reference, even if the queue is empty.</li>\n<li>Both <code>head</code> and <code>tail</code> initially point to the dummy node.</li>\n</ul>\n<ol>\n<li><strong>Enqueue Operation (<code>Queue_Enqueue</code>)</strong></li>\n</ol>\n<ul>\n<li>Allocates a new node and initializes it with the value to be inserted.</li>\n<li>Acquires the <code>tail_lock</code> to safely update the <code>tail</code> pointer and attach the new node to the end of the queue.</li>\n<li>Releases the <code>tail_lock</code> after completing the operation.</li>\n</ul>\n<ol>\n<li><strong>Dequeue Operation (<code>Queue_Dequeue</code>)</strong></li>\n</ol>\n<ul>\n<li>Acquires the <code>head_lock</code> to safely remove the node at the front of the queue.</li>\n<li>If the queue is empty (<code>head-&gt;next == NULL</code>), releases the lock and returns an error code (<code>-1</code>).</li>\n<li>Updates the <code>head</code> pointer to the next node and retrieves its value.</li>\n<li>Frees the dummy node after updating <code>head</code>.</li>\n<li>Releases the <code>head_lock</code>.</li>\n</ul>\n<p><img src=\"/img/image-20241202002324236.png\" alt=\"image-20241202002324236\"></p>\n<p><strong>Concurrency Features</strong></p>\n<ol>\n<li><strong>Separation of Locks</strong>:<ul>\n<li>The use of separate locks for the head and tail enables concurrent enqueue and dequeue operations.</li>\n<li>Enqueue operations access only the <code>tail_lock</code>.</li>\n<li>Dequeue operations access only the <code>head_lock</code>.</li>\n<li>This separation reduces contention and improves throughput.</li>\n</ul>\n</li>\n<li><strong>Dummy Node</strong>:<ul>\n<li>Simplifies the management of head and tail pointers by ensuring they never become <code>NULL</code>.</li>\n<li>Avoids special-case logic for handling an empty queue.</li>\n</ul>\n</li>\n<li><strong>Thread Safety</strong>:<ul>\n<li>All critical sections (modifications to <code>head</code> and <code>tail</code>) are protected by the corresponding locks, ensuring thread safety.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"29-4-Concurrent-Hash-Table\"><a href=\"#29-4-Concurrent-Hash-Table\" class=\"headerlink\" title=\"29.4 Concurrent Hash Table\"></a>29.4 Concurrent Hash Table</h2><p><strong>Key Components</strong></p>\n<ol>\n<li><strong>Data Structure</strong></li>\n</ol>\n<ul>\n<li>Hash Table (<code>hash_t</code>):<ul>\n<li>Consists of an array of buckets (<code>list_t lists[BUCKETS]</code>), each representing a linked list.</li>\n<li>Each bucket is initialized with its own lock for thread-safe operations.</li>\n</ul>\n</li>\n<li>Bucket Count (<code>BUCKETS</code>):<ul>\n<li>A fixed number of buckets (101 in this example).</li>\n<li>Keys are hashed to determine the appropriate bucket (<code>key % BUCKETS</code>).</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><strong>Initialization (<code>Hash_Init</code>)</strong></li>\n</ol>\n<ul>\n<li>Iterates through all buckets.</li>\n<li>Initializes each linked list (<code>List_Init(&amp;H-&gt;lists[i])</code>).</li>\n</ul>\n<ol>\n<li><strong>Insert Operation (<code>Hash_Insert</code>)</strong></li>\n</ol>\n<ul>\n<li>Computes the bucket index for the key (<code>key % BUCKETS</code>).</li>\n<li>Delegates the insertion to the list’s <code>List_Insert()</code> function.</li>\n</ul>\n<ol>\n<li><strong>Lookup Operation (<code>Hash_Lookup</code>)</strong></li>\n</ol>\n<ul>\n<li>Computes the bucket index for the key (<code>key % BUCKETS</code>).</li>\n<li>Delegates the search to the list’s <code>List_Lookup()</code> function.</li>\n</ul>\n<p><img src=\"/img/image-20241202002549213.png\" alt=\"image-20241202002549213\"></p>\n<p><strong>Concurrency Features</strong></p>\n<ol>\n<li><strong>Lock per Bucket</strong>:<ul>\n<li>Each bucket is a separate linked list with its own lock.</li>\n<li>Operations on different buckets can occur concurrently without contention.</li>\n</ul>\n</li>\n<li><strong>Scalability</strong>:<ul>\n<li>Multiple threads can insert or look up keys in different buckets simultaneously.</li>\n<li>Compared to a single lock for the entire hash table, this approach significantly reduces contention.</li>\n</ul>\n</li>\n<li><strong>Thread-Safe List Operations</strong>:<ul>\n<li>The <code>List_Insert()</code> and <code>List_Lookup()</code> functions for individual buckets are assumed to be thread-safe, ensuring consistency for operations within a bucket.</li>\n</ul>\n</li>\n</ol>\n<p><strong>Performance Analysis</strong></p>\n<p><strong>Figure 29.11 Results (Hypothetical Summary)</strong>:</p>\n<ol>\n<li><strong>Concurrent Updates</strong>:<ul>\n<li>Performance increases linearly with the number of threads for the hash table due to its fine-grained locking.</li>\n<li>Even with thousands of updates, the system scales well as most operations happen in independent buckets.</li>\n</ul>\n</li>\n<li><strong>Comparison with a Single-Lock Linked List</strong>:<ul>\n<li>The linked list suffers from contention as all operations are serialized due to a single lock.</li>\n<li>The performance of the linked list plateaus and degrades with an increasing number of threads.</li>\n</ul>\n</li>\n</ol>\n<p><img src=\"/img/image-20241202002603708.png\" alt=\"image-20241202002603708\"></p>\n<p><strong>Advantages</strong></p>\n<ol>\n<li><strong>Fine-Grained Locking</strong>:<ul>\n<li>Improves throughput by enabling parallelism across buckets.</li>\n<li>Avoids global contention, allowing better CPU utilization.</li>\n</ul>\n</li>\n<li><strong>Simplicity</strong>:<ul>\n<li>Straightforward design based on independent linked lists for buckets.</li>\n<li>Reuses the thread-safe list functions from previous implementations.</li>\n</ul>\n</li>\n<li><strong>Good Scalability</strong>:<ul>\n<li>As shown in Figure 29.11, the hash table’s performance scales well with increased concurrency.</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Chapter-30-Condition-Variables\"><a href=\"#Chapter-30-Condition-Variables\" class=\"headerlink\" title=\"Chapter 30: Condition Variables\"></a>Chapter 30: Condition Variables</h1>","feature":true,"text":"操作系统导论（中文版） | ostep-chinese Chapter 4: ProcessesOS provide the illusion of a nea...","permalink":"/post/OStep-note","photos":[],"count_time":{"symbolsCount":"197k","symbolsTime":"2:59"},"categories":[{"name":"理论","slug":"理论","count":4,"path":"api/categories/理论.json"}],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-4-Processes\"><span class=\"toc-text\">Chapter 4: Processes</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-1-Definition-of-a-Process\"><span class=\"toc-text\">4.1 Definition of a Process</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-2-Process-API\"><span class=\"toc-text\">4.2 Process API</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-3-Process-Creation\"><span class=\"toc-text\">4.3 Process Creation</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-4-Process-States\"><span class=\"toc-text\">4.4 Process States</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#4-5-Data-Structures\"><span class=\"toc-text\">4.5 Data Structures</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-5-Process-API\"><span class=\"toc-text\">Chapter 5: Process API</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-1-The-fork-System-Call\"><span class=\"toc-text\">5.1 The fork() System Call</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-2-The-wait-System-Call\"><span class=\"toc-text\">5.2 The wait() System Call</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-3-The-exec-System-Call\"><span class=\"toc-text\">5.3 The exec() System Call</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-4-Why-fork-and-exec-Motivating-the-API\"><span class=\"toc-text\">5.4 Why fork() and exec()? Motivating the API</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#5-5-Process-Control-and-Users-in-UNIX-Systems\"><span class=\"toc-text\">5.5 Process Control and Users in UNIX Systems</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-6-Limited-Direct-Execution\"><span class=\"toc-text\">Chapter 6:  Limited Direct Execution</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-1-Limited-Direct-Execution\"><span class=\"toc-text\">6.1 Limited Direct Execution</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-2-Problem-1-Restricted-Operations\"><span class=\"toc-text\">6.2 Problem #1: Restricted Operations</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#6-3-Problem-2-Switching-Between-Processes\"><span class=\"toc-text\">6.3 Problem #2: Switching Between Processes</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-7-Introduction-of-Scheduling\"><span class=\"toc-text\">Chapter 7:  Introduction of Scheduling</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-1-Workload-Assumptions\"><span class=\"toc-text\">7.1 Workload Assumptions</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-2-Scheduling-Metrics\"><span class=\"toc-text\">7.2 Scheduling Metrics</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-3-First-In-First-Out-FIFO\"><span class=\"toc-text\">7.3 First In, First Out (FIFO)</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-4-Shortest-Job-First-SJF\"><span class=\"toc-text\">7.4 Shortest Job First (SJF)</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-5-Shortest-Time-to-Completion-First-STCF\"><span class=\"toc-text\">7.5 Shortest Time-to-Completion First (STCF)</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-6-A-New-Metric-Response-Time\"><span class=\"toc-text\">7.6 A New Metric: Response Time</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-7-Round-Robin\"><span class=\"toc-text\">7.7 Round Robin</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#7-8-Incorporating-I-O\"><span class=\"toc-text\">7.8 Incorporating I&#x2F;O</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-8-The-Multi-Level-Feedback-Queue-in-Scheduling\"><span class=\"toc-text\">Chapter 8: The Multi-Level Feedback Queue in Scheduling</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#8-1-MLFQ-Basic-Rules\"><span class=\"toc-text\">8.1 MLFQ: Basic Rules</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#8-2-Attempt-1-How-To-Change-Priority\"><span class=\"toc-text\">8.2 Attempt #1: How To Change Priority</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#8-3-Attempt-2-The-Priority-Boost\"><span class=\"toc-text\">8.3 Attempt #2: The Priority Boost</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#8-4-Attempt-3-Better-Accounting\"><span class=\"toc-text\">8.4 Attempt #3: Better Accounting</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#8-5-Tuning-MLFQ-And-Other-Issues\"><span class=\"toc-text\">8.5 Tuning MLFQ And Other Issues</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-9-Proportional-Share\"><span class=\"toc-text\">Chapter 9: Proportional Share</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#9-1-Basic-Concept-Tickets-Represent-Your-Share\"><span class=\"toc-text\">9.1 Basic Concept: Tickets Represent Your Share</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#9-2-Ticket-Mechanisms\"><span class=\"toc-text\">9.2 Ticket Mechanisms</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#9-3-Implementation\"><span class=\"toc-text\">9.3 Implementation</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#9-4-An-Example\"><span class=\"toc-text\">9.4 An Example</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#9-5-How-To-Assign-Tickets\"><span class=\"toc-text\">9.5 How To Assign Tickets?</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#9-6-Stride-Scheduling\"><span class=\"toc-text\">9.6 Stride Scheduling</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-10-Multiprocessor-Scheduling-Advanced\"><span class=\"toc-text\">Chapter 10: Multiprocessor Scheduling (Advanced)</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#10-2-Don%E2%80%99t-Forget-Synchronization\"><span class=\"toc-text\">10.2 Don’t Forget Synchronization</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#10-3-One-Final-Issue-Cache-Affinity\"><span class=\"toc-text\">10.3 One Final Issue: Cache Affinity</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#10-4-Single-Queue-Scheduling\"><span class=\"toc-text\">10.4 Single-Queue Scheduling</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#10-5-Multi-Queue-Scheduling\"><span class=\"toc-text\">10.5 Multi-Queue Scheduling</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-13-Address-Spaces\"><span class=\"toc-text\">Chapter 13: Address Spaces</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#13-2-Multiprogramming-and-Time-Sharing\"><span class=\"toc-text\">13.2 Multiprogramming and Time Sharing</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#13-3-The-Address-Space\"><span class=\"toc-text\">13.3 The Address Space</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#13-4-Goals\"><span class=\"toc-text\">13.4 Goals</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-14-Memory-API\"><span class=\"toc-text\">Chapter 14: Memory API</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#14-1-Types-of-Memory\"><span class=\"toc-text\">14.1 Types of Memory</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#14-2-The-malloc-Call\"><span class=\"toc-text\">14.2 The malloc() Call</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#14-3-The-free-Call\"><span class=\"toc-text\">14.3 The free() Call</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#14-4-Common-Errors\"><span class=\"toc-text\">14.4 Common Errors</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-15-Address-Translation\"><span class=\"toc-text\">Chapter 15: Address Translation</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#15-1-Assumptions\"><span class=\"toc-text\">15.1 Assumptions</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#15-2-An-Example\"><span class=\"toc-text\">15.2 An Example</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#15-3-Dynamic-Hardware-based-Relocation\"><span class=\"toc-text\">15.3 Dynamic (Hardware-based) Relocation</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#15-4-Hardware-Support-A-Summary\"><span class=\"toc-text\">15.4 Hardware Support: A Summary</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#15-5-Operating-System-Issues\"><span class=\"toc-text\">15.5 Operating System Issues</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-16-Segmentation\"><span class=\"toc-text\">Chapter 16: Segmentation</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#16-1-Segmentation-Generalized-Base-Bounds\"><span class=\"toc-text\">16.1 Segmentation: Generalized Base&#x2F;Bounds</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#16-2-Which-Segment-Are-We-Referring-To\"><span class=\"toc-text\">16.2 Which Segment Are We Referring To?</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#16-3-What-About-The-Stack\"><span class=\"toc-text\">16.3 What About The Stack?</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#16-5-Fine-grained-vs-Coarse-grained-Segmentation\"><span class=\"toc-text\">16.5 Fine-grained vs. Coarse-grained Segmentation</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#16-6-OS-Support\"><span class=\"toc-text\">16.6 OS Support</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-17-Free-Space-Management\"><span class=\"toc-text\">Chapter 17: Free-Space Management</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#17-1-Assumptions\"><span class=\"toc-text\">17.1 Assumptions</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#17-2-Low-level-Mechanisms\"><span class=\"toc-text\">17.2 Low-level Mechanisms</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#17-3-Basic-Strategies\"><span class=\"toc-text\">17.3 Basic Strategies</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#17-4-Other-Approaches\"><span class=\"toc-text\">17.4 Other Approaches</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-18-Introduction-of-Paging\"><span class=\"toc-text\">Chapter 18: Introduction of Paging</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#18-1-A-Simple-Example-And-Overview\"><span class=\"toc-text\">18.1 A Simple Example And Overview</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#18-2-Where-Are-Page-Tables-Stored\"><span class=\"toc-text\">18.2 Where Are Page Tables Stored</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#18-3-What%E2%80%99s-Actually-In-The-Page-Table\"><span class=\"toc-text\">18.3 What’s Actually In The Page Table</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#18-4-Paging-Also-Too-Slow\"><span class=\"toc-text\">18.4 Paging: Also Too Slow</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#18-5-A-Memory-Trace\"><span class=\"toc-text\">18.5 A Memory Trace</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-19-Paging-Faster-Translations-TLBs\"><span class=\"toc-text\">Chapter 19: Paging: Faster Translations (TLBs)</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#19-1-TLB-Basic-Algorithm\"><span class=\"toc-text\">19.1 TLB Basic Algorithm</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#19-2-Example-Accessing-an-Array-with-TLB\"><span class=\"toc-text\">19.2 Example: Accessing an Array with TLB</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#19-3-Who-Handles-the-TLB-Miss\"><span class=\"toc-text\">19.3 Who Handles the TLB Miss?</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#19-4-TLB-Contents-What%E2%80%99s-In-There\"><span class=\"toc-text\">19.4 TLB Contents: What’s In There?</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#19-5-TLB-Issue-Context-Switches\"><span class=\"toc-text\">19.5 TLB Issue: Context Switches</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#19-6-TLB-Replacement-Policy\"><span class=\"toc-text\">19.6 TLB Replacement Policy</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#19-7-Real-TLB-Entry-The-MIPS-R4000\"><span class=\"toc-text\">19.7 Real TLB Entry: The MIPS R4000</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-20-Paging-Smaller-Tables\"><span class=\"toc-text\">Chapter 20: Paging: Smaller Tables</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#20-1-Simple-Solution-Bigger-Pages\"><span class=\"toc-text\">20.1 Simple Solution: Bigger Pages</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#20-2-Hybrid-Approach-Paging-and-Segments\"><span class=\"toc-text\">20.2 Hybrid Approach: Paging and Segments</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#20-3-Multi-level-Page-Tables\"><span class=\"toc-text\">20.3 Multi-level Page Tables</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-21-Beyond-Physical-Memory-Mechanisms\"><span class=\"toc-text\">Chapter 21: Beyond Physical Memory: Mechanisms</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#21-1-Swap-Space\"><span class=\"toc-text\">21.1 Swap Space</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#21-2-The-Present-Bit\"><span class=\"toc-text\">21.2 The Present Bit</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#21-3-The-Page-Fault\"><span class=\"toc-text\">21.3 The Page Fault</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#21-4-What-Happens-If-Memory-Is-Full\"><span class=\"toc-text\">21.4 What Happens If Memory Is Full?</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#21-5-Page-Fault-Control-Flow\"><span class=\"toc-text\">21.5 Page Fault Control Flow</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-22-Beyond-Physical-Memory-Policies\"><span class=\"toc-text\">Chapter 22:  Beyond Physical Memory: Policies</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#22-1-Cache-Management\"><span class=\"toc-text\">22.1 Cache Management</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#22-2-The-Optimal-Replacement-Policy\"><span class=\"toc-text\">22.2 The Optimal Replacement Policy</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#22-3-FIFO-First-In-First-Out\"><span class=\"toc-text\">22.3: FIFO (First-In, First-Out)</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#22-4-Random-Replacement\"><span class=\"toc-text\">22.4: Random Replacement</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#22-5-The-LRU-Policy\"><span class=\"toc-text\">22.5: The LRU Policy</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#22-6-Workload-Examples-and-Policy-Performance\"><span class=\"toc-text\">22.6: Workload Examples and Policy Performance</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#22-7-Implementing-Historical-Algorithms\"><span class=\"toc-text\">22.7: Implementing Historical Algorithms</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#22-8-Approximating-LRU\"><span class=\"toc-text\">22.8: Approximating LRU</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#22-9-Considering-Dirty-Pages\"><span class=\"toc-text\">22.9: Considering Dirty Pages</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-26-Introduction-of-Concurrency\"><span class=\"toc-text\">Chapter 26: Introduction of Concurrency</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#26-1-Why-Use-Threads\"><span class=\"toc-text\">26.1: Why Use Threads?</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#26-2-An-Example-Thread-Creation\"><span class=\"toc-text\">26.2: An Example: Thread Creation</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#26-3-Why-It-Gets-Worse-Shared-Data\"><span class=\"toc-text\">26.3: Why It Gets Worse - Shared Data</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#26-4-The-Heart-of-the-Problem-Uncontrolled-Scheduling\"><span class=\"toc-text\">26.4 The Heart of the Problem: Uncontrolled Scheduling</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#26-5-The-Wish-for-Atomicity\"><span class=\"toc-text\">26.5 The Wish for Atomicity</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-27-Thread-API\"><span class=\"toc-text\">Chapter 27: Thread API</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#27-1-Thread-Creation\"><span class=\"toc-text\">27.1 Thread Creation</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#27-2-Thread-Completion\"><span class=\"toc-text\">27.2 Thread Completion</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#27-3-Locks\"><span class=\"toc-text\">27.3 Locks</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#27-4-Condition-Variables\"><span class=\"toc-text\">27.4 Condition Variables</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-28-Lock\"><span class=\"toc-text\">Chapter 28: Lock</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-1-Locks-The-Basic-Idea\"><span class=\"toc-text\">28.1 Locks: The Basic Idea</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-2-Pthread-Locks\"><span class=\"toc-text\">28.2 Pthread Locks</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-3-Building-A-Lock\"><span class=\"toc-text\">28.3 Building A Lock</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-4-Evaluating-Locks\"><span class=\"toc-text\">28.4 Evaluating Locks</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-5-Controlling-Interrupts\"><span class=\"toc-text\">28.5 Controlling Interrupts</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-6-A-Failed-Attempt-Just-Using-Loads-Stores\"><span class=\"toc-text\">28.6 A Failed Attempt: Just Using Loads&#x2F;Stores</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-7-Building-Working-Spin-Locks-with-Test-And-Set\"><span class=\"toc-text\">28.7 Building Working Spin Locks with Test-And-Set</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-8-Evaluating-Spin-Locks\"><span class=\"toc-text\">28.8 Evaluating Spin Locks</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-9-Compare-And-Swap\"><span class=\"toc-text\">28.9 Compare-And-Swap</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-10-Load-Linked-and-Store-Conditional\"><span class=\"toc-text\">28.10: Load-Linked and Store-Conditional</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-11-Fetch-and-Add\"><span class=\"toc-text\">28.11: Fetch-and-Add</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-12-Too-Much-Spinning\"><span class=\"toc-text\">28.12: Too Much Spinning</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-13-Yielding-to-Reduce-Spinning\"><span class=\"toc-text\">28.13: Yielding to Reduce Spinning</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#28-14-Sleeping-Instead-of-Spinning\"><span class=\"toc-text\">28.14: Sleeping Instead of Spinning</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-29%EF%BC%9ALock-based-Concurrent-Data-Structures\"><span class=\"toc-text\">Chapter 29：Lock-based Concurrent Data Structures</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#29-1-Concurrent-Counters\"><span class=\"toc-text\">29.1 Concurrent Counters</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#29-2-Concurrent-Linked-Lists\"><span class=\"toc-text\">29.2 Concurrent Linked Lists</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#29-3-Concurrent-Queues\"><span class=\"toc-text\">29.3 Concurrent Queues</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#29-4-Concurrent-Hash-Table\"><span class=\"toc-text\">29.4 Concurrent Hash Table</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Chapter-30-Condition-Variables\"><span class=\"toc-text\">Chapter 30: Condition Variables</span></a></li></ol>","author":{"name":"Gueason","slug":"blog-author","avatar":"https://pic.quanjing.com/60/2a/QJ6771797507.jpg@!350h","link":"/","description":"小白，在成为“牛码”的路上","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"Script","uid":"f1f442c5eab80c7f5411f92281f65439","slug":"Script","date":"2024-11-01T00:00:00.000Z","updated":"2024-11-21T06:07:50.265Z","comments":true,"path":"api/articles/Script.json","keywords":null,"cover":null,"text":"","permalink":"/post/Script","photos":[],"count_time":{"symbolsCount":0,"symbolsTime":"1 mins."},"categories":[{"name":"其它","slug":"其它","count":1,"path":"api/categories/其它.json"}],"tags":[],"author":{"name":"Gueason","slug":"blog-author","avatar":"https://pic.quanjing.com/60/2a/QJ6771797507.jpg@!350h","link":"/","description":"小白，在成为“牛码”的路上","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}