{"title":"强化学习","uid":"4d1e3a4a4afff0b0d31e07df9ad79915","slug":"强化学习","date":"2025-03-01T00:00:00.000Z","updated":"2025-02-11T02:40:51.460Z","comments":true,"path":"api/articles/强化学习.json","keywords":null,"cover":[],"content":"<h1 id=\"一、基本概念\"><a href=\"#一、基本概念\" class=\"headerlink\" title=\"一、基本概念\"></a>一、基本概念</h1><h2 id=\"1-1术语\"><a href=\"#1-1术语\" class=\"headerlink\" title=\"1.1术语\"></a>1.1术语</h2><p>强化学习的框架主要由一个四元组 ( (S, A, R, P) ) 组成组成：</p>\n<p><strong>状态（State）：</strong>表示所有可能状态的集合，反映环境或系统当前的情况。</p>\n<p><strong>动作（Action）：</strong>智能体在特定状态下可以采取的操作，在一个应用里面执行动作的就是agent。</p>\n<p><strong>奖励（Reward）：</strong>一个数值反馈，用于量化智能体采取某一动作后环境的反应。( R(s, a, s’) ) 表示在状态 ( s ) 下采取动作 ( a ) 并转移到状态 ( s’ ) 时所获得的即时奖励。</p>\n<p><strong>策略（Policy）：</strong>一个映射函数，指导智能体在特定状态下应采取哪一动作。( P(s’ | s, a) ) 表示在状态 ( s ) 下采取动作 ( a ) 转移到状态 ( s’ ) 的概率。</p>\n<ul>\n<li><strong>目标策略（target policy）</strong>：智能体要学习的策略</li>\n<li><strong>行为策略（behavior policy）</strong>：智能体与环境交互的策略，即用于生成行为的策略</li>\n</ul>\n<p>Off-policy 是指行为策略和目标策略不是同一个策略，即智能体可以通过离线学习自己或别人的策略，来指导自己的行为；相反，on-policy 的行为策略和目标策略是同一个策略。</p>\n<p>这四个元素共同构成了马尔可夫决策过程（Markov Decision Process, MDP），这是强化学习最核心的数学模型。<br><img src=\"../img/2081af44d561d100a28510ce089fa638-17391841046251.png\" alt=\"file\"></p>\n<p><strong>回报(Return)</strong></p>\n<p>回报（cumulated future reward），一般表示为U，定义为</p>\n<p><img src=\"../img/image-20250210211525349.png\" alt=\"image-20250210211525349\"></p>\n<p>其中Rt表示第t时刻的奖励，agent的目标就是让Return最大化。</p>\n<p>未来的奖励不如现在等值的奖励那么好（比如一年后给100块不如现在就给），所以Rt+1的权重应该小于Rt。因此，强化学习通常用<strong>discounted return</strong>（折扣回报，又称cumulative discounted future reward），取γ为discount rate（折扣率），γ∈(0,1]，则有</p>\n<p><img src=\"../img/image-20250210211626770.png\" alt=\"image-20250210211626770\"></p>\n<p><strong>价值函数(Value Function)</strong></p>\n<ul>\n<li><strong>状态价值函数(State-value Function)</strong>：用来度量给定策略π的情况下，当前状态st的好坏程度。</li>\n<li><strong>动作价值函数(Action-value Function)</strong>：用来度量给定状态st和策略π的情况下，采用动作at的好坏程度。</li>\n</ul>\n<p><img src=\"../img/v2-01f2297f8677918d60f3976a2c5335d1_1440w.jpg\" alt=\"img\"></p>\n<h2 id=\"1-2随机性\"><a href=\"#1-2随机性\" class=\"headerlink\" title=\"1.2随机性\"></a>1.2随机性</h2><p><strong>动作随机</strong><br>  第一个随机性是根据动作来的，因为动作函数是根据policy函数π随机抽样得到的，我们用policy函数来控制agent，给定当前状态 S， agent的动作A是按照policy函数输出的概率来随机抽样，比如当前观测到的状态s，policy函数会告诉我们每个动作的概率有多大，agent有可能做其所存在的任何一种动作（向左，右，上）但这些动作的概率有大有小。</p>\n<p><strong>状态转移的随机性</strong><br>  假定agent做出了向上跳的动作，环境就要生成下一个状态S’，这个状态S’具有随机性，环境用状态转移函数p算出概率，然后用随机抽样得到下一个状态S^’，比如说下一个状态有两种可能，根据状态转移函数的计算，一种状态的概率是0.8，另外一种状态的概率是0.2，这两个都有可能成为下一种状态，系统会做一个随机抽样来决定下一个状态是什么。\n  </p>\n<h1 id=\"二、基于价值学习-Value-Based-Reinforcement-Learning-DQN\"><a href=\"#二、基于价值学习-Value-Based-Reinforcement-Learning-DQN\" class=\"headerlink\" title=\"二、基于价值学习 Value-Based Reinforcement Learning-DQN\"></a>二、基于价值学习 Value-Based Reinforcement Learning-DQN</h1><p><strong>Deep Q-Network</strong> </p>\n<p>Q-learning 是一种 off-policy TD 方法。</p>\n<p>TD 全称 Temporal Difference，中文名称：时序差分。</p>\n<p>最简单的 TD 方法是 TD(0)，每走一步进行更新，表达式如下：</p>\n<p><img src=\"../img/image-20250210230001497.png\" alt=\"image-20250210230001497\"></p>\n<p><a href=\"https://blog.csdn.net/november_chopin/article/details/107912720\">强化学习 7—— 一文读懂 Deep Q-Learning（DQN）算法_deep q learning-CSDN博客</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/108286901\">【强化学习】Deep Q-Network (DQN) - 知乎</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/365814943\">强化学习入门笔记——Q-learning从理论到实践 - 知乎</a></p>\n<h1 id=\"三、基于策略学习Policy-based-Reinforcement-Learning\"><a href=\"#三、基于策略学习Policy-based-Reinforcement-Learning\" class=\"headerlink\" title=\"三、基于策略学习Policy-based Reinforcement Learning\"></a>三、基于策略学习Policy-based Reinforcement Learning</h1><p><a href=\"https://zhuanlan.zhihu.com/p/648794033\">强化学习实战05 | 详解Policy-based RL：理论、案例及编程实战 - 知乎</a></p>\n","feature":true,"text":"一、基本概念1.1术语强化学习的框架主要由一个四元组 ( (S, A, R, P) ) 组成组成： 状态（State）：表示所有可能状态的集合，反映环境或系统当...","permalink":"/post/强化学习","photos":[],"count_time":{"symbolsCount":"1.7k","symbolsTime":"2 mins."},"categories":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","count":1,"path":"api/categories/Reinforcement-Learning.json"}],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%80%E3%80%81%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5\"><span class=\"toc-text\">一、基本概念</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-1%E6%9C%AF%E8%AF%AD\"><span class=\"toc-text\">1.1术语</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-2%E9%9A%8F%E6%9C%BA%E6%80%A7\"><span class=\"toc-text\">1.2随机性</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BA%8C%E3%80%81%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0-Value-Based-Reinforcement-Learning-DQN\"><span class=\"toc-text\">二、基于价值学习 Value-Based Reinforcement Learning-DQN</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%B8%89%E3%80%81%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0Policy-based-Reinforcement-Learning\"><span class=\"toc-text\">三、基于策略学习Policy-based Reinforcement Learning</span></a></li></ol>","author":{"name":"Gueason","slug":"blog-author","avatar":"https://pic.quanjing.com/60/2a/QJ6771797507.jpg@!350h","link":"/","description":"小白，在成为“牛码”的路上","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"OStep note","uid":"cc3ee0f3aa4d555f4b2c7e6d5e81b1c4","slug":"OStep-note","date":"2024-11-01T00:00:00.000Z","updated":"2024-12-01T16:28:18.479Z","comments":true,"path":"api/articles/OStep-note.json","keywords":null,"cover":[],"text":"操作系统导论（中文版） | ostep-chinese Chapter 4: ProcessesOS provide the illusion of a nea...","permalink":"/post/OStep-note","photos":[],"count_time":{"symbolsCount":"197k","symbolsTime":"2:59"},"categories":[{"name":"理论","slug":"理论","count":4,"path":"api/categories/理论.json"}],"tags":[],"author":{"name":"Gueason","slug":"blog-author","avatar":"https://pic.quanjing.com/60/2a/QJ6771797507.jpg@!350h","link":"/","description":"小白，在成为“牛码”的路上","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}