[{"id":"4d1e3a4a4afff0b0d31e07df9ad79915","title":"强化学习","content":"一、基本概念1.1术语强化学习的框架主要由一个四元组 ( (S, A, R, P) ) 组成组成：\n状态（State）：表示所有可能状态的集合，反映环境或系统当前的情况。\n动作（Action）：智能体在特定状态下可以采取的操作，在一个应用里面执行动作的就是agent。\n奖励（Reward）：一个数值反馈，用于量化智能体采取某一动作后环境的反应。( R(s, a, s’) ) 表示在状态 ( s ) 下采取动作 ( a ) 并转移到状态 ( s’ ) 时所获得的即时奖励。\n策略（Policy）：一个映射函数，指导智能体在特定状态下应采取哪一动作。( P(s’ | s, a) ) 表示在状态 ( s ) 下采取动作 ( a ) 转移到状态 ( s’ ) 的概率。\n\n目标策略（target policy）：智能体要学习的策略\n行为策略（behavior policy）：智能体与环境交互的策略，即用于生成行为的策略\n\nOff-policy 是指行为策略和目标策略不是同一个策略，即智能体可以通过离线学习自己或别人的策略，来指导自己的行为；相反，on-policy 的行为策略和目标策略是同一个策略。\n这四个元素共同构成了马尔可夫决策过程（Markov Decision Process, MDP），这是强化学习最核心的数学模型。\n回报(Return)\n回报（cumulated future reward），一般表示为U，定义为\n\n其中Rt表示第t时刻的奖励，agent的目标就是让Return最大化。\n未来的奖励不如现在等值的奖励那么好（比如一年后给100块不如现在就给），所以Rt+1的权重应该小于Rt。因此，强化学习通常用discounted return（折扣回报，又称cumulative discounted future reward），取γ为discount rate（折扣率），γ∈(0,1]，则有\n\n价值函数(Value Function)\n\n状态价值函数(State-value Function)：用来度量给定策略π的情况下，当前状态st的好坏程度。\n动作价值函数(Action-value Function)：用来度量给定状态st和策略π的情况下，采用动作at的好坏程度。\n\n\n1.2随机性动作随机  第一个随机性是根据动作来的，因为动作函数是根据policy函数π随机抽样得到的，我们用policy函数来控制agent，给定当前状态 S， agent的动作A是按照policy函数输出的概率来随机抽样，比如当前观测到的状态s，policy函数会告诉我们每个动作的概率有多大，agent有可能做其所存在的任何一种动作（向左，右，上）但这些动作的概率有大有小。\n状态转移的随机性  假定agent做出了向上跳的动作，环境就要生成下一个状态S’，这个状态S’具有随机性，环境用状态转移函数p算出概率，然后用随机抽样得到下一个状态S^’，比如说下一个状态有两种可能，根据状态转移函数的计算，一种状态的概率是0.8，另外一种状态的概率是0.2，这两个都有可能成为下一种状态，系统会做一个随机抽样来决定下一个状态是什么。\n  \n二、基于价值学习 Value-Based Reinforcement Learning-DQNDeep Q-Network \nQ-learning 是一种 off-policy TD 方法。\nTD 全称 Temporal Difference，中文名称：时序差分。\n最简单的 TD 方法是 TD(0)，每走一步进行更新，表达式如下：\n\n强化学习 7—— 一文读懂 Deep Q-Learning（DQN）算法_deep q learning-CSDN博客\n【强化学习】Deep Q-Network (DQN) - 知乎\n强化学习入门笔记——Q-learning从理论到实践 - 知乎\n三、基于策略学习Policy-based Reinforcement Learning强化学习实战05 | 详解Policy-based RL：理论、案例及编程实战 - 知乎\n","slug":"强化学习","date":"2025-03-01T00:00:00.000Z","categories_index":"Reinforcement Learning","tags_index":"","author_index":"Gueason"},{"id":"cc3ee0f3aa4d555f4b2c7e6d5e81b1c4","title":"OStep note","content":"操作系统导论（中文版） | ostep-chinese\nChapter 4: ProcessesOS provide the illusion of a nearly-endless supply of said CPUs by virtualizing the CPU, the OS can promote the illusion that many virtual CPUs exist when in fact there is only one physical CPU (or a few).\nThis basic technique, known as time sharing of the CPU, allows users to run as many concurrent processes as they would like; the potential cost is performance, as each will run more slowly if the CPU(s) must be shared.\nIn many operating systems, a common design paradigm is to separate high-level policies from their low-level mechanisms, which allows one easily to change policies without having to rethink the mechanism and is thus a form of modularity, a general software design principle.\n4.1 Definition of a Process\nA process is a program in execution.\nIt serves as the foundation for resource management in an operating system.\n\ncomponent of machine state that comprises a process:\nMemory (Address Space)\n\nContains the instructions of the running program.\nIncludes data that the program reads or writes during execution.\nDefines the range of memory a process can address.\n\nRegisters\n\nRegisters are used to execute instructions.\nSpecial registers include:\nProgram Counter (PC): Indicates the next instruction to execute.\nStack Pointer (SP) and Frame Pointer: Manage function parameters, local variables, and return addresses.\n\n\n\nPersistent Storage (I/O Information)\n\nIncludes details about files and storage devices the process interacts with, such as open files.\n\n4.2 Process APIThe Process API defines the interface provided by the operating system to manage processes. These functionalities are critical for process lifecycle and control, and they exist in some form on all modern operating systems.\nKey Components of the Process API:\n\nCreate\nFunction: Creates new processes.\nThe OS is responsible for initializing a new process and associating it with the requested program.\n\n\nDestroy\nFunction: Forcefully terminates processes.\nUsage:\nEnds processes that have completed execution naturally.\nHalts runaway or misbehaving processes via user command \n\n\n\n\nWait\nFunction: Allows a process to wait until another process finishes execution.\nUsage: Useful for synchronization between processes.\n\n\nMiscellaneous Control\nSuspend and Resume: Temporarily stop and restart processes.\nOther controls, depending on the OS, may include priority changes or signaling.\n\n\nStatus\nFunction: Retrieves process-related information.\nExamples:\nHow long the process has been running.\nCurrent process state (e.g., running, waiting, blocked).\n\n\n\n\n\n\n4.3 Process CreationSteps of Process Creation:\n\nLoading Code and Static Data\nPrograms reside on disk in an executable format.\nThe OS loads the program’s code and static data (e.g., initialized variables) into the process’s address space in memory.\nEager loading: Older systems load everything at once.\nLazy loading: Modern systems load code and data only as needed during execution (e.g., via paging).\n\n\n\n\nAllocating Runtime Stack\nThe stack is used for:\nLocal variables.\nFunction parameters.\nReturn addresses.\n\n\nThe OS initializes the stack with arguments to main() (e.g., argc and argv in C programs).\n\n\nAllocating Heap Memory\nThe heap is used for dynamically allocated data (e.g., via malloc() in C) and data structures such as linked lists, hash tables, trees.\nInitially small, it grows as the program requests more memory.\nThe OS manages heap expansion during runtime.\n\n\nSetting Up Input/Output (I/O)\nIn UNIX-like systems, every process starts with three default file descriptors:\nStandard input (stdin).\nStandard output (stdout).\nStandard error (stderr).\n\n\nThese descriptors allow programs to read from the terminal and print output easily.\n\n\nStarting Execution\nAfter loading and initialization, the OS starts execution by transferring control of the CPU to the process.\nThe process begins execution at its entry point, typically the main() function.\n\n\n\n4.4 Process StatesProcesses in an operating system exist in distinct states, which represent the progress of their execution and their interaction with system resources. At any given moment, a process can be in one of the following primary states:\n1. Running\n\nThe process is actively executing instructions on the CPU.\nThe OS scheduler allocates CPU time to the process, allowing it to progress.\n\n2. Ready\n\nThe process is prepared to run but is not currently executing because the OS has assigned the CPU to another process.\nThis state occurs due to scheduling decisions when multiple processes are competing for the CPU.\n\n3. Blocked\n\nThe process is waiting for an external event, such as an I/O operation (e.g., reading from a disk or network).\nWhile blocked, the process cannot execute until the event it depends on is completed.\n\nProcess State Transitions:\nThe transitions between these states are governed by the OS based on scheduling and system events. The transitions are as follows:\n\nRunning → Ready:The process is descheduled, typically because the OS preempts it to allow another process to run.\nReady → Running:The process is scheduled by the OS, gaining control of the CPU.\nRunning → Blocked:The process initiates an operation (e.g., I/O request) that requires it to wait, becoming blocked.\nBlocked → Ready:The event (e.g., I/O completion) occurs, making the process eligible to run again.\n\n\n\n\n4.5 Data StructuresOperating systems use specialized data structures to manage and track the state of processes. These structures are critical for enabling multitasking, handling process creation and termination, and performing context switches. Below are the key elements involved:\n1. Process Control Block (PCB)\nThe Process Control Block (PCB) (or process descriptor) is a data structure used to store information about each process in the system. Each process has its own PCB, which contains essential details such as:\n\nProcess State:Tracks the current state (e.g., running, ready, blocked, zombie).\nProcess ID (PID):A unique identifier for the process.\nRegister Context:Stores the contents of CPU registers when the process is stopped. This includes:\nInstruction pointer (eip): Where the process will resume execution.\nStack pointer (esp): Points to the process stack.\nOther general-purpose registers (ebx, ecx, etc.).\n\n\nMemory Information:\nPointer to the start of the process’s memory.\nSize of the memory allocated.\n\n\nParent Process:A reference to the process’s creator (useful for hierarchical process management).\nOpen Files and Current Directory:Tracks resources the process is using, including open file descriptors and the working directory.\nKernel Stack Pointer:A kernel-mode stack for handling system calls and interrupts.\nTrap Frame:Captures the state of the process during an interrupt.\n\n2. Process List\nThe process list (or task list) is a system-wide structure that maintains references to all PCBs. It organizes processes based on their states:\n\nReady List: Processes ready to run.\nBlocked List: Processes waiting for an event (e.g., I/O completion).\nRunning Process: Tracks the process currently using the CPU.\n\n3. Additional States\nWhile processes primarily cycle through running, ready, and blocked, additional states exist:\n\nUnused: Represents a process slot that has not been assigned to any process.\nEmbryo: Represents a process that is being created but has not yet started execution.\nZombie: Indicates a process has finished execution but is waiting for its parent to retrieve its exit status using a system call like wait(). This ensures the OS cleans up the process data correctly.\n\nChapter 5: Process API5.1 The fork() System CallThe fork() system call is one of the most essential yet unique operations in UNIX-based systems, allowing a process to create a nearly identical copy of itself, referred to as the child process.\nKey Concepts of fork()\n\nProcess Duplication:After calling fork(), the operating system creates a new child process, duplicating the parent’s address space, registers, and process control block (PCB). Both processes execute the same code starting right after the fork() call.\nReturn Values:\nThe parent process receives the PID (Process ID) of the child as the return value of fork().\nThe child process receives 0 as the return value.These differing return values allow the program to distinguish between the parent and child processes and handle them accordingly.\n\n\nAddress Space Isolation:The parent and child processes have independent address spaces, meaning changes in one process’s memory do not affect the other.\nNon-Deterministic Scheduling:After fork(), either the parent or the child can execute first. This depends on the CPU scheduler, making the order of output non-deterministic.\n\n123456789101112131415161718#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;int main(int argc, char *argv[]) &#123;    printf(&quot;hello world (pid:%d)\\n&quot;, (int) getpid());    int rc = fork();    if (rc &lt; 0) &#123; // fork failed        fprintf(stderr, &quot;fork failed\\n&quot;);        exit(1);    &#125; else if (rc == 0) &#123; // child process        printf(&quot;hello, I am child (pid:%d)\\n&quot;, (int) getpid());    &#125; else &#123; // parent process        printf(&quot;hello, I am parent of %d (pid:%d)\\n&quot;, rc, (int) getpid());    &#125;    return 0;&#125;\nCase 1:\nhello world (pid:29146)hello, I am parent of 29147 (pid:29146)hello, I am child (pid:29147)\nCase 2:\nhello world (pid:29146)hello, I am child (pid:29147)hello, I am parent of 29147 (pid:29146)\nThe operating system’s scheduler determines the order in which parent and child processes run, and scheduling is nondeterministic. Both Case 1 and Case 2 are perfectly correct behaviors, and the order depends on the scheduler.In multiprocess or multithreaded programming, such nondeterminism is common and is called a Race Condition. To deal with similar issues, the execution order of processes can be controlled through explicit synchronization, such as wait().\n5.2 The wait() System CallThe wait() system call is used in Unix-like operating systems to make a parent process pause its execution until one of its child processes finishes. This is especially useful for ensuring that a parent doesn’t exit before its child, and it also allows the parent to retrieve information about the child’s exit status.\nWith the wait() system call, we can be sure the child will always print first. If the child runs first, it prints before the parent. If the parent runs first, it immediately calls wait(), which makes it pause until the child finishes. Once the child exits, wait() returns, and then the parent prints its message.\n5.3 The exec() System CallThe exec() system call is a crucial part of process creation in operating systems, particularly when you need to run a different program than the one currently executing. Here’s an overview of its purpose and behavior:\n\nPurpose of exec():\nThe exec() system call allows a process to replace its own execution image with a different program. It is commonly used after a fork() to allow the child process to run a different program than the parent.\nUnlike fork(), which creates a new process, exec() replaces the current process with a new one. This means that the process ID (PID) and other resources like memory remain the same, but the program that is executing changes.\n\n\nHow exec() Works:\nThe exec() function takes the name of an executable file and its arguments as parameters. It loads the new program into the current process’s memory space, replacing the previous program.\nThe execution environment, including the stack and heap, is re-initialized, but the process itself remains the same (i.e., it does not create a new process).\nOnce exec() is called, the current process is effectively transformed into the new program, and the old program ceases to exist in that process.\nThe exec() call does not return if it is successful. If it fails, it returns an error.\n\n\nExample of exec() Usage:\nIn the example provided in the text, the child process calls execvp() to run the wc (word count) program on a file (p3.c).\nThe code snippet looks like this:\n\n\n\n123456char *myargs[3];myargs[0] = strdup(&quot;wc&quot;);  // The program to run: &quot;wc&quot; (word count)myargs[1] = strdup(&quot;p3.c&quot;);  // The file to count words inmyargs[2] = NULL;  // Marks the end of the argument listexecvp(myargs[0], myargs);  // Replaces the current process with &quot;wc&quot;\nAfter execvp() is called, the child process runs the wc program, counting the words, lines, and bytes in the file p3.c. The output is displayed as:\n29 107 1030 p3.c\n5.4 Why fork() and exec()? Motivating the APIThe separation of the fork() and exec() system calls might seem unusual, especially since they both deal with process creation. However, this design is fundamental to enabling flexible and powerful operations in UNIX-like systems, particularly for building tools like a shell.\n\nFlexibility in the Shell:\n\nThe key motivation for separating fork() and exec() is to allow the shell (or any process) to perform operations after the process is forked but before the new program is executed.\nThis separation provides flexibility by allowing the shell to manipulate the environment (e.g., file descriptors, working directory, or other aspects) before executing the new program.\n\n\nHow the Shell Works:\n\nThe shell prompts the user for input (e.g., a command to execute) and processes that input.\nIt calls fork() to create a new child process.\nThe child process can then be modified (e.g., by redirecting input/output, setting environment variables) before calling exec() to run the desired program.\nOnce the child process finishes, the parent waits for it to complete and then returns the prompt to the user.\n\n\nExample: Output Redirection:\n\nA common task in shells is redirecting output from a program to a file. The shell can accomplish this by closing the standard output (STDOUT_FILENO) and opening a file before calling exec().\n\nFor example, in the following command:\n1wc p3.c &gt; newfile.txt\n\n\n\n\nThe output of wc is redirected into newfile.txt instead of being printed to the terminal. This is done as follows:\n123close(STDOUT_FILENO);  // Close the current standard outputopen(&quot;./newfile.txt&quot;, O_CREAT | O_WRONLY | O_TRUNC, S_IRWXU);  // Open the fileexecvp(myargs[0], myargs);  // Run the &quot;wc&quot; program\n\nOutput of the Program:\n\n\nIn the example program (p4.c), fork() creates a child process, and before calling exec() to run the wc program, the standard output is redirected to the file p4.output.\nWhen the program is run:\n\n1./p4\nThe shell immediately returns the prompt (no output is shown), but the output of wc is written to the file p4.output. When the contents of the file are displayed:\n1cat p4.output\n\nPipes and Redirection:\n\n\nThe same principle is used for pipes, where the output of one process is directed as input to another. This is done via the pipe() system call, which creates an in-memory buffer (pipe) between two processes. For example:\n\n1grep -o foo file | wc -l\nHere, the output of grep (matching occurrences of “foo” in a file) is piped into wc -l (which counts the occurrences). The shell sets up the pipes and calls fork() and exec() for each process involved.\n\nWhy This Design Is Powerful:\n\n\nThe fork()/exec() design allows processes to easily spawn new programs while still having control over the environment and input/output. This capability is essential for building complex tools like shells, which can run multiple programs, redirect output, chain commands, and more.\n\n5.5 Process Control and Users in UNIX SystemsIn UNIX-like operating systems, process control involves managing and interacting with running processes. The system provides various system calls to control processes, manage their execution, and handle signals.\n\nSignals and Process Control:\nThe kill() system call allows processes to send signals to other processes. These signals can be used to interrupt, pause, or terminate processes.\nFor example, SIGINT (interrupt) is sent when a user presses Ctrl-C in a terminal, which typically causes a process to terminate.\nSIGTSTP (stop) is sent when Ctrl-Z is pressed, pausing the process and allowing it to be resumed later.\n\n\n\n\nSignal Handling:\nProcesses can catch certain signals by using the signal() system call. This allows them to handle signals in a custom way. For example, a process might catch SIGTERM (terminate) and perform cleanup tasks before exiting.\nSignals provide a mechanism for external events to interrupt or communicate with processes. The handling of signals is an important part of process control and interaction in UNIX systems.\n\n\nUser Permissions:\nWho can send signals to a process?\nGenerally, a user can only send signals to processes they own. This is important for system security and usability.\nIf arbitrary users could send signals to processes owned by others (e.g., sending SIGINT to kill another user’s process), it would pose a significant security risk and could disrupt the system’s functionality.\n\n\n\n\nUser Concept in UNIX:\nUNIX systems are designed with the concept of users who have different access levels to resources. A user logs into the system, typically with a password, to access resources such as CPU, memory, and files.\nOnce logged in, a user can create and control their own processes. For example, they can pause, resume, or kill the processes they own.\nResource Allocation: The operating system ensures fair and efficient distribution of system resources (like CPU and memory) among users and their processes, helping to maintain overall system stability and performance.\n\n\nSecurity and Process Control:\nBy limiting the ability to control processes to the user who owns them, the system ensures that each user’s actions are isolated from others. This helps maintain security and system integrity.\nThe operating system manages which processes can communicate with each other and ensures that users cannot interfere with or disrupt each other’s processes.\n\n\n\nChapter 6:  Limited Direct Execution6.1 Limited Direct ExecutionThe concept of limited direct execution is a technique that OS developers use to efficiently run programs on the CPU while still maintaining control over the system’s resources and processes. The technique involves running the user program as directly as possible on the CPU, but with constraints to ensure that the operating system can still manage and protect the system.\nBasic Direct Execution Protocol\nThe process starts by creating an entry for the process in the process list. Then:\n\nThe OS allocates memory for the program.\nThe program code is loaded into memory (from the disk).\nThe OS locates the entry point of the program (usually the main() function).\nThe OS then jumps to that entry point and starts executing the program, running the user code directly on the CPU.\n\nThis direct execution is shown in Figure 6.1 in the book and is straightforward: the OS does basic setup (e.g., memory allocation, stack setup) and then hands control over to the user program to execute. Once the program finishes execution, it returns control to the OS, freeing memory and removing the process from the process list.\n\nChallenges with Direct Execution\nWhile direct execution is efficient, it poses two key problems in the context of virtualizing the CPU:\n\nSafety and Control: If the OS simply runs a program directly on the CPU, how can it prevent the program from performing unsafe actions, such as accessing restricted memory or executing harmful instructions?\nSwitching Between Processes: In a multitasking system, how can the OS ensure that it can stop one running process and switch to another to ensure time sharing between processes?\n\nWhy “Limited”?\nThe term limited in “limited direct execution” comes from the fact that the OS must impose limits to prevent the user program from violating system integrity or monopolizing CPU resources. Without these limitations, the OS would be at the mercy of user programs, and the system would fail to provide the virtualization and resource management required for multiple programs to run safely and efficiently.\nIn essence, the operating system must control the execution of user programs, providing mechanisms for ensuring security, fairness, and process scheduling.\n6.2 Problem #1: Restricted OperationsDirect execution offers the benefit of running user programs quickly since they execute directly on the CPU. However, it introduces a problem: what if the program attempts to perform operations that are restricted or privileged, such as input/output (I/O) operations, or requesting additional system resources like memory or CPU time?\n\nThe User Mode and Kernel Mode\n\nTo address these issues, most systems introduce two modes of operation: user mode and kernel mode.\n\nUser mode: When a program runs in user mode, it is restricted in what it can do. For example, it cannot directly perform I/O operations. If it tries to, the CPU raises an exception, and the operating system is called to handle the operation. If the program tries to perform a restricted operation, the OS can terminate it for attempting to do something unsafe.\n\nKernel mode: In contrast, the operating system itself runs in kernel mode. In this mode, the OS has full access to all hardware resources and can perform privileged operations like issuing I/O requests, managing memory, and interacting with other processes.\n\n\n\nSystem Calls\n\nTo allow user programs to request privileged operations, the OS provides system calls, which enable programs to ask the OS to perform actions like reading from a disk or allocating memory.\nSystem calls are triggered by a special trap instruction. When a program needs to perform a system call, it executes the trap instruction, raising the privilege level and switching the CPU to kernel mode. The kernel then performs the requested operation and, once finished, uses a return-from-trap instruction to switch back to user mode and return control to the program.\nThis mechanism ensures that the OS controls access to privileged operations, protecting the system from malicious or faulty programs.\n\nTrap Table\n\nThe OS sets up a trap table during boot, which tells the CPU which code to run for events like system calls or interrupts (e.g., from a keyboard or disk). When a system call is made, the CPU looks up the handler in the trap table and jumps to the appropriate code in kernel mode.\nPrograms don’t directly specify where to jump in the kernel. Instead, they pass a system-call number, which the OS uses to find the correct code to execute. This prevents user programs from accessing arbitrary parts of the kernel, enhancing security.\nThe trap table is set up during boot and remains in place until the system is rebooted.\n\nSystem Call Security\n\nEven though system calls are carefully protected by the trap mechanism, there are still security considerations. For instance, the OS must ensure that the arguments passed to system calls are valid. If a user program passes a bad memory address to a system call (e.g., a pointer that points to kernel memory), the OS must reject the call to avoid security vulnerabilities. In secure systems, the OS must check all user inputs carefully to prevent malicious attacks.\nIn summary, the key idea in limited direct execution is that while user programs execute directly on the CPU for efficiency, the operating system controls access to restricted operations via the user mode/kernel mode distinction and system calls. The use of system calls, traps, and a trap table ensures that the OS can perform privileged operations on behalf of user programs without compromising security or control.\nLimited Direct Execution Protocol\n illustrates the flow of events during the lifecycle of a program in a system using the Limited Direct Execution (LDE) protocol. This approach balances the need for performance (letting programs run directly on hardware in user mode) and control (using the OS for privileged operations in kernel mode). \n\n\nOS @ Boot (Kernel Mode)\n\nThis section describes what the OS does when the system boots:\n\nInitialize trap table: The OS sets up the trap table, mapping events (e.g., system calls or interrupts) to their corresponding handlers.\nRemember syscall handler address: The trap table includes the address of the system-call handler, so the CPU knows where to jump during a system call.\n\n\nOS @ Run (Kernel Mode)\n\nWhen a program starts running, the OS sets up the necessary environment:\n\nCreate entry for process list: The OS tracks the process in a data structure.\nAllocate memory for program: The OS allocates memory for the program to run.\nLoad program into memory: The program’s code and data are loaded into RAM.\nSetup user stack with argv: The OS initializes the user stack with arguments passed to the program (e.g., argc and argv in main()).\nFill kernel stack with reg/PC: Registers and program counter (PC) are saved in the kernel stack to allow later context switches.\nreturn-from-trap: The CPU switches to user mode.\nRestore regs (from kernel stack): The CPU restores registers for the user program.\nMove to user mode: Execution switches from kernel to user mode.\nJump to main: The program begins execution at its main() function.\n\n\nRun Program (User Mode)\n\nThe program runs directly on the CPU until it needs a privileged operation:\n\nCall system call: The program requests a system call (e.g., file I/O or memory allocation).\nTrap into OS: The trap instruction causes a switch from user mode to kernel mode.\nSave regs (to kernel stack): CPU registers are saved in the kernel stack.\nMove to kernel mode: Privileges are elevated to allow execution of the system-call handler.\nJump to trap handler: The OS runs the system-call handler to process the request.\n\n\nHandle Trap (Kernel Mode)\n\nThe OS handles the system call:\n\nDo work of syscall: The OS performs the requested operation.\nreturn-from-trap: The system call completes, and control returns to the program.\nRestore regs (from kernel stack): CPU registers are restored.\nMove to user mode: Execution switches back to user mode.\nJump to PC after trap: The program resumes execution right after the system-call instruction.\n\n\nReturn from main() (Exit)\n\nWhen the program finishes, it makes an exit system call:\n\nTrap (via exit()): The program signals the OS it is done by invoking exit().\nFree memory of process: The OS deallocates memory used by the process.\nRemove from process list: The OS updates its data structures to mark the process as terminated.\n\n6.3 Problem #2: Switching Between ProcessesSwitching between processes is challenging because the OS cannot act while it is not running on the CPU. This creates the problem of how the OS can regain control of the CPU to switch tasks.\nA Cooperative Approach: Waiting for System Calls\n\nIn older systems like early Macintosh or Xerox Alto, processes were expected to voluntarily yield control by:\nMaking system calls (e.g., file operations or process creation).\nGenerating traps through illegal operations (e.g., dividing by zero or accessing invalid memory).\nExplicitly yielding via a specific system call.\n\n\n\nDrawbacks:\n\nIf a process enters an infinite loop without making system calls or errors, the OS cannot regain control. The only solution in such cases is to reboot the machine.\n\nA Non-Cooperative Approach: Using a Timer Interrupt\n\nKey Idea:\n A hardware timer generates interrupts periodically, allowing the OS to regain control.\n\nWhen the timer triggers, the running process is interrupted.\nThe hardware saves the process state and transfers control to an OS interrupt handler.\n\n\n\nAdvantages:\n\nThe OS can preempt uncooperative processes.\nIt ensures fairness and prevents rogue processes from monopolizing the CPU.\n\nContext Switching: Saving and Restoring Process State\n\nDecision Making: After regaining control, the OS decides whether to resume the current process or switch to another, based on scheduling policies.\nSteps in Context Switching:\nSave the state (registers, program counter, kernel stack pointer) of the current process.\nRestore the state of the next process.\nUse the restored state to resume execution of the next process.\n\n\n\nDetails:\n\nDuring an interrupt, the hardware implicitly saves user registers on the kernel stack.\nWhen switching processes, the OS explicitly saves kernel registers into the process structure of the current process and restores the next process’s state.\n\nSignificance of Timer Interrupts\n\nTimer interrupts are essential to prevent uncooperative behavior, allowing the OS to maintain system control.\nRebooting: While sometimes necessary (e.g., in infinite loops), rebooting ensures system reliability by resetting to a stable state and reclaiming leaked resources.\n\n\nChapter 7:  Introduction of Scheduling7.1 Workload AssumptionsThe following are the assumptions made about processes (or jobs) running in the system:\n\nUniform Job Duration:All jobs have the same runtime, simplifying comparisons and fairness among jobs.\n\nSimultaneous Arrival:All jobs arrive at the same time, eliminating complexities related to arrival time differences.\n\nNo Preemption (Run to Completion):Once a job starts, it runs to completion without interruptions.\n\nCPU-Only Jobs:Jobs exclusively use the CPU, without performing any I/O operations.\n\nKnown Job Runtimes:The runtime of each job is known beforehand, granting the scheduler perfect knowledge of job behavior.\n\n\nUnrealistic Assumptions\n\nKnown Job Runtimes:This assumption is especially unrealistic, as no scheduler in real life has omniscient knowledge of how long a job will take. In practice, this information must be inferred or estimated.\n\nCPU-Only Jobs:Real-world jobs often involve both CPU processing and I/O operations, introducing variability in their behavior and scheduling requirements.\n\nSimultaneous Arrival:Jobs typically arrive at different times in real systems, leading to challenges in dynamically managing the workload.\n\n\nPurpose of These Assumptions\n\nThese assumptions provide an idealized and highly controlled environment for studying scheduling policies.\nBy starting with such a simplified model, we can analyze scheduling algorithms without being distracted by the complexities of real-world systems.\nLater, these assumptions will be relaxed to explore more nuanced and practical scheduling policies.\n\n7.2 Scheduling MetricsScheduling policies need metrics to evaluate their effectiveness and to compare different approaches. This section introduces turnaround time as the primary metric for performance evaluation, with a brief discussion of the trade-offs between performance and fairness.\nPrimary Metric: Turnaround Time\nThe turnaround time measures how long a job takes to complete after it enters the system. Mathematically, it is defined as:\n\nT_{\\text{turnaround}} = T_{\\text{completion}} - T_{\\text{arrival}}Current Simplification\nGiven the assumption that all jobs arrive simultaneously\n\n(T_{\\text{arrival}} = 0), turnaround time reduces to:\n\nT_{\\text{turnaround}} = T_{\\text{completion}}This simplification allows us to focus solely on how completion times vary under different scheduling policies.\n7.3 First In, First Out (FIFO)First In, First Out (FIFO), also called First Come, First Served (FCFS), processes jobs in the order of their arrival. Its simplicity makes it straightforward to implement, but this can lead to inefficiencies, especially when job durations vary significantly.\n\n\nThe convoy effect is evident here: shorter jobs B and C are delayed significantly because the longer job AAA dominates CPU time. This highlights the inefficiency of FIFO when job lengths vary.\n7.4 Shortest Job First (SJF)Shortest Job First (SJF) scheduling, a technique inspired by operations research, prioritizes jobs based on their runtime, with shorter jobs running before longer ones. This approach significantly reduces average turnaround time compared to FIFO, especially for workloads with varying job durations.\n\n\n7.5 Shortest Time-to-Completion First (STCF)Shortest Time-to-Completion First (STCF), also known as Preemptive Shortest Job First (PSJF), is a scheduling algorithm that addresses the limitations of SJF by introducing preemption. STCF dynamically evaluates the job queue and schedules the job with the shortest remaining time. If a shorter job arrives during the execution of another, the scheduler preempts the running job and switches to the new one.\n\n7.6 A New Metric: Response TimeResponse time measures the time from when a job arrives in the system to the first time it is scheduled. It is crucial for assessing interactivity in systems where user inputs expect prompt feedback.\nThe formula for response time is:\n\nT_{\\text{response}} = T_{\\text{firstrun}} - T_{\\text{arrival}}Importance of Response Time\n\nIn early batch systems, turnaround time was the primary metric, and scheduling like STCF worked well for optimizing it.\nHowever, with the advent of time-sharing systems, users began interacting directly with computers. Quick responses became vital for user satisfaction.\nPoor response time can frustrate users, particularly in interactive systems where a delay can be perceived as a system failure.\n\n7.7 Round RobinRound Robin (RR) scheduling is a fair, time-sharing scheduling algorithm designed to optimize response time. It cycles through jobs, giving each a fixed time slice (or scheduling quantum) to run before switching to the next job. The process repeats until all jobs are completed.\nKey Features\n\nTime Slices:\nJobs are preempted after a specific time slice.\nThe time slice must be a multiple of the timer interrupt period (e.g., 10ms, 20ms, etc.).\n\n\nFairness:\nRR ensures all jobs receive CPU time in a cyclic manner.\n\n\nTime-Slicing Trade-Off:\nShort time slices improve response time but increase the cost of context switching.\nLong time slices reduce the overhead of context switching but degrade system responsiveness.\n\n\n\nExecution Order:\n\nA, B, and C are interleaved, and each gets a short turn to execute: ABC ABC ABC ABC ABC ……\n\n\n\n7.8 Incorporating I/ORelaxing Assumption 4 acknowledges that most programs perform I/O operations, which means they alternate between CPU and I/O usage. The scheduler must handle jobs that are blocked (waiting for I/O to complete) differently from jobs actively using the CPU\nKey Concepts\n\nBlocking I/O:\nWhen a job performs I/O, it cannot use the CPU and is moved to a blocked state.\nWhile blocked, the scheduler can assign the CPU to another job.\n\n\nHandling I/O Completion:\nWhen I/O completes, an interrupt is triggered.\nThe job is moved from the blocked state to the ready state.\nThe scheduler decides whether to immediately run this job or continue with the current job.\n\n\n\nExample Scenario\n\nJobs:\nA: Runs for 10ms, then performs I/O for 10ms, repeating this cycle 5 times (total CPU time = 50ms).\nB: CPU-intensive job that requires 50ms of uninterrupted CPU time.\n\n\nScheduler: Shortest Time-to-Completion First (STCF).\n\nNaive Scheduling:\n\nRun A until completion, then run B.\nThis results in poor resource utilization:\nThe CPU is idle while A performs I/O.\nTotal completion time: 100ms for A + 50ms for B = 150ms.\n\n\n\nImproved Scheduling (Overlapping I/O):\n\nTreat each 10ms CPU burst of A as an independent “sub-job.”\nAlternate between A and B, ensuring better CPU and disk utilization.\n\nMetrics:\n\nTotal completion time: 100ms (overlapping reduces idle time).\nA performs I/O while B uses the CPU, maximizing utilization.\n\n\nChapter 8: The Multi-Level Feedback Queue in Scheduling8.1 MLFQ: Basic RulesThe Multi-Level Feedback Queue (MLFQ) scheduling algorithm is designed to dynamically adjust job priorities based on their observed behavior. This approach ensures responsiveness for interactive jobs while still allowing long-running, CPU-bound tasks to execute eventually.\nKey Concepts\n\nQueues and Priorities:\n\nMLFQ consists of multiple queues, each representing a different priority level.\nJobs in higher-priority queues run before jobs in lower-priority queues.\nWithin the same queue, jobs are scheduled using round-robin (RR).\n\n\nRules of Scheduling:\n\nRule 1: If Priority(A)&gt;Priority(B), job A runs (and B does not).\nRule 2: If Priority(A)=Priority(B), A and B share the CPU in round-robin fashion.\n\n\n\nDynamic Priority Adjustment\nMLFQ learns job behavior and adjusts priorities dynamically:\n\nInteractive Jobs:\nJobs that frequently relinquish the CPU (e.g., waiting for keyboard input) are assumed to be interactive.\nThese jobs are kept at higher priority, ensuring responsiveness.\n\n\nCPU-Bound Jobs:\nJobs that use the CPU intensively without I/O are considered CPU-bound.\nTheir priority is lowered over time, allowing interactive jobs to run first.\n\n\n\nThis dynamic adjustment helps MLFQ balance system responsiveness with fairness.\n8.2 Attempt #1: How To Change PriorityPriority Adjustment Rules:\n\nRule 3: A new job starts at the highest priority (top queue).\nRule 4a: If a job uses its entire time slice, its priority is reduced (moved to a lower queue).\nRule 4b: If a job relinquishes the CPU before the time slice ends, it stays at the same priority level.\n\nExamples:\n\nSingle Long-Running Job:\n\nStarts at highest priority (Q2).\nUses the entire time slice (10 ms), so it is demoted to Q1.\nEventually moves to the lowest priority queue (Q0) and remains there.\nObservation: The MLFQ steadily lowers the priority of long-running jobs.\n\n\n\nInteractive vs. CPU-Intensive Job:\n\nJob A (long-running, CPU-bound) runs at Q0.\nJob B (short, interactive) arrives and starts at Q2.\nSince B completes quickly, it finishes in high-priority queues without being demoted.\nObservation: MLFQ prioritizes short jobs initially, mimicking Shortest Job First (SJF).\n\n\n\nI/O-Bound Job:\n\nInteractive Job B uses CPU briefly (e.g., 1 ms) before performing I/O.\nMLFQ keeps B in high priority (Q2) as it relinquishes the CPU early.\nObservation: MLFQ accommodates interactive jobs efficiently, ensuring they get CPU time promptly.\n\n\n\n\nProblems with the Current MLFQ Design:\n\nStarvation:\nA flood of interactive jobs can monopolize the CPU, leaving long-running jobs starved at lower priorities.\n\n\nGaming the Scheduler:\nA clever user could manipulate priority by:\nRelinquishing the CPU just before the time slice ends.\nStaying in higher queues, monopolizing CPU time.\n\n\nExample: Running for 99% of the time slice before issuing an I/O request.\n\n\nBehavior Change:\nA job that transitions from being CPU-bound to interactive cannot regain higher priority dynamically.\n\n\nSecurity Concerns:\nIn shared environments (e.g., data centers), poor scheduler design may allow one user to exploit the system, harming others.\nScheduling is thus a security-critical component.\n\n\n\n8.3 Attempt #2: The Priority BoostKey Concepts:\n\nPriority Boost (Rule 5):\nTo address starvation and better handle dynamic job behavior, periodically boost all jobs to the topmost priority queue.\nThis ensures that long-running jobs eventually receive CPU time, while jobs transitioning from CPU-bound to interactive are treated fairly.\n\n\nImplementation:\nAfter a specific time period S, move all jobs in the system to the highest-priority queue (Q2 in the example).\n\n\n\nEffects of Priority Boost:\n\nPrevention of Starvation:\nWithout a boost, long-running jobs in lower-priority queues may never get CPU time if interactive jobs dominate.\nWith a periodic boost, these jobs run in round-robin fashion in the top queue, ensuring they get some service.\n\n\nDynamic Job Behavior:\nIf a previously CPU-bound job becomes interactive, the priority boost allows the scheduler to treat it correctly.\n\n\n\nExample:\n\nScenario:\nA long-running job competes with two short interactive jobs.\n\n\nWithout Boost:\nThe long-running job is starved in the lowest-priority queue as interactive jobs dominate the CPU.\n\n\nWith Boost:\nEvery 50 ms (in the example), all jobs are moved to the top queue.\nThe long-running job periodically receives CPU time.\n\n\n\n\n8.4 Attempt #3: Better AccountingProblem Addressed:\n\nThe previous rules (including priority boost) improved fairness but still had inefficiencies:\nSome jobs could manipulate the system by becoming temporarily interactive to regain high priority.\nThe scheduler lacked precise tracking of CPU usage across jobs, leading to potential unfairness.\n\n\n\nProposed Solution - Better Accounting:\nThe idea is to improve accounting mechanisms to ensure:\n\nAccurate tracking of CPU usage.\nPrevention of gaming the system by processes.\n\n\n8.5 Tuning MLFQ And Other IssuesChallenges in Tuning MLFQ:\nThe Multi-Level Feedback Queue (MLFQ) scheduler requires careful parameter tuning to achieve an optimal balance between fairness and efficiency. However, finding the right configuration is non-trivial, leading to challenges like:\n\nNumber of Queues:\nToo few queues: Insufficient differentiation between job types.\nToo many queues: Increased complexity and overhead.\n\n\nTime Slice per Queue:\nHigh-priority queues (interactive jobs): Require shorter time slices (e.g., 10 ms) for responsiveness.\nLow-priority queues (CPU-bound jobs): Benefit from longer time slices (e.g., 100+ ms) to reduce context switching overhead.\n\n\nPriority Boost Frequency:\nFrequent boosts: Minimize starvation but may disrupt fairness.\nInfrequent boosts: Allow starvation, especially for long-running CPU-bound jobs.\n\n\nDynamic Behavior:\nWorkloads vary widely, making fixed parameters suboptimal in many cases.\n\n\n\n\nAvoiding “Voo-Doo Constants” (Ousterhout’s Law):\n\nA term coined by John Ousterhout, it warns against arbitrary constants (e.g., fixed time slice lengths or boost intervals).\nThese constants are often left unmodified, relying on defaults that may not suit every workload.\n\nPotential Solutions:\n\nLearning-Based Tuning:\n\nSystems can adapt by monitoring workloads and adjusting parameters dynamically.\nHowever, implementing such learning systems is complex.\n\n\nConfiguration Files:\n\nAllow experienced administrators to fine-tune parameters.\n\nThese often remain unused unless performance issues arise.\n\n\n\n\nExample: Solaris Time-Sharing Scheduler:\n\nTable-Based Configuration:\nSolaris MLFQ uses configurable tables to define:\nPriority adjustments over time.\nTime slice lengths (e.g., 20 ms for high-priority queues, hundreds of ms for low-priority queues).\nPriority boost intervals (~1 second).\n\n\nDefault: 60 queues with gradually increasing time slices.\n\n\n\nChapter 9: Proportional Share9.1 Basic Concept: Tickets Represent Your ShareCore Concept: Tickets\n\nDefinition: Tickets represent the share of a resource (e.g., CPU) allocated to a process or user.\nProportional Representation: The fraction of tickets a process holds corresponds to the percentage of the resource it should receive.\nExample:\nProcess A has 75 tickets, and Process B has 25 tickets.\nTotal tickets = 100.\nProcess A should receive 75% of the CPU, and Process B 25%.\n\n\n\n\n\nMechanism:\n\nLottery System:\nAt every scheduling event (e.g., time slice), a lottery is held.\nA random number (ticket) is drawn from 0 to the total number of tickets - 1.\nThe process holding the winning ticket gets to run.\n\n\nExample:\nProcess A: Tickets 0–74.\nProcess B: Tickets 75–99.\nWinning ticket determines the selected process:\nTicket 63 → A.\nTicket 85 → B.\n\n\nOver many time slices, the CPU usage approaches the desired proportions probabilistically.\n\n\n\nAdvantages of Lottery Scheduling:\n\nRandomness as a Robust Mechanism:\n\nAvoids Corner Cases:\nRandom approaches are less prone to worst-case scenarios compared to deterministic algorithms.\n\nExample: Random replacement avoids LRU’s worst-case performance in some cyclic workloads.\n\n\nMinimal State Tracking:\n\nTraditional scheduling requires tracking per-process CPU usage.\nLottery scheduling only needs to manage ticket counts.\n\n\n\n\nSimplicity and Speed:\n\nMinimal bookkeeping makes it lightweight and efficient.\nGenerating a random number is computationally inexpensive.\n\n\nFairness over Time:\n\nWhile randomness may cause deviations in the short term, fairness is achieved over time through probabilistic correctness.\n\n\n\nDrawbacks:\n\nShort-Term Inaccuracy:\nRandomness introduces variability. For example, a process may receive less than its share of the CPU in a small time window.\nExample: Process B should receive 25% but only gets 20% over 20 time slices.\n\n\n\n\nApplications of Tickets:\n\nProportional Resource Allocation:\nBeyond CPU scheduling, tickets can be used in other contexts:\nMemory Allocation in virtualized systems.\nI/O Scheduling to divide bandwidth among processes.\n\n\n\n\nReal-World Use Case:\nWaldspurger applied tickets in virtual memory management for hypervisors to control guest operating systems’ memory shares.\n\n\n\n9.2 Ticket Mechanisms1. Ticket Currency:\n\nPurpose: Allows users to manage tickets within their own namespace or “currency” while the system converts them to a global value.\nHow It Works:\nEach user allocates tickets in their local “currency.”\nThe system normalizes these tickets to the global scale.\nThis ensures fairness when multiple users share the same resource.\n\n\nExample:\nUser A:\nTotal tickets: 100.\nAllocates 500 tickets each to two jobs (A1 and A2) in local currency.\nConverted to 50 tickets each in global currency.\n\n\nUser B:\nTotal tickets: 100.\nAllocates 10 tickets to one job (B1) in local currency.\nConverted to 100 tickets in global currency.\n\n\nGlobal lottery:\nTotal tickets: 200.\nJobs compete proportionally based on their global ticket allocation.\n\n\n\n\n\n2. Ticket Transfer:\n\nPurpose: Temporarily transfers tickets between processes to reflect changing priorities or dependencies.\nUse Case:\nCommon in client/server systems.\nExample:\nA client process requests work from a server process.\nThe client transfers its tickets to the server to prioritize the server’s performance.\nAfter the task, the server transfers the tickets back to the client.\n\n\n\n\n\n3. Ticket Inflation:\n\nPurpose: Temporarily increases or decreases the number of tickets a process holds to adapt to specific needs.\nConstraints:\nUseful in trusted environments where processes cooperate.\nNot suitable for competitive scenarios, as greedy processes could exploit this mechanism.\n\n\nExample:\nA process anticipates heavy CPU usage and inflates its ticket count to signal its need for more resources.\nAfter completing the task, it deflates its tickets back to the original count.\n\n\n\n9.3 ImplementationKey Elements of Implementation:\nLottery scheduling is notable for its simplicity and elegance. The key components needed for implementation include:\n\nRandom Number Generator:\nUsed to pick a winning ticket number from the range [0, total_tickets-1].\n\n\nProcess List:\nA data structure (e.g., a linked list) to track all processes and their ticket allocations.\n\n\nTotal Ticket Count:\nThe sum of tickets across all processes, used as the range for random number selection.\n\n\n\nProcess Example:\n\nProcesses and Tickets:\nA: 100 tickets.\nB: 50 tickets.\nC: 250 tickets.\nTotal tickets: 400.\n\n\nWinning Ticket Selection:\nRandomly generate a number between 0 and 399 (e.g., 300).\n\n\nFinding the Winner:\nTraverse the process list, summing ticket counts with a counter.\nSteps:\nStart with counter = 0.\nAdd A’s tickets: counter = 100 (less than 300, continue).\nAdd B’s tickets: counter = 150 (still less than 300, continue).\nAdd C’s tickets: counter = 400 (greater than 300, C is the winner).\n\n\n\n\n\n123456789101112int counter = 0;                          // Tracks the sum of ticketsint winner = getrandom(0, totaltickets);  // Randomly pick the winning ticketnode_t *current = head;                   // Start at the head of the process listwhile (current) &#123;    counter += current-&gt;tickets;          // Add tickets of current process    if (counter &gt; winner)                 // Check if the current process wins        break;                            // Stop when the winner is found    current = current-&gt;next;              // Move to the next process&#125;// &#x27;current&#x27; points to the winning process; schedule it.\n9.4 An ExampleExperiment Setup:\n\nScenario:\nTwo jobs, each with:\n100 tickets.\nIdentical run times, denoted as R, which varies during the experiment.\n\n\n\n\nObjective:\nBoth jobs should ideally finish at the same time.\nMeasure the fairness of lottery scheduling, which uses randomness and does not guarantee deterministic behavior.\n\n\n\nFairness Metric:\n\nDefinition:\n\n\nF = \\frac{\\text{Completion time of the first job}}{\\text{Completion time of the second job}}\n\n\n\n\nF≈1: Perfect fairness (both jobs finish simultaneously).\n\nF&lt;1: First job finishes significantly earlier than the second job.\n\n\n\nExample:\n\nIf R=10:\n\nFirst job finishes at t=10, second at t=20.\n\n\nF = \\frac{10}{20} = 0.5\n\n\n\n\n\nObservations from Figure 9.2:\n\nShort Job Runs:\nFor small R, fairness (F) tends to be low.\nReason:\nWith fewer time slices, the randomness in lottery scheduling results in higher variability in job completion times.\nThere isn’t enough time for the probabilistic fairness to even out.\n\n\n\n\nLonger Job Runs:\nAs R increases, F approaches 1 (better fairness).\nReason:\nOver many time slices, the inherent randomness averages out, and the two jobs receive their fair share of CPU time proportional to their ticket allocation.\n\n\n\n\n\n\n9.5 How To Assign Tickets?This problem is a tough one, because of course how the system behaves is strongly dependent on how tickets are allocated. Thus, given a set of jobs, the “ticket-assignment problem” remains open\n9.6 Stride SchedulingStride Scheduling\n\nOverview:\n\nDeterministic alternative to lottery scheduling.\nEnsures exact proportional resource allocation based on ticket values.\n\n\nKey Concepts:\n\nStride:\n\nInversely proportional to the number of tickets a job holds.\n\nCalculated as:\n\n\\text{Stride} = \\frac{\\text{Large Number}}{\\text{Number of Tickets}}Example: With 10,000 as the large number:\n\nJob A (100 tickets): \nStride = \\frac{10,000}{100} =100\n\n\nJob B (50 tickets): \n\n  Stride = \\frac{10,000}{50} = 200.\nJob C (250 tickets): \nStride = \\frac{10,000}{250} = 40.Pass Value:\n\n\nTracks cumulative progress of each job.\n\nIncremented by the job’s stride after every time slice.\n\n\n\n\nScheduling Algorithm:\n\nSelect the job with the lowest pass value.\n\nAfter the job runs, update its pass value: \n\n\\text{New Pass Value} = \\text{Old Pass Value} + \\text{Stride}\n\n\n\n\nExample Execution (Figure 9.3):\n\n\nProportions achieved:\n\nC (250 tickets): Runs 5 times.\nA (100 tickets): Runs 2 times.\nB (50 tickets): Runs 1 time.\nReflects ticket allocation exactly.\n\n\nChapter 10: Multiprocessor Scheduling (Advanced)Single vs. Multi-CPU Systems\n\nSingle CPU System:\nUses hardware caches to speed up memory access.\nCache Hierarchy:\nSmall, fast cache memory holds copies of frequently accessed data.\nLarge, slower main memory stores all data.\n\n\nTemporal Locality: Frequently accessed data is likely to be accessed again soon.\nSpatial Locality: Access to an address makes nearby addresses more likely to be accessed.\n\n\nMulti-CPU System:\nShares a single main memory between multiple CPUs.\nEach CPU has its own private cache.\n\n\n\n\nCache Behavior in Multiprocessor Systems\n\nWhen a CPU modifies data, changes may remain in its private cache without immediately updating main memory.\n\nExample Problem:\n\nCPU 1 fetches a value D from memory (address A) and caches it.\nCPU 1 modifies D to D&#39; in its cache.\nThe program is moved to CPU 2, which fetches data from main memory.\nCPU 2 receives the old value D (not D&#39;).\n\nThis mismatch is called the Cache Coherence Problem.\n\n\nCache Coherence: Challenges and Solutions\n\nProblem: How to ensure all CPUs see a consistent view of shared memory.\nHardware Solution:\nBus Snooping:\nEach cache monitors memory updates on the bus connecting caches and memory.\nIf a cache sees a change to a memory address it holds:\nInvalidate the stale copy.\nOr Update the cache with the new value.\n\n\n\n\nComplexities arise with write-back caches, which delay updates to main memory.\n\n\n\n10.2 Don’t Forget SynchronizationCache Coherence and Synchronization\n\nCache coherence ensures that all processors have a consistent view of memory.\n\nHowever, \ncoherence alone is insufficient\n for managing \nshared data\n correctly.\n\nPrograms and the OS must still implement synchronization mechanisms when accessing or modifying shared data.\n\n\n\nKey Concept: Mutual Exclusion\n\nWhy Needed: Prevent race conditions where multiple threads or CPUs simultaneously modify shared data, causing inconsistencies.\nExample Problem:Removing an element from a shared linked list without synchronization:\n\n12345678int List_Pop() &#123;    Node_t *tmp = head; // store current head    int value = head-&gt;value; // get value    head = head-&gt;next; // move head pointer    free(tmp); // free old head    return value;&#125;\nIf two threads execute this code at the same time:\n\nBoth read the same head into their local tmp.\nBoth try to free the same memory, causing a double free.\nBoth return the same value, causing logical errors.\n\nSolution: Locks\n\nUse mutual exclusion (locks) to ensure only one thread executes the critical section at a time.\n\n1234567891011pthread_mutex_t m;int List_Pop() &#123;    lock(&amp;m);         // Acquire lock    Node_t *tmp = head;    int value = head-&gt;value;    head = head-&gt;next;    free(tmp);    unlock(&amp;m);       // Release lock    return value;&#125;\n10.3 One Final Issue: Cache AffinityA process, when run on a particular CPU, builds up a fair bit of state in the caches (and TLBs) of the CPU. The next time the process runs, it is often advantageous to run it on the same CPU, as it will run faster if some of its state is already present in the caches on that CPU. If, instead, one runs a process on a different CPU each time, the performance of the process will be worse, as it will have to reload the state each time it runs (note it will run correctly on a different CPU thanks to the cache coherence protocols of the hardware).\n10.4 Single-Queue SchedulingOverview of SQMS\n\nDefinition: SQMS uses a single queue to store all jobs that need scheduling, regardless of the number of CPUs.\nAdvantages:\nSimplicity: Easily adapted from single-CPU scheduling algorithms.\nUnified Policy: Centralized decision-making allows for consistent job selection.\n\n\n\nKey Challenges of SQMS\n\nScalability Issues\n\n\nLocks and Contention:\n\nA single queue requires locking mechanisms to ensure proper synchronization when multiple CPUs access the queue simultaneously.\n\nPerformance Degradation:\n As the number of CPUs increases, contention for the queue’s lock grows, leading to:\n\nLock overhead.\nCPUs spending more time waiting for the lock rather than executing jobs.\n\n\n\n\n\n\nCache Affinity\n\n\nProblem Description:\n\nJobs frequently migrate between CPUs when the next available job is picked from the global queue.\nThis job movement undermines cache affinity, forcing jobs to reload state into the cache each time they run on a new CPU.\n\n\nExample of Poor Affinity:\n\n\n\nAffinity Mechanisms in SQMS\n\nPreserving Affinity:\n\nImplement mechanisms to increase the likelihood that jobs continue to run on the same CPU.\n\nExample:\n\nJobs A through D stay on CPUs 0 through 3, respectively.\nJob E migrates across CPUs as needed for load balancing.\n\n\n\n\n\nAffinity Fairness:\n\nRotate job migrations to ensure fairness across jobs.\nFor example:\nJob E migrates first.\nIn subsequent cycles, a different job might be chosen to migrate.\n\n\n\n\nComplexity: These mechanisms increase the complexity of the scheduler and may not fully eliminate performance concerns.\n\n\n10.5 Multi-Queue SchedulingOverview of MQMS\n\nDefinition: In MQMS, each CPU is assigned its own scheduling queue. Jobs are distributed across these queues using heuristics, such as random assignment or balancing the number of jobs per queue.\nAdvantages:\nScalability: Reduces contention since queues operate independently, avoiding the bottlenecks of a single shared queue.\nCache Affinity: Jobs typically stay on the same CPU, preserving cached data and improving performance.\n\n\n\nKey Challenges of MQMS\n\nLoad Imbalance\n\n\nDescription:\nSome CPUs may finish their jobs earlier than others, leading to uneven CPU utilization.\nFor example:\nQueue 0: Jobs A, C\nQueue 1: Jobs B, D\nIf C finishes, CPU 0 may have only A left while CPU 1 still alternates between B and D.\n\n\nIf A finishes, CPU 0 becomes idle while CPU 1 continues to run jobs, resulting in underutilization of available CPUs.\n\n\n\n\n\nSolution: Job Migration\n\nMigration Mechanism:\n Jobs are moved between queues to balance the workload across CPUs.\n\nSimple Example:\n\nQueue 0: Empty\nQueue 1: Jobs B, D\nMigrate B or D to Queue 0, ensuring both CPUs remain active.\n\n\nComplex Example:\n\nQueue 0: Job A\n\nQueue 1: Jobs B, D\n\nAlternating migration might be needed to balance load:\n\nMove B to Queue 0 for a few time slices, allowing D to run alone on CPU 1.\nThen migrate B back, allowing A to share CPU 0 with D.\n\n\n\n\n\n\n\n\n\nWork-Stealing Technique\n\n\nDefinition: A queue low on jobs (source queue) “steals” jobs from a more loaded queue (target queue).\nBenefits: Balances the load dynamically without requiring centralized scheduling.\nChallenges:\nOverhead: Frequent checks for load imbalances increase system overhead, potentially impacting performance.\nInfrequent Checks: If checks are too rare, queues might become heavily imbalanced, negating the benefits of migration.\n\n\n\nChapter 13: Address Spaces13.2 Multiprogramming and Time SharingTransition to Time Sharing\n\nProblem with Multiprogramming: \nWhile multiprogramming increased CPU efficiency, it was not interactive enough for users. Program debugging was long and inefficient in batch processing, and real-time feedback was not possible.\n\nDemand for Interactivity:\n\nProgrammers and users wanted quicker responses and the ability to interact with running programs.\nThis led to the concept of time sharing, where multiple users could interact with a machine concurrently.\n\n\nTime Sharing Mechanism:\n\nA simple approach would be to run a process for a short period, save its state (including all memory) to disk, and load another process’s state to run next.\nChallenge: Saving and restoring all memory content to/from disk was slow and inefficient, particularly as memory sizes grew.\n\n\n\nEfficient Time Sharing with Memory Management\n\nImproved Time Sharing:\nInstead of saving and restoring entire memory contents, the OS keeps processes in memory and switches between them more efficiently.\nEach process is given a portion of physical memory, enabling concurrent execution and reducing the time overhead associated with disk I/O.\n\n\nExample Scenario (Figure 13.2):\nAssume a 512KB physical memory, divided among processes A, B, and C.\nThe OS runs one process (e.g., A) while the others (B and C) are waiting in the ready queue.\nThe CPU switches between processes, enabling multiple tasks to execute and improving system responsiveness.\n\n\n\n\nProtection and Isolation\n\nNew Challenges with Time Sharing:\nMemory Protection: Multiple processes sharing the same memory space raise concerns about security and stability.\nGoal: Prevent one process from accessing or modifying another process’s memory.\n\n\nImportance of Isolation:\nEnsures that processes cannot interfere with each other, which is crucial for system integrity and security.\n\n\n\n13.3 The Address SpaceAddress Space Abstraction\n\nDefinition: The address space is the running program’s view of memory as provided by the operating system (OS). This abstraction helps simplify the interaction between programs and the physical memory hardware.\nComponents of Address Space:\nCode Segment: Contains the executable instructions of the program.\nStack: Used for tracking function calls, local variables, and passing parameters and return values.\nHeap: Used for dynamically allocated memory, like data from malloc() in C or new in C++/Java.\nOther Elements: Includes static variables and other memory structures, but for simplicity, focus is often on code, stack, and heap.\n\n\n\nExample of Address Space (Figure 13.3)\n\n16KB Address Space Example:\nCode Segment: Occupies the first 1KB (top of the address space); it’s static and does not grow.\nHeap: Starts at 1KB and grows downward (allocations made via malloc()).\nStack: Starts at 16KB and grows upward (due to function calls and local data).\n\n\nGrowth Direction: The stack and heap grow toward each other, enabling dynamic expansion without interfering. This arrangement helps with efficient use of available space.\n\n\nVirtualization of Memory\n\nConcept: Memory virtualization is the OS’s ability to present each process with the illusion of a private, large address space, even when only a portion of the physical memory is used.\nPhysical vs. Virtual Memory:\nVirtual Address: The address used by a running program.\nPhysical Address: The actual location in physical memory where the process is loaded.\n\n\nOS and Hardware Support: When a program attempts to access a virtual address, the OS, with assistance from hardware (e.g., memory management unit), maps it to the corresponding physical address.\nWhy It Matters: Virtual memory allows multiple processes to share the same physical memory without interfering with each other. This abstraction is critical for process isolation, security, and efficient memory management.\n\n13.4 GoalsKey Goals of Virtual Memory (VM) Systems\nThe OS plays a crucial role in virtualizing memory to create a seamless and efficient experience for programs. To achieve this, there are three primary goals:\n\nTransparency\nDefinition: The OS should make the virtualization of memory invisible to running programs. Programs should operate as if they have access to their own dedicated physical memory, unaware that the memory they use is actually shared and virtualized.\nImportance: This ensures that programs can function without needing to manage or be aware of the underlying memory virtualization processes. This is critical for maintaining the simplicity of program design and operation.\n\n\nEfficiency\nDefinition: The OS must strive to virtualize memory efficiently, both in terms of time (speed of operations) and space (amount of memory used for managing the virtualization).\nTime Efficiency: The virtualization mechanism should not introduce significant delays or make programs run slower. Hardware support, such as Translation Lookaside Buffers (TLBs), is often leveraged to speed up memory address translation.\nSpace Efficiency: The OS must minimize the memory overhead used to manage the virtual memory system (e.g., data structures that map virtual to physical memory).\n\n\nProtection\nDefinition: The OS should ensure that processes are isolated from each other and from the OS itself. When a process performs memory operations (like load, store, or instruction fetch), it should not be able to access or interfere with the memory of other processes or the OS.\nPurpose: This isolation prevents one process from corrupting or compromising another, enhancing the system’s security and reliability. It also protects the OS from faulty or malicious processes.\nPrinciple of Isolation: Isolation ensures that if one process fails, it does not affect other processes or the OS. Some systems implement even stricter isolation, such as microkernels, which can further improve reliability by limiting interactions within the OS.\n\n\n\nTransparency Clarification\n\nCommon Misunderstanding: Transparency in this context means making the virtualization process invisible, not keeping information hidden. A transparent system is one that runs in the background without being noticed by applications, not one that is open or exposed.\nImplication: The OS should handle the mapping between virtual and physical addresses seamlessly, so applications don’t need to be aware of or interact with this process.\n\nImplications of Virtual Memory Goals\n\nIsolation and Security: Proper isolation enables a system where each process runs in its own space, preventing issues where one process could tamper with another. This is essential for system stability and for protecting sensitive data from potential threats.\nEfficiency Considerations: The OS must balance how to manage memory effectively, using algorithms and hardware to optimize space and speed. The goal is to avoid bottlenecks and ensure responsive performance.\n\nChapter 14: Memory API14.1 Types of MemoryTypes of Memory Allocation in C\n\nStack Memory (Automatic Memory)\n\nDefinition: Memory that is allocated and deallocated automatically by the compiler.\n\nManagement: The compiler takes care of stack memory management. When a function is called, space is allocated on the stack for local variables. When the function returns, this memory is deallocated automatically.\n\nDeclaration Example:\n1234void func() &#123;    int x; // Memory for integer x is allocated on the stack.    ...&#125;\n\nCharacteristics:\n\nShort-lived: The memory only persists as long as the function is running.\nScope: Limited to the function’s scope.\nUsage: Ideal for variables that don’t need to persist beyond the function call.\n\n\nLimitation: If you need data to persist after the function ends, storing it on the stack is not sufficient.\n\n\n\n\n\nHeap Memory (Dynamic Memory)\n\nDefinition: Memory that is explicitly allocated and deallocated by the programmer at runtime.\n\nManagement: The programmer is responsible for handling allocations and deallocations, which can lead to potential issues such as memory leaks or dangling pointers if not done correctly.\n\nAllocation Example:\n1234void func() &#123;    int *x = (int *) malloc(sizeof(int)); // Allocates an integer on the heap.    ...&#125;\n\nCharacteristics:\n\nLong-lived: The memory persists until it is explicitly deallocated using free().\nExplicit Control: The programmer must manage the memory lifecycle.\nUsage: Suitable for data that needs to exist beyond the function call or for variable-sized data structures (e.g., arrays).\n\nChallenges:\n\nMemory Leaks: Occur when memory is allocated but not properly deallocated.\nDangling Pointers: Occur when memory is deallocated but pointers still reference it.\nDebugging Complexity: Errors related to heap memory are often harder to find and fix.\n\n\n\n\n\nHow Memory Allocation Works\n\nStack Allocation:\nImplicit Management: Declaring int x; in a function automatically allocates space for x on the stack.\nLifetime: The memory is automatically reclaimed when the function exits.\n\n\nHeap Allocation:\nExplicit Management: The malloc() function allocates memory from the heap and returns a pointer to it.\nStorage: The pointer returned is stored on the stack, while the actual memory allocated resides on the heap.\nDeallocation: The programmer must use free() to release the memory and avoid leaks.\n\n\n\n14.2 The malloc() CallOverview of malloc()\n\nFunction Purpose: Allocates a specified number of bytes on the heap.\n\nReturn Value: Returns a void pointer to the allocated memory or NULL if the allocation fails.\n\nHeader File: To use malloc(), include #include &lt;stdlib.h&gt;, although the library is linked by default in C programs.\n\nPrototype:\n1void *malloc(size_t size);\n\n\nKey Points about malloc()\n\nParameter: Takes a single argument of type size_t, representing the number of bytes to allocate.\n\nUsage:\n\nIt is common practice to use the sizeof() operator to determine the amount of memory needed for data types or structures.\nExample for a single double:\n\n1double *d = (double *) malloc(sizeof(double));\n\n\nUnderstanding sizeof()\n\nCompile-Time Operator: sizeof() evaluates at compile time to determine the size of a type or variable in bytes.\n\nExample Use Cases:\n\nAllocating space for an array of integers:\n1int *x = malloc(10 * sizeof(int));  // Allocates space for 10 integers.\n\n\n\n\n\nChecking the size of a pointer (not the memory allocated):\n1printf(&quot;%d\\n&quot;, sizeof(x));  // Outputs the size of the pointer (e.g., 4 or 8 bytes, depending on the architecture).\n\n\n\nCommon Pitfall:\n\nUsing sizeof() on a pointer type (e.g., int *x) returns the size of the pointer itself, not the allocated space.\nCorrecting this: When you need the size of an allocated array or object, use sizeof() on the array or type, not on the pointer.\n\n\nStatic vs. Dynamic Memory Allocation:\n\nStatic Memory (e.g., int x[10];):\n1printf(&quot;%d\\n&quot;, sizeof(x));  // Outputs the total size of the array (e.g., 40 bytes for 10 integers on a 32-bit system).\n\n\n\n\nSpecial Considerations for Strings\n\nAllocation for Strings:\n\nWhen allocating memory for a string, remember to include space for the null terminator:\n1char *str = (char *) malloc(strlen(s) + 1);\n\nWhy Not sizeof()? \nUsing sizeof() on a string declared as char *str only gives the size of the pointer, not the length of the string it points to.\n\n\n\n\nPointer Casting and malloc()\n\nReturn Type: malloc() returns a void *, which is a generic pointer type.\n\nCasting:\n\nWhy Cast? Casting the result of malloc() (e.g., double *d = (double *) malloc(sizeof(double));) is not required for functionality but helps with code readability and ensures type safety, especially in C++.\n\nCaution: Casting does not affect the correctness or behavior of the program.\n\n\n\n\nTips for Using malloc()\n\nAlways Check for NULL: Verify that malloc() succeeded before using the allocated memory:\n1234int *arr = (int *) malloc(10 * sizeof(int));if (arr == NULL) &#123;    // Handle memory allocation failure&#125;\n\nFreeing Memory: Always use free() to release memory when it is no longer needed to avoid memory leaks:\n1free(arr);\n\n\n14.3 The free() CallTo free heap memory that is no longer in use, programmers simply call free():\n123int *x = malloc(10 * sizeof(int));...free(x);\n14.4 Common Errors\nForgetting to Allocate Memory\n\n\nError: Attempting to use strcpy() on an uninitialized pointer.\n\nIssue: Results in a segmentation fault, as there is no allocated space to copy data into.\n\nSolution: Always allocate memory before using a pointer, e.g., char *dst = (char *) malloc(strlen(src) + 1);.\n\n\n\nNot Allocating Enough Memory (Buffer Overflow)\n\n\nError: Allocating too little space for a string or array.\n\nIssue: Leads to writing past the allocated space, causing undefined behavior, potential crashes, or security vulnerabilities.\n\nExample: char *dst = (char *) malloc(strlen(src)); is insufficient for copying src, which includes a null terminator.\n\nSolution: Use malloc(strlen(src) + 1) to ensure enough space is allocated.\n\n\n\nForgetting to Initialize Allocated Memory\n\n\nError: Allocating memory without setting its contents.\n\nIssue: Reading uninitialized memory may yield random values, leading to unpredictable behavior.\n\nSolution: Initialize memory after allocation, using functions like memset() or explicit value assignment.\n\n\n\nForgetting to Free Memory (Memory Leaks)\n\n\nError: Failing to call free() after memory is no longer needed.\n\nIssue: Leads to memory leaks, which accumulate over time and can cause applications to run out of memory.\n\nSolution: Always free() memory when it is no longer needed, even in short-lived programs.\n\n\n\nFreeing Memory Before It’s Done (Dangling Pointers)\n\n\nError: Calling free() and then using the same pointer.\n\nIssue: Causes crashes or unexpected behavior if the pointer is dereferenced after being freed.\n\nSolution: Set the pointer to NULL after free() to prevent accidental use.\n\n\n\nFreeing Memory Multiple Times (Double Free)\n\n\nError: Calling free() on the same pointer more than once.\n\nIssue: Leads to undefined behavior, potential crashes, or data corruption.\n\nSolution: Avoid freeing the same memory block multiple times.\n\n\nBest Practices for Memory Management\n\nVerify Allocations: Always check if malloc() returns NULL to ensure memory was successfully allocated.\n\nUse sizeof() Properly: For correct memory allocation, use sizeof() to calculate the size of the type.\n\nFreeing Memory: Ensure memory is only freed once and set the pointer to NULL post-free.\n\nShort-Lived Programs: Memory leaks are less impactful, as the OS reclaims all memory when the process ends. However, developing good memory habits is essential for long-running programs, such as servers, where memory leaks can lead to crashes.\n\n\nThe Importance of free() and Garbage Collection\nIn contrast to languages with automatic garbage collection (e.g., Java or Python), C requires explicit management of memory. Improper use of malloc() and free() can lead to subtle, hard-to-find bugs.\nChapter 15: Address Translation15.1 Assumptions\nContiguous Allocation\n\n\nAssumption: The user’s address space is allocated contiguously in physical memory.\nImplication: This simplification allows the system to work without needing complex mapping between virtual and physical addresses.\n\n\nSmall Address Space\n\n\nAssumption: The size of the address space is less than the physical memory available.\nImplication: This constraint makes memory management simpler because there is no need to handle cases where the address space exceeds the physical memory capacity.\n\n\nUniform Address Space Size\n\n\nAssumption: Each address space is the same size.\nImplication: This simplifies memory management, as the system does not need to handle different sizes of address spaces, making allocation and mapping easier to manage.\n\n15.2 An ExampleThe example begins with a simple C code snippet that initializes a variable, increments it, and stores it back into memory:\n12345void func() &#123;    int x = 3000; // initial value    x = x + 3;   // line of code we are interested in    ...&#125;\nAssembly Code Representation\nThe compiler translates this code into assembly instructions, as shown in the x86 assembly snippet:\n123128: movl 0x0(%ebx), %eax ; load 0+ebx into eax132: addl $0x03, %eax      ; add 3 to eax135: movl %eax, 0x0(%ebx)  ; store eax back to memory\nMemory Accesses in the Process\nFrom the process’s perspective, the memory accesses during this code execution are:\n\nFetch the instruction at address 128.\n\nExecute the instruction, which loads the value at address 15 KB (variable x).\n\nFetch the instruction at address 132.\n\nExecute the instruction (no memory reference; it only modifies eax).\n\nFetch the instruction at address 135.\n\nExecute the instruction, which stores the updated value at address 15 KB.\n\n\nAddress Space and Physical Memory\n\nProcess’s Address Space: The process sees its own address space as starting from 0 and extending to a maximum of 16 KB. All memory references are assumed to be within this range.\n\nPhysical Memory Layout: The OS can place the process’s address space anywhere in physical memory, not necessarily starting at address 0. The section discusses how the process is relocated in physical memory without the process being aware of this relocation.\n\nFigure 15.2\n illustrates how physical memory might look after relocating the process. In the example:\n\nThe OS occupies the first slot of physical memory.\n\nThe process is placed starting at physical address 32 KB.\n\nOther slots in memory (e.g., 16 KB–32 KB and 48 KB–64 KB) are free.\n\n\n\n\n\nAddress Translation Mechanism\nThe main challenge here is how the OS can manage the translation of virtual addresses (used by the process) to physical addresses (actual locations in RAM). This translation must be transparent to the process to ensure that it can continue to execute as if it were working within its original virtual address space starting at 0.\nKey Concept: Interposition\n\nDefinition: Interposition is a powerful technique where an intermediary (in this case, the OS) steps in to modify or translate operations.\nApplication to Virtual Memory: The hardware interposes during each memory access made by the process, translating each virtual address to a physical address where the data is stored.\nBenefit: This translation is transparent to the process; it operates as if it has direct access to its original address space, even though the actual data might reside at different physical locations.\n\n15.3 Dynamic (Hardware-based) RelocationKey Concept: Base and Bounds\n\nDynamic Relocation (or Base and Bounds) uses hardware registers to translate virtual addresses (used by programs) into physical addresses (actual memory locations).\nEach CPU requires:\nBase Register: Contains the starting physical address where the process is loaded.\nBounds (Limit) Register: Defines the size of the address space, ensuring the process accesses memory within its range.\n\n\n\nAddress Translation Process\n\nBase Register: When a virtual address is generated by a process, it is added to the base register to calculate the physical address: \n\n\\text{Physical Address} = \\text{Virtual Address} + \\text{Base}\nBounds Register: Before translation, the CPU ensures that the virtual address is within the range defined by the bounds. If not:\n\nAn exception is raised, and the process is terminated.\n\n\nThese mechanisms provide:\n\nRelocation: Processes can run anywhere in physical memory.\nProtection: Prevents processes from accessing memory outside their allocated space.\n\n\n\nExample of Address Translation via base-and-bounds\n\nProcess loaded at 16 KB in physical memory, with a virtual address space of 4 KB:\nVirtual address 0 → Physical address 16 KB\nVirtual address 1 KB → Physical address 17 KB\nVirtual address 3 KB → Physical address 19 KB\nVirtual address 4.4 KB → Fault (out of bounds)\n\n\n\nAdvantages of Base and Bounds\n\nTransparency: Programs can operate as if starting at address 0, even when relocated in physical memory.\nProtection: Ensures processes only access their allocated memory.\nFlexibility: The OS can move processes to different memory locations dynamically.\n\nRole of the Memory Management Unit (MMU)\n\nThe MMU (part of the CPU) handles address translation using base and bounds registers.\nFuture memory management techniques extend the MMU’s functionality.\n\nNotes on Implementation\n\nBounds Register\n can operate in two modes:\n\nHolds the size of the address space (check before adding base).\nHolds the end physical address (check after adding base).\n\n\nBoth modes are logically equivalent; the first is used for simplicity.\n\n\n15.4 Hardware Support: A SummaryKey Hardware Requirements\nTo implement dynamic relocation effectively, hardware must support the following:\n\nCPU Modes:\n\nUser Mode: Applications run with restricted privileges to prevent unauthorized operations.\nKernel (Privileged) Mode: The OS runs with full machine access.\nA mode bit in the processor indicates the current mode, switching during system calls, interrupts, or exceptions.\n\n\nBase and Bounds Registers:\n\nBase Register: Stores the starting physical address of a process’s memory.\n\nBounds Register: Specifies the size (or end address) of the process’s memory.\n\nBoth registers are part of the \nMemory Management Unit (MMU)\n and ensure:\n\nAddress translation: Virtual address + Base → Physical address.\nAddress validation: Ensures memory references stay within bounds.\n\n\n\n\nPrivileged Instructions:\n\nThe OS must control the base and bounds registers using privileged instructions.\nThese instructions prevent user processes from modifying these registers and disrupting memory isolation.\n\n\nException Handling:\n\nThe CPU must raise exceptions when:\nA user process accesses an invalid address (out of bounds).\nA user process attempts to execute privileged instructions in user mode.\n\n\nThe OS must handle exceptions via exception handlers, such as terminating offending processes or logging violations.\n\n\nFree Memory Management:\n\nThe OS uses data structures like a free list to track unused memory regions, allowing efficient allocation to processes.\n\n\n\n15.5 Operating System IssuesKey Responsibilities of the OS\n\nMemory Management:\n\nProcess Creation:\nWhen a process is created, the OS allocates memory for its address space by:\nSearching a free list (a structure tracking unused memory regions).\nMarking the selected memory region as used.\n\n\n\n\nProcess Termination:\nUpon process termination, the OS:\nReclaims memory allocated to the process.\nReturns the memory to the free list for reuse.\n\n\n\n\n\n\nBase and Bounds Register Management:\n\nDuring \ncontext switches:\n\nThe OS saves the current process’s base and bounds register values to its Process Control Block (PCB).\nLoads the base and bounds register values for the next process.\n\n\nRelocating Processes:\n\nIf needed, the OS can move a process’s memory:\nDeschedules the process.\nCopies its address space to a new location.\nUpdates the saved base register in the PCB.\n\n\n\n\n\n\nException Handling:\n\nThe OS defines and installs exception handlers (e.g., illegal memory access handler) during system boot.\nHandles exceptions raised by the CPU, such as:\nOut-of-bounds memory access → OS terminates the process.\nIllegal instruction execution → OS terminates the process.\n\n\nProtects the system by responding firmly to misbehaving processes.\n\n\n\nOS Actions During System Boot\n\nInitializes key data structures and hardware:\nTrap Table: Defines addresses of handlers (e.g., for system calls, timers, exceptions).\nTimer: Sets up interrupt timers to ensure regular context switching.\nProcess Table: Tracks running and ready processes.\nFree List: Manages available memory regions.\n\n\n\nHardware-OS Interaction Timeline (Figures 15.5 and 15.6)\n\nAt Boot Time:\nThe OS sets up hardware and initializes data structures (e.g., free list, trap table).\n\n\nDuring Process Execution:\nThe OS sets base and bounds registers, then lets the process execute directly.\nMinimal OS intervention is needed unless:\nA timer interrupt occurs → triggers a context switch.\nThe process misbehaves (e.g., accessing invalid memory) → the OS handles the exception.\n\n\n\n\n\n\n\nChapter 16: Segmentation16.1 Segmentation: Generalized Base/BoundsKey Concepts in Segmentation\n\nSegments:\nA segment is a contiguous portion of the address space with a specific length and purpose.\nCommon logical segments:\nCode: Contains executable instructions.\nHeap: Used for dynamically allocated memory.\nStack: Manages function calls and local variables.\n\n\n\n\nAdvantages of Segmentation:\nEach segment can be placed independently in physical memory.\nAvoids allocating space for unused parts of virtual memory.\nEfficient for sparse address spaces (e.g., processes with large virtual address ranges but minimal actual usage).\n\n\n\nHow Segmentation Works\n\nMemory Layout:\n\nFigure 16.1: Virtual memory is divided into logical segments (e.g., Code, Heap, Stack).\n\n\nFigure 16.2: These segments are mapped into physical memory independently.\n\n\n\n\nMMU (Memory Management Unit):\n\nMaintains a set of base and bounds registers for each segment.\n\nExample from Figure 16.3:\n\nCode segment: Base = 32KB, Size = 2KB\n\nHeap segment: Base = 34KB, Size = 3KB\n\nStack segment: Base = 28KB, Size = 2KB\n\n\n\n\n\n\nAddress Translation:\n\nVirtual addresses are interpreted as segment + offset.\nHardware translates the virtual address to a physical address by:\nAdding the segment’s base value to the offset.\nChecking if the resulting address falls within the segment’s bounds.\n\n\n\n\n\nExample Address Translations\n\nCode Segment:\n\nVirtual address = 100 (falls in the Code segment, as per Figure 16.1).\nBase (Code) = 32KB.\nPhysical address = 100+32KB=32,868.\n\n\nHeap Segment:\n\nVirtual address = 4200.\nBase (Heap) = 34KB.\nOffset = 4200−4096=104 (Heap starts at 4KB in virtual memory).\nPhysical address = 34KB+104=34,920.\n\n\nInvalid Access:\n\nVirtual address = 7KB (outside the heap’s range of 4KB–7KB).\n\nHardware detects the out-of-bounds access and raises a segmentation fault.\n\nThe OS handles the fault, typically terminating the offending process.\n\n\n\n\n16.2 Which Segment Are We Referring To?Explicit Approach: Using Address Bits\n\nSegment Identification:\n\nThe top bits of the virtual address specify the segment.\n\nThe remaining bits represent the offset within the segment.\n\nExample (14-bit virtual address):\n\nBits 13-12: Segment identifier.\n\nBits 11-0: Offset.\n\n\n\n\n\n\nExample Translation:\n\nVirtual address: 4200\n\nSegment Identifier: Top 2 bits (010101) → Heap.\n\nOffset: Bottom 12 bits (0000_0110_1000) → 104\n\n\n\n\nTranslation steps:\n\nDetermine the segment (Heap).\nUse the segment’s base register (34KB) and bounds (3KB).\nCheck bounds: Offset 104&lt;3KB→ Valid.\nPhysical address: 34KB+104=34,920.\n\n\n\n\nHardware Logic (Simplified):\n123456789Segment = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SEG_SHIFT;Offset = VirtualAddress &amp; OFFSET_MASK;if (Offset &gt;= Bounds[Segment]) &#123;    RaiseException(PROTECTION_FAULT);&#125; else &#123;    PhysAddr = Base[Segment] + Offset;    Register = AccessMemory(PhysAddr);&#125;\n\nSEG_MASK: 0x3000(extract top 2 bits).\n\nSEG_SHIFT: 12 (shift to isolate the segment).\n\nOFFSET_MASK: 0xFFF(extract lower 12 bits).\n\n\n\nLimitations:\n\nUnused Segment: Using two bits for three segments leaves one unused.\n\nSegment Size Limit: Each segment is limited to 4KB (16KB/4).\n\n\n\n\nImplicit Approach: Inferring the Segment\n\nSegment Determination:\nCode segment: Addresses generated from the program counter.\nStack segment: Addresses based on the stack pointer.\nHeap segment: Any other addresses.\n\n\nAdvantages:\nAvoids reserving bits for segment identification.\nSupports dynamic segment sizes.\n\n\n\n16.3 What About The Stack?Key Adjustments for Backward-Growing Stacks\n\nGrowth Direction:\n\nMost segments (e.g., code, heap) grow forward (towards higher addresses).\n\nThe stack grows backward (towards lower addresses).\n\nTo accommodate this, the hardware includes an additional \ngrowth direction flag:\n\n1: Grows forward.\n0: Grows backward.\n\n\n\n\nSegment Register Example:\n\nEach segment tracks its base, size, and growth direction:\n\n\n\n\nStack Translation Example\n\nVirtual Address: 15KB = 0x3C00(binary: 11 1100 0000 0000).\n\nSegment Identifier: Top two bits (11) → Stack.\nOffset: 3KB\n\n\nTranslation Steps:\n\nIdentify the segment (Stack) and its base (28KB).\n\nConvert the offset for a \nbackward-growing segment:\n\nSubtract the max segment size (4KB) from the offset (3KB).\n3KB−4KB=−1KB\n\n\nCalculate the physical address:\n\nAdd the negative offset (−1KB) to the base (28KB).\n28KB−1KB=27KB.\n\n\n\n\n\n16.5 Fine-grained vs. Coarse-grained SegmentationCoarse-Grained Segmentation\n\nDefinition: Divides the virtual address space into a few large segments (e.g., code, heap, stack).\nExamples:\nSystems we’ve discussed so far (e.g., VAX/VMS).\nAddress space split into just three primary segments.\n\n\nAdvantages:\nSimple implementation with limited hardware requirements.\nEasy to understand and manage.\n\n\nDisadvantages:\nLess flexible.\nLarge segments may waste memory if their allocated space isn’t fully utilized.\n\n\n\nFine-Grained Segmentation\n\nDefinition: Breaks the address space into many smaller segments, offering more flexibility.\nHistorical Examples:\nMultics: Used fine-grained segmentation to improve flexibility and memory efficiency.\nBurroughs B5000: Supported thousands of segments, expecting compilers to split code and data into these smaller chunks.\n\n\nImplementation Requirements:\nA segment table stored in memory to manage the many segments.\nHardware capable of handling large numbers of segments and translating addresses efficiently.\n\n\nAdvantages:\nEnables more efficient memory use by identifying and utilizing only active segments.\nProvides flexibility for applications to organize code and data into logically independent segments.\n\n\nDisadvantages:\nIncreased complexity in hardware and OS support.\nMore overhead in managing segment tables.\n\n\n\n16.6 OS Support1. Context Switching\n\nRequirement: Segment registers must be saved and restored during a context switch.\nEach process has its own virtual address space.\nThe OS ensures the correct segment register values are loaded when a process is scheduled to run.\n\n\nWhy It Matters: Mismanaged segment registers could result in incorrect memory access, violating process isolation.\n\n2. Growing (or Shrinking) Segments\n\nDynamic Memory Requests:\nWhen a program requests additional memory (e.g., via malloc()), the heap may need to expand.\nIf the heap segment cannot accommodate the request, the OS must allocate more space.\nExample: The traditional UNIX sbrk() system call grows the heap and updates the segment size register.\n\n\nPossible OS Responses:\nGrant the Request: Update the segment size and allocate the required memory.\nReject the Request:\nIf there is insufficient physical memory.\nIf the process has already exceeded its allowed memory quota.\n\n\n\n\nKey Insight: The OS balances allocation requests while preventing overuse of physical memory.\n\n3. Managing Free Space\n\nChallenge: Physical memory fragmentation.\n\nSegments of varying sizes leave gaps in physical memory after allocation and deallocation.\n\nThis external fragmentation makes it difficult to find contiguous memory for new or growing segments.\n\n\n\n\n\nSolutions for Fragmentation:\n\nMemory Compaction:\nRearranges segments in physical memory to consolidate free space into a single contiguous block.\nSteps:\nStop processes.\nCopy segment data to new locations.\nUpdate segment registers to reflect the new physical addresses.\n\n\nDrawbacks:\nTime-consuming and processor-intensive.\nMay trigger additional rearrangement if segments grow.\n\n\n\n\nFree-List Management Algorithms:\nMaintain a list of free memory blocks to manage allocations more efficiently.\nAlgorithms:\nBest-Fit: Selects the smallest block that satisfies the request.\nWorst-Fit: Selects the largest available block.\nFirst-Fit: Allocates the first block that meets the size requirement.\nBuddy Algorithm: Divides memory into power-of-two-sized blocks, making splitting and merging efficient.\n\n\nLimitations: External fragmentation remains unavoidable, though good algorithms aim to minimize it.\n\n\n\nChapter 17: Free-Space Management17.1 Assumptions1. Memory Allocation Interface\n\nFunctions:\nmalloc(size_t size):\nTakes a size parameter (number of bytes requested).\nReturns a pointer (void*) to the allocated region of memory.\n\n\nfree(void* ptr):\nTakes a pointer and frees the associated memory chunk.\nKey Limitation: The size of the freed chunk is not provided; the allocator must infer it.\n\n\n\n\nImplication: Allocators must internally track the size of each allocated memory chunk to handle free() correctly.\n\n2. Heap and Free List\n\nHeap:\nThe managed memory region where allocation and deallocation occur.\n\n\nFree List:\nA data structure used to track available (free) memory chunks.\nFlexible Structure: While called a “list,” any structure that efficiently manages free space (e.g., trees, arrays) can be used.\n\n\n\n3. Focus on Fragmentation\n\nExternal Fragmentation:\nThe primary concern in allocator design.\nOccurs when free memory exists but is fragmented into non-contiguous chunks, preventing large allocations.\n\n\nInternal Fragmentation:\nOccurs when allocated chunks are larger than requested, leaving unused space within the chunk.\nAssumption: This chapter focuses on external fragmentation for simplicity and because it poses more interesting challenges.\n\n\n\n4. No Memory Relocation\n\nOnce memory is allocated to a client:\nIt cannot be moved until explicitly freed.\nThis lack of relocation precludes compaction, a common method to reduce fragmentation in OS-level memory management.\n\n\nWhy It Matters:\nFragmentation accumulates over time, as memory cannot be reshuffled to consolidate free space.\n\n\n\n5. Contiguous Region Management\n\nAllocators manage a single, contiguous region of memory.\n\nGrowth of the Heap:\n\nWhile allocators might request additional memory from the OS (e.g., via sbrk()), the assumption here is a fixed-size heap for simplicity.\n\n\n\n17.2 Low-level MechanismsSplitting and Coalescing\n\nFree List: A data structure that tracks free space in the heap. For example:\n\nHeap: \n\n\nFree List:\n\n\n\n\nSplitting: When allocating memory smaller than a free chunk:\n\nExample: Request 1 byte from a 10-byte free chunk at address 20.\n\nResult:\n\n\n\n\nCoalescing: Combines adjacent free chunks into one:\n\nExample: Freeing address 10 in [free][used][free].\n\nBefore: Three chunks (0–9, 10–19, 20–29).\n\n\nAfter Coalescing: One chunk (0–29).\n\n\n\n\n\nTracking Allocated Region Size\n\nHeader Information: Each allocated block contains a header before the user’s data.\n\nExample Header:\n\n1234typedef struct &#123;    int size;    int magic;&#125; header_t;\nThe size field indicates the allocated memory size, and magic is for integrity checks.\n\n\nAllocation with Header:\n\nMemory layout:\n\nHeader: Contains size (20) and magic number.\nAllocated Space: User’s 20 bytes.\n\n\nFree space searched for 28 bytes (20 + header size).\n\n\n\n\n\nEmbedding a Free List\n\nThe free list is embedded directly in the heap.\n\nEach free chunk includes metadata:\nA node in the free list is defined to contain:\n\nsize: the size of the chunk of free memory.\nnext: a pointer to the next node in the list.\n\n\n\nCode Structure:\n1234typedef struct __node_t &#123;    int              size;    struct __node_t *next;&#125; node_t;\n\nInitialization:\n\nStart with a single chunk covering the entire heap:\n\nExample: A 4096-byte heap minus header size.\n\n\n\n\n\nInitializing the Free List\n\nMemory Allocation: The heap is typically initialized with a system call such as mmap() to acquire a chunk of memory.\n\nSingle Free Node: Initially, the free list contains one entry that spans the entire heap size (minus the space needed for the header).\n\nCode Example:\n1234node_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, -1, 0);head-&gt;size = 4096 - sizeof(node_t);  // Adjust size for the node header.head-&gt;next = NULL;  // No other nodes in the list yet.\n\n\n\n\nMemory Layout:\n\nAt head, we have a node_t structure with:\nsize: 4088 bytes (4096 minus the size of node_t).\nnext: NULL (indicating no subsequent free chunks).\n\n\n\n\nHandling Allocations\n\nRequest for Memory: When a program requests, say, 100 bytes, the library searches for a free chunk that can satisfy this request.\n\nSplitting the Chunk:\n\nThe chosen chunk (4088 bytes) is split into:\nAllocated: 108 bytes (100 bytes + header size).\nRemaining Free Chunk: 3980 bytes.\n\n\n\n\nMemory Layout Post-Allocation:\n\nHeader: 8 bytes (size and magic number).\nAllocated Space: 100 bytes.\nUpdated Free List: Now points to the 3980-byte chunk.\n\n\n\n\nFurther Allocations and Fragmentation\n\nMultiple Allocations:\n\nAllocating additional 100-byte chunks leads to more headers and smaller free chunks.\n\nThe heap becomes fragmented, with free nodes potentially spread out.\n\nExample (Figure 17.5):\n\nThree 100-byte allocations, each with its header.\n\nFree space: One large chunk (3764 bytes).\n\n\n\n\n\n\n\nMemory Deallocation and Fragmentation\n\nFreeing a Chunk:\n\nWhen memory is freed (e.g., free(16500)), the library determines the chunk size and adds it back to the free list.\n\nVisualization After Freeing\n (Figure 17.6):\n\nThe free list now points to a 3764-byte chunk and possibly other smaller chunks.\n\n\n\n\n\n\nUncoalesced Free List:\n\nAfter freeing chunks, the list may contain non-contiguous free chunks, leading to fragmentation (Figure 17.7).\n\n\nProblem: Although the memory is technically available, it appears fragmented and cannot be used as a single large block.\n\nCoalescing: To solve fragmentation, adjacent free chunks must be merged to form larger contiguous free blocks.\n\n\n\n\n17.3 Basic StrategiesThis section discusses various strategies for managing free space in memory allocators, highlighting the trade-offs between speed and fragmentation reduction. Here’s an overview of the key strategies:\n 1. Best Fit\n\nDescription: Searches the entire free list to find the smallest block that fits the requested size.\nAdvantages:\nReduces internal fragmentation by minimizing wasted space within allocated blocks.\n\n\nDisadvantages:\nRequires a full scan of the free list, leading to high performance costs.\nOften results in many small, unusable free chunks, increasing external fragmentation.\n\n\n\n2. Worst Fit\n\nDescription: Selects the largest free block to satisfy the allocation request.\nAdvantages:\nTries to avoid creating many small chunks, leaving larger contiguous free spaces.\n\n\nDisadvantages:\nRequires a full scan of the free list, leading to high overhead.\nStudies show it performs poorly, often resulting in excessive fragmentation.\n\n\n\n3. First Fit\n\nDescription: Starts from the beginning of the free list and allocates the first block that is large enough.\nAdvantages:\nFaster than exhaustive strategies (like Best Fit or Worst Fit) as it stops searching as soon as it finds a suitable block.\n\n\nDisadvantages:\nCan “pollute” the beginning of the free list with small leftover chunks.\n\n\nOptimization: Using address-based ordering of the free list simplifies coalescing and reduces fragmentation.\n\n4. Next Fit\n\nDescription: Similar to First Fit, but instead of starting at the beginning of the list for each allocation, it resumes from where the previous search ended.\nAdvantages:\nSpreads searches more uniformly across the free list, reducing small leftover chunks at the start.\n\n\nDisadvantages:\nSimilar to First Fit in terms of search cost and fragmentation tendencies.\n\n\n\nExamples\n\nInitial Free List: 10 → 30 → 20 → NULL (block sizes in the list)\nAllocation Request: 15 bytes\nBest Fit: Chooses block 20 (smallest fit) → Remaining list: 10 → 30 → 5 → NULL\nWorst Fit: Chooses block 30 (largest fit) → Remaining list: 10 → 15 → 20 → NULL\nFirst Fit: Also chooses block 30 in this example, as it’s the first block large enough → Remaining list: 10 → 15 → 20 → NULL\n\n\n\n17.4 Other Approaches1. Segregated Lists\n\nDescription: Maintains separate free lists for different object sizes. Frequently requested sizes are handled by dedicated lists, while other sizes use a general-purpose allocator.\nAdvantages:\nReduces fragmentation by dedicating memory pools to specific sizes.\nImproves allocation and deallocation speed for common sizes due to the reduced need for complex searches.\n\n\nChallenges:\nBalancing memory allocation between specialized pools and the general allocator.\nManaging space when specialized pools run low or are underutilized.\n\n\nExample: The slab allocator in the Solaris kernel, created by Jeff Bonwick:\nAllocates memory in slabs (multiples of page size).\nKeeps freed objects in an initialized state to reduce initialization and destruction costs.\nReclaims slabs with all zeroed reference counts when memory is needed elsewhere.\n\n\n\n2. Buddy Allocation\n\nDescription: A hierarchical allocation system based on powers of two. Free memory is split recursively into halves until a block of the required size is found.\n\nKey Operations:\n\nAllocation: Divides memory until the smallest power-of-two block that can accommodate the request is found.\n\nDeallocation: Checks if the “buddy” (block of the same size) is free and coalesces recursively up the tree.\n\n\n\n\nAdvantages:\n\nSimplifies coalescing, as the buddy of a block can be determined by flipping a specific bit in the address.\nEfficient for block merging and splitting.\n\n\nDisadvantages:\n\nCan lead to internal fragmentation, as memory is allocated only in power-of-two block sizes.\n\n\n\n3. Other Advanced Techniques\n\nScaling Challenges:\nBasic list-based allocators struggle to scale, especially in systems with high allocation demands or multiple processors.\nTo address this, advanced allocators use more complex data structures, such as:\nBalanced binary trees\nSplay trees\nPartially-ordered trees\n\n\n\n\nConcurrency-Friendly Allocators:\nModern systems with multi-threaded workloads require scalable allocators designed for multiprocessor environments.\nExamples:\nBerger et al. (2000): A scalable allocator for multi-threaded systems.\nEvans (2006): Focuses on performance in parallel environments.\n\n\n\n\n\nChapter 18: Introduction of Paging18.1 A Simple Example And OverviewAddress Space &amp; Pages:\n\nVirtual Address Space: Divided into pages. Each page has a fixed size, making memory management straightforward.\n\nExample: A tiny 64-byte address space has 4 pages, each 16 bytes.\n\n\n\n\nPhysical Memory: Divided into page frames. Each frame can hold one page.\n\nExample: A 128-byte physical memory has 8 frames.\n\n\n\n2. Mapping Virtual to Physical Memory\nVirtual addresses are mapped to physical memory through a page table.\n\nPage Table: A data structure maintained by the operating system to track the mapping of virtual pages to physical frames.\n\nExample\n: In this scenario:\n\nVirtual page 0 → Physical frame 3\n\nVirtual page 1 → Physical frame 7\n\nVirtual page 2 → Physical frame 5\n\nVirtual page 3 → Physical frame 2\n\n\n\n\n\n\n\n3. Address Translation Process\nProblem: How to translate a virtual address VA=21 into a physical address?\n(1) Break Down the Virtual Address\n\nVirtual address structure:\n\nAddress space is 64 bytes → Requires 6 bits (2^6 = 64).\nPage size is 16 bytes → Offset within a page requires 4 bits.\nTop 2 bits are the Virtual Page Number (VPN); bottom 4 bits are the offset.\n\n\nVirtual address VA = 21 in binary: 010101:\n\nVPN: 010101 (Virtual page 1).\n\nOffset: 010101 (5th byte within the page).\n\n\n\n\n\n(2) Look Up the Page Table\n\nFrom the page table, find the physical frame for VPN 01:\nVirtual page 1 → Physical frame 7 (111).\n\n\n\n(3) Construct the Physical Address\n\nReplace the virtual page number with the physical frame number:\n\nVirtual address: 010101 (VPN 01, Offset 0101).\nPhysical address: 1110101(PFN 111, Offset 0101).\n\n\nResulting Physical Address: 1110101(2)=117(10)\n\n\n\n(4) Access Memory\n\nThe system accesses data at physical address 117, completing the memory operation.\n\n18.2 Where Are Page Tables Stored1. The Size Problem of Page Tables\nPage tables can grow to enormous sizes, especially in systems with large address spaces. Let’s break it down:\n\n32-bit Address Space Example:\n\nAddress space: 2^32bytes.\nPage size: 4 KB (2^12).\nVirtual Address Structure:\n20-bit VPN (Virtual Page Number).\n12-bit Offset.\n\n\nTotal Pages: 2^20\n\n\nMemory Requirement:\n\nAssume each Page Table Entry (PTE) takes 4 bytes.\n\nTotal memory for one page table: \n\n2^{20} \\times 4 \\, \\text{bytes} = 4 \\, \\text{MB}\nFor 100 processes: \n\n100 \\times 4 \\, \\text{MB} = 400 \\, \\text{MB}\nModern Challenge: Even with gigabytes of memory, dedicating hundreds of MB to page tables is inefficient.\n\n\n\n\n2. Why Not Store Page Tables in On-Chip Memory?\n\nLimited On-Chip Space: Hardware like the Memory Management Unit (MMU) does not have enough space for full page tables.\nInstead, page tables for each process are stored in physical memory.\nLater, we’ll explore how virtualization of OS memory allows page tables themselves to be paged or swapped to disk.\n\n\n\n18.3 What’s Actually In The Page Table1. Overview of Page Table Structure\nThe page table is a data structure used by the operating system to map virtual addresses to physical addresses. Specifically, it translates Virtual Page Numbers (VPNs) to Physical Frame Numbers (PFNs).\n\nSimple Implementation:\nA linear page table is the most straightforward form.\nIt is implemented as an array, where:\nThe VPN serves as the index.\nThe corresponding Page Table Entry (PTE) stores the mapping to the physical frame.\n\n\nThis design is simple but inefficient for large address spaces, as it requires significant memory.\n\n\n\n2. Key Components of a Page Table Entry (PTE)\nEach PTE contains several important bits to manage memory effectively. Let’s break them down:\na. Valid Bit\n\nPurpose: Indicates whether the page’s translation is valid.\nUsage:\nPages that the process is not allowed to access (e.g., unallocated pages in a sparse address space) are marked invalid.\nIf an invalid page is accessed, the CPU triggers a trap to the OS.\nExample:\nAt program startup, code and heap occupy one part of the address space, and the stack another.\nUnused memory in between is marked invalid to save physical memory.\n\n\n\n\n\nb. Protection Bits\n\nDefine how the page can be accessed:\nRead: Indicates if the page can be read.\nWrite: Determines if the page can be modified.\nExecute: Specifies if instructions can be executed from the page.\n\n\nTrap on Violation: If the process attempts an operation not allowed by the protection bits, a trap to the OS occurs.\n\nc. Present Bit\n\nIndicates whether the page is currently in physical memory or has been swapped to disk.\nIf not present:\nA page fault occurs.\nThe OS retrieves the page from disk and loads it into physical memory.\n\n\n\nd. Dirty Bit\n\nTracks whether the page has been modified since it was loaded into memory.\nPurpose: Helps the OS determine whether it needs to write the page back to disk during eviction.\n\ne. Reference/Accessed Bit\n\nTracks whether the page has been accessed recently.\nUsage: Critical for page replacement algorithms to decide which pages to keep in memory.\n\nf. Caching Bits\n\nHardware-Specific: Control how the hardware caches the page.\n\n3. Example: x86 Page Table Entry\n\n\nP (Present): Indicates if the page is in memory.\nR/W (Read/Write): Determines if writes are allowed.\nU/S (User/Supervisor): Controls access based on privilege level.\nA (Accessed): Indicates if the page has been accessed.\nD (Dirty): Tracks if the page has been modified.\nPFN (Page Frame Number): Points to the actual location in physical memory.\nPWT (Page Write-Through): Determines write-through caching policy.\nPCD (Page Cache Disable): Disables caching for the page.\nPAT (Page Attribute Table): Additional control for caching.\nG (Global): Prevents the TLB from being flushed on context switches.\n\n18.4 Paging: Also Too Slow1. The Problem with Page Tables in Memory\nUsing page tables introduces two key challenges:\n\nSpace Overhead: Page tables can be very large, especially for processes with sparse address spaces.\n\nPerformance Overhead:\nAccessing memory becomes slower because each memory reference now requires \ntwo memory accesses:\n\nOne to retrieve the Page Table Entry (PTE).\nOne to fetch the actual data.\n\n\n\n2. Example Walkthrough of a Memory Access\nConsider the instruction:\n1movl 21, %eax\nThe process requires data from virtual address 21, which must be translated to the corresponding physical address before fetching the data.\nStep-by-Step Translation Process\n\nExtract the Virtual Page Number (VPN):\n\nUse a VPN_MASK to isolate the VPN bits from the virtual address.\n\nShift these bits right by SHIFT to form the integer VPN:\n1VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT\n\n\n\nLocate the Page Table Entry (PTE):\n\nUse the Page Table Base Register (PTBR) to find the physical address of the page table.\n\nAdd the VPN (multiplied by the size of each PTE) to the PTBR to locate the specific PTE:\n1PTEAddr = PTBR + (VPN * sizeof(PTE))\n\n\n\nFetch the PTE from Memory:\n\nRetrieve the PTE by accessing memory at PTEAddr:\n1PTE = AccessMemory(PTEAddr)\n\n\n\nValidate the Access:\n\nCheck the Valid bit of the PTE:\n\nIf False, trigger a segmentation fault.\n\n\nCheck the \nprotection bits:\n\nIf access is not allowed, trigger a protection fault.\n\n\n\n\nForm the Physical Address (PhysAddr):\n\nExtract the offset from the virtual address:\n1offset = VirtualAddress &amp; OFFSET_MASK\n\nConcatenate the \nPage Frame Number (PFN) from the PTE with the offset:\n1PhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offset\n\n\n\nFetch the Desired Data:\n\nAccess memory at PhysAddr to retrieve the data and store it in the register:\n1Register = AccessMemory(PhysAddr)\n\n\n\n\nSummary of Translation Protocol\nThe process is illustrated in Figure 18.6:\n123456789101112131415161718191 // Extract the VPN from the virtual address2 VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT34 // Form the address of the page-table entry (PTE)5 PTEAddr = PTBR + (VPN * sizeof(PTE))67 // Fetch the PTE8 PTE = AccessMemory(PTEAddr)910 // Check if process can access the page11 if (PTE.Valid == False)12     RaiseException(SEGMENTATION_FAULT)13 else if (CanAccess(PTE.ProtectBits) == False)14     RaiseException(PROTECTION_FAULT)15 else16     // Access is OK: form physical address and fetch it17     offset = VirtualAddress &amp; OFFSET_MASK18     PhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offset19     Register = AccessMemory(PhysAddr)\n3. Performance Issue: Double Memory Access\n\nTwo memory references are required per instruction:\nFetch the PTE.\nFetch the data using the translated physical address.\n\n\nThis effectively doubles memory access time, leading to significant performance degradation.\n\n18.5 A Memory Trace1. Program and Context\n\nThe program initializes an integer array:\n1234int array[1000];...for (i = 0; i &lt; 1000; i++)    array[i] = 0;\n\nEach iteration of the loop sets one array element to 0.\n\n\n2. Assembly Instructions\nDisassembling the program yields the following assembly code:\n\nmovl $0x0, (%edi, %eax, 4)\nWrites 0 into the memory location calculated as (%edi + %eax * 4).\n%edi: Base address of the array.\n%eax: Current array index.\n\n\nincl %eax: Increments %eax to point to the next array element.\ncmpl $0x03e8, %eax: Compares %eax with 1000 (end condition).\njne 0x1024: Jumps back to the first instruction if %eax is not yet 1000.\n\nEach loop iteration processes one array element, requiring several memory operations.\n3. Paging Mechanism\nAssumptions:\n\nVirtual Address Space: 64KB (unrealistically small for clarity).\nPage Size: 1KB.\nPage Table Location: Physical address 1024 (1KB).\nMappings:\nInstructions: Virtual page 1 (VPN 1) maps to physical frame 4 (PFN 4).\nArray: Virtual pages 39–42 (VPN 39–42) map to physical frames 7–10 (PFN 7–10).\n\n\n\nPage Table Format:\n\nA linear page table (array-based) is used.\nVirtual addresses are translated by looking up their respective entries in the page table\n\n4. Memory Access Breakdown\nEach memory access involves two steps:\n\nPage Table Lookup: Translate the virtual address to a physical address.\nData Access: Use the physical address to fetch the instruction or data.\n\nExample for One Loop Iteration:\n\nFetch the movl instruction:\n\nTranslate virtual address (VA) for the instruction using the page table.\nFetch the instruction from physical memory.\n\n\nFetch the array element address:\n\nTranslate the VA of the array element (e.g., 40000 for the first element).\nFetch the page table entry for VPN 39.\nFetch the physical address (7232 for the first element).\n\n\nWrite 0 to the physical address (array data).\n\nFetch and execute the incl, cmpl, and jne\ninstructions:\n\nEach requires translating their respective VAs to PAs.\n\n\n\nTotal Memory Accesses Per Iteration:\n\nInstruction fetches: 8 (page table lookup 4 + data access 4).\nExplicit update: 2(page table lookup + data write).\nTotal: 10 memory accesses.\n\n5. Visualization (Figure 18.7)\n\nX-axis: Memory access sequence across five iterations.\nY-axis:\nBlack: Virtual and physical addresses of instructions.\nDark gray: Virtual and physical addresses of array data.\nLight gray: Physical addresses for page table lookups.\n\n\n\nThe graph shows:\n\nInstruction fetches are repeated every iteration.\nArray accesses shift to new virtual pages (VPN 39–42) as the loop progresses.\n\n\nChapter 19: Paging: Faster Translations (TLBs)19.1 TLB Basic Algorithm1. Virtual Address Translation Begins\n\nThe CPU generates a virtual address (VA).\nThe virtual address is divided into:\nVirtual Page Number (VPN): Determines which virtual page is being accessed.\nOffset: Specifies the exact byte within the page.\n\n\n\n2. Check the TLB\n\nThe hardware checks if the translation for the VPN exists in the TLB:\nTLB Hit:\nThe desired translation is found in the TLB.\nThe hardware retrieves the Page Frame Number (PFN) from the TLB entry.\nThe physical address (PA) is constructed by combining the PFN with the original offset.\nThe memory is accessed, completing the operation quickly.\n\n\nTLB Miss:\nThe desired translation is not in the TLB.\nThe hardware must now access the page table to find the correct translation.\n\n\n\n\n\n3. Handling a TLB Miss\n\nThe hardware performs these additional steps:\nAccesses the page table in physical memory using the VPN to find the corresponding PFN.\nPerforms protection checks to ensure the process has the appropriate access rights (e.g., read/write).\nUpdates the TLB with the new translation (VPN → PFN), so future accesses to this page are faster.\nRetries the memory access, which now succeeds because the TLB holds the required translation.\n\n\n\n4. Costs of a TLB Miss\n\nA TLB hit is fast because the TLB is close to the CPU and optimized for speed.\nA TLB miss incurs a high cost because:\nThe page table must be accessed, which requires at least one additional memory access (and possibly more if multi-level page tables are used).\nUpdating the TLB takes time.\n\n\n\n19.2 Example: Accessing an Array with TLBSystem Setup\n\nVirtual Address Space: 8 bits (256 possible addresses).\nPage Size: 16 bytes.\nVPN: 4 bits (16 virtual pages).\nOffset: 4 bits (16 bytes per page).\n\n\nArray: 10 integers, each 4 bytes, starting at virtual address 100.\nTotal size: 10×4 = 40 bytes.\n\n\n\nArray Placement in Virtual Memory\nThe array starts at address 100 (binary: 01100100). Breaking this into VPN and offset:\n\nVPN = 0110 (Page 6)\nOffset = 0100 (Byte 4 in the page)\n\nArray Layout:\n\nAccess Pattern in C\n1234int sum = 0;for (int i = 0; i &lt; 10; i++) &#123;    sum += a[i];&#125;\nDetailed TLB Walkthrough\nInitial Condition: TLB is empty at the start.\nAccess 1: a[0]\n\nVirtual Address: 100 → VPN = 06, Offset = 04.\nTLB Lookup: Miss.\nAction: Hardware fetches the physical frame number (PFN) for VPN = 06 from the page table and updates the TLB.\nCost: High (page table lookup).\n\nAccess 2: a[1]\n\nVirtual Address: 104 → VPN = 06, Offset = 08.\nTLB Lookup: Hit.\nAction: Address translation is done using the cached TLB entry for VPN = 06.\nCost: Low (TLB hit).\n\nAccess 3: a[2]\n\nVirtual Address: 108 → VPN = 06, Offset = 12.\nTLB Lookup: Hit.\nCost: Low.\n\nAccess 4: a[3]\n\nVirtual Address: 112 → VPN = 07, Offset = 00.\nTLB Lookup: Miss.\nAction: Fetch the translation for VPN = 07 from the page table and update the TLB.\nCost: High.\n\nAccess 5-7: a[4], a[5], a[6]\n\nAll these accesses are within VPN = 07.\nTLB Lookup: Hits for all.\nCost: Low.\n\nAccess 8: a[7]\n\nVirtual Address: 128 → VPN = 08, Offset = 00.\nTLB Lookup: Miss.\nAction: Fetch the translation for VPN = 08 and update the TLB.\nCost: High.\n\nAccess 9-10: a[8], a[9]\n\nBoth accesses are within VPN = 08.\nTLB Lookup: Hits for both.\nCost: Low.\n\nTLB Activity Summary\n\nAccess Pattern: Miss, Hit, Hit, Miss, Hit, Hit, Hit, Miss, Hit, Hit.\n\nHit Rate:\n\n\\text{Hit Rate} = \\frac{\\text{Hits}}{\\text{Total Accesses}} = \\frac{7}{10} = 70\\%.\n\n19.3 Who Handles the TLB Miss?1. Hardware-Managed TLBs\n\nDescription:\n\nIn hardware-managed TLBs, the hardware itself resolves the TLB miss without any direct involvement from the operating system.\nThe hardware must have detailed knowledge of the page table’s location and structure to perform this task.\n\n\nProcess:\n\nThe hardware detects a TLB miss during address translation.\nIt automatically walks the page table, retrieving the physical frame number (PFN) associated with the requested virtual page.\nThe retrieved translation is inserted into the TLB.\nThe instruction that caused the miss is retried, now resulting in a TLB hit.\n\n\nKey Features:\n\nRequires a page-table base register (e.g., CR3 register in Intel x86) to locate the page table.\nThe hardware assumes a fixed, predefined format for the page table (e.g., multi-level page tables in x86).\nThe entire process is handled without invoking the OS.\n\n\n\n2. Software-Managed TLBs\n\nDescription:\n\nModern architectures like MIPS R10k or Sun SPARC v9 handle TLB misses using the operating system.\nThe hardware raises a trap or exception on a TLB miss, transferring control to the operating system’s TLB miss handler.\n\n\nProcess:\n\nOn a TLB miss, the hardware raises an exception and enters kernel mode.\n\nControl is passed to the OS, which executes a TLB miss handler\n This handler:\n\nLocates the translation in the page table.\nUpdates the TLB with the correct virtual-to-physical mapping using privileged instructions.\n\n\nThe OS returns from the trap, resuming execution at the instruction that caused the TLB miss, ensuring it now hits in the TLB.\n\n\n\nKey Features:\n\nGreater flexibility: The OS can implement any page table structure or data format it prefers.\nThe TLB miss handler is carefully managed to avoid infinite TLB misses, often by keeping handler code in unmapped physical memory or using reserved TLB entries.\n\n\nExample: RISC processors (e.g., MIPS or SPARC) use software-managed TLBs to simplify hardware design and allow OS control over memory management.\n\n\nComparison: Hardware vs. Software Management\n\n\n\n\nAspect\nHardware-Managed TLBs\nSoftware-Managed TLBs\n\n\n\n\nComplexity\nMore complex hardware design.\nSimpler hardware, relies on OS.\n\n\nFlexibility\nFixed page table structure.\nOS decides the page table structure.\n\n\nPerformance\nFaster on a miss due to direct hardware handling.\nSlightly slower as it involves an exception.\n\n\nExamples\nIntel x86 (older), CISC architectures.\nMIPS, SPARC, RISC architectures.\n\n\n\n\n19.4 TLB Contents: What’s In There?1. TLB Entry Structure\nA typical TLB entry contains a few key components:\n\nVPN (Virtual Page Number): This is the virtual page number, which is the part of the virtual address that is translated into a physical address. It’s the index into the page table.\nPFN (Physical Frame Number): The physical frame number is the corresponding physical address for the page, which is what the memory management unit (MMU) uses to access the actual memory location.\nOther Bits: These additional bits control and define the behavior of the entry. These can include:\nValid Bit: This bit indicates whether the entry contains a valid translation. If the valid bit is 0, it means the entry is not valid, and a TLB miss occurs. If it’s 1, the translation is valid and ready to use.\nProtection Bits: These determine the types of access allowed for the page, similar to the protection flags in the page table. For instance:\nRead/Write (RW): Indicates whether the page can be read from or written to.\nRead/Execute (RX): Specifies if the page can be executed as code.\nRead-Only (RO): Some pages are marked read-only, preventing write operations.\n\n\nAddress-Space Identifier (ASID): Some TLBs include an ASID to distinguish between different address spaces, useful for multi-tasking environments. This ensures that one process’s TLB entries don’t accidentally mix with another’s.\nDirty Bit: This bit tracks whether the page has been modified (i.e., written to) since it was last loaded into memory. This helps the operating system determine if the page needs to be written back to disk (for paging or swapping) when it’s evicted from memory.\n\n\n\n2. Example of TLB Entry\nA simple TLB entry might look like this:\n\n\n\n\nVPN\nPFN\nValid Bit\nProtection Bits\nASID\nDirty Bit\n\n\n\n\n0x12345\n0x6789A\n1\nRead/Write\n0x01\n0\n\n\n\n\n\nVPN: The virtual page number (0x12345).\nPFN: The physical frame number (0x6789A).\nValid Bit: Indicates that this entry is valid (1).\nProtection Bits: This page can be read and written to.\nASID: Belongs to address space 0x01.\nDirty Bit: This page hasn’t been modified (0).\n\n19.5 TLB Issue: Context SwitchesThe Problem with Context Switching\nConsider the following scenario:\n\nProcess P1 is running and has its 10th virtual page (VPN 10) mapped to physical frame 100.\nProcess P2 is about to be run, and it has its 10th virtual page (VPN 10) mapped to physical frame 170.\n\nThe TLB could have entries like:\n\n\n\n\nVPN\nPFN\nValid\nProtection\n\n\n\n\n10\n100\n1\nrwx\n\n\n10\n170\n1\nrwx\n\n\n\n\nHere, the TLB is ambiguous: it contains entries for VPN 10 that map to different physical frames (100 and 170). When the hardware searches for a translation, it cannot determine which entry belongs to P1 or P2. This can lead to incorrect translations and errors when the TLB is used for address lookup.\nApproaches to Managing TLB on Context Switch\n1. Flushing the TLB\n\nConcept: Clear all entries in the TLB upon context switches.\n\nMechanism:\n\nThis can be achieved using a privileged hardware instruction in a software-managed system or automatically by changing the page-table base register (PTBR) in a hardware-managed TLB.\nThe flush operation sets all valid bits to 0, invalidating all entries.\n\n\nPros:\n\nEnsures that there are no stale or incorrect translations in the TLB for the new process.\n\n\nCons:\n\nIntroduces performance overhead, as the new process will incur TLB misses when accessing data and code pages for the first time.\n\nIf context switches are frequent, the performance cost can be significant due to the repeated TLB misses.\n\n\n\n\n2. Using Address Space Identifiers (ASIDs)\n\nConcept: Add an ASID field to TLB entries to differentiate between translations for different processes.\nMechanism:\nEach TLB entry includes an ASID (a process identifier, typically smaller in size than a PID, e.g., 8 bits).\nThe TLB can store entries for multiple processes simultaneously, and the ASID helps the hardware distinguish which translations belong to the currently running process.\n\n\nExample:\n\n\n\n\n\nVPN\nPFN\nValid\nProtection\nASID\n\n\n\n\n10\n100\n1\nrwx\n1\n\n\n10\n170\n1\nrwx\n2\n\n\n\n\n\nPros:\nReduces the need to flush the TLB on context switches, as translations for other processes are ignored based on the ASID.\n\n\nCons:\nThe OS must update a privileged register with the current process’s ASID during a context switch.\nThe TLB needs additional logic to filter entries based on the ASID, which can add complexity.\n\n\n\nTLB Sharing Across Processes\nASIDs also enable TLB sharing in cases where multiple processes share a physical page:\n\n\n\n\nVPN\nPFN\nValid\nProtection\nASID\n\n\n\n\n10\n101\n1\nr-x\n1\n\n\n50\n101\n1\nr-x\n2\n\n\n\n\n\nExample: Process 1 (P1) and Process 2 (P2) share physical page 101, but map it to different virtual pages (VPN 10 and 50).\nBenefits:\nCode sharing between processes, such as shared libraries, reduces memory overhead.\nThe TLB can hold separate entries for each process’s mapping, differentiated by the ASID.\n\n\n\n19.6 TLB Replacement PolicyCommon TLB Replacement Policies\n1. Least Recently Used (LRU)\n\nConcept: The least recently used (LRU) entry is evicted when a new entry needs to be installed.\nRationale: This policy assumes that entries that have not been accessed recently are less likely to be used in the near future, leveraging the concept of temporal locality in memory access.\nLimitations: Can perform poorly in certain scenarios, such as when a program loops over n + 1 pages with a TLB of size n, resulting in a miss for every access (known as the “thrashing” problem). In such cases, LRU is suboptimal.\n\n2. Random Replacement Policy\n\nConcept: A TLB entry is evicted at random when a new entry needs to be installed.\nRationale: This approach is simple and avoids some edge cases where more sophisticated policies, like LRU, might behave poorly.\nUse Case: Random replacement is particularly effective when the TLB size is small relative to the working set of the program.\n\n3. Other Policies\nWhile LRU and Random are among the most common, other replacement strategies are also possible:\n\nFirst-In, First-Out (FIFO): Evicts the oldest entry in the TLB. It’s simpler than LRU but may not be as effective in optimizing for cache hits.\nLeast Frequently Used (LFU): Evicts the entry that has been accessed the fewest times. This policy can be complex to implement due to the need for tracking access counts.\nClock Policy: A practical approximation of LRU, using a circular buffer and a “clock hand” to check and evict entries based on their reference bits. If an entry’s reference bit is 1, it’s cleared and the hand moves to the next entry. If it’s 0, the entry is evicted.\n\n19.7 Real TLB Entry: The MIPS R4000The MIPS R4000 is a modern system that uses software-managed TLBs. This section discusses the structure of a TLB entry in the MIPS R4000 and the associated concepts.\nStructure of a MIPS TLB Entry:\n\nAddress Space: The MIPS R4000 supports a 32-bit address space with 4KB pages. The virtual address is split into:\n20-bit Virtual Page Number (VPN).\n12-bit Offset (used within the page).\n\n\nVPN Size: While 20 bits are expected for the VPN in a general 32-bit virtual address, only 19 bits are used for user addresses. This is because half of the 32-bit address space is reserved for the kernel, reducing the VPN size to 19 bits.\nPhysical Frame Number (PFN): The PFN has up to 24 bits, supporting up to 64 GB of physical memory (with 4KB pages).\n\n\nTLB Management in MIPS R4000:\n\nSoftware-Managed TLB: Unlike hardware-managed TLBs, the MIPS R4000 requires software to manage TLB entries. The OS must explicitly update and maintain the TLB contents.\nInstructions for Managing the TLB:\nTLBP: Probes the TLB to check if a particular translation exists.\nTLBR: Reads a TLB entry’s contents into registers.\nTLBWI: Writes to a specific TLB entry.\nTLBWR: Replaces a random TLB entry.\n\n\nPrivileged Instructions: These TLB management instructions are privileged and should only be used by the OS. Allowing a user process to modify the TLB could lead to severe security issues, such as executing arbitrary code or disrupting system operations.\n\nChapter 20: Paging: Smaller Tables20.1 Simple Solution: Bigger PagesOne way to reduce the size of the page table is by using larger page sizes. This approach helps minimize the number of entries needed in the page table, thus reducing its total size. Let’s consider the following example for a 32-bit address space:\n\nExample with 16KB Pages:\nThe address space would be divided into a 18-bit Virtual Page Number (VPN) and a 14-bit offset.\nIf each Page Table Entry (PTE) takes 4 bytes, the linear page table would have 2182^{18} entries.\nThis results in a total page table size of 1MB, which is a factor of four smaller than a table with 4KB pages (which would have 2202^{20} entries for the same 32-bit address space).\n\n\n\nChallenges with Larger Pages:\n\nInternal Fragmentation: One significant drawback of using larger pages is internal fragmentation, where space within a page is wasted if applications do not fully use the entire page. This inefficiency happens because the allocated pages may only be partially filled, leading to unused memory within those pages.\n\nMemory Utilization: Applications may need to allocate pages but end up using only a small portion of each. This can lead to the memory filling up with many partially used large pages, reducing overall memory efficiency.\n\n\n20.2 Hybrid Approach: Paging and SegmentsConcept of Hybrid Paging and Segmentation:\nA hybrid solution involves using multiple page tables—one for each logical segment of the address space (e.g., code, heap, stack). Instead of having a single large page table for all segments, there are separate tables for each segment, reducing wasted space.\nExample of a 32-bit Virtual Address Space:\n\nSegmentation: A 32-bit address is split into:\n\nTop bits: Used to identify the segment (e.g., 01 for code, 10 for heap, 11 for stack).\n\nVPN (Virtual Page Number): Identifies the page within the segment.\n\nOffset: The offset within the page.\n\n\n\n\nBase and Bounds Registers:\n\nEach segment has a base register pointing to the physical address of its page table.\nA bounds register indicates the number of valid pages in the segment.\n\n\nAddress Translation:\n\nWhen a memory access occurs and a TLB miss happens, the hardware uses the segment bits to select the appropriate base and bounds register for the segment.\nThe hardware combines the segment’s base address with the VPN to find the page table entry (PTE).\n\n123SN = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SN_SHIFTVPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; VPN_SHIFTAddressOfPTE = Base[SN] + (VPN * sizeof(PTE))\nThis process resembles the linear page table lookup but uses multiple segment base registers.\n\n\nBenefits:\n\nMemory Efficiency: Reduces memory waste by only allocating page tables for used segments.\nSmaller Page Tables: Unused pages between segments do not consume space in a page table.\n\nChallenges:\n\nLimited Flexibility: Segmentation assumes a certain usage pattern. For instance, a large but sparsely used heap can still lead to waste within the segment’s page table.\nExternal Fragmentation: Page tables can be of variable size (multiples of PTEs), making it harder to find contiguous space in physical memory to store them.\n\n20.3 Multi-level Page TablesConcept Overview\nA multi-level page table divides the page table into smaller, page-sized units. If an entire page of page table entries (PTEs) is invalid (i.e., unused), it is not allocated in memory. To manage these pages, a page directory is introduced. The page directory tracks whether a page of the page table exists in memory and points to it if valid.\nPage Directory: Contains entries (Page Directory Entries, PDEs) that indicate the validity and address of the corresponding pages of the page table. Each PDE minimally has:\n\nA valid bit indicating if the page table it points to has valid entries.\nA page frame number (PFN), which points to the physical location of the page table in memory.\n\nWhen a PDE is marked as valid, it means that at least one entry on the referenced page of the page table is valid.\n\nStructure and Benefits\n\nSpace Efficiency: Multi-level tables allocate space proportional to the actual address space in use, avoiding memory allocation for unused regions.\nMemory Management: Each level of the table can be managed within a single memory page, simplifying allocation and growth.\nFlexibility: Page table pages can be scattered in physical memory, unlike a linear page table that requires contiguous memory.\n\nHowever, multi-level tables involve more complex lookups:\n\nTLB Miss: Accessing a virtual address results in two memory loads (one for the PDE and one for the PTE), compared to one load in a linear page table. This represents a trade-off between space savings and lookup time.\n\nExample of Multi-Level Page Table\nConsider a 16KB address space with 64-byte pages, resulting in a 14-bit virtual address space (8 bits for the VPN and 6 bits for the offset). A linear page table for this space would need 256 entries, each 4 bytes in size, totaling 1KB. This table can be divided into 16 pages, each holding 16 PTEs.\n\nPage Directory:\n\nFor the given example, we divide the page table into 16 pages and create a page directory with 16 entries, each pointing to a page of the page table.\n\nIndexing:\n\n\nThe top 4 bits of the VPN are used to index into the page directory (PDIndex). The address of the PDE is calculated as:\n1PDEAddr = PageDirBase + (PDIndex * sizeof(PDE))\n\n\n\n\nIf the PDE is valid, the PFN points to the page table page. The remaining bits of the VPN are used to index within that page to get the PTE:\n1PTEAddr = (PDE.PFN &lt;&lt; SHIFT) + (PTIndex * sizeof(PTE))\n\n\nPage Tables:\n\nEach valid entry in the page directory points to a page of the page table. This page table contains entries that map specific VPNs to PFNs.\nIn the example, there are two page tables:\nPage Table at PFN 100: Contains entries for VPNs 0 to 15. Entries for VPNs 0 and 1 are valid (code segment), and VPNs 4 and 5 are also valid (heap segment). The remaining entries are marked as invalid.\nPage Table at PFN 101: Contains entries for the last 16 VPNs. Only VPNs 254 and 255 are valid (stack segment), while the other entries are invalid.\n\n\n\n\nExample Address Translation:\n\nTo translate a virtual address like 0x3F80 (binary 1111 1111 1000 0000), we:\nExtract the PDIndex (1111) to access the PDE in the page directory.\nUse the PFN from the PDE to locate the page table page at PFN 101.\nExtract the PTIndex (1110) to index into the page table and find the correct PTE.\n\n\n\nThe Need for More Than Two Levels\nThe goal of a multi-level page table is to keep each piece of the table within a single page, but as the address space grows, the page directory can become too large to fit into one page.\nGiven:\n\nPage size: 512 bytes\nPTE size: 4 bytes\n\nYou can fit 128 PTEs on one page (512 / 4 = 128). The VPN needs 7 bits (log₂128) to index these entries within one page.\nWith a 30-bit address, the VPN’s higher-order bits point to the page directory index. In this case, there are 14 bits left to address the page directory. This results in a page directory with 2^14 entries, which is too large to fit in one page.\n\nTo handle this, the concept of a multi-level page table is introduced:\n\nThe page directory itself is divided into multiple pages.\n\nA higher-level page directory is added to point to these sub-pages of the main directory.\n\n\nVirtual Address Breakdown\nThe virtual address is split as follows:\n\nPD Index 0: Top bits used to index into the top-level page directory.\n\nPD Index 1: Next set of bits used to index into the second-level page directory.\n\nPage Table Index: The remaining bits to index into the page table.\n\n\n\nStep-by-Step Translation\nWhen translating a virtual address:\n\nCheck the TLB: If there’s a TLB hit, the physical address can be formed directly without accessing the page table.\nTLB Miss:\nUse the top bits (PD Index 0) to look up the first-level page directory and obtain the physical address (PDBR).\nUse the second set of bits (PD Index 1) to look up the second-level page directory.\nCombine the physical frame number from the first-level directory with the second-level index to access the page table.\nFinally, use the page table index to find the PTE and obtain the physical frame number.\nIf any lookup fails or the PTE is invalid, a segmentation fault is raised.\n\n\n\nChapter 21: Beyond Physical Memory: Mechanisms21.1 Swap SpaceWhat is Swap Space?\nSwap space is a reserved area on the disk used for temporarily holding pages of memory that are swapped out of RAM. It allows the system to maintain more memory pages in use than the size of the physical memory.\n\nPurpose: To extend the apparent size of the system’s memory.\nMechanism: Pages from RAM are written to swap space (swapped out), freeing up physical memory for other uses. Pages can later be read back into memory (swapped in) when needed.\n\nCharacteristics of Swap Space\n\nDisk Address Tracking:\nThe OS must keep track of the disk location for every swapped-out page.\n\n\nSize:\nDetermines the maximum number of memory pages that can be swapped out.\nFor simplicity, assume swap space is very large in examples.\n\n\nSwapping Unit:\nPages are swapped in and out in fixed-size units (page-sized).\n\n\n\nExample from Figure 21.1\n\nPhysical Memory:\n\nFour physical frames (PFN 0–3), each holding one page.\nActive processes share this memory, with some of their pages swapped out to disk.\n\n\nSwap Space:\n\nContains eight blocks, holding swapped-out pages for processes.\n\nOne block is free for new swap operations.\n\n\n\n\n\nProcesses in this example:\n\nProc 0:\n\nVPN 0 is in physical memory.\nVPN 1 and VPN 2 are in swap space (Block 0 and Block 1).\n\n\nProc 1:\n\nVPN 2 and VPN 3 are in physical memory.\nVPN 0 and VPN 1 are in swap space (Block 3 and Block 4).\n\n\nProc 2:\n\nVPN 0 is in physical memory.\nVPN 1 is in swap space (Block 6).\n\n\nProc 3:\n\nAll pages are in swap space, meaning the process is not currently running.\n\n\n\nKey Insights\n\nSimulating Larger Memory:\nSwap space enables the system to pretend it has more memory than available physical RAM, supporting more active processes or larger applications.\n\n\nFreeing Memory:\nBy swapping out infrequently used pages, the system can prioritize memory for active processes.\n\n\n\nCode Pages and Swap Space\n\nCode Pages:\n\nPages containing program instructions (like binaries) are initially stored on disk.\nThese are loaded into memory when the program executes.\n\n\nReclaiming Memory:\n\nCode pages can be safely evicted from RAM to make space for other needs, as they can always be reloaded from the binary on disk. These pages typically don’t occupy swap space.\n\n\n\n21.2 The Present BitPresent Bit: Adding Disk Swapping Support\nThe present bit is a field in each Page Table Entry (PTE) used to indicate whether a page is currently in physical memory or stored on disk.\n\nPresent Bit Values:\n1: The page is in physical memory.\n0: The page is not in memory but resides on disk.\n\n\n\nThis mechanism allows the system to support swapping, where pages not in use are moved to disk to free up physical memory.\nMemory Reference Process Recap\n\nVirtual Memory Reference:\nA process generates a virtual address for an instruction or data access.\nThe system must translate this virtual address into a physical address to access the data.\n\n\nTranslation Lookaside Buffer (TLB):\nThe TLB caches mappings from Virtual Page Numbers (VPNs) to Physical Frame Numbers (PFNs) for faster translation.\nA TLB hit avoids accessing the page table in memory, speeding up the process.\n\n\nPage Table Lookup (if TLB miss):\nThe hardware locates the page table using the page table base register.\nIt uses the VPN to index into the table and retrieve the corresponding PTE.\n\n\nValid and Present Pages:\nIf the PTE is valid and the present bit is 1, the page is in memory.\nThe PFN is extracted, the TLB is updated, and the memory access is retried, resulting in a TLB hit.\n\n\n\n21.3 The Page FaultPage Fault Handling\nA page fault occurs when a process accesses a page that is not present in physical memory. The OS handles page faults in the following steps:\n\nTrigger the Page-Fault Handler:\nWhen a page fault occurs, the page-fault handler (a part of the OS) is invoked to determine the next steps.\n\n\nLocate the Missing Page:\nThe page that caused the fault may have been swapped out to disk.\nThe OS checks the page table entry (PTE) for information about the page’s location on disk. This disk address can be stored in the bits of the PTE that would typically store the page frame number (PFN).\n\n\nFetch the Page from Disk:\nThe OS issues a request to the disk to fetch the page back into physical memory.\n\n\nUpdate the Page Table:\nAfter the disk I/O completes, the OS updates the page table:\nMarks the page as present.\nUpdates the PFN field with the page’s new physical memory location.\n\n\n\n\nRetry the Instruction:\nThe OS retries the instruction that caused the page fault:\nThe retry may generate a TLB miss, which will be serviced by updating the TLB with the new translation.\nThe final retry will succeed, fetching the desired data or instruction from memory.\n\n\n\n\n\nWhy Does the OS Handle Page Faults?\nAlthough hardware often takes the lead in managing the TLB, it defers page fault handling to the OS for two key reasons:\n\nDisk I/O Latency:\nDisk operations are slow. Even if the OS executes many instructions to service the page fault, the overhead is negligible compared to the time it takes to complete a disk I/O.\n\n\nHardware Simplicity:\nHandling page faults requires knowledge of the swap space, issuing I/O operations, and managing disk access. Offloading this complexity to the OS simplifies hardware design.\n\n\n\nProcess State During a Page Fault\n\nWhile the OS services the page fault, the process that caused the fault enters the blocked state.\nThe OS schedules other ready processes to run during this time, maximizing the CPU’s utilization. This overlapping of I/O and computation is a key advantage of multiprogrammed systems.\n\nTerminology Clarification\n\nPage Faults vs. Illegal Memory Access:\nA page fault occurs during a valid memory access when the requested page is not in physical memory.\nIllegal memory access refers to attempts to access memory outside the allocated virtual address space, leading to a segmentation fault or similar error.\n\n\n\n21.4 What Happens If Memory Is Full?\nKey Problem:\nWhen all physical memory is in use, there is no free space to load a page from swap space during a page fault.\n\n\nSolution:\nThe operating system uses a page-replacement policy to decide which page(s) to evict from memory to make room for the new page.\n\n\n\nPage-Replacement Policy\n\nDefinition:\nA strategy used by the OS to select a page to remove from physical memory when space is needed.\n\n\nSignificance:\nChoosing the wrong page can severely degrade performance, as frequent disk accesses (caused by poor decisions) are orders of magnitude slower than memory accesses.\n\n\n\nPerformance Implications\n\nGood Policy:\nReduces the number of page faults and improves overall program performance.\n\n\nBad Policy:\nCauses thrashing, where a program spends more time swapping pages in and out than executing, leading to disk-like speeds (10,000–100,000 times slower than memory).\n\n\n\nPage-Fault Control Flow Algorithm\nThe control flow when a process accesses memory involves handling TLB hits, TLB misses, and page faults. Below is a simplified explanation of Figure 21.2:\n\nTLB Lookup:\n\nThe system checks the Translation Lookaside Buffer (TLB) for a match.\n\nIf the Virtual Page Number (VPN) is found (\nTLB hit ):\n\nValidate the access permissions. If valid, compute the physical address and access memory.\nIf permissions are invalid, raise a protection fault.\n\n\nIf the VPN is not found (TLB miss), proceed to the page table.\n\n\n\nPage Table Lookup:\n\nCompute the Page Table Entry (PTE) address.\nCheck the PTE validity:\nInvalid PTE: Raise a segmentation fault (invalid memory access).\nValid PTE:\nCheck access permissions; if invalid, raise a protection fault.\nIf the page is present in memory, update the TLB and retry the instruction.\nIf the page is not present, raise a page fault.\n\n\n\n\n\n\nPage Fault Handling:\n\nThe OS takes over to handle the page fault:\nUse the page-replacement policy if memory is full.\nLoad the required page from swap space to physical memory.\nUpdate the PTE and TLB to reflect the new page’s presence.\n\n\n\n\n\n\n21.5 Page Fault Control FlowHardware Control Flow\nThe hardware handles memory access up to the point of detecting a TLB miss. The control flow splits into three possible cases:\n\n\nValid and Present Page:\nThe TLB miss handler checks the page table.\nIf the page is valid and present (lines 18–21):\nThe PFN is retrieved from the PTE.\nThe TLB is updated, and the instruction is retried.\nThis retry results in a TLB hit, and memory access succeeds.\n\n\n\n\nValid but Not Present Page:\nIf the page is valid but not present (lines 22–23):\nA page fault is triggered, and control is handed to the OS page-fault handler.\nThe OS resolves the issue by loading the page from disk into memory.\n\n\n\n\nInvalid Page:\nIf the page is invalid (lines 13–14):\nThe hardware traps this access as an error.\nThe OS trap handler likely terminates the process, as this typically indicates a bug.\n\n\n\n\n\nSoftware Control Flow\nWhen a page fault occurs, the OS handles it as follows (Figure 21.3):\n\n\nFind a Free Physical Page:\nThe OS searches for an available physical frame for the faulted page.\nIf no free frame exists, the page replacement algorithm runs, evicting a page from memory to free up space.\n\n\nFetch Page from Disk:\nThe OS issues a disk I/O request to load the page from the swap space into memory.\nThe process is blocked during this operation.\n\n\nUpdate the Page Table:\nAfter the I/O completes, the OS updates the page table:\nMarks the page as present.\nSets the PFN field to indicate the page’s new location in memory.\n\n\n\n\nRetry the Instruction:\nThe OS retries the instruction that caused the fault:\nThe retry will result in a TLB miss, as the translation is not yet cached in the TLB.\nOn another retry, the TLB hit occurs, and the hardware completes the memory access.\n\n\n\n\n\nChapter 22:  Beyond Physical Memory: Policies22.1 Cache ManagementKey Concepts\n\nMain Memory as a Cache:\n\nMain memory holds only a subset of all virtual memory pages.\nDisk acts as the backing store for pages not in memory.\nMinimizing disk accesses (cache misses) is critical because disk access is orders of magnitude slower than memory access.\n\n\nAMAT(Average Memory Access Time ) Calculation:\n\nAMAT Formula:\n\nAMAT = T_M + (P_{Miss} \\cdot T_D)where:\n\nTM: Memory access time (e.g., 100 nanoseconds).\nPMiss: Probability of a cache miss.\nTD: Disk access time (e.g., 10 milliseconds).\n\n\nA high hit rate significantly reduces AMAT, as misses incur the costly overhead of disk I/O.\n\n\n\nHit and Miss Rates:\n\nHit Rate (PHit): Fraction of memory accesses found in main memory.\nMiss Rate (PMiss): Fraction of accesses requiring a disk fetch.\nPHit+PMiss= 1.\n\n\n\nExample Scenario\n\nSystem Configuration:\n\nAddress space: 4 KB.\nPage size: 256 bytes.\nVPN: 4 bits (16 virtual pages).\nOffset: 8 bits.\n\n\nMemory References:\n\nSequence: 0x000,0x100,0x200,0x300,0x400,0x500,0x600,0x700,0x800,0x900\nPages in memory: All except virtual page 3.\nBehavior:\nHits: 9 times (pages in memory).\nMisses: 1 time (page 3).\n\n\n\n\nHit and Miss Rates:\n\nHit rate: 90%\nMiss rate: 10%\n\n\nAMAT Calculation:\n\nGiven:\n\nTM=100 ns\nTD=10 ms\nPMiss=0.1\n\n\n\nAMAT = 100 \\, \\text{ns} + (0.1 \\cdot 10 \\, \\text{ms}) = 1.0001\n\n\n\n\nIf PMiss=0.00(99.9% hit rate):\n\n\nAMAT = 100 \\, \\text{ns} + (0.001 \\cdot 10 \\, \\text{ms}) = 10.1 \\, \\mu \\text{s}.\n\n\n\nPerformance Implications\n\nDisk Cost Dominates AMAT:\nThe high cost of disk access makes even small miss rates dramatically increase AMAT.\nFor example, a 0.1%miss rate results in AMAT ∼100×lower than a 10% miss rate.\n\n\nImportance of Replacement Policies:\nOptimizing hit rates with intelligent policies is essential for performance.\nPoor replacement policies can result in frequent misses, throttling system performance to disk speeds.\n\n\n\n22.2 The Optimal Replacement PolicyKey Concepts\n\nOptimal Replacement Policy:\nReplace the page that will not be accessed for the longest time in the future.\nMinimizes cache misses, making it the best possible policy in terms of performance.\n\n\nUsefulness of Optimal as a Benchmark:\nHelps in evaluating new policies by providing a reference point.\nExample: If a policy achieves an 80% hit rate and the optimal policy achieves 82%, the new policy is close to the ideal.\n\n\nIntuition:\nPages accessed sooner are more important to keep.\nEvicting the page needed furthest in the future minimizes future misses.\n\n\n\nExample Walkthrough\n\nAccess Sequence: 0,1,2,0,1,3,0,3,1,2,1\nCache Size: 3 pages.\n\nTrace Analysis:\n\nPerformance Metrics:\n\nHit Rate: \n{Hit Rate} = \\frac{\\text{Hits}}{\\text{Hits} + \\text{Misses}} = \\frac{6}{6 + 5} = 54.5\\%\n\n\nHit Rate Excluding Compulsory Misses:\n\nCompulsory Misses: First access to pages 0,1,2,3\n\nEffective Hit Rate: \n\n\\frac{6}{6 + 1} = 85.7\\%.\n\n\n\n\nTypes of Cache Misses\n\nCompulsory Misses:\nOccur on the first reference to a page.\nExample: Misses for pages 0,1,20, 1, 2 in the empty cache.\n\n\nCapacity Misses:\nOccur when the cache is full, and a page must be evicted.\nExample: Miss for page 33 when replacing page 22.\n\n\nConflict Misses:\nRelevant only in hardware caches with restrictions on placement.\nNot applicable to fully-associative OS page caches.\n\n\n\n22.3: FIFO (First-In, First-Out)Example Trace Consider the following sequence of virtual page references: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1 Assuming a cache size of 3, FIFO behaves as follows:\n\nHit Rate\n\nTotal Hits: 4\n\nTotal Misses: 7\n\nHit Rate: \n\n\\frac{\\text{Hits}}{\\text{Hits + Misses}} = \\frac{4}{11} \\approx 36.4\\%\n\n\nHit Rate (Excluding Compulsory Misses): \n\\frac{\\text{Hits}}{\\text{Hits + Non-Compulsory Misses}} = \\frac{4}{7} \\approx 57.1\\%\n\n22.4: Random ReplacementExample Trace Using the same reference sequence as before: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1 And assuming a cache size of 3, the behavior of Random Replacement might look like this (depending on the random eviction decisions):\n\nPerformance\n\nTotal Hits: 4\nTotal Misses: 7\nHit Rate: 411≈36.4%\\frac{4}{11} \\approx 36.4\\%\n\nObservation: Random achieves similar results to FIFO in this specific trial but could perform better or worse depending on the random eviction choices.\nStatistical Analysis In a study of Random’s behavior over 10,000 trials (using the same reference sequence), the following outcomes were observed:\n\nNumber of Hits: Ranges from 2 to 6 hits per trial.\nBest Case: 6 hits (equivalent to the optimal policy), occurring in about 40% of trials.\nWorst Case: 2 hits or fewer, occurring in the lower tail of trials.\nDistribution: Random’s performance is highly variable, with a mean hit rate slightly better than FIFO but significantly worse than optimal or LRU.\n\n\n22.5: The LRU PolicyHistorical Basis\n\nTemporal Locality: LRU relies on the observation that programs often revisit recently accessed pages, demonstrating temporal locality.\nSpatial vs. Temporal Locality:\nSpatial locality refers to accessing nearby pages (e.g., an array).\nTemporal locality focuses on the recurrence of accessing the same page.\n\n\n\nHow LRU Works LRU maintains an ordered list or queue of pages in memory, where the most recently accessed page is at the front and the least recently accessed one is at the back. When a new page needs to be loaded and the cache is full, the page at the back of the list (the least recently used) is evicted.\nExample of LRU Trace Given the reference stream 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1 and a cache size of 3, LRU’s decision-making can be shown as:\n\nPerformance\n\nResults: LRU can perform similarly to the optimal policy, achieving high hit rates.\nCorrect Decisions: LRU correctly evicts pages that will not be accessed soon, which increases the likelihood of hits in subsequent accesses.\n\nWhy LRU Outperforms FIFO and Random\n\nFIFO and Random: These policies can evict pages that are still useful (e.g., FIFO evicting the first-in page regardless of usage).\nLRU: By keeping track of recency, it avoids such mistakes and maintains a better hit rate. In the example above, LRU’s decisions are informed by the history of accesses, leading to fewer misses.\n\n22.6: Workload Examples and Policy Performance\nNo-Locality Workload\n\n\nDescription: In this workload, 100 unique pages are accessed in a random manner out of 10,000 total accesses. This kind of workload has no specific pattern, so each access is independent.\n\n\n\n\n80-20 Workload\n\n\nDescription: This workload exhibits locality where 80% of the accesses are made to 20% of the pages (the “hot” pages), while 20% of the accesses are made to the remaining 80% of pages (the “cold” pages).\n\n\n\n\nLooping-Sequential Workload\n\n\nDescription: This workload involves 50 pages accessed sequentially from 0 to 49 and then loops back to 0, continuing for 10,000 total accesses.\n\n\n\n22.7: Implementing Historical AlgorithmsChallenges of Implementing LRU\n\nData Structure Management: To implement LRU perfectly, each time a page is accessed, the system must update a data structure to move that page to the front, representing the most recently used (MRU) side. This requires frequent updates on each memory access, which can be computationally expensive.\nComparison with FIFO: Unlike LRU, FIFO (First-In, First-Out) only requires updates when a new page is added to the end of the list or when an existing page is evicted from the beginning. This is simpler and less costly to manage than LRU’s continuous repositioning of pages.\nAccounting Costs: The constant accounting for LRU can significantly impact system performance, especially if the method for tracking access history is not optimized.\n\nHardware Support for LRU\nOne approach to optimize the implementation of LRU is to leverage hardware support:\n\nTime Stamps: Hardware could update a time field in memory whenever a page is accessed. These time fields could be part of a per-process page table or stored in a separate array, with one entry per physical page.\nPage Replacement: When the OS needs to evict a page, it would scan the time fields to identify the page that has not been accessed for the longest time.\n\nIssue with Large-Scale Systems:\n\nScalability Problems: The solution of scanning an array of time fields to find the least-recently-used page becomes impractical as the number of pages increases. For instance, on a system with 4GB of memory divided into 4KB pages, there would be 1 million pages. Scanning such a large array to find the LRU page would be prohibitively slow, even on modern CPUs.\n\n22.8: Approximating LRUThe Use Bit Concept\n\nDefinition: The use bit is a simple binary flag associated with each page. When a page is accessed (read or written), the hardware sets the use bit to 1. The hardware does not reset this bit to 0; instead, this is managed by the operating system (OS).\n\nPurpose: The use bit is used by the OS to approximate LRU behavior, helping the system identify pages that have not been accessed recently.\n\n\nThe Clock Algorithm\nA common approach to approximate LRU is the Clock Algorithm, which is inspired by the idea of a clock with a hand that points to a specific page. The algorithm works as follows:\n\nCircular Arrangement: All pages in memory are arranged in a circular list, and the “clock hand” points to a page.\n\nReplacement Decision:\n\nThe OS checks the use bit of the page pointed to by the clock hand.\nIf the use bit is 1, the page has been accessed recently. The OS resets the use bit to 0 and moves the clock hand to the next page.\nIf the use bit is 0, the page has not been accessed recently and is chosen for replacement.\n\n\nCycle: If all pages have their use bit set to 1 during the scan, the OS clears all use bits and starts the scan again, ensuring no page is unfairly excluded from the search.\n\n\nAdvantages of the Clock Algorithm\n\nEfficient Scanning: The clock algorithm avoids repeatedly scanning the entire memory for the least-recently-used page, making it more efficient than a full scan.\nSimplicity: The algorithm is straightforward to implement and operates with minimal overhead.\n\n22.9: Considering Dirty PagesWhy Consider Dirty Pages?\n\nPerformance Impact: Evicting a dirty page requires writing its contents back to disk, which incurs significant I/O overhead and impacts system performance.\n\nClean Pages: In contrast, evicting a clean page is more efficient because no data needs to be written to disk; the physical frame can simply be reused.\n\n\nThe Modified (Dirty) Bit\n\nDefinition: A modified bit (or dirty bit) is a hardware-supported flag that indicates whether a page has been modified since it was brought into memory.\n\nSetting the Bit: This bit is set whenever a page is written to. When the page is read-only or has not been modified, the bit remains clear (indicating that the page is clean).\n\n\nIncorporating Dirty Page Considerations in the Clock Algorithm\n\nThe clock algorithm can be enhanced to consider the status of the dirty bit:\nInitial Check: When the clock hand points to a page for potential eviction, the algorithm first checks if the page is both unused (use bit = 0) and clean (dirty bit = 0). If such a page is found, it is evicted, as it requires no I/O overhead.\nFallback: If no clean, unused pages are found, the algorithm then looks for pages that are unused but dirty (dirty bit = 1). While this page must be written back to disk, it is still a candidate for eviction if no clean pages are available.\nRepeat: If the algorithm cannot find a suitable page to evict, it may scan through the entire memory set (clearing use bits as needed) until an appropriate page is found.\n\n\n\nChapter 26: Introduction of Concurrency26.1: Why Use Threads?1. Parallelism\n\nDefinition: Parallelism leverages multiple processors (or cores) to perform different parts of a task simultaneously.\nExample: Consider adding two large arrays or incrementing each element of an array. A single-threaded program will process each element sequentially, one by one.\nWith Threads: On a multi-processor system, the workload can be divided among threads, each handling a portion of the data. For instance, four threads could each process 25% of the array, utilizing all available CPUs and speeding up execution.\nParallelization: Transforming a single-threaded program into one that utilizes multiple CPUs through threads is a common technique to improve performance on modern hardware.\n\n2. Responsiveness and Avoiding I/O Blocking\n\nChallenge with I/O: Programs often need to wait for slow I/O operations\nSolution with Threads:\nInstead of making the entire program wait, threads allow the program to continue performing other tasks. For example:\nOne thread can wait for I/O completion.\nAnother thread can utilize the CPU to perform computations or issue additional I/O requests.\n\n\nThis overlap of I/O and computation enhances the program’s overall responsiveness and efficiency.\n\n\n\nThreads vs. Processes\n\nThreads:\nShare the same address space, making it easy to share data.\nSuitable for tasks that require shared memory or frequent data exchange between threads.\nLightweight compared to processes, with less overhead for creation and context switching.\n\n\nProcesses:\nOperate in separate address spaces, offering stronger isolation.\nIdeal for logically separate tasks where minimal data sharing is needed.\nHeavier than threads due to separate memory and context.\n\n\n\n26.2: An Example: Thread CreationCode Overview\nThe program demonstrates creating two threads, each running a function mythread() that prints a character string passed as an argument.\nKey Code Snippets\n\nThread Function (mythread):\n1234void *mythread(void *arg) &#123;    printf(&quot;%s\\n&quot;, (char *) arg);    return NULL;&#125;\n\nThe function takes a generic pointer arg and prints the string it points to.\nIt returns NULL after completing.\n\n\nMain Function:\n12345678910int main(int argc, char *argv[]) &#123;    pthread_t p1, p2;    printf(&quot;main: begin\\n&quot;);    Pthread_create(&amp;p1, NULL, mythread, &quot;A&quot;);    Pthread_create(&amp;p2, NULL, mythread, &quot;B&quot;);    Pthread_join(p1, NULL);    Pthread_join(p2, NULL);    printf(&quot;main: end\\n&quot;);    return 0;&#125;\n\npthread_t: Represents a thread identifier.\n\nPthread_create: Creates a new thread. The arguments are:\n\nPointer to the thread identifier (p1 or p2).\nAttributes (set to NULL for default attributes).\nFunction pointer (mythread) to run in the thread.\nArgument passed to the thread function (&quot;A&quot; or &quot;B&quot;).\n\n\nPthread_join: Waits for the thread to finish. The main thread waits for p1 and p2 to complete before continuing.\n\n\n\n\nExecution Ordering\n\nThe thread scheduler determines the execution order, which can vary across runs. Possible orderings:\n\nSequential (Figure 26.3):\n\nMain thread runs first, creating Thread 1 and then Thread 2.\nThread 1 runs and prints “A”.\nThread 2 runs and prints “B”.\nThe main thread resumes and prints “main: end”.\n\n\nThread 1 First (Figure 26.4):\n\nMain thread creates Thread 1, which starts running immediately and prints “A”.\n\nThe main thread then creates Thread 2, which runs and prints “B”.\n\nMain thread resumes, printing “main: end”.\n\n\n\n\nThread 2 First (Figure 26.5):\n\nMain thread creates both threads.\n\nThread 2 runs first and prints “B”.\n\nThread 1 runs later and prints “A”.\n\nMain thread resumes and prints “main: end”.\n\n\n\n\n\n\n\n26.3: Why It Gets Worse - Shared DataCode Overview\nThe code in Figure 26.6 demonstrates a global shared variable, counter, being updated by two threads concurrently. Each thread attempts to increment counter 10 million times.\nKey Aspects\n\nGlobal Shared Variable:\n1static volatile int counter = 0;\n\nstatic: Limits the scope of counter to the current file.\nvolatile: Prevents compiler optimizations on counter, ensuring direct memory access each time.\n\n\nThread Function:\n12345678void *mythread(void *arg) &#123;    printf(&quot;%s: begin\\n&quot;, (char *) arg);    for (int i = 0; i &lt; 1e7; i++) &#123;        counter = counter + 1;    &#125;    printf(&quot;%s: done\\n&quot;, (char *) arg);    return NULL;&#125;\n\nEach thread adds 1 to counter in a loop.\nThe desired final value of counter is 20,000,000.\n\n\nMain Function:\n12345678910int main(int argc, char *argv[]) &#123;    pthread_t p1, p2;    printf(&quot;main: begin (counter = %d)\\n&quot;, counter);    Pthread_create(&amp;p1, NULL, mythread, &quot;A&quot;);    Pthread_create(&amp;p2, NULL, mythread, &quot;B&quot;);    Pthread_join(p1, NULL);    Pthread_join(p2, NULL);    printf(&quot;main: done with both (counter = %d)\\n&quot;, counter);    return 0;&#125;\n\nTwo threads are created, each running mythread.\nThe main thread waits for both threads to finish using pthread_join.\n\n\n\nExpected vs. Actual Output\n\nExpected Output:\n123456main: begin (counter = 0)A: beginB: beginA: doneB: donemain: done with both (counter = 20000000)\n\nActual Output (Examples):\n\ncounter = 19345221\ncounter = 19221041\n\n\n\nWhy It Happens\nThe discrepancy occurs due to race conditions when counter is incremented by multiple threads simultaneously. Let’s break it down:\n\nIncrement Operation:\n1counter = counter + 1;\nThis statement is not atomic; it involves:\n\nRead: Load the current value of counter into a register.\nModify: Increment the value in the register.\nWrite: Store the incremented value back into counter.\n\n\nInterleaved Execution:\n\nWhen threads execute concurrently, their instructions can interleave. For example:\nThread 1 reads counter = 0.\nThread 2 reads counter = 0.\nThread 1 writes counter = 1.\nThread 2 writes counter = 1. (Overwrites Thread 1’s increment.)\n\n\nThe final value is 1 instead of 2.\n\n\nNon-Deterministic Behavior:\n\nThe exact interleaving depends on the OS scheduler, leading to different results each time the program runs.\n\n\n\n26.4 The Heart of the Problem: Uncontrolled SchedulingTo understand why the issue occurs, we must examine the sequence of instructions generated by the compiler for the code that updates the shared variable counter. For instance, in x86 assembly, adding 1 to counter might translate to:\n123mov 0x8049a1c, %eax   ; Load the value of counter into register eaxadd $0x1, %eax        ; Increment the value in register eaxmov %eax, 0x8049a1c   ; Store the updated value back into memory\nHere, we assume counter is stored at memory address 0x8049a1c. This sequence illustrates how updating counter requires three instructions: fetching the current value into a register, modifying it, and writing it back to memory.\nRace Condition Example:\nImagine two threads, Thread 1 and Thread 2, concurrently executing this code to increment counter, initially set to 50.\n\nThread 1 begins execution:\nIt executes mov, loading counter‘s value (50) into its private register eax. Now, eax = 50.\nIt then increments eax to 51.\n\n\nA context switch occurs:\nThe OS interrupts Thread 1 before it stores the updated value back into counter. The state of Thread 1 (including its eax register) is saved.\n\n\nThread 2 starts execution:\nIt executes the same mov instruction, fetching the still-unchanged value of counter (50) into its register. Now, eax = 50 for Thread 2.\nThread 2 increments eax to 51 and completes the third mov instruction, writing the updated value (51) back to counter.\n\n\nAnother context switch occurs:\nThread 1 resumes execution. It finishes its previously interrupted sequence by executing the final mov instruction, writing its eax value (51) back to counter.\n\n\n\n\nCritical Section and Mutual Exclusion:\nThe problematic code segment is an example of a critical section, where multiple threads access a shared resource (like counter). When executed without proper synchronization, this can lead to race conditions.\nTo address this, we need mutual exclusion, ensuring that only one thread can execute the critical section at a time. Synchronization primitives (e.g., locks or atomic operations) can achieve this.\nAtomic Operations:\nA fundamental solution to race conditions is the use of atomic operations, which ensure that a series of actions either occur entirely or not at all, without interruption. This principle underpins synchronization in concurrent code, as well as broader applications like file systems and database transactions.\n26.5 The Wish for AtomicityOne potential solution to the race condition problem is to have more advanced instructions that can perform all necessary operations in a single, uninterrupted step. This would eliminate the risk of an unexpected interrupt occurring mid-operation. For example, consider a hypothetical instruction:\n1memory-add 0x8049a1c, $0x1\nAssuming this instruction atomically adds a value to a specified memory location, the hardware guarantees that it completes without being interrupted mid-operation. This guarantee means that when an interrupt occurs, the instruction has either not started at all or has completed fully, with no intermediate state. This type of atomic behavior ensures the consistency and correctness of the operation.\nAtomicity Explained:\nIn this context, atomicity means performing an operation as a single, indivisible unit. This is often summarized as “all or none”—either the entire operation is completed, or none of it is. The goal is to execute the three-step code sequence below atomically:\n123mov 0x8049a1c, %eaxadd $0x1, %eaxmov %eax, 0x8049a1c\nIf we had a single instruction that could do this entire sequence atomically, we could simply use that instruction to ensure consistency. However, in most cases, such a specialized instruction is not feasible or practical. For instance, in complex data structures like a concurrent B-tree, we wouldn’t want hardware to have a dedicated instruction for an “atomic update of a B-tree.”\nPractical Solution:\nInstead, hardware supports a limited set of atomic operations that serve as building blocks for synchronization primitives. By leveraging these primitives in combination with assistance from the operating system, we can develop multi-threaded code that accesses critical sections safely and produces the correct results, even under the complexity of concurrent execution.\nChapter 27: Thread API27.1 Thread CreationCreating new threads is the foundational step for writing multi-threaded programs. In POSIX-compliant systems, this is done using the pthread_create() function, which provides an interface for thread creation. The function is defined as follows:\n123#include &lt;pthread.h&gt;int pthread_create(pthread_t *thread, const pthread_attr_t *attr,                   void *(*start_routine)(void*), void *arg);\nThe function takes four arguments:\n\nthread: This is a pointer to a pthread_t structure, which is used to interact with the thread. You pass this structure to pthread_create() to initialize it.\n\nattr: This argument specifies any attributes the thread might have, such as stack size or scheduling priority. Attributes are initialized using pthread_attr_init(). However, most programs will use the default settings, so you can pass NULL if you don’t need custom attributes.\n\nstart_routine: This is the function that the thread will start running. It is declared as a function pointer with the signature void *(*start_routine)(void*). This indicates that the function:\n\nTakes a single argument of type void *.\nReturns a value of type void *.\n\nIf the thread function needed to take an integer as an argument, the declaration would change to:\n1int pthread_create(..., void *(*start_routine)(int), int arg);\nIf the function returned an integer instead, the declaration would be:\n1int pthread_create(..., int (*start_routine)(void *), void *arg);\n\narg: This is the argument passed to the start_routine function when the thread begins execution. The use of void * for this argument and the return type allows for flexibility, enabling you to pass and return any type of data.\n\n\n27.2 Thread CompletionOnce a thread is created, you may need to wait for it to complete before proceeding. This is done using the pthread_join() function, which allows one thread to wait for another thread to finish its execution.\nFunction Signature:\n1int pthread_join(pthread_t thread, void **value_ptr);\nParameters:\n\nthread: The thread identifier to wait for, which is initialized by pthread_create() when you pass a pointer to it.\nvalue_ptr: A pointer to where the thread’s return value should be stored. Since the return value can be of any type, this parameter is a pointer to void. The pthread_join() function modifies the value at value_ptr, so it needs to be a pointer, not a direct value.\n\nExample of Waiting for a Thread\nIn the example below (Figure 27.1), a thread is created, passed arguments packed into a myarg_t structure, and then pthread_join() is called to wait for its completion. The thread returns values, which are stored in a myret_t structure. The main thread can then access these returned values after pthread_join() returns.\n1234567891011121314151617181920typedef struct &#123; int a; int b; &#125; myarg_t;typedef struct &#123; int x; int y; &#125; myret_t;void *mythread(void *arg) &#123;    myret_t *rvals = malloc(sizeof(myret_t));    rvals-&gt;x = 1;    rvals-&gt;y = 2;    return (void *) rvals;&#125;int main(int argc, char *argv[]) &#123;    pthread_t p;    myret_t *rvals;    myarg_t args = &#123; 10, 20 &#125;;    pthread_create(&amp;p, NULL, mythread, &amp;args);    pthread_join(p, (void **) &amp;rvals);    printf(&quot;returned %d %d\\n&quot;, rvals-&gt;x, rvals-&gt;y);    free(rvals);    return 0;&#125;\nImportant Points:\n\nArgument and Return Simplification: If a thread does not need complex arguments or return values, you can pass NULL when creating the thread or in pthread_join() if you don’t need the return value.\n\nAvoid Stack Allocation: Never return a pointer to a local variable allocated on the thread’s stack. Once the function returns, the stack memory is deallocated, leading to undefined behavior. Example of what not to do:\n12345678void *mythread(void *arg) &#123;    myarg_t *args = (myarg_t *) arg;    printf(&quot;%d %d\\n&quot;, args-&gt;a, args-&gt;b);    myret_t oops; // Allocated on stack: BAD!    oops.x = 1;    oops.y = 2;    return (void *) &amp;oops; // Dangerous: points to deallocated memory&#125;\n\nSimpler Argument Passing: If a thread only needs to pass a simple value, you don’t have to package it into a structure. Example (Figure 27.3):\n1234567891011121314void *mythread(void *arg) &#123;    long long int value = (long long int) arg;    printf(&quot;%lld\\n&quot;, value);    return (void *) (value + 1);&#125;int main(int argc, char *argv[]) &#123;    pthread_t p;    long long int rvalue;    pthread_create(&amp;p, NULL, mythread, (void *) 100);    pthread_join(p, (void **) &amp;rvalue);    printf(&quot;returned %lld\\n&quot;, rvalue);    return 0;&#125;\n\n\nWhen to Use pthread_join():\n\nNot all multi-threaded programs use pthread_join(). For example, a web server with worker threads might not need it, as the main thread handles request acceptance and passes tasks to the worker threads indefinitely.\n\nHowever, in programs that create threads to perform specific tasks in parallel, pthread_join() is essential to ensure that all work is completed before exiting or moving to the next stage of computation.\n\n\n27.3 LocksBasic Lock Operations\n\nLocking and Unlocking:\n12int pthread_mutex_lock(pthread_mutex_t *mutex);int pthread_mutex_unlock(pthread_mutex_t *mutex);\n\npthread_mutex_lock() attempts to acquire the lock. If it’s already locked by another thread, the calling thread waits until it is released.\npthread_mutex_unlock() releases the lock, allowing other threads to acquire it.\n\n\n\nProper Initialization\n\nStatic Initialization:\n1pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\nThis method sets up the lock to default values, making it ready for use.\n\nDynamic Initialization:\n12int rc = pthread_mutex_init(&amp;lock, NULL);assert(rc == 0); // Check for successful initialization\nHere, pthread_mutex_init() allows runtime initialization, with NULL as the second argument to use default attributes.\n\nDestruction: When done using a lock, ensure you call pthread_mutex_destroy(&amp;lock) to clean up resources.\n\n\nError Handling\n\nError Code Checking: Always check the return codes from pthread_mutex_lock() and pthread_mutex_unlock()\n to ensure they succeed:\n1234void Pthread_mutex_lock(pthread_mutex_t *mutex) &#123;    int rc = pthread_mutex_lock(mutex);    assert(rc == 0); // Ensure successful lock&#125;\nThis approach ensures that your code doesn’t proceed if there is an issue with lock acquisition.\n\n\nAdvanced Lock Functions\n\npthread_mutex_trylock(): Tries to acquire the lock without waiting. Returns immediately if the lock is already held.\npthread_mutex_timedlock(): Tries to acquire the lock within a specified time. Returns after the timeout or if the lock is acquired.\n\nThese advanced functions help avoid indefinite blocking, which is essential for handling deadlocks or specific use cases where you need non-blocking behavior.\n27.4 Condition VariablesBasic Concepts\n\nCondition Variables: Used to signal between threads, allowing one thread to wait for another to notify it of an event.\nAssociated Lock: Condition variables require a mutex to prevent race conditions when checking or updating shared variables.\n\nKey Routines\n\npthread_cond_wait():\n\nSuspends the calling thread and releases the lock while it waits.\n\nRe-acquires the lock when it wakes up.\n\nTypical usage:\n12345678pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;pthread_cond_t cond = PTHREAD_COND_INITIALIZER;Pthread_mutex_lock(&amp;lock);while (ready == 0) &#123;    Pthread_cond_wait(&amp;cond, &amp;lock);&#125;Pthread_mutex_unlock(&amp;lock);\n\n\n\npthread_cond_signal():\n\nWakes up one waiting thread.\n\nThe lock should be held when signaling to avoid race conditions.\n\nExample:\n1234Pthread_mutex_lock(&amp;lock);ready = 1;Pthread_cond_signal(&amp;cond);Pthread_mutex_unlock(&amp;lock);\n\n\n\n\nImportant Points\n\nLock Holding: pthread_cond_wait() releases the lock while putting the thread to sleep, which allows other threads to acquire the lock and signal the condition. When the waiting thread is woken, it re-acquires the lock before continuing.\n\nWhile Loop for Safety: The condition should be checked inside a while loop, not an if statement. This is because some implementations of pthread_cond_wait() might spuriously wake up a thread without a change in the condition. Using a while loop ensures the condition is re-checked before proceeding.\n\nPerformance Considerations: Simple flags (e.g., using busy-waiting loops) can be tempting but are inefficient and error-prone:\n1while (ready == 0) ; // spin (bad practice)\nThis approach wastes CPU cycles and increases the risk of synchronization bugs.\n\n\nChapter 28: Lock28.1 Locks: The Basic IdeaWhat is a Lock?\n\nDefinition: A lock is a synchronization primitive that controls access to shared resources by multiple threads.\nState: A lock has two states:\nAvailable (Unlocked/Free): No thread holds the lock; it can be acquired by any thread.\nAcquired (Locked/Held): A thread holds the lock, preventing other threads from acquiring it until it is released.\n\n\n\nBasic Lock Operations\n\nLock Operation (lock(&amp;mutex)): This routine tries to acquire the lock. If no thread currently holds it, the calling thread becomes the owner and can enter the critical section.\nUnlock Operation (unlock(&amp;mutex)): This routine releases the lock, making it available for other threads to acquire. If there are threads waiting to acquire the lock, one will be selected (based on scheduling policies) to proceed.\n\nCode Example\nConsider a simple critical section updating a shared variable:\n12345lock_t mutex; // Declare a globally-allocated lock variable...lock(&amp;mutex);       // Try to acquire the lockbalance = balance + 1; // Critical sectionunlock(&amp;mutex);     // Release the lock\nHow Locks Work:\n\nLock Acquisition:\nWhen lock() is called, if the lock is free, the thread acquires it and becomes the “owner”.\nIf the lock is already acquired by another thread, the calling thread waits until it becomes free.\n\n\nLock Release:\nWhen unlock() is called by the owner, the lock is released, and it transitions to the available state.\nIf other threads are waiting, one of them will acquire the lock and proceed with its critical section.\n\n\n\nBenefits of Using Locks\n\nControlled Access: Locks prevent multiple threads from simultaneously executing a critical section, ensuring data consistency and avoiding race conditions.\nThread Safety: Ensures that only one thread at a time can execute a specific section of code, protecting shared resources from concurrent modifications.\nScheduling Control: While the OS controls thread scheduling, using locks allows the programmer to manage the execution order within critical sections, giving more predictable behavior in multi-threaded programs.\n\n28.2 Pthread LocksThe POSIX library uses the term “mutex” for a lock, which stands for mutual exclusion. This is crucial for ensuring that only one thread accesses a critical section at a time, preventing other threads from entering until the current thread has completed its operation. The code snippet provided demonstrates how to use a POSIX mutex:\n123451 pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;23 Pthread_mutex_lock(&amp;lock); // wrapper; exits on failure4 balance = balance + 1;5 Pthread_mutex_unlock(&amp;lock);\nIn this code, lock is passed to Pthread_mutex_lock and Pthread_mutex_unlock, allowing different variables to be protected by different locks. This strategy, known as fine-grained locking, enhances concurrency compared to using a single large lock for all critical sections, which is termed coarse-grained locking. The fine-grained approach allows more threads to execute locked code simultaneously, improving efficiency.\n28.3 Building A LockUnderstanding how to build a lock goes beyond just using them; it involves knowing the hardware and OS support needed. Efficient locks ensure mutual exclusion with minimal overhead and may possess additional desirable properties. Building an effective lock requires hardware support—using specific CPU instructions—and OS-level assistance to create a comprehensive locking library. While the detailed implementation of these hardware primitives is a topic for computer architecture courses, understanding their use is essential for building synchronization primitives like locks.\n28.4 Evaluating LocksBefore creating locks, it’s important to define evaluation criteria. A lock’s primary function is mutual exclusion—ensuring that only one thread can enter a critical section at a time. The lock should be assessed for fairness, which means ensuring that each thread contending for the lock has a fair chance to acquire it, preventing starvation (where a thread never gets access). Performance is another key criterion, focusing on the overhead introduced by using the lock. Performance should be measured in different scenarios:\n\nNo contention: What overhead does the lock introduce when only one thread is active?\nSingle CPU with contention: How does the lock behave when multiple threads compete on a single CPU?\nMultiple CPUs with contention: How does the lock perform when multiple threads on different CPUs try to access it?\n\nEvaluating these cases provides insight into the performance impact of various locking techniques.\n28.5 Controlling InterruptsOne early method for providing mutual exclusion was disabling interrupts in a single-processor system:\n1234561 void lock() &#123;2 DisableInterrupts();3 &#125;4 void unlock() &#123;5 EnableInterrupts();6 &#125;\nDisabling interrupts ensures that code within a critical section runs without interruption, making it effectively atomic. Once the critical section is completed, interrupts are re-enabled, resuming normal program operation.\nAdvantages:\n\nSimplicity: This method is straightforward and ensures code executes without interference.\n\nDrawbacks:\n\nPrivileged Operation: Allowing threads to disable interrupts introduces trust issues. Malicious or poorly designed programs could monopolize the processor by calling lock() at the start of execution or enter an infinite loop, effectively halting the system.\nMultiprocessor Limitation: In multi-CPU systems, disabling interrupts on one CPU does not prevent threads on other CPUs from running, making this method ineffective.\nLost Interrupts: Extended interruption masking can lead to missed events, such as the completion of I/O operations, causing the OS to lose track of essential events.\nPerformance Issues: Operations to mask and unmask interrupts are slower than normal CPU instructions.\n\nDue to these limitations, disabling interrupts is used sparingly, often within the operating system itself to ensure atomic operations for critical data structures. This is safe because the OS can trust its own code to perform privileged operations correctly.\nFigure 28.1: First Attempt: A Simple Flag\n123456789101112131415161 typedef struct __lock_t &#123; int flag; &#125; lock_t;23 void init(lock_t *mutex) &#123;4 // 0 -&gt; lock is available, 1 -&gt; held5 mutex-&gt;flag = 0;6 &#125;78 void lock(lock_t *mutex) &#123;9 while (mutex-&gt;flag == 1) // TEST the flag10 ; // spin-wait (do nothing)11 mutex-&gt;flag = 1; // now SET it!12 &#125;1314 void unlock(lock_t *mutex) &#123;15 mutex-&gt;flag = 0;16 &#125;\nThis simple lock implementation uses a flag to indicate whether the lock is available (0) or held (1). The lock() function continuously checks the flag in a spin-wait loop until it is set to 0, then sets it to 1 to claim the lock. The unlock() function resets the flag to 0, releasing the lock.\nLimitations:\n\nThis basic implementation can lead to busy-waiting, which wastes CPU resources.\nThis method only works for single-processor systems; in multi-CPU systems, it won’t prevent other CPUs from accessing the critical section.\n\n28.6 A Failed Attempt: Just Using Loads/Stores\nObjective: Build a lock using a single variable (flag) accessed via normal loads and stores.\nApproach:\nA flag variable indicates lock possession: 0 means the lock is free; 1 means the lock is held.\nThe lock() function checks if the flag is 0. If true, it sets the flag to 1 (acquiring the lock). Otherwise, it waits in a loop until the lock is released.\nThe unlock() function resets the flag to 0, releasing the lock.\n\n\nIssues:\nCorrectness Problem: The approach fails to ensure mutual exclusion.\nExample: Two threads could simultaneously read flag = 0, and both set it to 1, leading both to enter the critical section.\n\n\nPerformance Problem: Spin-waiting wastes CPU cycles, especially on uniprocessors, as the waiting thread cannot proceed until a context switch occurs.\n\n\n\n28.7 Building Working Spin Locks with Test-And-Set\nHardware Support: Early systems introduced instructions like TestAndSet to solve locking issues.\n\nDefinition: TestAndSet performs two actions atomically: retrieves the current value at a memory location and updates it to a new value.\n\nExample C code:\n12345int TestAndSet(int *old_ptr, int new) &#123;    int old = *old_ptr; // Fetch the old value    *old_ptr = new;    // Set the new value    return old;        // Return the old value&#125;\n\n\n\nSpin Lock Implementation:\n\nStructure:\n12345678910111213141516typedef struct __lock_t &#123;    int flag;&#125; lock_t;void init(lock_t *lock) &#123;    lock-&gt;flag = 0; // 0: available, 1: held&#125;void lock(lock_t *lock) &#123;    while (TestAndSet(&amp;lock-&gt;flag, 1) == 1)        ; // Spin-wait&#125;void unlock(lock_t *lock) &#123;    lock-&gt;flag = 0;&#125;\n\nMechanism:\n\nA thread calls lock(). If the flag is 0, TestAndSet returns 0 and sets flag = 1, acquiring the lock.\nIf the lock is already held (flag = 1), TestAndSet repeatedly returns 1, causing the thread to spin until the lock is released.\nOnce the critical section is completed, unlock() sets flag = 0.\n\n\n\n\n\n28.8 Evaluating Spin Locks\nCorrectness: Spin locks ensure mutual exclusion; only one thread can enter the critical section.\nFairness:\nSpin locks provide no guarantees of fairness. A thread may spin indefinitely under contention, leading to starvation.\n\n\nPerformance:\nSingle CPU: Inefficient. If the thread holding the lock is preempted, others spin needlessly.\nMultiple CPUs: More effective, especially if the number of threads matches the number of CPUs.\n\n\n\n28.9 Compare-And-Swap\nDefinition:\n\nCompares a memory location’s current value with an expected value. If they match, updates the memory with a new value; otherwise, no action is taken.\n\nExample C code:\n123456int CompareAndSwap(int *ptr, int expected, int new) &#123;    int original = *ptr;    if (original == expected)        *ptr = new;    return original;&#125;\n\n\n\nUsage in Locks:\n\nReplace TestAndSet with CompareAndSwap\n in the spin-lock implementation:\n1234void lock(lock_t *lock) &#123;    while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) == 1)        ; // Spin&#125;\n\nEnsures mutual exclusion by atomically swapping the flag value only when it matches the expected value (0).\n\n\n\n\n28.10: Load-Linked and Store-ConditionalSome computer architectures, like MIPS, offer load-linked (LL) and store-conditional (SC) instructions to implement synchronization primitives such as locks. These instructions work as follows:\n\nLoad-Linked (LL): Loads a value from memory into a register.\nStore-Conditional (SC): Stores a new value to the same memory address only if no other thread has modified it since the LL operation. It returns 1 on success and 0 on failure.\n\nExample: Building a Lock with LL/SCUsing LL/SC, locks can be implemented by spinning until the flag indicating lock availability becomes 0. The thread then attempts to set the flag to 1 atomically:\n12345678910void lock(lock_t *lock) &#123;    while (1) &#123;        while (LoadLinked(&amp;lock-&gt;flag) == 1); // Spin until flag is 0        if (StoreConditional(&amp;lock-&gt;flag, 1) == 1) return; // Acquire lock    &#125;&#125;void unlock(lock_t *lock) &#123;    lock-&gt;flag = 0; // Release lock&#125;\nOptimized Implementation: A concise version uses short-circuit boolean conditionals:\n123void lock(lock_t *lock) &#123;    while (LoadLinked(&amp;lock-&gt;flag) || !StoreConditional(&amp;lock-&gt;flag, 1));&#125;\nThis ensures only one thread can successfully update the flag to 1.\n28.11: Fetch-and-AddThe Fetch-and-Add (FAA) instruction atomically increments a value at a memory address while returning its old value. This can be leveraged to implement ticket-based locks that ensure fairness.\nExample: Ticket Locks A ticket lock uses a ticket and turn variable:\n\nA thread fetches its ticket number (myturn) using FAA.\nIt waits until myturn equals turn to enter the critical section.\nOn unlock, turn is incremented to allow the next thread to proceed.\n\n123456789101112131415161718typedef struct __lock_t &#123;    int ticket;    int turn;&#125; lock_t;void lock_init(lock_t *lock) &#123;    lock-&gt;ticket = 0;    lock-&gt;turn = 0;&#125;void lock(lock_t *lock) &#123;    int myturn = FetchAndAdd(&amp;lock-&gt;ticket);    while (lock-&gt;turn != myturn); // Spin until turn matches&#125;void unlock(lock_t *lock) &#123;    lock-&gt;turn++;&#125;\n28.12: Too Much SpinningIssue with Spinning Spinning wastes CPU cycles, especially in single-processor systems where the lock-holding thread must finish before other threads can proceed. This inefficiency escalates with more threads contending for a lock.\n28.13: Yielding to Reduce SpinningYielding A thread finding a lock held can yield the CPU to allow another thread to run. This reduces wasted CPU time:\n12345678void lock() &#123;    while (TestAndSet(&amp;flag, 1) == 1)        yield(); // Give up CPU&#125;void unlock() &#123;    flag = 0;&#125;\nWhile better than spinning, yielding can still waste time with excessive context switches and doesn’t prevent starvation.\n28.14: Sleeping Instead of SpinningUsing Queues A better approach involves sleeping waiting threads and waking them when the lock is available. This ensures efficient CPU usage and prevents starvation.\nExample: Queue-Based Locks This approach uses:\n\nA guard lock to manage access to critical lock variables.\nA queue to track waiting threads.\nOS primitives like park() (to put a thread to sleep) and unpark() (to wake a thread).\n\n123456789101112131415161718192021222324252627282930313233typedef struct __lock_t &#123;    int flag;    int guard;    queue_t *q;&#125; lock_t;void lock_init(lock_t *m) &#123;    m-&gt;flag = 0;    m-&gt;guard = 0;    queue_init(m-&gt;q);&#125;void lock(lock_t *m) &#123;    while (TestAndSet(&amp;m-&gt;guard, 1) == 1); // Acquire guard lock    if (m-&gt;flag == 0) &#123;        m-&gt;flag = 1; // Acquire lock        m-&gt;guard = 0;    &#125; else &#123;        queue_add(m-&gt;q, gettid());        m-&gt;guard = 0;        park(); // Sleep    &#125;&#125;void unlock(lock_t *m) &#123;    while (TestAndSet(&amp;m-&gt;guard, 1) == 1); // Acquire guard lock    if (queue_empty(m-&gt;q)) &#123;        m-&gt;flag = 0; // Release lock    &#125; else &#123;        unpark(queue_remove(m-&gt;q)); // Wake next thread    &#125;    m-&gt;guard = 0;&#125;\nAdvantages\n\nNo unnecessary CPU usage during contention.\nFairness ensured by servicing threads in queue order.\nPrevents starvation and optimizes system responsiveness.\n\nChapter 29：Lock-based Concurrent Data Structures29.1 Concurrent Counters1. Non-Synchronized Counter (Figure 29.1)\nThe simplest implementation of a counter lacks thread safety. If multiple threads access and modify the counter concurrently, race conditions may occur, leading to incorrect results.\n12345678typedef struct __counter_t &#123;    int value;&#125; counter_t;void init(counter_t *c) &#123; c-&gt;value = 0; &#125;void increment(counter_t *c) &#123; c-&gt;value++; &#125;void decrement(counter_t *c) &#123; c-&gt;value--; &#125;int get(counter_t *c) &#123; return c-&gt;value; &#125;\n2. Lock-Based Counter (Figure 29.2)\nTo make the counter thread-safe, a mutex lock is added. Each method locks the counter during an operation, ensuring that only one thread can modify the counter at a time.\n12345678910111213141516171819202122232425262728typedef struct __counter_t &#123;    int value;    pthread_mutex_t lock;&#125; counter_t;void init(counter_t *c) &#123;    c-&gt;value = 0;    pthread_mutex_init(&amp;c-&gt;lock, NULL);&#125;void increment(counter_t *c) &#123;    pthread_mutex_lock(&amp;c-&gt;lock);    c-&gt;value++;    pthread_mutex_unlock(&amp;c-&gt;lock);&#125;void decrement(counter_t *c) &#123;    pthread_mutex_lock(&amp;c-&gt;lock);    c-&gt;value--;    pthread_mutex_unlock(&amp;c-&gt;lock);&#125;int get(counter_t *c) &#123;    pthread_mutex_lock(&amp;c-&gt;lock);    int rc = c-&gt;value;    pthread_mutex_unlock(&amp;c-&gt;lock);    return rc;&#125;\nTrade-Off: While this implementation ensures correctness, performance suffers as the number of threads increases. The mutex introduces contention: threads must wait for others to release the lock.\nExample Walkthrough (Figure 29.3)\n\nSetup: A system with 4 CPUs (L1 to L4 are local counters) and a global counter (G). Threshold S is set to 5.\n\nProcess:\n\nAt Time 1, L3 is incremented.\nAt Time 2, L1 and L3 are incremented again.\nAt Time 6, L1 reaches the threshold (5), its value is transferred to G, and L1 is reset to 0.\nAt Time 7, L4 reaches the threshold, transfers its value to G, and resets.\n\n\n\n\n3. Approximate Counters (Figure 29.4)\nTo address scalability issues, an approximate counter is introduced. Instead of using a single shared counter, local counters are maintained for each CPU/core, and updates to the global counter occur periodically.\nKey Components:\n\nLocal Counters: Each CPU has its own counter, which threads running on that CPU increment. This reduces contention since threads update different counters.\nGlobal Counter: Periodically updated by transferring the values of local counters.\nThreshold (S): Determines when the local counter value is transferred to the global counter. A higher threshold improves performance but reduces accuracy.\n\n12345678910111213141516171819202122232425262728293031323334353637typedef struct __counter_t &#123;    int global;                     // global count    pthread_mutex_t glock;          // global lock    int local[NUMCPUS];             // per-CPU local counters    pthread_mutex_t llock[NUMCPUS]; // locks for local counters    int threshold;                  // threshold for global update&#125; counter_t;void init(counter_t *c, int threshold) &#123;    c-&gt;threshold = threshold;    c-&gt;global = 0;    pthread_mutex_init(&amp;c-&gt;glock, NULL);    for (int i = 0; i &lt; NUMCPUS; i++) &#123;        c-&gt;local[i] = 0;        pthread_mutex_init(&amp;c-&gt;llock[i], NULL);    &#125;&#125;void update(counter_t *c, int threadID, int amt) &#123;    int cpu = threadID % NUMCPUS;  // map thread to CPU    pthread_mutex_lock(&amp;c-&gt;llock[cpu]);    c-&gt;local[cpu] += amt;    if (c-&gt;local[cpu] &gt;= c-&gt;threshold) &#123;        pthread_mutex_lock(&amp;c-&gt;glock);        c-&gt;global += c-&gt;local[cpu];        pthread_mutex_unlock(&amp;c-&gt;glock);        c-&gt;local[cpu] = 0;    &#125;    pthread_mutex_unlock(&amp;c-&gt;llock[cpu]);&#125;int get(counter_t *c) &#123;    pthread_mutex_lock(&amp;c-&gt;glock);    int val = c-&gt;global;    pthread_mutex_unlock(&amp;c-&gt;glock);    return val; // approximate value&#125;\nPerformance Analysis\n\nPrecise Counter (Lock-Based):\nScales poorly with increasing threads.\nEven two threads lead to a massive slowdown due to lock contention.\n\n\nApproximate Counter:\nReduces contention by using local counters and periodic updates.\nThreshold SS determines performance and accuracy:\nLower SS: Higher accuracy, lower performance.\nHigher SS: Lower accuracy, higher performance.\n\n\n\n\n\nExample Trace (Threshold S=5S = 5):\n\nLocal counters increment until they reach 5.\nOnce the threshold is hit, the local counter transfers its value to the global counter.\n\n\n29.2 Concurrent Linked Lists\nBasic Concurrent Linked List Design\n\n\nThe initial implementation (Figure 29.7) locks the entire list during critical sections like insertion and lookup.\n\n\nExceptional paths (e.g., malloc() failure) complicate this design since the lock must be released before returning, increasing the likelihood of bugs.\n\nA redesign (Figure 29.8) optimizes this by:\n\nLocking only the shared critical section during an insert.\n\nUsing a single return path for lookup to reduce error-prone unlock scenarios.\n\n\n\n\n\n\nLocking Strategies\n\n\nThe redesigned List_Insert defers locking until it reaches the critical section, assuming malloc() is thread-safe.\nList_Lookup minimizes lock and unlock points by consolidating logic.\n\n\nScaling with Hand-Over-Hand Locking\n\n\nConcept:\n Instead of a single lock for the list, assign a lock per node. As the traversal proceeds:\n\nAcquire the lock for the next node.\nRelease the lock for the current node.\n\n\nChallenges:\n\nHigh locking overhead during traversal.\nIn practice, single-lock approaches often outperform due to simplicity and reduced overhead.\n\n\n\n\nPractical Considerations\n\n\nReducing complexity in control flow (e.g., common exit points) is critical in concurrent programming.\nHand-over-hand locking might be better suited for edge cases or hybrid designs where locks are distributed strategically (e.g., every few nodes).\n\n29.3 Concurrent QueuesKey Components\n\nData Structure\n\n\nNode (node_t):\nEach node contains a value (int value) and a pointer to the next node (node_t *next).\n\n\nQueue (queue_t):\nIncludes pointers to the head and tail nodes (node_t *head, *tail).\nTwo separate locks:\nhead_lock for dequeue operations.\ntail_lock for enqueue operations.\n\n\n\n\n\n\nInitialization (Queue_Init)\n\n\nA dummy node is allocated during initialization, ensuring the head and tail always have a valid node to reference, even if the queue is empty.\nBoth head and tail initially point to the dummy node.\n\n\nEnqueue Operation (Queue_Enqueue)\n\n\nAllocates a new node and initializes it with the value to be inserted.\nAcquires the tail_lock to safely update the tail pointer and attach the new node to the end of the queue.\nReleases the tail_lock after completing the operation.\n\n\nDequeue Operation (Queue_Dequeue)\n\n\nAcquires the head_lock to safely remove the node at the front of the queue.\nIf the queue is empty (head-&gt;next == NULL), releases the lock and returns an error code (-1).\nUpdates the head pointer to the next node and retrieves its value.\nFrees the dummy node after updating head.\nReleases the head_lock.\n\n\nConcurrency Features\n\nSeparation of Locks:\nThe use of separate locks for the head and tail enables concurrent enqueue and dequeue operations.\nEnqueue operations access only the tail_lock.\nDequeue operations access only the head_lock.\nThis separation reduces contention and improves throughput.\n\n\nDummy Node:\nSimplifies the management of head and tail pointers by ensuring they never become NULL.\nAvoids special-case logic for handling an empty queue.\n\n\nThread Safety:\nAll critical sections (modifications to head and tail) are protected by the corresponding locks, ensuring thread safety.\n\n\n\n29.4 Concurrent Hash TableKey Components\n\nData Structure\n\n\nHash Table (hash_t):\nConsists of an array of buckets (list_t lists[BUCKETS]), each representing a linked list.\nEach bucket is initialized with its own lock for thread-safe operations.\n\n\nBucket Count (BUCKETS):\nA fixed number of buckets (101 in this example).\nKeys are hashed to determine the appropriate bucket (key % BUCKETS).\n\n\n\n\nInitialization (Hash_Init)\n\n\nIterates through all buckets.\nInitializes each linked list (List_Init(&amp;H-&gt;lists[i])).\n\n\nInsert Operation (Hash_Insert)\n\n\nComputes the bucket index for the key (key % BUCKETS).\nDelegates the insertion to the list’s List_Insert() function.\n\n\nLookup Operation (Hash_Lookup)\n\n\nComputes the bucket index for the key (key % BUCKETS).\nDelegates the search to the list’s List_Lookup() function.\n\n\nConcurrency Features\n\nLock per Bucket:\nEach bucket is a separate linked list with its own lock.\nOperations on different buckets can occur concurrently without contention.\n\n\nScalability:\nMultiple threads can insert or look up keys in different buckets simultaneously.\nCompared to a single lock for the entire hash table, this approach significantly reduces contention.\n\n\nThread-Safe List Operations:\nThe List_Insert() and List_Lookup() functions for individual buckets are assumed to be thread-safe, ensuring consistency for operations within a bucket.\n\n\n\nPerformance Analysis\nFigure 29.11 Results (Hypothetical Summary):\n\nConcurrent Updates:\nPerformance increases linearly with the number of threads for the hash table due to its fine-grained locking.\nEven with thousands of updates, the system scales well as most operations happen in independent buckets.\n\n\nComparison with a Single-Lock Linked List:\nThe linked list suffers from contention as all operations are serialized due to a single lock.\nThe performance of the linked list plateaus and degrades with an increasing number of threads.\n\n\n\n\nAdvantages\n\nFine-Grained Locking:\nImproves throughput by enabling parallelism across buckets.\nAvoids global contention, allowing better CPU utilization.\n\n\nSimplicity:\nStraightforward design based on independent linked lists for buckets.\nReuses the thread-safe list functions from previous implementations.\n\n\nGood Scalability:\nAs shown in Figure 29.11, the hash table’s performance scales well with increased concurrency.\n\n\n\nChapter 30: Condition Variables","slug":"OStep-note","date":"2024-11-01T00:00:00.000Z","categories_index":"理论","tags_index":"","author_index":"Gueason"},{"id":"f1f442c5eab80c7f5411f92281f65439","title":"Script","content":"","slug":"Script","date":"2024-11-01T00:00:00.000Z","categories_index":"其它","tags_index":"","author_index":"Gueason"},{"id":"8a3e3c56ca704c10863ee2d485165156","title":"操作系统408（搬运）","content":"参考与转载链接\n【王道】操作系统 知识点总结（合集）【超详细！】 - Zyyyyyyyyy - 博客园 (cnblogs.com)\n王道操作系统知识点总结_住在天上的云的博客-CSDN博客\n《王道操作系统》学习笔记总目录+思维导图_王道操作系统思维导图-CSDN博客\n5万字、97 张图总结操作系统核心知识点 - 程序员cxuan - 博客园 (cnblogs.com)\n一篇就学会操作系统（最全操作系统笔记总结） - 知乎 (zhihu.com)\n计算机操作系统知识点总结（有这一篇就够了！！！）-CSDN博客\n杨一涛操作系统.pdf\n一、操作系统概论1.1基本概念现代计算机硬件系统由一个或多个处理器、主存、打印机、键盘、鼠标、显示器、网络接口以及各种输入/输出设备构成的系统。在硬件的基础之上，安装了一层软件，这层软件能够根据用户输入的指令达到控制硬件的效果，从而满足用户的需求，这样的软件称为 操作系统。\n\n控制和管理整个计算机系统的硬件和软件资源\n合理地组织调度计算机的工作和资源的分配\n作为中间层提供给用户和其他软件方便的接口和环境\n计算机系统中最基本的系统软件\n\n\n大部分计算机有两种运行模式：内核态 和 用户态，软件中最基础的部分是操作系统，它运行在 内核态 中。操作系统具有硬件的访问权，可以执行机器能够运行的任何指令。软件的其余部分运行在 用户态 下。\n功能：\n\n\n处理机管理\n\n​        即对进程的管理，包括进程控制、进程同步、进程通信、死锁处理、处理机调度等。\n\n存储器管理\n方便程序运行、用户使用及提高内存的利用率，包括内存分配与回收、地址映射、内存保护与共享和内存扩充等功能。\n\n文件管理\n计算机中的信息都是以文件的形式存在的，操作系统中负责文件管理的部分称为文件系统。\n文件管理包括文件存储空间的管理、目录管理及文件读写管理和保护等。\n\n设备管理\n设备管理的主要任务是完成用户的I/O请求，方便用户使用各种设备，并提高设备的利用率，主要包括缓冲管理、设备分配、设备处理和虚拟设备等功能。\n\n\n\n命令接口\n用户利用这些操作命令来组织和控制作业的执行。\n\n\n  联机命令接口：即交互式命令接口，适用于分时或实时系统。\n  “雇主”说一句话，“工人”做一件事，并做出反馈，这就强调了交互性。\n  脱机命令接口：即批处理命令接口，适用于批处理系统\n  “雇主”把要“工人”做的事写在清单上，“工人”按照清单命令逐条完成这些事，这就是批处理。\n\n程序接口\n程序接口由一组系统调用组成。是为编程人员提供的接口。普通用户不能直接使用程序接口，只能通过程序代码间接使用。\n用户通过在程序中使用系统调用命令请求OS为其提供服务。\n\n\n\nGUl：图形化用户接口（Graphical User Interface）\n用户可以使用形象的图形界面进行操作，而不再需要记忆复杂的命令、参数。\n\n\n\n裸机：没有任何软件支持的计算机称为裸机，它仅构成计算机系统的物质基础。\n在裸机上安装的操作系统，可以提供资源管理功能和方便用户的服务功能，将裸机改造成功能更强、使用更方便的机器。\n\n\n​       通常把覆盖了软件的机器成为扩充机器，又称之为虚拟机。\n1.2特征并发\n并发指两个或多个事件在同一时间间隔内发生。这些事件宏观上是同时发生的，但微观上是交替发生的。\nOS的并发性是通过分时实现的。\n\n单核CPU同一时刻只能执行一个程序，各个程序只能并发地执行\n多核CPU同一时刻可以同时执行多个程序，多个程序可以并行地执行\n\n并行：两个或多个事件在同一时刻发生，需要硬件支持，如多流水线或多处理机硬件环境。\n共享\n共享即资源共享，是指系统中的资源可供内存中多个并发执行的进程共同使用。\n\n互斥共享：资源一个时间段内只允许一个进程访问。\n临界资源(独占资源)：在一段时间内只允许一个进程访问的资源，计算机中大多数物理设备及某些软件中的栈、变量和表格都属于临界资源，它们被要求互斥共享\n\n\n\n同时共享：系统中资源的分时共享，允许多个进程宏观上同时对资源进行访问，微观上交替地对资源进行访问\n\n并发和共享互为存在的条件：\n\n资源共享是以程序的并发为条件的，失去并发性，则不存在共享性；\n\n若系统不能对资源共享，则程序无法并发执行。\n\n\n虚拟\n把一个物理上的实体变为若干个逻辑上的对应物。物理实体是实际存在的，而逻辑上对应物是用户感受到的。\n\n虚拟处理器：通过时分复用技术，让多道程序并发执行的方法，来分时使用一个处理器的。\n\n虚拟存储器：通过空分复用技术，将一台机器的物理存储器变为虚拟存储器，以便从逻辑上扩充存储器的容量。\n\n虚拟I/O设备：采用虚拟设备技术将一台物理I/O设备虚拟为多台逻辑上的I/O设备，并允许每个用户占用一台逻辑上的I/O设备，使原来仅允许在一段时间内由一个用户访问的设备（即临界资源）变为在一段时间内允许多个用户同时访问的共享设备。\n\n\n异步\n在多道程序环境下，允许多个程序并发执行，但由于资源有限，进程的执行不是一贯到底的，而是走走停停，以不可预知的速度向前推进，这就是进程的异步性。\n1.3分类与发展\n\n单道批处理系统：引入脱机输入输出技术\n\n特征：\n自动性。在顺利的情况下，磁带上的一批作业能自动地逐个运行，而无须人工干预。\n顺序性。磁带上的各道作业顺序地进入内存，各道作业的完成顺序与它们进入内存的顺序在正常情况下应完全相同，亦即先调入内存的作业先完成。\n单道性。内存中仅有一道程序运行，即监督程序每次从磁带上只调入一道程序进入内存运行，当该程序完成或发生异常情况时，才换入其后继程序进入内存运行。\n\n优点：缓解人机速度矛盾。\n\n缺点：资源利用率仍低，内存中只有高速CPU等待低速I/O。\n\n\n多道批处理系统：操作系统开始出现\n多道程序设计：允许多个程序同时进入内存并在CPU中交替地运行，这些程序共享系统中的各种硬/软件资源。当一道程序因I/O请求而暂停运行时，CPU便立即转去运行另一道程序。\n\n特点：\n多道：计算机内存中同时存放多道相互独立的程序。\n宏观上并行：同时进入系统的多道程序都处于运行过程中，即它们先后开始各自的运行，但都未运行完毕。\n微观上串行：内存中的多道程序轮流占有CPU，交替执行。\n间断性：由于多道程序之间需要共享和竞争系统资源，因此每个程序的执行过程不是连续的，而是有间断的。\n共享性：多道程序之间需要共享系统的各种资源，如CPU、内存、外设等。\n制约性：多道程序之间存在相互制约的关系，如同步、互斥、优先级等。\n\n优点：\n资源利用率高，多道程序并发执行，共享计算机资源\n系统吞吐量大，CPU和其他资源保持”忙碌”\n\n缺点：用户响应时间长、开销大，无交互能力\n\n\n分时操作系统\n分时技术：计算机以时间片为单位轮流为各个用户/作业服务，各个用户可通过终端与计算机进行交互，并共享一台主机。\n\n特点\n同时性。同时性也称多路性，指允许多个终端用户同时使用一台计算机，即一台计算机与若干台终端相连接，终端上的这些用户可以同时或基本同时使用计算机。\n交互性。用户能够方便地与系统进行人机对话，即用户通过终端采用人机对话的方式直接控制程序运行，与同程序进行交互。\n独立性。系统中多个用户可以彼此独立地进行操作，互不干扰\n及时性。用户请求能在很短时间内获得响应。\n\n优点\n用户请求可以被即时响应，解决了人机交互问题。\n允许多个用户同时使用一台计算机，并且用户对计算机的操作相互独立，感受不到别人的存在。\n\n缺点：无法优先处理紧急任务\n\n\n实时操作系统\n能在某个时间限制内完成某些紧急任务而不需要时间片排队。\n\n分类：\n软实时系统：能够接受偶尔违反时间规定且不会引起永久性的损害。\n如飞机订票系统、银行管理系统。\n硬实时系统：某个动作必须绝对地在规定的时刻（或规定的时间范围）发生。\n如飞行器的飞行自动控制系统。\n\n特点\n及时性，可靠性\n\n优点：能够优先处理紧急任务\n\n\n1.4体系结构\n宏内核\n宏内核是将操作系统功能作为一个紧密结合的整体放到内核。\n由于各模块共享信息，因此有很高的性能。\n微内核\n由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。\n在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在用户态。\n1.5运行机制\nCPU执行两种不同性质的程序：\n操作系统内核程序：是用户自编程序的管理者，“管理程序”（即内核程序）要执行一些特权指令。\n用户自编程序：即系统外层的应用程序，或简称“应用程序”，“被管理程序”（即用户自编程序）出于安全考虑不能执行这特权指令。\n\n\n\n特权指令和非特权指令\n特权指令：是指不允许用户直接使用的指令，如/O指令、置中断指令，存取用于内存保护的寄存器、送程序状态字到程序状态字寄存器等的指令。\n非特权指令：是指允许用户直接使用的指令，它不能直接访问系统中的软硬件资源，仅限于访问用户的地址空间，这也是为了防止用户程序对系统造成破坏。\n\n\n\nCPU的运行模式\n用户态（目态）：CPU处于用户态，此时CPU只能执行非特权指令。\n核心态（又称管态、内核态）：CPU处于核心态，此时CPU可以执行特权指令，切换到用户态的指令也是特权指令。\n应用程序运行在用户态，操作系统内核程序运行在核心态。\n\n\n   内核态一&gt;用户态：执行特权指令，修改PSW（程序状态字寄存器），用二进制位表示，1内核态0用户态\n   用户态一&gt;内核态：应用程序向操作系统请求服务时通过使用访管指令，从而产生一个中断事件将操作系统转换为核心态。\n   访管指令\n   访管指令是一条可以在用户态下执行的指令。在用户程序中，因要求操作系统提供服务而有意识地使用访管指令，从而产生一个中断事件（自愿中断)，将操作系统转换为核心态，称为访管中断。\n   访管指令本身不是特权指令，其基本功能是让程序拥有“自愿进管”的手段，从而引起访管中断。\n   陷入指令\n   原则上可看作访管指令，但是从操作系统的角度定义的。访管强调的是cpu从用户态切换到了核心态，可以执行指令集中的所有指令。而陷入（自陷、陷阱）指令强调程序从用户程序从用户台切换到内核态(以下简称切换到操作系统)，陷入指令即汇编中的中断指令。执行陷入指令程序会中断，跳转到中断服务程序（操作系统的代码）。所以访管强调的是可以执行特权指令，陷入强调的是进程放弃cpu，交还给操作系统。\n1.6中断和异常，抢占和上下文，子程序调用\n中断作用\n让操作系统内核强行夺回CPU的控制权；使CPU从用户态变为内核态的方式。\n中断和异常的分类\n异常：又称内中断，指来自CPU执行指令内部的事件，如程序的非法操作码、地址越界、运算溢出、虚存系统的缺页及专门的陷入指令等引起的事件。\n\n故障（Fault）通常是由指令执行引起的异常，如非法操作码、缺页故障、除数为0、运算溢出等。\n\n陷入（Trap）是一种事先安排的“异常”事件，用于在用户态下调用操作系统内核程序，如条件陷阱指令。\n\n终止（Abort）是指出现了使得CPU无法继续执行的硬件故障，如控制器出错、存储器校验错等。\n\n\n注：缺页（Page Fault）通常发生在用户态，而它的处理是在核心态完成的。\n中断：又称外中断，指来自CPU执行指令外部的事件，通常用于信息输入/输出，如I/O中断，时钟中断。\n故障和陷入异常属于软件中断（程序性异常），终止异常（内中断）和外中断属于硬件中断。\n异常不能被屏蔽，一旦出现，就应立即处理。\n中断和异常的处理过程\n\n当CPU在执行用户程序的第i条指令时检测到异常事件，或发现中断请求信号\n\n如果是外部中断，保护被中断进程的CPU环境（如PSW、PC、寄存器）到PCB中。\n\nCPU打断当前用户程序，执行对应的中断或异常处理程序。\n\n恢复秩序\n\n返回第i+1条指令：由自陷（Trap）引起的内中断；如系统调用。由外部设备引起的外中断，如键盘\n返回第i条指令：由故障（Fault）引起的内中断；如缺页等。\n\n\n若中断或异常处理程序发现是不可恢复的致命错误，则终止用户程序。\n\n\n中断与抢占\n上下文是从英文context翻译过来，是指某一时刻处理器的运行状态，包括进程运行所需的所有CPU相关信息。这些信息通常存储在进程的 PCB（进程控制块） 中，用于支持进程的切换和恢复。\n一般来说，CPU处于以下三种状态：\n(1) 运行于用户空间，执行用户进程；(2) 运行于内核空间，处于进程上下文（进程可睡眠，用信号量同步）；(3) 运行于内核空间，处于中断上下文（不关联进程，不可睡眠，只能用锁同步）。\n中断：发生在中断上下文，无进程切换，CPU执行中断处理程序后返回。\n抢占：涉及进程上下文切换，一个进程主动释放或被迫让出CPU。\n子程序调用\n\n1.7系统调用\n系统调用与库函数\n\n系统调用（System Call），又称广义指令，是用户程序向操作系统内核请求服务的接口，通过触发陷入（Trap），允许程序执行特权操作（如文件操作、内存分配、进程管理等），通常需要切换到内核模式\n库函数则是开发者在应用层编写的函数，封装了常用的功能，可以直接被程序调用，无需内核模式切换。\n分类\n\n设备管理：完成设备的请求或释放，以及设备启动等功能。\n\n文件管理：完成文件的读、写、创建及删除等功能。\n\n进程控制：完成进程的创建、撤销、阻塞及唤醒等功能。\n\n进程通信：完成进程之间的消息传递或信号传递等功能。\n\n内存管理：完成内存的分配、回收以及获取作业占用内存区大小及始址等功能。\n\n\n系统调用过程\n系统调用的处理需要由操作系统内核程序负责完成，要运行在核心态。\n系统调用通常通过触发软中断来实现，用户程序可以执行陷入指令（又称访管指令或trap指令）来发起系统调用，请求操作系统提供服务。\n\n访管指令不是特权指令，访管指令是在用户态使用的，所以它不可能是特权指令。\n\n传递系统调用参数→执行陷入（trap）指令→执行相应的服务程序→返回用户态\n注：系统调用在用户态，处理执行在核心态\n\n当需要管理程序服务时，系统则通过硬件中断机制进入核心态，运行管理程序；\n\n也可能是程序运行出现异常情况，被动地需要管理程序的服务，这时就通过异常处理来进入核心态。\n\n管理程序运行结束时，用户程序需要继续运行，此时通过相应的保存的程序现场退出中断处理程序或异常处理程序，返回断点处继续执行\n\n\n\n1.8 原语原语是一种在执行过程中不允许被中断的操作，通常用于实现操作系统的关键功能（如锁的获取/释放、进程调度）。\n它是由一组程序模块所组成，必须在内核态下执行，并且常驻内存，而个别系统有一部分不在管态下运行。\n原语和广义指令都可以被进程所调用。两者的差别在于原语有不可中断性，它是通过在执行过程中关闭中断实现的，且一般由系统进程调用。许多广义指令的功能都可在用户态下运行的系统进程完成，而不一定要在管态下完成，例如文件的建立、打开、关闭、删除等广义指令，都是借助中断进入管态程序，然后转交给相应的进程，最终由进程实现其功能。引进原语的主要目的是为了实现进程的通信和控制。\n1.9 快照什么是快照？ 快照与备份有什么区别？_快照和备份的区别-CSDN博客\n快照介绍和原理-阿里云开发者社区\n\n二、进程管理2.1进程与线程2.1.1 进程的概念和特征一个进程就是一个正在执行的程序的实例。从概念上来说，每个进程都有各自的虚拟 CPU，但是实际情况是 CPU 会在各个进程之间进行来回切换。在任何一个给定的瞬间仅有一个进程真正运行。\n\n进程的概念\n进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。\n程序：是静态的，就是个存放在磁盘里的可执行文件，如：QQ.exe。\n进程：是动态的，是程序的一次执行过程，或者是一个正在运行的程序，如：可同时启动多次QQ程序。\n进程实体：即进程映像，是静态的，可理解为进程的一次时刻的状态。\n作业：用户向计算机提交的一项任务，是静态的，它通常是一个批处理程序或一个后台程序。\n进程实体的组成\n1.程序控制块PCB(Processing Control Block)\nPCB是进程存在的唯一标志，当进程被创建时，操作系统为其创建PCB，当进程结束时，会回收其PCB。\nOS是根据PCB来对并发执行的进程进行控制和管理的，PCB 存于内存的内核区，注意内存的内核区和 OS 的内核态的区别，内核程序运行在内核态。\nPCB包含的内容：\n\n进程描述信息\n进程标识符PID：当进程被创建时，操作系统会为该进程分配一个唯一的、不重复的ID号—PID（Process ID，进程ID）\n用户标识符UID\n\n\n进程控制和管理信息\nCPU、磁盘、网络流量使用情况统计…\n进程当前状态：就绪态/阻塞态/运行态.…\n进程优先级，抢占处理机的优先级\n\n\n资源分配清单\n程序段指针\n数据段指针\nI/O设备\n\n\n处理机相关信息（CPU现场信息）：如PSW（program status word）、PC等等各种寄存器的值（用于实现进程切换）\n\n2.程序段：程序的代码（指令序列）\n3.数据段：运行过程中产生的各种数据（如：程序中定义的变量）\n\nPCB 是给操作系统用的，程序段和数据段是给进程自己用的。\n引入进程实体的概念后，可把进程定义为是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。\n\n进程的特征\n\n动态性：进程是程序的一次执行过程，是动态地产生、变化和消亡的；动态性是进程最基本的特征。\n并发性：内存中有多个进程实体，各进程可并发执行\n独立性：进程是能独立运行、独立获得资源、独立接受调度的基本单位\n异步性：各进程按各自独立的、不可预知的速度向前推进，异步性会导致并发程序执行结果的不确定性。\n结构性：每个进程都会配置一个PCB。结构上看，进程由程序段、数据段、PCB组成\n\n2.1.2 进程的状态与转换基本状态\n\n运行态。占有CPU，并在CPU上运行；有其他所需资源\n单核处理机环境每个时刻只有一个进程处于运行态\n\n\n\n就绪态。已具有运行条件，但无空闲CPU，暂时不能运行；有其他所需资源\n系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。\n\n\n\n阻塞态，又称等待态。因等待某一事件暂时不能运行；无其他所需资源\n系统通常将处于阻塞态的进程也排成一个队列，甚至根据阻塞原因的不同，设置多个阻塞队列。\n\n\n\n创建态。进程正在被创建，尚未转到就绪态。OS为进程分配系统资源、初始化PCB\n\n首先申请一个空白PCB，并向PCB中填写用于控制和管理进程的信息\n然后为该进程分配运行时所必须的资源\n最后把该进程转入就绪态并插入就绪队列\n\n但是，如果进程所需的资源尚不能得到满足，如内存不足，则创建工作尚未完成，进程此时所处的状态称为创建态。\n\n\n\n终止态。进程正从系统中消失，进程正常结束或其他原因退出运行。OS回收进程拥有的资源，撤销PCB\n\n进程状态的转换\n\n(1) 运行态 → 就绪态\n\n触发原因:\n中断:\n时间片到期引发时钟中断：将当前运行的进程放回就绪队列，交由调度器选择下一个进程。\n优先级抢占引发中断：一个优先级更高的进程需要运行。\n主动放弃CPU ：则通过系统调用（软中断）实现\n\n\n用户态/核心态:\n核心态：中断处理程序运行在核心态。\n当前进程被中断后进入就绪态，系统返回用户态运行新的进程。\n\n\n\n\n\n(2) 运行态 → 阻塞态\n\n触发原因:\n系统调用:\n进程主动请求某些需要等待的操作，如I/O操作（read()、write()）、资源申请（wait()）。这些操作通常需要操作系统完成，进程因此进入阻塞态。\n\n\n用户态/核心态:\n发起系统调用时，进程从用户态切换到核心态，调用内核服务后进入阻塞态，随后切回核心态或另一个进程运行。\n\n\n\n\n\n(3) 阻塞态 → 就绪态（唤醒）\n\n触发原因:\n中断:\nI/O中断：I/O操作完成后，硬件设备触发中断，通知操作系统相关进程可以继续执行。\n\n\n用户态/核心态:\n中断处理程序在核心态运行；进程被唤醒并转入就绪队列时，仍然由核心态切换至用户态以恢复正常执行。\n\n\n\n\n\n(4) 就绪态 → 运行态\n\n触发原因:\n调度器分配CPU：调度程序选择当前就绪队列中的某个进程，将其切换到运行态。\n用户态/核心态:\n进程在被调度器选中时，先进入核心态以执行必要的上下文切换操作，完成后进入用户态以运行程序。\n\n\n\n\n\n2.1.3进程的组织方式链接方式\n链接方式是将同一状态的进程的PCB组成一个双向链表，称为进程队列。\n\n结构：每个队列的队首和队尾都有一个指针，指向第一个和最后一个PCB。每个PCB中也有两个指针，分别指向前一个和后一个PCB。这样，就可以方便地在队列中插入或删除PCB。\n\n优点：简单、灵活\n\n缺点：查找效率低，需要遍历链表\n\n\n\n索引方式\n索引方式是将所有的PCB存放在一张索引表中，每个表项包含一个PCB的地址和状态信息。\n\n结构：索引表可以是顺序表或散列表，可以按照进程号或其他关键字进行排序或散列。\n优点：查找效率高，可以快速定位到某个PCB\n缺点：需要额外的空间存储索引表，且索引表的大小受限于内存容量\n\n\n2.1.4进程的控制创建进程的方式\n\n系统初始化（init）：启动操作系统时，通常会创建若干个进程。\n正在运行的程序执行了创建进程的系统调用（比如 fork）\n用户请求创建一个新进程：在许多交互式系统中，执行命令、双击图标等触发新进程创建。\n初始化一个批处理工作\n\n进程的创建过程\n\n创建原语：操作系统创建一个进程时使用的原语，其操作如下；创建态→就绪态\n\n申请空白PCB\n为新进程分配所需资源\n初始化PCB\n将PCB插入就绪队列\n\n\n引起进程创建的事件\n\n用户登录：分时系统中，用户登录成功，系统会建立为其建立一个新的进程\n作业调度：多道批处理系统中，有新的作业放入内存时，会为其建立一个新的进程\n提供服务：用户向操作系统提出某些请求时，会新建一个进程处理该请求\n应用请求：由用户进程主动请求创建一个子进程\n\n\n父子进程\n允许一个进程创建另一个进程，此时创建者称为父进程，被创建的进程称为子进程。\n\n进程可以继承父进程所拥有的资源。\n当子进程被撤销时，应将其从父进程那里获得的资源归还给父进程。\n在撤销父进程时，通常也会同时撤销其所有的子进程。\n\n\n\n进程的终止原因\n\n正常退出(自愿的) ： 多数进程是由于完成了工作而终止。当编译器完成了所给定程序的编译之后，编译器会执行一个系统调用告诉操作系统它完成了工作。\n错误退出(自愿的)：比如执行一条不存在的命令，于是编译器就会提醒并退出。\n严重错误(非自愿的)\n被其他进程杀死(非自愿的) ： 某个进程执行系统调用告诉操作系统杀死某个进程。\n\n进程的终止过程\n\n撤销原语：其操作如下；\n就绪态/阻塞态/运行态→终止态→无\n\n从PCB集合中找到终止进程的PCB\n若进程正在运行，立即剥夺CPU，将CPU分配给其他进程\n终止其所有子进程\n将该进程拥有的所有资源归还给父进程或操作系统\n删除PCB\n\n\n引起进程终止的事件\n\n正常结束：进程自已请求终止（exit系统调用）\n异常结束：整数除以0、非法使用特权指令，然后被操作系统强行杀掉\n外界干预：用户选择杀掉进程\n\n\n\n进程的阻塞\n\n阻塞原语：其操作如下；\n运行态→阻塞态\n\n找到要阻塞的进程对应的PCB\n保护进程运行现场，将PCB状态信息设置为“阻塞态\n将PCB插入相应事件的等待队列\n\n\n引起进程阻塞的事件\n\n需要等待系统分配某种资源\n需要等待相互合作的其他进程完成工作\n\n\n\n进程的唤醒\n\n唤醒原语：其操作如下；\n阻塞态→就绪态\n\n在事件等待队列中找到PCB\n将PCB从等待队列移除，设置进程为就绪态\n将PCB插入就绪队列，等待被调度\n\n\n引起进程唤醒的事件\n\n等待的事件发生：因何事阻塞，就应由何事唤醒\n\n\n\n阻塞原语唤醒原语必须成对使用\n进程的切换\n\n切换原语：其操作如下；\n运行态→就绪态，就绪态→运行态\n\n将运行环境信息存入PCB\nPCB移入相应队列选择\n另一个进程执行，并更新其PCB\n根据PCB恢复新进程所需的运行环境\n\n\n引起进程切换的事件\n\n当前进程时间片到\n有更高优先级的进程到达\n当前进程主动阻塞\n当前进程终止\n\n\n\n\n调度是指决定资源分配给哪个进程的行为，是一种决策行为切换是指实际分配的行为，是执行行为一般来说现有资源调度，后有进程切换\n操作系统为了执行进程间的切换，会维护着一张表，这张表就是 进程表(process table)。每个进程占用一个进程表项。该表项包含了进程状态的重要信息，包括程序计数器、堆栈指针、内存分配状况、所打开文件的状态、账号和调度信息，以及其他在进程由运行态转换到就绪态或阻塞态时所必须保存的信息。\n下面展示了一个典型系统中的关键字段\n\n第一列内容与进程管理有关，第二列内容与 存储管理有关，第三列内容与文件管理有关。\n与每一 I/O 类相关联的是一个称作 中断向量(interrupt vector) 的位置（靠近内存底部的固定区域），指向中断服务程序入口地址。以磁盘中断为例，假设用户进程正在运行：\n\n硬件层操作\n压入程序计数器和状态字到堆栈。\n跳转至中断向量指定地址。\n\n\n软件层操作\n汇编代码保存寄存器值并设置新堆栈。\nC 中断服务程序处理中断（如缓存写入）。\n调用调度程序选择下一个运行的进程。\n\n\n恢复运行\n汇编代码恢复寄存器值和内存映射。\n进程从中断前状态继续运行。\n\n\n\n2.1.5进程的通信低级通信方式：PV操作。高级通信方式：共享存储、消息传递、管道通信。\n1.共享存储\n设置一个共享空间进行读/写操作实现信息交换，一次只能有一个进程进行读或写操作\n在对共享空间进行写/读操作时，需要使用同步互斥工具（如PV操作）。\n\n共享存储分为两种：\n\n低级方式：基于数据结构的共享\n比如共享空间里只能放一个长度为10的数组。这种共享方式速度慢、限制多，是一种低级通信方式\n\n高级方式：基于存储区的共享\n操作系统在内存中划出一块共享存储区，数据的形式、存放位置都由通信进程控制，而不是操作系统。这种共享方式速度很快，是一种高级通信方式。\n\n\n\n进程之间共享空间需要通过特殊的系统调用实现；进程内线程共享进程空间。\n\n2.消息传递\n在消息传递系统中，进程间的数据交换以格式化的消息（Message）为单位。\n\n进程通过操作系统提供的“发送消息/接收消息”两个原语进行数据交换。\n\n在微内核操作系统中，微内核与服务器之间的通信就采用了消息传递机制。\n\n消息格式：\n\n消息头：发送进程ID、接受进程ID、消息长度等格式化的信息\n消息体\n\n通信方式：\n\n直接通信方式：发送进程直接把消息发送给接收进程，并将它挂在接收进程的消息缓冲队列上，接收进程从消息缓冲队列中取得消息。\n\n间接通信方式：发送进程通过信箱间接地通信，将消息发送到某个中间实体，接收进程从中间实体取得消息。该通信方式广泛应用于计算机网络中。\n\n注：可以多个进程往同一个信箱 send 消息，也可以多个进程从同一个信箱中 receive 消息。\n用发送原语和接收原语实现基于信箱的进程间通信\n\n\n\n3.管道通信\n管道是一种特殊的共享文件（又称 pipe 文件），本质上是内存中的固定大小缓冲区。\n一个进程向这个通道里写入字节流，另一个进程从这个管道中读取字节流。读空时，读进程阻塞；写满时，写进程阻塞。\nshell 中的管线 pipelines 就是用管道实现的\n1sort &lt;f | head\nShell 创建两个进程（sort 和 head），并建立管道，使 sort 的输出直接作为 head 的输入。管道满时，系统暂停 sort，直到 head 读取数据。\n管道实际上就是 |，两个应用程序不知道有管道的存在，一切都是由 shell 管理和控制的。\n\n管道通信按生产者-消费者方式通信，需互斥访问管道：\n\n写满不能写，读空不能读\n未写满时不能读，未读空不能写\n\n\n一个管道只能实现半双工通信；实现双向同时通信要建立两个管道\n\n管道只能由创建进程所访问，父进程创建的管道会被子进程继承，用于父子进程间通信。\n2.1.6线程和多线程模型1.线程的基本概念\n线程是CPU是程序执行运算的基本单位。一个进程可以包含一个或多个线程，它们共享同一进程的资源，但每个线程有自己的执行上下文。线程不拥有系统资源也无法分配。\n引入线程的原因：\n\n多线程之间会共享同一块地址空间和所有可用数据的能力\n线程要比进程更轻量级，方便创建，撤销。\n简化进程间的通信，以小的开销提高进程内的并发程度\n\n2.进程（Process）与线程（Thread）比较\n进程：系统进行资源分配和管理的基本单位。\n线程：CPU运行调度的最小单位。\n区别与联系：\n\n一个进程可以有一个或多个线程\n线程包含在进程之中，是进程中实际运行工作的单位\n进程的线程共享进程资源\n一个进程可以并发多个线程，每个线程执行不同的任务。\n进程作为系统资源的分配单位，线程则作为处理机的分配单元\n\n\n线程不像是进程那样具备较强的独立性。同一个进程中的所有线程都会有完全一样的地址空间，这意味着它们也共享同样的全局变量。由于每个线程都可以访问进程地址空间内每个内存地址，因此一个线程可以读取、写入甚至擦除另一个线程的堆栈。线程之间除了共享同一内存空间外，还具有如下不同的内容\n\n3.线程的实现方式\n线程的实现可以分为两类：用户级线程 和 内核级线程。内核级线程又称内核支持的线程。\n\n用户级线程(User-Level Thread,UTL)\n线程管理由用户程序在用户空间完成，内核无法感知线程的存在。“用户级线程”就是“从用户视角看能看到的线程。线程切换在用户态完成，无需os内核干预。\n\n\n若系统中只有用户级线程，则处理机的调度对象是进程\n\n\n\n\n内核级线程(Kernel-Level Thread, KTL)\n内核级线程是在内核的支持下运行的，线程管理的所有工作也是在内核空间内实现的。“内核级线程”就是“从操作系统内核视角看能看到的线程”\n\n\n线程管理由操作系统内核完成，线程调度和切换需要进入核心态。\n操作系统会为每个内核级线程建立相应的TCB（Thread Control Block，线程控制块）通过TCB对线程进行管理。\n\n\n\n注：内核级线程才是处理机分配的单位\n组合方式\n在组合实现方式中，内核支持多个内核级线程的建立、调度和管理，同时允许用户程序建立、调度和管理用户级线程。也只有内核级线程才是处理及分配的单位。\n\n一个内核级线程可以对应多个用户级线程，用户级线程通过时分多路复用来共享内核级线程。\n\n同一进程中的多个线程可以同时在多处理机上并行执行，且在阻塞一个线程时不需要将整个进程阻塞\n\n线程库\n线程库是为程序员提供创建和管理线程的API。实现方式有以下两种。\n用户级线程库：完全在用户空间中实现，调用时不会涉及内核系统调用。\n内核级线程库：由操作系统提供，调用时会进行系统调用，操作系统在内核空间管理线程。\n\n\n\n\n5.多线程模型\n\n一对一模型\n一个用户级线程映射到一个内核级线程。每个用户进程有与用户级线程同数量的内核级线程。\n\n\n\n优点：支持多线程并发，阻塞一个线程不会影响其他线程。\n\n缺点：线程切换成本高，占用内核资源多。\n\n\n\n\n多对一模型\n多个用户级线程映射到一个内核级线程。且一个进程只被分配一个内核级线程。\n\n优点：用户级线程的切换在用户空间即可完成，线程管理开销小，切换效率高\n\n缺点：一个线程阻塞会导致整个进程阻塞，并发度低。\n\n\n\n\n\n\n多对多模型\nn用户及线程映射到m个内核级线程（n&gt;=m）。每个用户进程对应m个内核级线程。\n克服了多对一模型并发度不高的缺点（一个阻塞全体阻塞），又克服了一对一模型中一个用户进程占用太多内核级线程，开销太大的缺点。还拥有上述两种模型各自的优点。\n\n\n\n6.线程的状态与转换\n\n7.线程的组织与控制\n\n\n线程控制块（TCB）\n\n线程标识符\n\n\n寄存器集合（如程序计数器、状态寄存器等）\n\n线程运行状态，用于描述线程正处于何种状态\n优先级\n线程专有存储区\n堆栈指针（保存局部变量及返回地址）\n\n所有线程共享进程的地址空间和全局变量，能访问彼此的堆栈。\n\n\n\n线程的创建\n程序启动时通常有一个初始化线程，负责创建其他线程。创建线程时需要提供线程主程序入口、堆栈大小和优先级等参数。创建函数返回一个线程标识符。\n\n\n\n线程的终止\n线程完成任务后，可以通过特定函数（如 thread_exit）终止。被终止的线程不立即释放资源，直到调用分离函数后才释放。\n被终止但尚未释放资源的线程仍可被其他线程调用，以使被终止线程重新恢复运行。\n\n\n\n线程系统调用\n通过调用库函数（thread_create）创建新的线程。创建的线程通常都返回一个线程标识符作为新线程的名字。\n当线程完成工作后，通过调用一个函数（thread_exit）退出。可以通过调用函数例如 thread_join ，表示一个线程可以等待另一个线程退出。\n调用 thread_yield，允许线程自动放弃 CPU 从而让另一个线程运行。不同于进程，线程是无法利用时钟中断强制让线程让出 CPU 的。\n\n\n2.2 处理机调度2.2.1 调度的概念调度的基本概念\n当计算机系统支持多道程序设计时，会有多个进程或线程争夺 CPU 时间片。操作系统通过 调度程序 (scheduler)来决定哪个进程或线程获得 CPU 执行时间。调度程序使用的算法称为 调度算法(scheduling algorithm)。\n处理机调度的目的是合理分配 CPU 时间，按照一定的算法（如公平性和效率）从就绪队列中选择一个进程或线程运行，实现进程的 并发 执行。\n调度的层次\n一个作业从提交开始直到完成，要经历以下三级调度，如下图所示。\n\n\n高级调度（作业调度）\n当内存空间有限时，从外存的作业后备队列中选择作业调入内存，并创建进程。每个作业调入一次，调出一次。作业调入时会建立PCB，调出时才撤销PCB。\n\n作业：一个具体的任务\n\n\n发生频率最低 外存→内存（面向作业）\n\n\n\n\n中级调度（内存调度）\n内存不足时，将某些进程的数据调出外存，待内存空闲或进程需要运行时，再从挂起队列中选择进程调入内存。\n\n暂时调到外存等待的进程状态为挂起状态。被挂起的进程PCB会被组织成挂起队列。\n\n\n外存→内存（面向进程）\n\n\n\n\n低级调度（进程调度）\n在内存中从就绪队列中选择进程，将处理机分配给它。\n\n发生频率高 内存→CPU\n\n\n\n三级调度的联系\n\n七状态模型\n\n\n\n2.2.2 调度的目标 不同的调度算法具有不同的特性，在选择调度算法时，必须考虑算法的特性。评价标准如下。\n\nCPU利用率：指CPU“忙碌”的时间占总时间的比例。\n\n利用率=\\frac{忙碌的时间}{总时间}\n系统吞吐量：单位时间内完成作业的数量。\n\n系统吞吐量=\\frac{总共完成了多少道作业}{总共花了多少时间}\n周转时间：指从作业被提交给系统开始，到作业完成为止的这段时间间隔。\n\n周转时间=作业完成时间-作业提交时间平均周转时间：指多个作业周转时间的平均值。\n\n平均周转时间=\\frac{各个作业周转时间之和}{作业数}带权周转时间：作业周转时间与作业实际运行时间的比值。带权周转时间必然≥1\n\n带权周转时间=\\frac{作业周转时间}{作业实际运行时间}=\\frac{作业完成时间-作业提交时间}{作业实际运行时间}平均带权周转时间：多个作业带权周转时间的平均值。\n\n平均带权周转时间=\\frac{各个作业带权周转时间之和}{作业数}\n等待时间\n等待时间，指进程/作业处于等待处理机状态时间之和，等待时间越长，用户满意度越低。\n\n等待时间=周转时间-运行时间\n对于进程来说，等待时间就是指进程建立后等待被服务的时间之和。\n对于作业来说，不仅要考虑建立进程后的等待时间，还要加上作业在外存后备队列中等待的时间。\n\n平均等待时间：各个进程/作业等待时间的平均值。\n\n平均等待时间=\\frac{各个进程/作业等待时间之和}{进程/作业数}\n响应时间：从用户提交请求到首次产生响应所用的时间。\n\n\n2.2.3 调度的实现1.调度程序（调度器）\n用于调度和分派CPU 的组件称为调度程序，它通带由三部分组成\n\n\n排队器：按策略管理就绪进程队列，将新就绪进程插入队列。\n分派器：从就绪队列中取出选定进程，将CPU分配给它。\n上下文切换器：完成进程切换，包括保存当前进程上下文并恢复新进程上下文。\n\n2.调度的时机\n\n需要调度\n\n主动放弃：进程终止、异常、主动阻塞。（比如等待IO）\n被动放弃：时间片用完；I/O中断；更高优先级进程到来进入就绪队列\n\n\n不能调度\n\n处理中断的过程中\n中断处理程序正在执行，此时系统需优先完成中断服务，不能进行调度。\n\n进程在内核程序的临界区中\n内核临界区访问关键资源（如就绪队列）。\n为确保资源的完整性和操作的原子性，不允许调度和切换。\n用户态的普通临界区对系统的影响较小，因此允许发生调度。\n\n原子操作未完成时\n原子操作是不可中断的基本单元，必须完整执行完毕，调度必须等待其结束。\n\n\n\n\n3.进程调度方式\n\n非剥夺调度方式\n又称非抢占方式。当前进程会一直使用处理机直到该进程终止或主动要求进入阻塞态。\n适合批处理系统，简单但无法及时处理紧急任务。\n\n剥夺调度方式\n又称抢占方式。高优先级进程可抢占CPU，支持分时和实时系统，能处理紧急任务。\n\n\n4.进程切换\n\n上下文切换：切换CPU到另一个进程需要保存当前进程状态并恢复另一个进程的状态。\n\n对原来运行进程各种数据的保存\n对新的进程各种数据的恢复（如：程序计数器、程序状态字、各种数据寄存器等处理机现场信息，这些信息一保存在进程控制块）\n\n\n\n  切换流程：\n\n保存当前进程的上下文（寄存器、程序计数器等）。\n更新当前进程的PCB。\n\n\n切换就绪队列，选择新进程。\n加载新进程的上下文并恢复运行。\n\n\n\n5.闲逛进程\n调度程序永远的备胎，没有其他就绪进程时，运行闲逛进程（idle）\n特性：\n\n优先级最低；\n可以是0地址指令，占一个完整的指令周期（指令周期末尾例行检查中断）\n能耗低\n\n\n闲逛进程不需要CPU之外的资源，它不会被阻塞。\n\n6.两种线程的调度\n\n用户级线程调度。由于内核并不知道线程的存在，所以内核还是和以前一样，选择一个进程，并给予时间控制。由进程中的调度程序决定哪个线程运行。\n内核级线程调度。内核选择一个特定线程运行，通常不用考虑该线程属于哪个进程。对被选择的线程赋予一个时间片，如果超过了时间片，就会强制挂起该线程。\n\n\n用户级线程的线程切换在同一进程中进行，仅需少量的机器指令；\n内核级线程的线程切换需要完整的上下文切换、修改内存映像、使高速缓存失效，这就导致了若干数量级的延迟。\n\n2.2.4 典型的调度算法先来先服务（FCFS，first-come,first-serverd）\n\n算法规则：按作业/进程到达的先后顺序进行服务。第一个任务启动后可以运行任意长时间，不会因运行时间过长而中断。\n当其他作业进入时，排到就绪队列尾部。当正在运行的进程阻塞，处于等待队列的第一个进程就开始运行。\n阻塞的进程恢复时也排就绪队列的队尾。\n\n\n\n优缺点：\n\n优点：公平、算法实现简单，一个单链表记录了所有就绪进程\n缺点：排在长作业（进程）后面的短作业需要等待很长时间，带权周转时间很大，FCFS算法对长作业有利，对短作业来说用户体验不好。\n\n\n非抢占式的算法；不会导致饥饿\n\n\n\n短作业优先（SJF，Shortest Job First）\n\n算法规则：最短的作业/进程优先得到服务\n\n优缺点\n\n优点：\n“最短的”平均等待时间、平均周转时间;\n\n在所有进程都几乎同时到达且都可以运行时，采用SJF调度算法的平均等待时间、平均周转时间最少；\n\n\n缺点：必须先知道作业的运行时间。对短作业有利，对长作业不利。可能产生饥饿现象。\n\n\n\n抢占式的算法；会导致饥饿\n\nSJF和SPF是非抢占式的算法。但是也有抢占式的版本：最剩间优先算法（SRTN，Shortest Remaining Time Next）\n每当有进程加入就绪队列改变时就需要调度，如果新到达的进程剩余时间比当前运行的进程剩余时间更短，则由新进程抢占处理机，当前运行进程重新回到就绪队列。另外，当一个进程完成时也需要调度\n\n\n\n\n\n\n优先级调度算法\n\n算法规则：每个作业/进程有各自的优先级，调度时选择优先级最高的作业/进程\n\n优缺点\n\n优点：用优先级区分紧急程度、重要程度，适用于实时操作系统。可灵活地调整对各种作业/进程的偏好程度\n缺点：若源源不断地有高优先级进程到来，则可能导致饥饿\n\n\n抢占式/非抢占式的算法；会导致饥饿\n\n抢占式、非抢占式都有。做题时的区别在于：非抢占式只需在进程主动放弃处理机时进行调度即可，而抢占式还需在就绪队列变化时，检查是否会发生抢占。\n\n\n优先级分类：根据优先级是否可以动态改变，可将优先级分为静态优先级和动态优先级两种。\n\n静态优先级：创建进程时确定，之后一直不变\n动态优先级：创建进程时有一个初始值，之后会根据情况动态地调整优先级。\n\n\n就绪队列未必只有一个，可以按照不同优先级来组织。另外，也可以把优先级高的进程排在更靠近队头的位置\n\n\n\n非抢占\n\n抢占\n\n时间片轮转调度算法（RR）\n\n算法思想：公平轮流地为各个进程服务，让每个进程在一定时间间隔内都可以得到响应\n\n算法规则：按照各进程到达就绪队列的顺序，轮流让各个进程执行一个时间片（如100ms）。若进程未在一个时间片内执行完，则剥夺处理机，将进程重新放到就绪队列队尾重新排队。\n\n优缺点\n\n优点：公平；响应快，适用于分时操作系统；\n缺点：由于高频率的进程切换，因此有一定开销；不区分任务的紧急程度。\n\n\n抢占式的算法；不会导致饥饿\n\n若进程未能在时间片内运行完，将被强行剥夺处理机使用权，因此时间片轮转调度算法属于抢占式的算法。由时钟装置发出时钟中断来通知CPU时间片已到\n\n\n\n\n\n\n高响应比优先（HRRN）\n\n算法思想：要综合考虑作业/进程的等待时间和要求服务的时间\n\n算法规则：在每次调度时先计算各个作业/进程的响应比，选择响应比最高的作业/进程为其服务\n\n响应比=\\frac{等待时间+要求服务时间}{要求服务时间}高响应比优先算法：非抢占式的调度算法，只有当前运行的进程主动放弃CPU（主动阻塞），调度时计算所有就绪进程的响应比，选响应比最高的进程上处理机。\n\n优缺点\n\n综合考虑了等待时间和运行时间（要求服务时间）等待时间相同时，要求服务时间短的优先（SJF的优点）；\n要求服务时间相同时，等待时间长的优先（FCFS的优点）\n对于长作业来说，随着等待时间越来越久，其响应比也会越来越大，从而避免了长作业饥饿的问题\n\n\n非抢占式的算法；不会导致饥饿\n\n非抢占式的算法。因此只有当前运行的作业/进程主动放弃处理机时，才需要调度，计算响应比\n\n\n\n\n\n多级队列调度算法\n\n系统中按进程类型设置多个队列，进程创建成功后插入某个队列\n\n\n\n队列之间可采取固定优先级，或时间片划分\n\n固定优先级：高优先级空时低优先级进程才能被调度\n时间片划分：如三个队列分配时间50%、40%、10%\n\n\n各队列可采用不同的调度策略，如\n系统进程队列采用优先级调度、交互式队列采用RR、批处理队列采用FCFS\n\n\n多级反馈队列调度算法\n\n\n算法思想：对其他调度算法的折中权衡\n\n算法规则：\n\n1.设置多级就绪队列，各级队列优先级从高到低，时间片从小到大\n2.新进程到达时先进入第1级队列，按FCFS原则排队等待被分配时间片，若用完时间片进程还未结束，则进程进入下一级队列队尾。如果此时已经是在最下级的队列，则重新放回该队列队尾\n3.只有第k级队列为空时，才会为k+1级队头的进程分配时间片\n\n\n用于作业/进程调度：用于进程调度\n\n优缺点\n\n对各类型进程相对公平（FCFS的优点）；\n每个新到达的进程都可以很快就得到响应（RR优点）；\n短进程只用较少的时间就可完成（SPF优点）；\n不必实现估程运时间（避用户作假）；\n可灵活地调整对各类进程的偏好程度，比如CPU密集型进程、IO密集型进程\n\n\n拓展：可以将因I/O而阻塞的进程重新放回原队列，这样I/O型进程就可以保持较高优先级\n\n\n抢占式的算法；会导致饥饿\n\n在k级队列的进程运行过程中，若更上级的队列（1~k-1级）中进入了一个新进程，则由于新进程处于优先级更高的队列中，因此新进程会抢占处理机，原来运行的进程放回k级队列队尾。\n\n\n例：\n\n(2019年408第27题)系统采用二级反馈队列调度算法进行进程调度。就绪队列Q1采用时间片轮转调度算法，时间片为10ms；就绪队列Q2采用短进程优先调度算法，系统优先调度Q1队列中的进程，当Q1为空时系统才会调度Q2中的进程；新创建的进程首先进入Q1；Q1中的进程执行一个时间片后，若未结束，则转入Q2。若当前Q1、Q2为空，系统依次创建进程P1、P2后即开始进程调度，P1、P2需要的 CPU 时间分别为 30ms 和 20ms，则进程P1、P2在系统中的平均等待时间为( 15ms )。\n\n\n\n\nP1等待时间 = P1周转时间 - P1运行时间 = 50-30 = 20ms\nP2等待时间 = P2周转时间 - P2运行时间 = 30-20 = 10ms\nP1、P2在系统中的平均等待时间 = (P1等待时间+P2等待时间)/2 = 15ms\n\n经典调度算法解析-CSDN博客\n2.2.4 操作系统之作业/进程调度算法（FCFS先来先服务、SJF短作业优先、HRRN高响应比优先）_llf算法-CSDN博客\n2.2.5 操作系统之调度算法（时间片轮转调度算法、优先级调度算法、多级反馈队列调度算法）_操作系统时间调度表-CSDN博客\n\n\n\n\n\n先来先服务\n短作业优先\n高响应比优先\n时间片轮转\n多级反馈队列\n\n\n\n\n能否是可抢占\n否\n能\n能\n能\n队列内算法不一定\n\n\n能否是非抢占\n能\n能\n能\n否\n队列内算法不一定\n\n\n优点\n公平，实现简单\n平均等待时间最少，效率最高\n兼顾长短作业\n兼顾长短作业\n兼顾长短作业， 有较好的的响应时间， 可行性强\n\n\n缺点\n不利于短作业\n长作业会饥饿， 估计时间不易确定\n计算响应比的开销大\n平均等待时间较长， 上下文切换浪费时间\n无\n\n\n适用于\n无\n作业调度， 批处理系统\n无\n分时系统\n相当通用\n\n\n默认决策模式\n非抢占\n非抢占\n非抢占\n抢占\n抢占\n\n\n\n\n2.3 同步与互斥2.3.1 同步与互斥的基本概念\n临界资源：一个时间段内只允许一个进程使用的资源。各进程需要互斥地访问临界资源。\n临界区：访问临界资源的那段代码。\n临界区是系统提供的一种数据结构，程序中可以声明一个该类型的变量，之后用它来实现对资源的互斥访问。当进程访问某一临界资源时，先将该临界区加锁（若临界区不空闲则等待），用完该资源后，将临界区释放。\n\n\n  为了保证临界资源的正确使用，可把临界资源的访问过程分成4个部分：\n\n1）进入区。为了进入临界区使用临界资源，在进入区要检查可否进入临界区，若能进入临界区，则应设置正在访问临界区的标志，以阻止其他进程同时进入临界区。\n2）临界区。进程中访问临界资源的那段代码，又称临界段。\n3）退出区。将正在访问临界区的标志清除。\n4）剩余区。代码中的其余部分。\n123456do&#123;    entry section;\t\t\t//进入区    critical section;\t\t//临界区    exit section;\t\t\t//退出区    remainder section;\t\t//剩余区&#125;while(true)\n\n\n同步\n\n同步亦称直接制约关系，它是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而产生的制约关系。进程间的直接制约关系就是源于它们之间的相互合作。\n读进程和写进程并发地运行，由于并发必然导致异步性，因此“写数据”和“读数据”两个操作执行的先后顺序是不确定的。而实际应用中，又必须按照“写数据→读数据”的顺序来执行的。\n\n互斥\n 互斥也称间接制约关系。当一个进程进入临界区使用临界资源，另一进程必须等待当占用临界资源的进程退出临界区后，另一进程才能访问此临界资源。\n遵循原则：\n\n空闲让进。临界区空闲时，可以允许一个请求进入临界区的进程立即进入临界区；\n忙则等待。当已有进程进入临界区时，其他进程进入临界区进必须等待\n有限等待。对请求访问的进程，应保证能在有限时间内进入临界区（保证不会饥饿）\n让权等待。当进程不能进入临界区时，应立即释放处理机，防止进程忙等待。\n\n2.3.2 实现临界区互斥的基本方法1.软件实现方法\n单标志法（违背“空闲让进”原则）\n两个进程在访问完临界区后会把使用临界区的权限转交给另一个进程。也就是说每个进程进入临界区的权限只能被另一个进程赋予。\n该算法设置一个公用整型变量turn，用于指示被允许进入临界区的进程编号。\n\n\n双标志法先检查（违背“忙则等待”原则）\n在每一个进程访问临界区资源之前，先查看一下临界区资源是否正被访问，若正被访问，该进程需等待：否则，进程才进入自己的临界区。\n\n\n优点：不用交替进入，可连续使用\n缺点：按序列①⑤②⑥执行时，会同时进入临界区（违背“忙则等待”），Pi进程和Pj进程可能同时进入临界区；检查和修改操作不能一次进行。\n\n双标志法后检查\n\n\n缺点：当两个进程几乎同时都想进入临界区时，它们分别将自己的标志值设置为TRUE，并且同时检测对方的状态，发现对方也要进入临界区，于是双方互相谦让，结果谁也进不了临界区，从而导致“饥饿”现象。违背了“空闲让进”和“有限等待”产生饥饿\n\nPeterson算法\n结合双标志法、单标志法的思想。如果双方都争着想进入临界区，那可以让进程尝试“孔融让梨”（谦让）。做一个有礼貌的进程。\n\n\n\n为了防止两个进程为进入临界区而无限期等待，又设置了变量turn，每个进程在先设置自己的标志后再设置turn标志。这时，再同时检测另一个进程状态标志和允许进入标志，以便保证两个进程同时要求进入临界区时，只允许一个进程进入临界区。\n进程在进入区要做的步骤： ① 主动争取 ② 主动谦让 ③ 检查对方是否也想使用，且最后一次是不是自己说了客气话\n存在问题：Peterson 算法用软件方法解决了进程互斥问题， 遵循 “空闲让进”、“忙则等待”、“有限等待” 三个原则，但是依然 未遵循 “让权等待” 的原则。\n\n2.硬件实现方法\n中断屏蔽方法\n当一个进程正在使用处理机执行它的临界区代码时，要防止其他进程再进入其临界区访问的最简单的方法是：禁止一切中断发生，或称之为屏蔽中断、关中断。\n\n硬件指令法\n\nTestAndSet指令\n简称TS指令，也有地方称为TestAndSetLock指令，或TSL指令。TSL指令是用硬件实现的。TS是原子操作，执行的过程不允许被中断，只能一气呵成。以下是用C语言描述的逻辑。\n执行TSL指令时，它的内部运转逻辑：\n\n假设lock现在为false，代表临界资源A空闲，那么我就可以访问这个资源，同时将lock=true，提醒别的进程，这个临界资源A我正在使用，让他们等等\n假设lock为true，代表临界资源正在有人使用，所以我必须等待，并且将lock=true，并不影响什么，所以没关系，只是为了让lock为false时可以上锁，将上锁与检查在一个TSL指令完成。\n\n\n\n\n\nSwap指令\n\n\n\n逻辑上来看 Swap 和 TSL 并无太大区别，都是先记录下此时临界区是否已经被上锁（记录在 old 变量上），再将上锁标记 lock 设置为 true，最后检查 old，如果 old 为 false 则说明之前没有别的进程对临界区上锁，则可跳出循环，进入临界区。Swap 指令优点缺点和TSL指令相同。\n\n\n硬件方法的优点\n适用于任意数目的进程，不管是单处理机还是多处理机；简单、容易验证其正确性。\n可以支持进程内有多个临界区，只需为每个临界区设立一个布尔变量。\n\n\n硬件方法的缺点\n进程等待进入临界区时要耗费处理时间，不能实现让权等待。\n从等待进程中随机选择一个进入临界区，有进程可能一直选不上，从而导致“饥饿”现象。\n\n\n\n2.3.3 互斥锁 解决临界区最简单的工具就是互斥锁（mutex lock）。一个进程在进入临界区时获得锁；在退出临界区时释放锁。函数acquire()获得锁，而函数release()释放锁。acquire()和release()是原子操作，由硬件机制完成。\n 每个互斥锁有一个布尔变量available，表示锁是否可用。如果锁是可用的，调用acquire()会成功，且锁不再可用。当一个进程试图获取不可用的锁时，会被阻塞，直到锁被释放。\n1234567acquire()&#123;    while(!available);\t\t//忙等待    avilable = false;\t\t//获得锁&#125;release()&#123;    available = true;\t\t//释放锁&#125;\n\n优点：等待期间不用切换进程上下文，多处理器系统中，若上锁的时间短，则等待代价很低\n缺点：需忙等，进程时间片用完才下处理机，违反“让权等待”；不太适用于单处理机系统，忙等的过程中不可能解锁\n\n2.3.4 信号量 信号量机制是一种功能较强的机制,可用来解决互斥与同步问题，它只能被两个标准的原语wait(S)和signal(S)访问，也可记为”P操作”和”V操作”。\n 信号量其实就是一个变量（可以是一个整数，也可以是更复杂的记录型变量），可以用一个信号量来表示系统中某种资源的数量，比如：系统中只有一台打印机，就可以设置一个初值为1的信号量。\n 原语是一种特殊的程序段，其执行只能一气呵成，不可被中断。原语是由关中断/开中断指令实现的。软件解决方案的主要问题是由“进入区的各种操作无法一气呵成”，因此如果能把进入区、退出区的操作都用“原语”实现，使这些操作能“一气呵成”就能避免问题。\n整型信号量\n用一个整数型的变量作为信号量，用来表示系统中某种资源的数量。wait(S)、signal(S)可描述为：\n\n\n与普通整数变量的区别：对信号量的操作只有三种，即初始化、P操作、V操作\n\nwait(S) 原语，“检查”和“上锁”一气呵成，避免了并发、异步导致的问题。\n存在的问题： 不满足 “让权等待” 原则，会发生 忙等。\n\n记录型信号量\n整型信号量存在“忙等”问题，因此人们又提出了“记录型信号量”，即用记录型数据结构表示的信号量。\n除了需要用于代表资源数目的整型变量value外，再增加一个进程链表L，用于链接所有等待该资源的进程。\n\n\nwait：如果剩余资源数不够使用block原语使进程从运行态进入阻塞态，并把挂到信号量S的等待队列（即阻塞队列）中。\nsignal：释放资源后，若还有别的进程在等待这种资源，则使用wakeup原语唤醒等待队列中的一个进程，该进程从阻塞态变为就绪态。\nS.value的初值表示系统中某种资源的数目。\nP操作：\n对信号量S的一次P操作意味着进程请求一个单位的该资源，因此需要执行S.value-，表示资源数减1\n当S.value&lt;0时表示该类资源已分配完毕，因此进程应调用bock原语进行自我阻塞（当前运行的进程从运行态→阻塞态），主动放弃处理机，并插入该类资源的等待队列S.L中。\n可见，该机制遵循了“让权等待”原则，不会出现“忙等”现象。\n\n\nV操作：\n对信号量S的一次V操作意味进程释放一个单位的该资源，因此需要执行S.value.+，表示资源数加1，\n若加1后仍是S.value&lt;=0，表示依然有进程在等待该类资源，因此应调用wakeup原语唤醒等待队列中的第一个进程（被唤醒进程从阻塞态→就绪态）\n\n\n\n2.3.4 操作系统之信号量机制（整型信号量、记录型信号量P、V）_llf算法-CSDN博客\n\n例：某计算机系统中有1台打印机，则可在初始化信号量 S 时将 S.value 的值设为 1，队列 S.L 设置为空。\n\n① CPU 为 P0 服务，S.value —，值为 0，P0开始使用打印机。② CPU 为 P1 服务，S.value —，值为 -1，无资源执行 block 原语( wait原语 )。阻塞队列( P1 )，S.value = -1 说明有1个进程在等待资源。③ CPU 为 P2 服务，S.value —，值为 -2，无资源执行 block 原语。阻塞队列( P1→P2 )，S.value = -2 说明有2个进程在等待资源。④ CPU 为 P0 服务，S.value ++，S.value = -1 ≤ 0，说明有进程在等待该资源。因此应调用 wakeup 原语(signal原语)唤醒等待队列中的第一个进程P1，将释放资源给 P1，P1从阻塞态变为就绪态，等待被 CPU 服务(CPU顺序执行)。阻塞队列( P2 )⑤ CPU 为 P1 服务，P1 使用完打印机，S.value ++，S.value = 0，调用 wakeup 原语唤醒 P2。阻塞队列()。⑥ CPU 为 P2 服务， P2是用完打印机，S.value ++，S.value = 1。\n\n信号量机制实现进程互斥\n\n伪代码如下所示：\n设 S 为实现进程 P1、P2 互斥的信号量，由于只允许一个进程进入临界区，所以 S 的初值应设为 1。然后把临界区置于 P(S) 和 V(S) 之间，进入区之前申请资源（P操作），退出区之前释放资源（ V操作 ），即可实现两个进程对临界资源的互斥访问。\n\n\n操作：\n\n1.分析并发进程的关键活动，划定临界区（如：对临界资源打印机的访问就应放在临界区）\n2.设置互斥信号量mutex，初值为1\n3.在进入区P（mutex）一一申请资源\n4.在退出区V（mutex）一一释放资源\n\n\n注意\n\n对不同的临界资源需要设置不同的互斥信号量。\nP、V操作必须成对出现。缺少P（mutex）就不能保证临界资源的互斥访问。缺少V（mutex）会导致资源永不被释放，等待进程永不被唤醒。\n\n\n\n信号量机制实现进程同步\n\n程序\n\n\n\n\n步骤\n\n\n信号量机制实现前驱关系\n分析问题，画出前驱图，把每一对前驱关系都看成一个同步问题\n\n问题：下图是一个前驱图，其中 S1, S2, S3, … ,S6 是进程 P1, P2, P3,…, P6 中的程序段，这些程序段要求按如下前驱图所示的顺序来执行：\n\n\n\n2.3.5 管程进程同步工具，确保每一次只有一个进程访问共享空间\n引入管程原因\n管程的引入让程序员写程序时不需要再关注复杂的PV操作，从而避免了传统信号量机制存在的很多问题。\n定义：由一组数据及定义在这组数据之上的对这组数据的操作组成的软件模块，这组操作能初始化并改变管程中的数据和同步进程。\n管程的组成\n\n局限于管程的共享数据结构说明\n对该数据结构进行操作的一组过程（函数）\n对局部于管程的共享数据设置初始值的语句\n管程的名字\n\n1234567891011121314151617181920monitor Demo&#123;//定义一个名称为&quot;Demo&quot;的管程    //定义共享数据结构，对应系统中的某种共享资源    共享数据结构 S;    //对共享数据结构初始化的语句    init_code()&#123;        S=5;\t\t//初始资源数等于5    &#125;    //过程1：申请一个资源    take_away()&#123;        对共享数据结构x的一系列处理;        s--;\t\t//可用资源-1        ...    &#125;    //过程2：归还一个资源    give_back()&#123;        对共享数据结构x的一系列处理;        s++;\t\t//可用资源+1        ...    &#125;&#125;\n管程的基本特征\n\n局部于管程的数据只能被局部于管程的过程所访问\n一个进程只有通过调用管程内的过程才能进入管程访问共享数据\n每次仅允许一个进程在管程内执行某个内部过程\n\n\n注：过程其实就是函数，如下面这个 People 类，People 是管程的名字，username 和 str 是局部于管程的共享数据结构，login 方法是该数据结构进行操作的过程。\n12345678910public class People&#123;\tprivate String username = &quot;admin&quot;; // 用户名\tprivate String str= &quot;123456&quot;; // 密码\t\tpublic void login()&#123;\t\tif(&quot;admin&quot;.equals(username) &amp;&amp; &quot;123456&quot;.equals(str))&#123;\t\t\tSystem.out.println(&quot;登录成功！&quot;);\t\t\t&#125;\t&#125;&#125;\n\n条件变量\n条件变量condition，是表示管程阻塞原因的变量。\n 通常，一个进程被阻塞的原因可以有多个，因此在管程中设置了多个条件变量。每个条件变量保存了一个等待队列，用于记录因该条件变量而阻塞的所有进程，对条件变量只能进行两种操作，即wait和signal。\n\nx.wait：当x对应的条件不满足时，正在调用管程的进程调用x.wait将自己插入x条件的等待队列，并释放管程。此时其他进程可以使用该管程。\nx.signal：x对应的条件发生了变化，则调用x.signal，唤醒一个因x条件而阻塞的进程。\n\n12345678910111213monitor Demo&#123;    共享数据结构 S;    condition x；\t\t\t\t\t//定义一个条件变量x    init_code()&#123;...&#125;    take_away()&#123;        if(S&lt;0) x.wait();\t\t\t//资源不够，在条件变量x上阻塞等待        资源足够，分配资源，做一系列处理；    &#125;    give_back()&#123;        归还资源，做一系列相应处理;\t\tif(有进程在等待）x.signal();\t//唤醒一个阻塞进程    &#125;&#125;\n\n条件变量和信号量的比较：\n相似点：条件变量的wait/signal操作类似于信号量的P/V操作，可以实现进程的阻塞/唤醒。\n不同点：条件变量是“没有值”的，仅实现了“排队等待”功能；而信号量是“有值”的，信号量的值反映了剩余资源数，而在管程中，剩余资源数用共享数据结构记录。\n\n\n\n2.3.6 经典同步问题1.生产者-消费者问题\n产生问题：当两者并发执行时可能出差错，导致预期的结果与真实的结果不相符：当执行生产者+1和消费者-1操作之后，缓冲区的值从10变为了11。\n\n\n问题描述\n系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品放入缓冲区，消费者进程每次从缓冲区中取出一个产品并使用。（注：这里的“产品”理解为某种数据）\n生产者、消费者共享一个初始为空、大小为n的缓冲区。\n\n只有缓冲区没满时，生产者才能把产品放入缓冲区，否则必须等待。\n只有缓冲区不空时，消费者才能从中取出产品，否则必须等待。\n\n缓冲区是临界资源，各进程必须互斥地访问。\n\n\n问题分析\n\n\n1.关系分析。找出题目中描述的各个进程，分析它们之间的同步、互斥关系。\n同步关系：缓冲区没满，生产者生产；缓冲区没空，消费者消费。\n互斥关系：各进程互斥访问缓冲区。\n\n2.整理思路。根据各进程的操作流程确定P、V操作的大致顺序。\n\n\n\n\n\n3.设置信号量。并根据题目条件确定信号量初值。（互斥信号量初值一般为1，同步信号量的初始值要看对应资源的初始值是多少）\n123semaphore mutex = 1; //互斥信号量，实现对缓冲区的互斥访问semaphore empty = n; //同步信号量，表示空闲缓冲区的数量semaphore full = 0;  //同步信号量，表示产品的数量，也即非空缓冲区的数量\n\n\n\n进程描述\n\n\n能否改变相邻P、V操作的顺序？\n不能，会发生死锁\n\n\n\n\n能否只设置一个同步信号量\n不能。原因在于：两个信号量 empty 和 full，其中 empty 用于制约生产者生产，full 用于制约消费者消费。如果只设置一个信号量，如 full，那么生产者会无限的生产，起不到制约作用。\n\n\n1234567891011121314151617181920mutex,full,empty:semaphoremutex :=1;full:=0;empty:=n; 生产者P：Wait(empty);Wait(mutex);Buffer(in)=nextp;in:=(in+1) mod n;Signal(mutex);Signal(full); 消费者C:Wait(full);Wait(mutex);netxc=buffer(out);out:=(out+1) mod n;Signal(mutex);Signal(empty);\n2.多生产者多消费者问题\n\n问题描述\n桌子上有一只盘子，每次只能向其中放入一个水果。爸爸专向盘子中放苹果，妈妈专向盘子中放橘子，儿子专等着吃盘子中的橘子，女儿专等着吃盘子中的苹果。只有盘子空时，爸爸或妈妈才可向盘子中放一个水果。仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出水果。\n\n问题分析\n\n1.关系分析\n同步关系：① 父亲将苹果放入盘子，女儿才能取苹果；     ② 母亲将句子放入盘子，儿子才能取橘子；     ③ 只有盘子为空，父亲或者母亲才能放水果。互斥关系：对缓冲区(盘子)的访问要互斥的进行。\n\n2.整理思路\n分析同步要以 事件 的角度分析，不要以 进程 的角度分析。\n\n3.信号量的设置\n1234semaphore mutex = 1;  //实现互斥访问盘子（缓冲区）semaphore apple = 0;  //盘子中有几个苹果semaphore orange = 0; //盘子中有几个橘子semaphore plate = 1;  //盘子中还可以放多少个水果\n\n\n\n进程描述\n实现方法\n\n\n  \n\n能否不用互斥信号量\n如果缓冲区大小为1，在任何时刻，apple、orange、plate 三个信号量中最多只有一个是1。因此在任何时刻，最多只有一个进程的P操作不会被阻塞，并顺利地进入临界区。  如果缓冲区大小大于1，数据可能存在相互覆盖的情况。如：父亲在向盘子放橘子的同时，母亲也可以往盘子里放橘子，有可能导致两个进程写入缓冲区的数据相互覆盖的情况。  因此，当缓冲区大小等于1，有可能不设置互斥变量。当缓冲区大小大于1，必须设置互斥变量。是否不用设置互斥信号量主要观察，同一时刻信号量是否最多一个1，建议设置互斥信号量。\n但需要注意的是，实现互斥的P操作一定要在实现同步的P操作之后，否则可能引起“死锁”。\n\n\n\n如果有两个盘子plate\n\n\n分析\n在分析同步问题（一前一后问题）的时候不能从单个进程行为的角度来分析，要把“一前一后”发生的事看做是两种“事件”的前后关系。\n如果从 单个进程的角度 来考虑的话，会有以下结论：\n\n① 如果盘子里装有苹果，那么一定要女儿取走苹果后父亲或母亲才能再放入水果；\n② 如果盘子里装有橘子，那么一定要儿子取走橘子后父亲或母亲才能再放入水果。\n\n这就意味着要 设置四个同步信号量 分别实现这 四个一前一后的关系，较为复杂。\n\n\n  若从 事件的角度 来考虑，我们可以把上述四对进程行为的前后关系抽象为 一对事件 的前后关系，即：盘子变空事件 → 放入水果事件。\n  \n\n总结:在生产者_消费者问题中，如果缓冲区大小为1，那么有可能不需要设置互斥信号量就可以实现互斥访问缓冲区的功能。当然，这不是绝对的，要具体问题具体分析。\n建议:在考试中如果来不及仔细分析，可以加上互斥信号量，保证各进程一定会互斥地访问缓冲区。但需要注意的是，·实现互斥的P操作一定要在实现同步的P操作之后·，否则可能引起·“死锁”·。\n\n3.读者-写者问题\n\n问题描述\n有读者和写者两组并发进程，共享一个文件，当两个或两个以上的读进程同时访问共享数据时不会产生副作用，但若某个写进程和其他进程（读进程或写进程）同时访问共享数据时则可能导致数据不一致的错误。因此要求：\n\n①允许多个读者可以同时对文件执行读操作\n②只允许一个写者往文件中写信息\n③任一写者在完成写操作之前不允许其他读者或写者工作\n④写者执行写操作前，应让已有的读者和写者全部退出\n\n\n\n\n问题分析\n\n两类进程：写进程、读进程\n互斥关系：写进程一写进程、写进程一读进程。读进程与读进程不存在互斥问题\n\n\n\n\n\n进程描述\n\n方案1\n方案设置 rw 和 mutex 两个信号量。rw 信号量 用于实现 读进程与写进程、写进程与写进程 对共享文件的互斥访问。mutex 信号量 用于保证对 count 变量的互斥访问。\n\n加mutex原因：\n比如：当count=0时，第一个读者进程执行到p(rw),rw=0,假设此时时间片到了，切换到第二个读者进程,第二个进程发现count=0,则执行p(rw)，但是此时rw=0，于是第二个进程被堵在p（rw）这里，同理，后面的可能会有多个进程堵在p(rw)，只有当第一个进程再次获得时间片，执行count++,让count不为0，然后其他进程就可以直接绕过if直接进行count++来访问文件，但是第三个读者进程和后面的几个可能堵在p(rw)的多个读者进程则必须得等count–为0后才可以再次和写进程竞争来访问文件，对count的访问没有做到一气呵成，会导致本来一些进程一直堵在p（rw）。\n\n\n\n\n若没有设置 mutex 信号量，两个读进程并发执行到 if 条件且都满足，都会执行 P(rw)，会造成其中一个读进程阻塞的情况。设置 mutex 信号量，使得 count 信号量的检查和赋值操作一气呵成，保证了对 count 信号量访问的互斥性。\n\n\n\n **方案 1 存在的问题：** 只要有读进程还在读，写进程就要一直阻塞等待，可能 “饿死”。因此，这种算法中，读进程是优先的。\n\n方案2\n方案 2 是对方案 1 问题的修正，添加了 w 信号量，保证了 读写公平 。如：假设对共享文件的访问顺序是：读者1→读者2→ 写者1 → 读者3 ，读者 2 执行完后，写者 1 将会进行写文件，读者 3 进程将会被阻塞。待写者1写完文件后，读者 3 进行读写者 1 访问后的文件。\n\n算法 核心思想 在于设置了一个 计数器 count 用来记录当前正在访问共享文件的读进程数。我们可以用 count 的值来判断当前进入的进程是否是第一个/最后一个读进程，从而做出不同的处理。另外，还需考虑 count 变量的互斥性。\n\n\n4.哲学家进餐问题\n\n问题描述\n一张圆桌上坐着5名哲学家，每两个哲学家之间的桌上摆一根筷子，桌子的中间是一碗米饭。哲学家们倾注毕生的精力用于思考和进餐，哲学家在思考时，并不影响他人。只有当哲学家饥饿时，才试图拿起左、右两根筷子（一根一根地拿起）。如果筷子已在他人手上，则需等待。饥饿的哲学家只有同时拿起两根筷子才可以开始进餐，当进餐完毕后，放下筷子继续思考。\n\n\n\n问题分析\n\n1.关系分析\n系统中有5个哲学家进程，5位哲学家与左右邻居对其中间筷子的访问是互斥关系。\n\n2.整理思路\n哲学家进餐问题中 只有互斥关系，但与之前遇到的问题不同点在于，每个哲学家进程需要同时持有两个临界资源才能开始吃饭。如何避免临界资源分配不当造成的死锁现象，是哲学家问题的关键。\n\n3.信号量的设置\n定义互斥信号量数组 chopstick[5]={1,1,1,1,1} 用于实现对 5 个筷子的互斥访问。并对哲学家按0~4编号，哲学家 i 左边的筷子编号为 i，右边的筷子编号为 (i+1)%5。此外，还需要设置 互斥信号量mutex，用以保证哲学家进程左右两支筷子都可用。\n\n\n\n\n\n进程描述\n算法保证，一个哲学家再拿到筷子拿到一半时被阻塞，也不会有别的哲学家尝试拿筷子，即至少有一个哲学家进程不阻塞。\n\n\n其他方案：① 对哲学家进程施加一些限制条件，如最多允许四个哲学家同时进餐。这样可以保证至少有一个哲学家是可以拿到左右两只筷子的。② 要求奇数号哲学家先拿左边的筷子，然后再拿右边的筷子，而偶数号哲学家刚好相反。用这种方法可以保证如果相邻的两个奇偶号哲学家都想吃饭，那么只会有其中一个可以拿起第一只筷子，另一个会直接阻塞。这就避免了占有一支后再等待另一只的情况。\n\n\n\n有n（n ≥ 3）名哲学家围坐在一张圆桌边，每名哲学家交替地就餐和思考。再圆桌中心有m(m ≥ 1)个碗，每两名哲学家之间有一根筷子。每名哲学家必须取到一个碗和两侧的筷子后，才能就餐，进餐完毕，将碗和筷子放回原位，并继续思考。为使尽可能多的哲学家同时就餐，且防止出现死锁现象，请使用信号量的P、V操作[wait()，signal()操作]描述上述过程中的互斥与同步，并说明所用信号量及初始值的含义。\n12345678910111213141516171819202122232425semaphore bowl,mutex=1；\t\t\t\tsemaphore chopsticks[n] = &#123;1&#125;;\t//n个筷子，for(int i = 0; i &lt; n; i++)\tchopsticks[i] = 1;\t\t\t//每个哲学家一侧筷子个数为1bowl = min(m,n-1);\t\t\t\t//限制访问资源的人数最多为n-1Process Philosopher()&#123;\twhile(true)\t&#123;\t\t思考\t\tP(mutex);\t\tP(bowl);\t\t\t\t//取碗\t\tP(chopsticks[i]);\t\t//左手筷子\t\tP(chopsticks[(i+1)%n])\t//右手筷子\t\t\t\t干饭\t\t\t\tV(chopsticks[i]);\t\tV(chopsticks[(i+1)%n]);\t\tV(bowl);        V(mutex);\t&#125;&#125;\n吸烟者问题\n\n问题描述\n假设一个系统有 三个抽烟者进程 和 一个供应者进程。每个抽烟者不停地卷烟并抽掉它，但是要卷起并抽掉一支烟，抽烟者需要有三种材料：烟草、纸和胶水。三个抽烟者中，第一个拥有烟草、第二个拥有纸、第三个拥有胶水。供应者进程无限地提供三种材料，供应者每次将两种材料放桌子上，拥有剩下那种材料的抽烟者卷一根烟并抽掉它，并给供应者进程一个信号告诉完成了，供应者就会放另外两种材料在桌上，这个过程一直重复（让三个抽烟者轮流地抽烟）。\n\n\n\n问题分析\n\n1.关系分析\n同步关系：① 桌上有组合一，第一个抽烟者取走东西     ② 桌上有组合二，第二个抽烟者取走东西     ③ 桌上有组合三，第三个抽烟者取走东西     ④ 抽烟者抽完发出完成信号，供应者将下一个组合放到桌上互斥关系：对缓冲区的访问要互斥的进行。\n\n2.整理思路\n\n\n注：由于缓冲区大小为1，任意时刻同步信号量和互斥信号量最多只有一个1，因此互斥信号量可以不设置。\n\n\n3.信号量的设置\n12345semaphore offer1 = 0; //桌上组合一的数量semaphore offer2 = 0; //桌上组合二的数量semaphore offer3 = 0; //桌上组合三的数量semaphore finish = 0; //抽烟是否完成int i = 0; //用于实现“三个抽烟者轮流抽烟”\n\n\n\n进程描述\n\n\n能否从进程角度思考？\n 不可以。    同多生产者多消费者问题，假设从进程角度思考，那么第一个抽烟者抽完后，供应者再将第一个组合放到桌上；第二个抽烟者抽完后，供应者再将第二个组合放到桌上；第三个抽烟者抽完后，供应者再将第三个组合放到桌上。这样相比于从事件考虑的一个一前一后的关系，多出了多个关系，并且较为复杂。因此要从事件的角度思考 PV 关系。\n\n\n2.4 死锁\n2.4.1 死锁的概念1.死锁的定义\n 在并发环境下，各进程互相等待对方手里的资源，导致各进程都阻塞，都无法向前推进的现象，就是“死锁”。\n2.死锁、饥饿、死循环的区别\n\n共同点：都是进程无法顺利向前推进的现象（故意设计的死循环除外)\n\n区别：\n\n死锁：两个及以上进程间互相等待对方手里的资源，导致各进程都阻塞，无法向前推进的现象。\n\n饥饿：可能只有单个进程由于长期得不到想要的资源，某进程无法向前推进的现象。\n\n死循环：某进程执行过程中一直跳不出某个循环的现象。是被管理者出现的问题。\n\n\n\n\n3.死锁产出的必要条件\n\n互斥条件：互斥使用的资源（要么空闲、要么被一个进程占用）才会导致死锁。像内存、扬声器这样可以同时让多个进程使用的资源是不会导致死锁。\n占有和等待条件：已得到资源的进程可再请求新的资源且占用已有的资源不放。\n不可抢占（不剥夺）条件：进程所获得的资源不可被抢占，只能主动释放。\n循环等待条件：至少由两个进程组成环路，其中每一个进程都在等待下一个进程被占用的资源。\n\n注：发生死锁时一定有循环等待，但是发生循环等待时未必死锁，即 循环等待是死锁的必要不充分条件。\n\n如果同类资源数大于1，则即使有循环等待，也未必发生死锁（如上图 Pn 可以同时请求 P1 或者 Pk 的资源，得到 Pk 资源后，不会发生死锁）。 但如果系统中每类资源都只有一个，那循环等待就是死锁的充分必要条件了。\n5.死锁处理策略\n\n死锁预防。设置某些限制条件，破坏产生死锁的4个必要条件中的一个或几个。\n避免死锁。在资源的动态分配过程中，用某种方法防止系统进入不安全状态。\n死锁的检测及解除。无须采取任何限制性措施，允许进程在运行过程中发生死锁。通过系统的检测机构及时地检测出死锁的发生，然后采取某种措施解除死锁。\n\n2.4.2 死锁预防1.破坏互斥条件\n把只能互斥使用的资源改造为允许共享使用，则系统不会进入死锁状态。如: SPOOLing技术。使用 SPOOLing 技术可以把 独占设备在逻辑上改造成共享设备。比如，用SPOOLing技术将打印机改造为共享设备…\n2.破坏不剥夺条件\n\n提供两种方案：\n申请资源得不到时，主动释放所占有资源，需要再申请。\n申请资源被其他进程占用时，由 OS 协助剥夺。\n\n\n策略的缺点：\n实现起来比较复杂；\n释放已获得的资源可能造成前一阶段工作的失效；\n反复地申请和释放资源会增加系统开销，降低系统吞吐量；\n可能导致进程饥饿。\n\n\n\n3.破坏请求和保持（占有和等待）条件\n采用 静态分配方法，即进程 在运行前一次申请完它所需要的全部资源，在它的资源未满足前，不让它投入运行。一旦投入运行后，这些资源就一直归它所有，该进程就不会再请求别的任何资源了。\n策略的缺点： 进程在整个运行期间都一直保持着所有资源，就会造成严重的资源浪费，资源利用率极低。另外，该策略也有 可能导致某些进程饥饿。\n4.破坏循环等待条件\n采用 顺序资源分配法。首先给系统中的资源编号，要求进程只能按编号递增顺序请求资源。\n原理分析： 一个进程只有已占有小编号的资源时，才有资格申请更大编号的资源。按此规则，已持有大编号资源的进程不可能逆向地回来申请小编号的资源，从而就不会产生循环等待的现象(类比拓扑排序)。\n策略的缺点： 不方便增加新的设备，因为可能需要重新分配所有的编号；进程实际使用资源的顺序可能和编号递增顺序不一致，会导致资源浪费；必须按规定次序申请资源，用户编程麻烦。\n2.4.3 死锁避免系统安全状态\n\n安全序列：是指如果系统按照这种序列分配资源，则每个进程都能顺利完成。\n安全状态：系统如果存在安全序列，则处于安全状态，安全状态一定不发生死锁。安全序列可能有多个。\n不安全状态：如果分配了资源之后，系统中找不出任何一个安全序列，系统就进入了不安全状态。可能发生死锁（处于不安全状态未必就是发生了死锁，但发生死锁时一定是在不安全状态）\n\n安全序列的计算方法：\n例子：假设系统中有三个进程p1 , p2 , p3 ，一共有12台磁带机。进程p1共需要10台，进程p2共需要4台，进程p3共需要9台。假设在某个时刻，进程p1 , p2 , p3已分别获得5台、2台和2台，还有3台没分配。\n\n\n\n\n进程\n最大需求\n已分配\n可用\n\n\n\n\np1\n10\n5\n3\n\n\np2\n4\n2\n\n\n\np3\n9\n2\n\n\n\n\n系统进入不安全状态后，便可能进入死锁状态；反之，只要系统处于安全状态，系统变可避免进入死锁。\n分配2个资源给p2，满足p 2的最大需求，待p2释放所持资源，此时空闲资源共计5个单位；\n分配5个资源给p1，满足p 1的最大需求，待p1释放所持资源，此时空闲资源共计10个单位；\n分配7个资源给p3，满足p 3的最大需求，待p3释放所持资源，此时空闲资源共计12个单位；\n最后得到安全序列p2 , p1 , p3这种情况下不会发生死锁。\n但是如果先给p3分配一个资源，此时系统进入不安全状态。之后再将剩下的两个资源分配给p2，等待p2进程运行完毕，回收其资源，此时空闲资源共有4个单位。此时4个单元无法满足p1或者p3 ，则此时无法再找到一个安全序列，陷入僵局，最终导致死锁。\n并不是所有的不安全状态都是死锁状态，但是当系统进入不安全状态的时候，就有可能出现死锁情况。反之，如果系统处于安全状态，系统便可以找到安全序列，避免死锁状态。\n银行家算法\n一句话+一张图说清楚——银行家算法-CSDN博客\n【操作系统】银行家算法的实现_银行家算法实现-CSDN博客\n核心思想： 在分配资源前，预先判断这次分配是否会导致系统进入不安全状态，以此来决定是否答应资源分配请求，从而使得系统避免死锁。\n\n数据结构描述：\n(1) 可利用资源向量 Available。这是一个含有 m 个元素的数组，其中的每一个元素代表一类可利用的资源数目，其初始值是系统中所配置的该类全部可用资源的数目，其数值随该类资源的分配和回收而动态地改变。如果 Available[j] = K，则表示系统中现Rj类资源K个。\n\n\n  (2) 最大需求矩阵Max。这是一个n x m的矩阵，它定义了系统中n个进程中的每个进程对m类资源的最大需求。如果Max[i,j] = K，则表示进程i需要Rj 类资源的最大数目为K。\n  分配矩阵 Allocation。这也是一个n x m的矩阵，它定义了系统中每一类资源当前已分配给每一进程的资源数。如果 Allocation[i,jl = K，则表示进程i当前己分得Rj类资源的数目为K。\n  (4) 需求矩阵Need.这也是一个n×m的矩阵，用以表示每一个进程尚需的各类资源数。如果Need[i,j] = K，则表示进程i还需要Rj类资源K个方能完成其任务。  上述三个矩阵间存在下述关系:  　　　　　　　　　　　　　　Need[i,j] = Max[i,j] - allocation[i, j]\n\n银行家算法描述\n设 Request；是进程Pi的请求向量，如果 Requesti[j] = K，表示进程Pi需要K个Rj类型的资源。当Pi发出资源请求后，系统按下述步骤进行检査:(1) 如果 Requesti[j] ≤ Need[i,j]便转向步骤(2)；否则认为出错，因为它所需要的资源数已超过它所宣布的最大值。\n\n\n  (2) 如果 Requesti[j] ≤ Available[j]，便转向步骤(3)；否则，表示尚无足够资源，Pi须等待。\n  (3) 系统试探着把资源分配给进程Pi，并修改下面数据结构中的数值  　　　　Available[j] = Available[j] - Requesti[j];  　　　　Allocation[i,j] = Allocation[i,j] + Requesti[j];  　　　　Need[i,j] = Need[i,j] - Requesti[j];\n  (4) 系统执行安全性算法，检查此次资源分配后系统是否处于安全状态。若安全，才正式将资源分配给进程Pi，以完成本次分配；否则，将本次的试探分配作废，恢复原来的资源分配状态，让进程Pi等待。\n  注：安全性算法是银行家算法的核心。\n\n安全性算法描述\n(1) 设置两个向量:①工作向量Work，它表示系统可提供给进程继续运行所需的各类资源数目，它含有m个元素，在执行安全算法开始时，Work = Available；② Finish:它表示系统是否有足够的资源分配给进程，使之运行完成。开始时先做 Finish[i] = false；当有足够资源分配给进程时，再令Finish[i] = true。\n\n\n  (2) 从进程集合中找到一个能满足下述条件的进程  　　　　① Finish[i] = false;  　　　　② Need[i,j] ≤ Work[j];  若找到，执行步骤(3)，否则，执行步骤(4)。\n  (3)当进程Pi获得资源后，可顺利执行，直至完成，并释放出分配给它的资源，故应执行:  　　　　Work[j] = Work[j] + Allocation[i,j];  　　　　Finish[i] = true;  　　　　go to step 2;(goto语句不推荐使用 _ )\n  (4)如果所有进程的 Finish[i] =true都满足，则表示系统处于安全状态；否则，系统处于不安全状态。\n  2.4操作系统之死锁详解(预防、避免、检测、解除)+思维导图_llf算法-CSDN博客\n王道书解法：\n\n\n2.4.4 死锁检测和解除 如果系统既不采取预防死锁的措施，也不采取避免死锁的措施，系统就很可能发生死锁。在这种情况下，系统应当提供死锁检测和解除的手段。\n资源分配图\n系统死锁可利用 资源分配图 来描述。圆代表一个进程，框代表一类资源，框中一个圆代表一类资源中的一个资源。\n\n\n两种结点：\n进程结点：对应一个进程\n资源结点：对应一类资源，一类资源可能有多个\n\n\n两种边：\n请求边：表示进程想申请几个资源（每条边代表一个）\n分配边：表示已经为进程分配了几个资源（每条边代表一个）\n\n\n\n死锁定理\n简化资源分配图可检测系统状态是否为死锁状态。简化方法如下：\n① 在资源分配图中，找出 既不阻塞又不是孤点的进程 Pi。\n\n不阻塞：表示进程申请的资源可以被满足，如 P1 进程。由于 R2 资源除分配给 P2 进程一个资源外，还剩有一个资源，因此 P1 进程申请的 R2 资源可以被满足。相反，P2 进程申请 R1 资源则不会被满足，由于 R1 资源全部被分配完。\n不是孤点：表示与该进程节点至少一个边相连。\n\n② 消去进程所有的请求边和分配边，使之成为孤点。\n重复以上步骤，若能消去图中所有的边，则称该图是可完全简化的。\n\n注：并不是系统中所有的进程都是死锁状态，用死锁检测算法 化简资源分配图后，还连着边的那些进程就是死锁进程。\n\n死锁定理：如果某时刻系统的资源分配图是不可完全简化的，那么此时系统死锁\n死锁解除\n一旦检测出死锁的发生，就应该立即解除死锁。解除死锁的主要方法有：\n\n资源剥夺法\n挂起（暂时放到外存上）某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但是 应防止被挂起的进程长时间得不到资源而饥饿。\n\n撤销进程法（或称终止进程法）\n强制撤销部分甚至全部死锁进程，并剥夺这些进程的资源。这种方式的优点是实现简单，但所付出的代价可能会很大。因为有些进程可能已经运行了很长时间，已经接近结束了，一旦被终止可谓功亏一篑，以后还得从头再来。撤销的原则可以按进程优先级和撤销进程代价的高低进行。\n\n进程回退法\n让一个或多个死锁进程回退到足以避免死锁的地步。进程回退时，自愿释放资源而非剥夺。这就要求系统要记录进程的历史信息，设置还原点。\n\n\n\n注：撤销进程法中参考的优先级，应考虑：进程优先级、已执行多长时间、还要多久能完成、进程已经使用了多少资源、进程是交互式的还是批处理式的等因素。\n\n\n三、内存管理3.1 内存管理概念3.1.1 内存管理的基本原理和要求1.内存管理的概念\n内存管理是操作系统对内存空间进行合理划分和动态分配的机制，主要功能包括：\n\n内存分配与回收：记录已分配与空闲区域，回收进程结束后的内存。\n内存扩充：使用虚拟内存技术扩展逻辑内存。\n地址转换：将逻辑地址转换为物理地址（地址重定位）。\n内存保护：保证进程间互不干扰。\n\n2.程序执行过程\n 创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，通常需要以下几个步骤：\n\n编译：将源代码翻译成目标模块（机器语言）。\n链接：将目标模块与库函数链接为完整可执行文件。\n装入：将可执行文件装入内存。\n\n\n\n 编译后，每个目标模块从 0 开始编址，称为 逻辑地址。链接程序将多个模块按相对地址连接，构成统一的从 0 开始的 逻辑地址空间。程序员只需使用逻辑地址，内存管理机制对其透明。不同进程可拥有相同的逻辑地址，因其映射到内存的不同物理位置。\n 物理地址空间 是内存中实际单元的集合，是地址转换后的最终地址。程序运行时，指令和数据需通过物理地址访问主存。装入程序通过 地址重定位 将逻辑地址转换为物理地址，动态重定位则在指令执行时完成。\n\n3.程序的链接\n\n静态链接\n在运行前链接所有目标模块与库函数，生成完整的可执行文件，之后不再拆开。\n\n\n装入时动态链接\n装入内存时链接目标模块。\n\n\n运行时动态链接\n程序运行时按需链接模块，便于模块共享和更新。\n\n\n\n4.程序的装载\n\n绝对装载\n在编译与链接后，得到的装入模块指定 直接使用了绝对地址。\n\n\n\n\n可重定位装载\n装入时将逻辑地址一次性转换为物理地址。\n\n静态重定位的特点： 在一个作业装入内存时，必须分配其要求的全部内存空间，如果没有足够的内存，就不能装入该作业。作业一旦进入内存后，在运行期间就不能再移动，也不能再申请内存空间。\n\n\n\n动态运行时装载\n地址转换推迟到指令执行时，支持程序在内存中移动和按需装载。\n\n\n\n  \n  \n  动态重定位特点： 可以将程序分配到不连续的存储区中；在程序运行前只需装入它的部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内存；便于程序段的共享，可以向用户提供一个比存储空间大得多的地址空间；采用动态重定位时允许程序在内存中发生移动。\n\n注：链接的作用是形成了完整的装入模块与逻辑地址，但逻辑地址到物理地址的转换过程是重定位，而不是装入。\n\n5.内存映像\n 一个进程的内存映像一般有几个要素：\n\n代码段：即程序的二进制代码，代码段是只读的，可以被多个进程共享。\n数据段：即程序运行时加工处理的对象，包括全局变量和静态变量。\n进程控制块（PCB）：存放在系统区。操作系统通过PCB来控制和管理进程。\n堆：用来存放动态分配的变量。通过调用malloc 函数动态地向高地址分配空间。\n栈：用来实现函数调用。从用户空间的最大地址往低地址方向增长。\n\n\n代码段和数据段在程序调入内存时就指定了大小，而堆和栈不一样。\n当调用像malloc和free这样的C标准库函数时，堆可以在运行时动态地扩展和收缩。\n用户栈在程序运行期间也可以动态地扩展和收缩，每次调用一个函数，栈就会增长；从一个函数返回时，栈就会收缩。\n\n\n 上图是一个进程在内存中的映像。\n\n其中，共享库用来存放进程用到的共享函数库代码，如printf函数等。\n在只读代码段中，.iit是程序初始化时调用的_init函数；.text是用户程序的机器代码；.rodata是只读数据。\n在读/写数据段中，.data是已初始化的全局变量和静态变量；.bss是未初始化及所有初始化为0的全局变量和静态变量。\n\n6.内存保护\n确保每个进程都有一个单独的内存空间。内存分配前，需要保护操作系统不受用户进程的影响，同时保护用户进程不受其他用户进程的影响。内存保护可采取两种方法：\n\n在CPU中设置一对上、下限寄存器，存放用户作业在主存中的下限和上限地址，每当CPU要访问一个地址时，分别和两个寄存器的值相比，判断有无越界。\n\n采用重定位寄存器（又称基地址寄存器）和界地址寄存器（又称限长寄存器）来实现这种保护。\n重定位寄存器含最小的物理地址值，界地址寄存器含逻辑地址的最大值。内存管理机构动态地将逻辑地址与界地址寄存器进行比较，若未发生地址越界，则加上重定位寄存器的值后映射成物理地址，再送交内存单元，如下图所示。\n\n\n重定位寄存器是用来“加”的，逻辑地址加上重定位寄存器中的值就能得到物理地址；\n界地址寄存器是用来“比”的，通过比较界地址寄存器中的值与逻辑地址的值来判断是否越界。\n\n 加载重定位寄存器和界地址寄存器时必须使用特权指令，只有操作系统内核才可以加载这两个存储器。这种方案允许操作系统内核修改这两个寄存器的值，而不允许用户程序修改。\n\n\n7.内存共享\n并不是所有的进程内存空间都适合共享，只有那些只读的区域才可以共享。\n可重入代码又称纯代码，是一种允许多个进程同时访问但不允许被任何进程修改的代码。\n在实际运行时，每个进程有自己的私有数据段，可以更改自己私有的数据区数据，不可改变共享的代码。\n\n例：考虑一个可以同时容纳40个用户的多用户系统，他们同时执行一个文本编辑程序，若该程序有160KB代码区和40KB数据区，则共需8000KB的内存空间来支持40个用户。如果160KB代码是可分享的纯代码，则不论是在分页系统中还是在分段系统中，整个系统只需保留一份副本即可，此时所需的内存空间仅为40KB×40+160KB=1760KB。\n对于分页系统，假设页面大小为4KB，则代码区占用40个页面、数据区占用10个页面。为实现代码共享，应在每个进程的页表中都建立40个页表项，它们都指向共享代码区的物理页号。此外，每个进程还要为自己的数据区建立10个页表项，指向私有数据区的物理页号。\n对于分段系统，由于是以段为分配单位的，不管该段有多大，都只需为该段设置一个段表项（指向共享代码段始址，以及段长160KB）。由此可见，段的共享非常简单易行。\n\n3.1.2 覆盖与交换覆盖与交换技术是在多道程序环境下用来扩充内存的两种方法。\n覆盖\n\n基本思想：\n程序运行时并非需要所有部分，内存分为固定区和覆盖区。\n经常使用的部分放入固定区，其他部分根据调用需求装入覆盖区，替换已有内容。\n\n特点：\n\n允许未装入全部程序时运行，但无法突破内存限制。\n覆盖区内容可更新，固定区内容常驻内存。\n对用户和程序员不透明。\n\n\n\n交换\n\n基本思想：将等待状态的进程移出内存（换出），将准备运行的进程调入内存（换入）。\n交换过程：例如，有一个CPU采用时间片轮转调度算法的多道程序环境。时间片到，内存管理器将刚刚执行过的进程换出，将另一进程换入刚刚释放的内存空间。同时，CPU调度器可以将时间片分配给其他已在内存中的进程。每个进程用完时间片都与另一进程交换。在理想情况下，内存管理器的交换过程速度足够快，总有进程在内存中可以执行。\n问题：\n需要足够大的外存（如磁盘）存储内存映像。\n交换时间需尽量短，保证CPU效率。\n换出进程需完全空闲，交换空间独立于文件系统。\n常用于内存紧张时，但负载降低时会暂停。\n\n\n\n交换过程\n下面是一个交换过程\n\n刚开始的时候，只有进程 A 在内存中，然后从创建进程 B 和进程 C 或者从磁盘中把它们换入内存，然后在图 d 中，A 被换出内存到磁盘中，最后 A 重新进来。因为图 g 中的进程 A 现在到了不同的位置，所以在装载过程中需要被重新定位，或者在交换程序时通过软件来执行；或者在程序执行期间通过硬件来重定位。基址寄存器和变址寄存器就适用于这种情况。\n\n交换在内存创建了多个 空闲区(hole)，内存会把所有的空闲区尽可能向下移动合并成为一个大的空闲区。这项技术称为内存紧缩(memory compaction)。但是这项技术通常不会使用，因为这项技术会消耗很多 CPU 时间。\n区别\n交换技术主要在不同进程（或作业）之间进行，而覆盖则用于同一个程序或进程中。对于主存无法存放用户程序的矛盾，现代操作系统是通过虚拟内存技术来解决的，覆盖技术则已成为历史；而交换技术在现代操作系统中仍具有较强的生命力。\n3.1.3 连续分配管理方式单一连续分配\n在单一连续分配方式中，内存被分为 系统区和用户区。系统区通常位于内存的低地址部分，用于存放操作系统相关数据；用户区用于存放用户进程相关数据。内存中只能有一道用户程序，用户程序独占整个用户区空间。\n\n优点： 实现简单；无外部碎片；可以采用覆盖技术扩充内存；无需采取内存保护，因为内存中永远只有一道程序。\n缺点：只能用于单用户、单任务的操作系统中；有内部碎片；存储器利用率极低。\n\n内部碎片：分配给某进程的内存区域中，如果有些部分没有用上。\n外部碎片：是指内存中的某些空闲分区由于太小而难以利用。\n\n固定分区分配\n固定分区分配是最简单的一种多道程序存储管理方式，它将整个用户空间划分为若干个固定大小的分区，在每个分区中只装入一道作业。当有空闲分区时，便可从外存的后备作业队列中选择适当大小的作业装入该分区，如此循环。划分分区有两种方法：\n\n分区大小相等。程序太小会造成浪费，程序太大又无法装入，缺乏灵活性。\n分区大小不等。划分为多个较小的分区、适量的中等分区和少量大分区，增加了灵活性。\n\n\n 为了便于分配，建立一张分区使用表，通常按分区大小排队，各表项包括每个分区的起始地址、大小及状态（是否已分配），如下图所示。\n\n\n分配内存时，便检索该表，以找到一个能满足要求且尚未分配的分区分配给装入程序，并将对应表项的状态置为“已分配”；若找不到这样的分区，则拒绝分配。\n回收内存时，只需将对应表项的状态置为“未分配”即可。\n\n优点： 实现简单，无外部碎片。\n缺点： 当用户程序太大时，可能所有的分区都不能满足需求，此时不得不采用覆盖技术来解决，但这又会降低性能；会产生内部碎片，内存利用率低。\n动态分区分配\n动态分区分配 又称为 可变分区分配。这种分配方式不会预先划分内存分区，而是在进程装入内存时，根据进程的大小动态地建立分区，并使分区的大小正好适合进程的需要。因此，系统分区的大小和数目是可变的。\n\n例：如图所示，系统有64MB内存空间，其中低8MB固定分配给操作系统，其余为用户可用内存。\n开始时装入前三个进程，它们分别分配到所需的空间后，内存仅剩4MB，进程4无法装入。\n在某个时刻，内存中没有一个就绪进程，CPU出现空闲，操作系统就换出进程2，换入进程4。由于进程4比进程2小，这样在主存中就产生了一个6MB的内存块。\n之后CPU又出现空闲，需要换入进程2，而主存无法容纳进程2，操作系统就换出进程1，换入进程2。\n\n\n紧凑技术：动态分区在开始时是很好的，但随着时间的推移，内存中会产生越来越多的外部碎片。需要通过紧凑技术来解决，即操作系统不时地对进程进行移动和整理。但这需要动态重定位寄存器的支持，且相对费时。\n 在进程装入或换入主存时，若内存中有多个足够大的空闲块，则操作系统必须确定分配哪个内存块给进程使用，这就是动态分区的分配策略。考虑以下几种算法：\n\n首次适应（FirstFit）算法\n\n算法思想：每次都从低地址开始查找，找到第一个能满足大小的空闲分区。\n如何实现：空闲分区以地址递增的次序链接。分配内存时，从链首开始顺序查找，找到大小能满足要求的第一个空闲分区分配给作业。\n算法简单，最好最快，回收分区后一般不需要对空闲分区队列重新排序\n\n\n最佳适应（BestFit）算法\n\n算法思想：优先使用更小的分区，以保留更多大分区。\n如何实现：空闲分区按容量递增的次序形成空闲分区链，找到第一个能满足要求且最小的空闲分区分配给作业，避免“大材小用”。\n缺点：产生大量小的、难以利用的外部碎片\n\n\n最坏适应（WorstFit）算法\n（最大适应算法）\n\n算法思想：优先使用更大的分区，以防止产生太小的不可用的碎片。\n如何实现：空闲分区以容量递减的次序链接，找到第一个能满足要求的，即最大的分区，从中分割一部分存储空间给作业。\n缺点：如果之后有“大进程”到达，无足够大连续内存空间分配。\n\n\n邻近适应（NextFit）算法\n（循环首次适应算法）\n\n算法思想：由首次适应演变而来，每次从上次查找结束位置开始查找。\n如何实现：空闲分区以地址递增的顺序排列（可排成一个循环链表）。每次分配内存时从上次查找结束的位置开始查找空闲分区链（或空闲分区表），找到大小能满足要求的第一个空闲分区。\n缺点：导致无论低地址、高地址部分的空闲分区都有相同的概率被使用，也就导致了高地址部分的大分区更可能被使用，划分为小分区，最后导致无大分区可用。\n\n\n\n\n\n\n\n算法\n算法思想\n分区排列顺序\n优点\n缺点\n\n\n\n\n首次适应\n从头到尾找适合的分区\n空闲分区以地址递增次序排列\n性能最好 算法开销小\n\n\n\n最佳适应\n优先使用更小的分区\n空闲分区以容量递增次序排列\n保留更大分区\n产生大量碎小的外部碎片；算法开销大\n\n\n最坏适应\n优先使用更大的分区\n空闲分区以容量递减次序排列\n减少难以利用的碎片\n大分区容易被用完；算法开销大\n\n\n邻近适应\n每次从上次查找结束位置开始查找\n空闲分区以地址递增次序排列（可排列成循环链表)\n空闲分区有相同概率被使用，算法开销小\n使高地址大分区也被用完\n\n\n\n\n\n\n注：动态分区分配没有内部碎片，但是有外部碎片。\n\n\n分区的分配与回收\n回收内存分区时，有可能遇到四种情况：\n\n回收区的后面有一个相邻的空闲分区。\n回收区的前面有一个相邻的空闲分区。\n回收区的前、后各有一个相邻的空闲分区。\n回收区的前、后都没有相邻的空闲分区。\n\n无论那种情况，都要遵循相邻的空闲分区要合并的原则。\n\n\n3.1.4 基本分页存储管理 固定分区会产生内部碎片，动态分区会产生外部碎片，这两种技术对内存的利用率都比较低。为了避免碎片的产出，引出了分页的思想。​ 分页的思想：把主存空间划分为大小相等且固定的块，块相对较小，作为主存的基本单位。每个进程也以块为单位进行划分，进程在执行时，以块为单位逐个申请主存中的块空间。\n 分页管理与固定分区类似，不会产生外部碎片；进程运行按块申请主存空间，只会在最后一块有内部碎片，每个进程平均只有半个块的内部碎片（页内碎片）。\n基本概念\n\n页面和页面大小\n\n进程中的块称为页或页面（Page)，\n内存中的块称为页框或页帧（Page Frame)\n外存也以同样的单位进行划分，直接称为块或盘块（Block)。\n\n进程在执行时需要为每个页面分配主存中的可用页框，这就产生了页和页框的一一对应。\n\n\n\n将内存空间分为一个个大小相等的分区，每个分区就是一个页框 。每个页框有一个编号，即 页框号，页框号 从 0 开始。\n将进程的逻辑地址空间也分为与页框大小相等的一个个部分，每个部分称为一个 页或页面。每个页面也有一个编号，即 页号，页号也是 从 0 开始。\n\n页框=页帧=内存块=物理块=物理页面\n\n为方便地址转换，页面大小应是2的整数幂。同时页面大小应该适中，\n\n页面太小会使进程的页面数过多，这样页表就会过长，占用大量内存，而且也会增加硬件地址转换的开销，降低页面换入/换出的效率；\n页面过大又会使页内碎片增多，降低内存的利用率。\n\n\n地址结构\n 地址结构决定了虚拟内存的寻址空间有多大。\n分页存储管理的 逻辑地址结构 如下所示：\n\n 地址结构包含两个部分：前一部分为页号，后一部分为页内偏移量 W。\n 在上图所示的例子中，地址长度为 32 位，其中 0 ~ 11位 为页内偏移量(或称页内地址)，即每页大小为 4KB；12~31 位为页号，进程地址空间最多允许 220 页。\n\n页表\n为了能知道进程的每个页面在内存中存放的位置，操作系统要为每个进程建立一张 页表。页表通常存在 PCB (进程控制块，在操作系统的内核地址空间)中。页表记录进程 页面 和实际存放的 内存块 之间的 映射关系。\n  \n\n\n\n一个进程对应一张页表。\n\n进程的每个页面对应一个页表项。\n\n每个页表项由页号和块号组成。\n每个页表项的长度是相同的。\n\n例：假设某系统物理内存大小为 4 GB，页面大小为 4 KB，则每个页表项至少应该为多少字节？\n\n内存块大小=页面大小=4KB=212B\n4GB的内存总共会被分为232／212=220个内存块\n内存块号的范围应该是0～220-1\n内存块号至少要用20 bit来表示\n至少要用3B来表示块号（3＊8=24 bit＞20bit）\n\n页表项在内存中是连续存放，因此页号是可以隐藏的，不占内存空间，页表项占 3 个字节。\n\n注：如果未特别强调，默认计算机按字节编址。\n\n\n\n地址转换\n分页存储特点： 虽然进程的各个页面是离散存放的，但是页面内部是连续存放的。\n页号 = 逻辑地址 / 页面长度页内偏移量 = 逻辑地址 % 页面长度\n如果要访问逻辑地址 A 的物理块，则\n\n确定逻辑地址 A 对应的页号 P\n找到 P 号页面在内存中的起始地址（需要查页表）\n确定逻辑地址 A 的页内偏移量 W\n\n\n\n基本地址变换机构\n基本地址变换机构可以借助进程的页表将逻辑地址转换为物理地址。变换机构如下图所示。\n\n通常会在系统中设置一个 页表寄存器(PTR)，存放 页表在内存中的起始地址 F 和页表长度 M。进程未执行时，页表的始址 和 页表长度 放在进程控制块(PCB)中，当进程被调度时，操作系统内核会把它们放到页表寄存器中。\n\n设页面大小为 L ，逻辑地址 A 到物理地址 E 的变换过程如下：\n\n\n计算页号P和页内偏移量W\n如果用十进制数手算，则\n\nP=A/L，W=A%L但是在计算机实际运行时，逻辑地址结构是固定不变的，因此计算机硬件可以更快地得到二进制表示的页号、页内偏移量\n\n判断页号是否越界\n比较页号P和页表长度M，若P≥M，则产生越界中断，否则继续执行。\n注意：页号是从0开始的，而页表长度至少是1，因此P=M时也会越界\n\n查页表，找页号对应的页表项，确定内存块号\n\n页表中页号P对应的页表项地址=页表起始地址F+页号P∗页表项长度取出该页表项内容b，即为内存块号。\n\n注意区分页表项长度、页表长度、页面大小的区别。\n页表长度指的是这个页表中总共有几个页表项，即总共有几个页；\n页表项长度指的是每个页表项占多大的存储空间；\n页面大小指的是一个页面占多大的存储空间\n\n\n用内存块号和偏移量得到物理地址\n计算$E=b*L+W$，用得到的物理地址E去访存。\n如果内存块号、页面偏移量是用二进制表示的，那么把二者拼接起来就是最终的物理地址了\n\n访问目标内存单元\n\n\n在分页存储管理（页式管理）的系统中，页是信息的物理单位，分页完全是系统行为，因此 页的大小由系统决定，逻辑地址在计算机的视角很好确定。所以，页式管理中地址是一维的。即，只要给出一个逻辑地址，系统就可以自动地算出页号、页内偏移量 两个部分，并不需要显式地告诉系统这个逻辑地址中，页内偏移量占多少位。\n\n\n具有快表的地址变换机构\n快表，又称联想寄存器(TLB)，是一种 访问速度比内存快很多的高速缓存器，用来存放最近访问的页表项的副本，可以加速地址变换的速度。与此对应，内存中的页表常称为慢表。\n\n\n注：TLB 不是内存；快表与 Cache(高速缓冲器) 的区别在于，块表中只有页表项的副本，而普通 Cache 中可能有其他各种数据的副本，可以把快表理解为一种特殊的 Cache。\n\n\n设某进程执行过程中要访问 (0,4) 这个逻辑地址，访问过程如下：\n\n\nCPU 给出逻辑地址，由硬件进行地址转换，将页号与快表中的所有页号进行比较。\n如果找到匹配的页号，说明要访问的页表项在快表中有副本，则直接从中取出该页对应的内存块号，再将内存块号与页内偏移量拼接形成物理地址，最后再访存。因此，若快表命中，存取数据仅一次访存。\n如果没有找到匹配的页号，则需要访问内存中的页表。找到对应页表项，得到页面存放的内存块号，再将内存块号与页内偏移量拼接形成物理地址，最后再访存。因此，若快表未命中，存取数据需两次访存。\n\n\n\n注：在找到页表项后，应同时将其存入快表，以便后面可能的再次访问。但若快表已满，则必须按照一定算法对旧的页表项进行替换（局部性原理）。\n两级页表\n两级页表的分配管理方式属于基本分页存储管理范畴，其用于解决页表项占据连续页框的问题。\n\n单级页表存在的问题\n问题一：页表必须连续存放，因此当页表很大时，需要占用很多个连续的页框。\n解决：可建立两级页表，一级页表为页目录表，二级页表离散存储。\n问题二：没有必要让整个页表常驻内存，因为进程在一段时间内可能只需要访问某几个特定的页面。\n解决：可以在需要访问页面时才把页面调入内存（虚拟存储技术）。可以在页表项中增加一个标志位，用于表示该页面是否已经调入内存\n\n两级页表的原理、地址结构\n二级页表实际上是在原有页表结构上再加上一层页表，如下图所示。\n\n 建立多级页表的目的在于建立索引，以便不用浪费主存空间去存储无用的页表项，也不用盲目地顺序式查找页表项。\n\n\n例：某系统按字节寻址，支持 30 位的逻辑地址，采用分页存储管理，页面大小为 4KB，页表项长度为 4B，试问逻辑地址的结构。\n页面大小为4KB=212B，则页内偏移量要用12位表示。\n30-12=18，则顷号用18位表示，即进程最多有218个页面，一共需要218个页表项来记录这些页面与物理块的映射关系，且页号范围是：0~218-1。\n页表项长度是4B，一个内存块（页框）最多存储4K/4=212/4=210个页表项。\n218个页表项则需要218/210=28个内存块才能存储。\n即需要专门给进程分配28=256个连续的物理块（页框）来存放它的页表。\n为避免连续占用内存块问题，可以设置28=256个二级页表，并用一级页表来记录这些二级页表，因此一级页号占8位。\n\n\n地址变换\n例： 将逻辑地址 (00000000,0000000001,111111111111) 转换为物理地址（用十进制表示）。\n\n首先，按照地址结构将逻辑地址拆分成三部分\n\n从 PCB 中读出页目录表始址，再根据一级页号查页目录表，找到下一级页表在内存中的存放位置。\n根据二级页号查二级页表，找到最终想访问的内存块号。\n结合页内偏移量得到物理地址。\n\n最终要访问的内存块号为 4，该内存块的起始地址为 4*4096 = 16384 页内偏移量为 4095。\n最终的物理地址为：16384 + 4095= 20479。\n\n两次页表，若采用“快表”，需要3次访存。\n\n第一次：访问页目录表。\n第二次：访问内存中的二级页表。\n第三次：访问目标内存单元。\n\n\n\n多级页表\n若分为两级页表后，页表依然很长，则可以采用更多级页表。并且，若采用多级页表机制，则各级页表的大小不能超过一个页面。\n\n例：某系统按字节编址，采用 40 位逻辑地址，页面大小为 4KB，页表项大小为 4B，假设采用纯页式存储，则要采用 (   ) 级页表，页内偏移量为 (   ) 位？\n\n页面大小=4KB=212B，按字节编址，因此页内偏移量为12位。\n页号=40-12=28位\n页面大小=212B，页表项大小=4B，则每个页面可存放212/4=210个页表项。\n因此，各级页表最多包含210个页表项，需要10位二进制位才能映射到210个页表项。\n因此每一级的页表对应页号应为10位。总共28位的页号至少要分为3级。\n\n此外，若未用“快表”，N 级页表机制，需要 N+1 次访问内存。\n\n\n3.1.5 基本分段式存储管理 分页管理方式是从计算机的角度考虑设计的，目的是提高内存的利用率，提升计算机的性能。分页通过硬件机制实现，对用户完全透明。\n 分段管理方式的提出则考虑了用户和程序员，以满足方便编程、信息保护和共享、动态增长及动态链接等多方面的需要。\n分段\n段式管理方式按照用户进程中的自然段划分逻辑空间。\n\n\n例如，用户进程由主程序段、两个子程序段、栈段和数据段组成，于是可以把这个用户进程划分为5段，每段从0开始编址，并分配一段连续的地址空间。\n\n段内要求连续，段间不要求连续，因此整个作业的地址空间是二维的。\n其逻辑地址由段号S与段内偏移量W两部分组成，如下图所示分段系统中的逻辑地址结构。\n\n其中，段号为16位，段内偏移量为16位，因此一个作业最多有216=65536段，最大段长为64KB。\n\n在页式系统中，逻辑地址的页号和页内偏移量对用户是透明的，但在段式系统中，段号和段内偏移量必须由用户显式提供，在高级程序设计语言中，这个工作由编译程序完成。\n\n段表\n程序分多个段，各段离散地装入内存，为了保证程序能正常运行，就必须能从物理内存中找到各个逻辑段的存放位置。为此，需为每个进程建立一张段映射表，简称段表。\n\n段表用于实现从逻辑段到物理内存区的映射。\n特点：\n\n① 每个段对应一个段表项，其中 记录了该段在内存中的起始位置（又称“基址”）和段的长度。\n② 各个段表项的长度是相同的。\n③ 由于段表项长度相同，在内存中是连续存放，因此段号可以是隐含的，不占存储空间。\n④ 段内要求连续，段间不要求连续，因此整个作业的地址空间是二维的。\n\n地址变换机构\n 分段系统的地址变换过程如图所示。为了实现进程从逻辑地址到物理地址的变换功能，在系统中设置了段表寄存器，用于存放段表始址F和段表长度M。从逻辑地址A到物理地址E之间的地址变换过程如下：\n\n\n① 根据逻辑地址得到段号，段内地址\n从逻辑地址A中取出前几位为段号S，后几位为段内偏移量W。\n\n② 判断段号是否越界\n比较段号S和段表长度M，若 $段号S≥段表长度M$ ，则产生越界中断，否则继续执行。\n\n③ 查询段表，找到对应段表项\n段表中段号S对应的 $段表项地址=段表始址F+段号S×段表项长度$ 。\n\n④ 检查段内地址是否超过段长\n取出该段表项的前几位得到段长C。若$段内偏移量W≥段长C$，则产生越界中断，否则继续执行。\n\n⑤ 计算得到物理地址\n取出段表项中该段的始址b，计算 $物理地址E=段基址b+偏移量W$ ，得到物理地址E。\n\n⑥ 访问目标内存单元\n用得到的物理地址E去访问内存。\n\n\n段的共享与保护\n\n共享\n在分段系统中，段的共享是通过两个作业的段表中相应表项指向被共享的段的同一个物理副本来实现的。\n不能修改的代码称为纯代码或可重入代码（它不属于临界资源），这样的代码和不能修改的数据可以共享，而可修改的代码和数据不能共享。\n\n保护\n分段管理的保护方法主要有两种：一种是存取控制保护，另一种是地址越界保护。\n\n存取控制保护：指在段表的每个表项中，设置“存取控制”字段，规定对该段的访问方式。\n地址越界保护：指在进行存储访问时，要检查逻辑地址是否超出了进程的地址空间。\n\n\n\n分段、分页管理的对比\n\n\n\n\n存储信息\n地址空间\n信息保护\n访存次数\n\n\n\n\n\n分页管理\n页是信息的物理单位 对用户透明 系统行为\n一维 记忆符()\n不易\n分页(单级页表)需两次访问 页表+目标内存单元\n\n\n分段管理\n段是信息的逻辑单位 对用户可见 用户需求\n二维 段名+段内地址([D]\\\n)\n容易 纯代码\n分段需两次访问 段表+目标内存单元\n\n\n\n\n\n3.1.6 段页式管理段页式管理结构\n 分页存储管理能有效地提高内存利用率，而分段存储管理能反映程序的逻辑结构并有利于段的共享和保护。将这两种存储管理方法结合起来，便形成了段页式存储管理方式。\n 段页式存储管理方式，将作业的地址空间首先被分成若干逻辑段，每段都有自己的段号，然后将每个段分成若干大小固定的页，内存空间分为大小一个个大小相等的分区。如下图所示。\n\n 在段页式系统中，作业的逻辑地址分为三部分：段号、页号和页内偏移量。如下图所示。\n\n 段号的位数决定了每个进程最多可以分几个段，页号位数决定了每个段最大有多少页，页内偏移量决定了页面大小、内存块大小是多少。\n 在一个进程中，段表只有一个，而页表可能有多个。\n\n例：如下图所示的段页式格式，\n\n\n段号16位，因此进程中最多有216=64K个段。\n页号4位，因此每个段最多有24=16页。\n页内偏移量有12位，因此每个内存块大小为212=2KB\n\n\n分段对用户是可见的，程序员编程时需要显式地给出段号、段内地址。而将各段分页对用户是不可见的。系统会根据段内地址自动划分页号和页内偏移量。因此，段页式管理的地址结构是二维的。\n地址转换\n 在进行地址变换时，首先通过段表查到页表始址，然后通过页表找到页号，最后形成物理地址。\n\n 如下图所示，进行一次访问实际需要三次访问主存，这里同样可以使用快表来加快查找速度，其关键字由段号、页号组成，值是对应的页帧号和保护码。\n\n\n根据逻辑地址得到段号、页号、页内偏移量\n\n判断段号是否越界若S≥M，则产生越界中断，否则继续执行\n\n查询段表找到对应的段表项，段表项的存放地址为\n\nF+S×段表顶长度\n检查页号是香越界，若页号≥页表长度，则发生越界中断，否则继续执行\n\n根据页表存放块号、页号查询页表找到对应页表项\n\n根据内存块号页内偏移量得到最终的物理地址\n\n访问目标内存单元\n\n\n3.1.7 空闲内存管理在进行内存动态分配时，操作系统必须对其进行管理。大致上说，有两种监控内存使用的方式\n\n位图(bitmap)\n空闲列表(free lists)\n\n使用位图的存储管理\n使用位图方法时，内存可能被划分为小到几个字或大到几千字节的分配单元。每个分配单元对应于位图中的一位，0 表示空闲， 1 表示占用（或者相反）。一块内存区域和其对应的位图如下\n\n位图提供了一种简单的方法在固定大小的内存中跟踪内存的使用情况，因为位图的大小取决于内存和分配单元的大小。这种方法有一个问题是，当决定为把具有 k 个分配单元的进程放入内存时，内容管理器(memory manager) 必须搜索位图，在位图中找出能够运行 k 个连续 0 位的串。在位图中找出制定长度的连续 0 串是一个很耗时的操作，这是位图的缺点。（可以简单理解为在杂乱无章的数组中，找出具有一大长串空闲的数组单元）\n使用链表进行管理\n另一种记录内存使用情况的方法是，维护一个记录已分配内存段和空闲内存段的链表，段会包含进程或者是两个进程的空闲区域。可用上面的图 c 来表示内存的使用情况。链表中的每一项都可以代表一个 空闲区(H) 或者是进程(P)的起始标志，长度和下一个链表项的位置。\n\n当按照地址顺序在链表中存放进程和空闲区时，有几种算法可以为创建的进程（或者从磁盘中换入的进程）分配内存。我们先假设内存管理器知道应该分配多少内存，最简单的算法是使用 首次适配(first fit)。内存管理器会沿着段列表进行扫描，直到找个一个足够大的空闲区为止。 除非空闲区大小和要分配的空间大小一样，否则将空闲区分为两部分，一部分供进程使用；一部分生成新的空闲区。首次适配算法是一种速度很快的算法，因为它会尽可能的搜索链表。\n首次适配的一个小的变体是 下次适配(next fit)。它和首次匹配的工作方式相同，只有一个不同之处那就是下次适配在每次找到合适的空闲区时就会记录当时的位置，以便下次寻找空闲区时从上次结束的地方开始搜索，而不是像首次匹配算法那样每次都会从头开始搜索。\n另外一个著名的并且广泛使用的算法是 最佳适配(best fit)。最佳适配会从头到尾寻找整个链表，找出能够容纳进程的最小空闲区。\n3.2 虚拟内存管理3.2.1 虚拟内存的基本概念传统存储管理方式的特征\n\n传统存储管理方式\n连续分配\n单一连续分配\n固定分区分配\n动态分区分配\n\n\n非连续分配\n基本分页存储管理\n基本分段存储管理\n基本段页式存储管理\n\n\n\n\n\n特征：\n\n一次性\n作业必须一次性全部装入内存后，才能开始运行\n这会导致两种情况：\n\n当作业很大而不能全部被装入内存时，将使该作业无法运行；\n当大量作业要求运行时，由于内存不足以容纳所有作业，只能使少数作业先运行，导致多道程序度的下降。\n\n\n驻留性：作业被装入内存后，就一直驻留在内存中，其任何部分都不会被换出，直至作业运行结束。运行中的进程会因等待IO而被阻塞，可能处于长期等待状态。\n\n\n由以上分析可知，许多在程序运行中不用或暂时不用的程序（数据）占据了大量的内存空间，而一些需要运行的作业又无法装入运行，显然浪费了宝贵的内存资源。\n局部性原理\n快表、页高速缓存及虚拟内存技术都属于高速缓存技术，这个技术所依赖的原理就是局部性原理。\n\n时间局部性。\n程序中的某条指令一且执行，不久后该指令可能再次执行；某数据被访问过，不久后该数据可能再次被访问。产生的原因是程序中存在着大量的循环操作。\n\n空间局部性。\n一旦程序访问了某个存储单元，在不久后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式聚存储的。\n\n\n虚拟存储器的定义和特征\n尽管基址寄存器和变址寄存器用来创建地址空间的抽象，但是这有一个其他的问题需要解决：管理软件的不断增大(managing bloatware)。虚拟内存的基本思想是，每个程序都有自己的地址空间，这个地址空间被划分为多个称为页面(page)的块。每一页都是连续的地址范围。这些页被映射到物理内存，但并不是所有的页都必须在内存中才能运行程序。当程序引用到一部分在物理内存中的地址空间时，硬件会立刻执行必要的映射。当程序引用到一部分不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令。\n程序不需全部装入即可运行，运行时根据需要动态调入数据，若内存不够，还需换出一些数据。系统好像为用户提供了一个比实际内存容量大得多的存储器，称为虚拟存储器。\n\n多次性：无需在作业运行时一次性全部装入内存，而是允许被分成多次调入内存。\n对换性：无需在作业运行时一直常驻内存，而是允许在作业运行过程中，将作业换入、换出。\n虚拟性：从逻辑上扩充了内存的容量，使用户看到的内存容量，远大于实际的容量。\n\n虚拟内存技术的实现\n虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。\n访问的信息不在内存时，由操作系统负责将所需信息从外存调入内存（请求调页功能）\n内存空间不够时，将内存中暂时用不到的信息换出到外存（页面置换功能）\n\n虚拟内存的实现方式\n\n请求分页存储管理\n请求分段存储管理\n请求段页式存储管理\n\n\n所需要的\n硬件支持\n\n一定容量的内存和外存。\n页表机制（或段表机制），作为主要的数据结构。\n中断机构，当用户程序要访问的部分尚未调入内存时，则产生中断。\n地址变换机构，逻辑地址到物理地址的变换。\n\n\n\n分页\n大部分使用虚拟内存的系统中都会使用一种 分页(paging) 技术。在任何一台计算机上，程序会引用使用一组内存地址。当程序执行\n1MOV REG,1000\n这条指令时，它会把内存地址为 1000 的内存单元的内容复制到 REG 中（或者相反，这取决于计算机）。地址可以通过索引、基址寄存器、段寄存器或其他方式产生。\n这些程序生成的地址被称为 虚拟地址(virtual addresses) 并形成虚拟地址空间(virtual address space)，在没有虚拟内存的计算机上，系统直接将虚拟地址送到内存中线上，读写操作都使用同样地址的物理内存。在使用虚拟内存时，虚拟地址不会直接发送到内存总线上。相反，会使用 MMU(Memory Management Unit) 内存管理单元把虚拟地址映射为物理内存地址，像下图这样\n\n下面这幅图展示了这种映射是如何工作的\n\n页表给出虚拟地址与物理内存地址之间的映射关系。每一页起始于 4096 的倍数位置，结束于 4095 的位置，所以 4K 到 8K 实际为 4096 - 8191 ，8K - 12K 就是 8192 - 12287\n在这个例子中，我们可能有一个 16 位地址的计算机，地址从 0 - 64 K - 1，这些是虚拟地址。然而只有 32 KB 的物理地址。所以虽然可以编写 64 KB 的程序，但是程序无法全部调入内存运行，在磁盘上必须有一个最多 64 KB 的程序核心映像的完整副本，以保证程序片段在需要时被调入内存。\n页表\n虚拟页号可作为页表的索引用来找到虚拟页中的内容。由页表项可以找到页框号（如果有的话）。然后把页框号拼接到偏移量的高位端，以替换掉虚拟页号，形成物理地址。\n\n因此，页表的目的是把虚拟页映射到页框中。从数学上说，页表是一个函数，它的参数是虚拟页号，结果是物理页框号。\n\n通过这个函数可以把虚拟地址中的虚拟页转换为页框，从而形成物理地址。\n页表项的结构\n下面我们探讨一下页表项的具体结构，上面你知道了页表项的大致构成，是由页框号和在/不在位构成的，现在我们来具体探讨一下页表项的构成\n\n页表项的结构是与机器相关的，但是不同机器上的页表项大致相同。上面是一个页表项的构成，不同计算机的页表项可能不同，但是一般来说都是 32 位的。页表项中最重要的字段就是页框号(Page frame number)。毕竟，页表到页框最重要的一步操作就是要把此值映射过去。下一个比较重要的就是在/不在位，如果此位上的值是 1，那么页表项是有效的并且能够被使用。如果此值是 0 的话，则表示该页表项对应的虚拟页面不在内存中，访问该页面会引起一个缺页异常(page fault)。\n保护位(Protection) 告诉我们哪一种访问是允许的，啥意思呢？最简单的表示形式是这个域只有一位，0 表示可读可写，1 表示的是只读。\n修改位(Modified) 和 访问位(Referenced) 会跟踪页面的使用情况。当一个页面被写入时，硬件会自动的设置修改位。修改位在页面重新分配页框时很有用。如果一个页面已经被修改过（即它是 脏 的），则必须把它写回磁盘。如果一个页面没有被修改过（即它是 干净的），那么重新分配时这个页框会被直接丢弃，因为磁盘上的副本仍然是有效的。这个位有时也叫做 脏位(dirty bit)，因为它反映了页面的状态。\n访问位(Referenced) 在页面被访问时被设置，不管是读还是写。这个值能够帮助操作系统在发生缺页中断时选择要淘汰的页。不再使用的页要比正在使用的页更适合被淘汰。这个位在后面要讨论的页面置换算法中作用很大。\n最后一位用于禁止该页面被高速缓存，这个功能对于映射到设备寄存器还是内存中起到了关键作用。通过这一位可以禁用高速缓存。具有独立的 I/O 空间而不是用内存映射 I/O 的机器来说，并不需要这一位。\n3.2.2 请求分页管理方式请求分页系统建立在基本分页系统基础之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。\n页表机制\n 请求分页系统在一个作业运行之前不要求全部一次性调入内存，因此在作业的运行过程中，必然会出现要访问的页面不在内存中的情况。因此在请求页表项中增加了 4个字段，如下图所示。\n\n\n状态位P：用于指示该页是否已调入内存，供程序访问时参考。\n访问字段A：用于记录本页在一段时间内被访问的次数，或记录本页最近已有多长时间未被访问，供置换算法换出页面时参考。\n修改位M：标识该页在调入内存后是否被修改过，以确定页面置换时是否写回外存。\n外存地址：用于指出该页在外存上的地址，通常是物理块号，供调入该页时参考。\n\n缺页中断机构\n 在请求分页系统中，每当所要访问的页面不在内存中时，便产生一个缺页中断，请求操作系统将所缺的页调入内存。\n\n缺页中断执行过程\n先将缺页的进程阻塞（调页完成唤醒)，\n若内存中有空闲块，则分配一个块，将要调入的页装入该块，并修改页表中的相应页表项，\n若此时内存中没有空闲块，则要淘汰某页（若被淘汰页在内存期间被修改过，则要将其写回外存）。\n\n\n缺页中断和一般中断的区别：\n在指令执行期间而非一条指令执行完后产生和处理中断信号，属于内部异常。\n一条指令在执行期间，可能产生多次缺页中断。\n\n\n\n地址变换机构\n 请求分页系统中的地址变换机构，是在分页系统地址变换机构的基础上，为实现虚拟内存，又增加了某些功能而形成的，如产生和处理缺页中断，及从内存中换出一页的功能等等。\n\n新增步骤1：请求调页（查到页表项时进行判断）\n新增步骤2：页面置换（需要调入页面，但没有空闲内存块时进行)\n新增步骤3：需要修改请求页表中新增的表项\n\n请求分页管理的地址变换过程，如下图所示，红框部分为新增步骤：\n\n\n只有“写指令”才需要修改“修改位”。并且，一般来说只需修改快表中的数据，只有要将快表项删除时才需要写回内存中的慢表。这样可以减少访存次数。\n和普通的中断处理一样，缺页中断处理依然需要保留CPU现场。\n需要用某种“页面置换算法”来决定一个换出页面（下节内容）\n换入/换出页面都需要启动慢速的I/O操作，可见，如果换入换出太频繁，会有很大的开销。\n页面调入内存后，需要修改慢表，同时也需要将表项复制到快表中。\n\n3.2.3 页框分配驻留集大小\n给一个进程分配的物理页框的集合就是这个进程的驻留集。\n\n分配给一个进程的页框越少，驻留在主存中的进程就越多，从而可提高CPU的利用率。\n若一个进程在主存中的页面过少，则尽管有局部性原理，缺页率仍相对较高。\n若分配的页框过多，则由于局部性原理，对该进程的缺页率没有太明显的影响。\n\n内存分配策略\n 在请求分页系统中，可采取两种内存分配策略，即固定和可变分配策略。在进行置换时，也可采取两种策略，即全局置换和局部置换。\n\n固定分配：操作系统为每个进程分配一组固定数目的物理块，在进程运行期间不再改变。即，驻留集大小不变\n可变分配：先为每个进程分配一定数目的物理块，在进程运行期间，可根据情况做适当的增加或减少。即驻留集大小可变\n局部置换：发生缺页时只能选进程自己的物理块进行置换。\n全局置换：可以将操作系统保留的空闲物理块分配给缺页进程，也可以将别的进程持有的物理块置换到外存，再分配给缺页进程。\n\n\n固定分配VS可变分配：区别在于进程运行期间驻留集大小是否可变\n局部置换VS全局置换：区别在于发生缺页时是否只能从进程自己的页面中选择一个换出\n\n三种组合方案：\n\n固定分配局部置换\n它为每个进程分配一定数目的物理块，在整个运行期间都不改变。\n若进程在运行中发生缺页，则只能从该进程在内存中的页面中选出一页换出，然后再调入需要的页面。\n\n可变分配全局置换\n为系统中的每个进程分配一定数目的物理块，操作系统自身也保持一个空闲物理块队列。\n当某进程发生缺页时，系统从空闲物理块队列中取出物理块分配给该进程，井将欲调入的页装入其中。\n\n可变分配局部置换\n它为每个进程分配一定数目的物理块，当某进程发生缺页时，只允许从该进程在内存的页面中选出一页换出动态变换，频繁缺页，分配物理块，缺页率低，减少物理块\n\n\n物理块调入算法\n采用固定分配策略时，将系统中的空闲物理块分配给各个进程，可采用下述几种算法。\n\n平均分配算法，将系统中所有可供分配的物理块平均分配给各个进程。\n按比例分配算法，根据进程的大小按比例分配物理块。\n优先权分配算法，为重要和紧迫的进程分配较多的物理块。通常采取的方法是把所有可分配的物理块分成两部分：一部分按比例分配给各个进程；一部分则根据优先权分配。\n\n调入页面的时机\n为确定系统将进程运行时所缺的页面调入内存的时机，可采取以下两种调页策略：\n\n预调页策略：将预计在不久后便会被访问的页面预先调入内存；主要用于进程的首次调入，由程序员指出应先调入哪些页。\n请求调页策略：进程在运行中需要访问的页面不再内存而提出请求，由系统将所需页面调入内存。每次仅调入一页，增加了磁盘I/O开销。\n\n从何处调入页面\n 请求分页系统中的外存分为两部分：用于存放文件的文件区和用于存放对换页面的对换区。\n 对换区采用连续分配方式，而文件区采用离散分配方式，因此对换区的磁盘IO速度比文件区的更快。这样，当发生缺页请求时，系统从何处将缺页调入内存就分为三种情况：\n\n系统拥有足够的对换区空间\n可以全部从对换区调入所需页面，以提高调页速度。为此，在进程运行前，需将与该进程有关的文件从文件区复制到对换区。\n\n系统缺少足够的对换区空间\n凡是不会被修改的文件都直接从文件区调入；而当换出这些页面时，由于它们未被修改而不必再将它们换出。但对于那些可能被修改的部分，在将它们换出时须调到对换区，以后需要时再从对换区调入（因为读比写的速度快）。\n\nUNIX方式\n运行之前进程有关的数据全部放在文件区，故未使用过的页面，都可从文件区调入。若被使用过的页面需要换出，则写回对换区，下次需要时从对换区调入。进程请求的共享页面若被其他进程调入内存，则无须再从对换区调入。\n\n\n如何调入页面\n\n当进程所访问的页面不在内存中时（存在位为0)，便向CPU发出缺页中断，中断响应后便转入缺页中断处理程序。\n\n该程序通过查找页表得到该页的物理块，此时如果内存未满，则启动磁盘I/O，将所缺页调入内存，并修改页表。\n\n如果内存已满，则先按某种置换算法从内存中选出一页准备换出；\n\n如果该页未被修改过（修改位为0)，则无须将该页写回磁盘；\n如果该页已被修改（修改位为1)，则必须将该页写回磁盘，\n\n然后将所缺页调入内存，并修改页表中的相应表项，置其存在位为1。\n\n调入完成后，进程就可利用修改后的页表形成所要访问数据的内存地址。\n\n\n3.2.4 页面置换算法 进程运行时，若其访问的页面不在内存中而需将其调入，但内存已无空闲空间时，就需要从内存中调出一页程序或数据，送入磁盘的对换区，选择调出页面的算法就称为页面置换算法。\n最佳置换算法（OPT）\n 选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面，这样可以保证获得最低的缺页率。\n\n最优的页面置换算法的工作流程如下：在缺页中断发生时，这些页面之一将在下一条指令（包含该指令的页面）上被引用。其他页面则可能要到 10、100 或者 1000 条指令后才会被访问。每个页面都可以用在该页首次被访问前所要执行的指令数作为标记。\n最优化的页面算法表明应该标记最大的页面。如果一个页面在 800 万条指令内不会被使用，另外一个页面在 600 万条指令内不会被使用，则置换前一个页面，从而把需要调入这个页面而发生的缺页中断推迟。计算机也像人类一样，会把不愿意做的事情尽可能的往后拖。\n这个算法最大的问题时无法实现。当缺页中断发生时，操作系统无法知道各个页面的下一次将在什么时候被访问。这种算法在实际过程中根本不会使用。\n先进先出置换算法（FIFO，First-In,First-Out）\n优先淘汰最早进入内存的页面，即在内存中驻留时间最久的页面。\n\n该算法实现简单，只需把调入内存的页面根据先后次序链接成队列，设置一个指针总指向最早的页面。\n\nBelady异常一一当为进程分配的物理块数增大时，缺页次数不减反增的异常现象。\n只有FIFO算法回产生Belady异常，算法性能差。\n该算法与进程实际运行时的规律不适应，因为在进程中，有的页面经常被访问。\n\n\n二次机会页面置换算法\n我们上面学到的 FIFO 链表页面有个缺陷，那就是出链和入链并不会进行 check 检查，这样就会容易把经常使用的页面置换出去，为了避免这一问题，我们对该算法做一个简单的修改：我们检查最老页面的 R 位，如果是 0 ，那么这个页面就是最老的而且没有被使用，那么这个页面就会被立刻换出。如果 R 位是 1，那么就清除此位，此页面会被放在链表的尾部，修改它的装入时间就像刚放进来的一样。然后继续搜索。\n这种算法叫做 第二次机会(second chance)算法，就像下面这样，我们看到页面 A 到 H 保留在链表中，并按到达内存的时间排序。\n\na）按照先进先出的方法排列的页面；b）在时刻 20 处发生缺页异常中断并且 A 的 R 位已经设置时的页面链表。\n假设缺页异常发生在时刻 20 处，这时最老的页面是 A ，它是在 0 时刻到达的。如果 A 的 R 位是 0，那么它将被淘汰出内存，或者把它写回磁盘（如果它已经被修改过），或者只是简单的放弃（如果它是未被修改过）。另一方面，如果它的 R 位已经设置了，则将 A 放到链表的尾部并且重新设置装入时间为当前时刻（20 处），然后清除 R 位。然后从 B 页面开始继续搜索合适的页面。\n寻找第二次机会的是在最近的时钟间隔中未被访问过的页面。如果所有的页面都被访问过，该算法就会被简化为单纯的 FIFO 算法。具体来说，假设图 a 中所有页面都设置了 R 位。操作系统将页面依次移到链表末尾，每次都在添加到末尾时清除 R 位。最后，算法又会回到页面 A，此时的 R 位已经被清除，那么页面 A 就会被执行出链处理，因此算法能够正常结束。\n最近最久未使用置换算法（LRU，Least Recently Used）\n 选择最近最长时间未访问过的页面予以淘汰，它认为过去一段时间内未访问过的页面，在最近的将来可能也不会被访问。\n\n 该算法为每个页面设置一个访问字段，来记录页面自上次被访问以来所经历的时间，淘汰页面时选择现有页面中值最大的予以淘汰。\n 该算法的实现需要专门的硬件支持，虽然算法性能好，但是实现困难，开销大。\nNFU(Not Frequently Used，最不常用)\n算法。它需要一个软件计数器来和每个页面关联，初始化的时候是 0 。在每个时钟中断时，操作系统会浏览内存中的所有页，会将每个页面的 R 位（0 或 1）加到它的计数器上。这个计数器大体上跟踪了各个页面访问的频繁程度。当缺页异常出现时，则置换计数器值最小的页面。\n只需要对 NFU 做一个简单的修改就可以让它模拟 LRU，这个修改有两个步骤\n\n首先，在 R 位被添加进来之前先把计数器右移一位；\n第二步，R 位被添加到最左边的位而不是最右边的位。\n\n修改以后的算法称为 老化(aging) 算法，下图解释了老化算法是如何工作的。\n\n我们假设在第一个时钟周期内页面 0 - 5 的 R 位依次是 1，0，1，0，1，1，（也就是页面 0 是 1，页面 1 是 0，页面 2 是 1 这样类推）。也就是说，在 0 个时钟周期到 1 个时钟周期之间，0，2，4，5 都被引用了，从而把它们的 R 位设置为 1，剩下的设置为 0 。在相关的六个计数器被右移之后 R 位被添加到 左侧 ，就像上图中的 a。剩下的四列显示了接下来的四个时钟周期内的六个计数器变化。\n\nCPU正在以某个频率前进，该频率的周期称为时钟滴答或时钟周期。一个 100Mhz 的处理器每秒将接收100,000,000个时钟滴答。\n\n当缺页异常出现时，将置换（就是移除）计数器值最小的页面。如果一个页面在前面 4 个时钟周期内都没有被访问过，那么它的计数器应该会有四个连续的 0 ，因此它的值肯定要比前面 3 个时钟周期内都没有被访问过的页面的计数器小。\n这个算法与 LRU 算法有两个重要的区别：看一下上图中的 e，第三列和第五列\n\n最近未使用页面置换算法（NRU）\n为了能够让操作系统收集页面使用信息，大部分使用虚拟地址的计算机都有两个状态位，R 和 M，来和每个页面进行关联。每当引用页面（读入或写入）时都设置 R，写入（即修改）页面时设置 M，这些位包含在每个页表项中，就像下面所示\n\n因为每次访问时都会更新这些位，因此由硬件来设置它们非常重要。一旦某个位被设置为 1，就会一直保持 1 直到操作系统下次来修改此位。\n如果硬件没有这些位，那么可以使用操作系统的缺页中断和时钟中断机制来进行模拟。当启动一个进程时，将其所有的页面都标记为不在内存；一旦访问任何一个页面就会引发一次缺页中断，此时操作系统就可以设置 R 位(在它的内部表中)，修改页表项使其指向正确的页面，并设置为 READ ONLY 模式，然后重新启动引起缺页中断的指令。如果页面随后被修改，就会发生另一个缺页异常。从而允许操作系统设置 M 位并把页面的模式设置为 READ/WRITE。\n可以用 R 位和 M 位来构造一个简单的页面置换算法：当启动一个进程时，操作系统将其所有页面的两个位都设置为 0。R 位定期的被清零（在每个时钟中断）。用来将最近未引用的页面和已引用的页面分开。\n当出现缺页中断后，操作系统会检查所有的页面，并根据它们的 R 位和 M 位将当前值分为四类：\n\n第 0 类：没有引用 R，没有修改 M\n第 1 类：没有引用 R，已修改 M\n第 2 类：引用 R ，没有修改 M\n第 3 类：已被访问 R，已被修改 M\n\n尽管看起来好像无法实现第一类页面，但是当第三类页面的 R 位被时钟中断清除时，它们就会发生。时钟中断不会清除 M 位，因为需要这个信息才能知道是否写回磁盘中。清除 R 但不清除 M 会导致出现一类页面。\nNRU(Not Recently Used) 算法从编号最小的非空类中随机删除一个页面。此算法隐含的思想是，在一个时钟内（约 20 ms）淘汰一个已修改但是没有被访问的页面要比一个大量引用的未修改页面好，NRU 的主要优点是易于理解并且能够有效的实现。\n时钟置换算法（CLOCK）/最近未用算法（NRU）\n简单的CLOCK算法实现方法：\n\n为每个页面设置一个访问位，再将内存中的页面都通过链接指针链接成一个循环队列。\n当某页被访问时，其访问位置为1。\n当需要淘汰一个页面时，只需检查页的访问位。如果是0，就选择该页换出；如果是1，则将它置为0，暂不换出，继续检查下一个页面，\n若第一轮扫描中所有页面都是1，则将这些页面的访问位依次置为0后，再进行第二轮扫描（第二轮扫描中一定会有访问位为0的页面，因此简单的CLOCK算法选择一个淘汰页面最多会经过两轮扫描）\n\n\n改进型的时钟置换算法\n\n简单时钟问题：简单的时钟置换算法仅考虑到一个页面最近是否被访问过。事实上，如果被淘汰的页面没有被修改过，就不需要执行I/O操作写回外存。只有被淘汰的页面被修改过时，才需要写回外存。\n因此，除了考虑一个页面最近有没有被访问过之外，操作系统还应考虑页面有没有被修改过。在其他条件都相同时，应优先淘汰没有修改过的页面，避免I/O操作。这就是改进型的时钟置换算法的思想。\n修改位=0，表示页面没有被修改过：修改位=1，表示页面被修改过。\n\n算法规则：将所有可能被置换的页面排成一个循环队列，用（访问位R，修改位M）表示各页面状态。\n\n替换帧优先级：\n\n1类R=0，M=0：最近未被访问且未被修改，是最佳淘汰页。\n2类R=0，M=1：最近未被访问，但已被修改，不是很好的淘汰页。\n3类R=1，M=0：最近已被访问，但未被修改，可能再被访问。\n4类R=1，M=1：最近已被访问且已被修改，可能再被访问。\n\n\n\n第一轮：第一优先级——最近设访问，且没修改的页面\n从当前位置开始扫描到第一个(0，0)的帧用于替换。本轮扫描不修改任何标志位\n\n第二轮：第二优先级——最近没访问，但修改过的页面\n若第一轮扫描失败，则重新扫描，查找第一个（0，1）的帧用于替换。本轮将所有扫描过的帧访问位设为0\n\n第三轮：第三优先级——最近访问过，但没修改的页面\n若第二轮扫描失败，则重新扫描，查找第一个（0，0）的帧用于替换。本轮扫描不修改任何标志位\n\n第四轮：第四优先级——最近访问过，且修改过的页面\n若第三轮扫描失败，则重新扫描，查找第一个（0，1）的帧用于替换。\n\n\n由于第二轮己将所有帧的访问位设为0，因此经过第三轮、第四轮扫描定会有一个帧被选中，因此改进型CLOCK置换算法选择一个淘汰页面最多会进行四轮扫描。\n\n性能：算法开销较小，性能也不错\n\n\n3.2.5 抖动和工作集抖动\n\n定义：抖动，又称颠簸，指在页面置换过程中，刚刚换出的页面马上又要换入主存，刚刚换入的页面马上又要换出主存。\n抖动发生的原因：系统中同时运行的进程太多，由此分配给每个进程的物理块太少，不能满足进程正常运行的基本要求，致使每个进程在运行时频繁地出现缺页，必须请求系统将所缺页面调入内存。\n抖动的危害：\n使得在系统中排队等待页面调入/调出的进程数目增加。\n对磁盘的有效访问时间也随之急剧增加，造成每个进程的大部分时间都用于页面的换入/换出，而几乎不能再去做任何有效的工作，\n进而导致发生处理机的利用率急剧下降并趋于零的情况。\n\n\n\n工作集\n由于抖动的发生与系统为进程分配物理块的多少有关，于是又提出了关于进程工作集的概念。\n工作集是指在某段时间间隔内，进程要访问的页面集合。\n 基于局部性原理，可以用最近访问过的页面来确定工作集。一般来说，工作集$W$可由时间$t$和工作集窗口大小$Δ$来确定。例如，某进程对页面的访问次序如下：\n\n 假设系统为该进程设定的工作集窗口大小$Δ$为5，则在$t_1$时刻，进程的工作集为{2,3,5}，在$t_2$时刻，进程的工作集为{1,2,3,4}。\n 工作集大小一般会比窗口小很多，工作集反映了进程在接下来的一段时间内很有可能会频繁访问的页面集合，因此，若分配给进程的物理块小于工作集大小，则该进程就很有可能频繁缺页。\n 一般来说分配给进程的物理块数（即驻留集大小）要大于工作集大小。\n3.2.6 内存映射文件 内存映射文件（Memory-MappedFiles）与虚拟内存有些相似，将磁盘文件的全部或部分内容与进程虚拟地址空间的某个区域建立映射关系，便可以直接访问被映射的文件，而不必执行文件 I/O 操作，也无须对文件内容进行缓存处理。这种特性非常适合用来管理大尺寸文件。\n特性\n\n进程可使用系统调用，请求操作系统将文件映射到进程的虚拟地址空间\n以访问内存的方式读写文件\n进程关闭文件时，操作系统负责将文件数据写回磁盘，并解除内存映射\n多个进程可以映射同一个文件，方便共享\n\n\n优点\n\n程序员编程更简单，已建立映射的文件，只需按访问内存的方式读写即可\n文件数据的读入/写出完全由操作系统负责，I\\O效率可以由操作系统负责优化\n\n3.2.7 虚拟存储器性能影响因素页面大小\n根据局部性原理，页面较大则缺页率较低，页面较小则缺页率较高。\n\n页面较小时，一方面减少了内存碎片，有利于提高内存利用率；另一方面，也会使每个进程要求较多的页面，导致页表过长，占用大量内存。\n页面较大时，虽然可以减少页表长度，但会使页内碎片增大。\n\n分配给进程的物理块\n分配给进程的物理块数越多，缺页率就越低，但是当物理块超过某个数目时，再为进程增加一个物理块对缺页率的改善是不明显的。\n页面置换算法\n好的页面置换算法可使进程在运行过程中具有较低的缺页率。\n选择LRU、CLOCK等置换算法，将未来有可能访问的页面尽量保留在内存中，从而提高页面的访问速度。\n写回磁盘的频率\n换出已修改过的页面时，应当写回磁盘，如果每当一个页面被换出时就将它写回磁盘，那么每换出一个页面就需要启动一次磁盘，效率极低。\n建立一个已修改换出页面的链表，对每个要被换出的页面（已修改），可以暂不将它们写回磁盘，而将它们挂在该链表上，仅当被换出页面数达到给定值时，才将其写回磁盘。\n局部化程度\n编写程序的局部化程度越高，执行时的缺页率就越低。如果存储采用的是按行存储，访问时就要尽量采用相同的访问方式，避免按列访问造成缺页率过高的现象。\n3.2.8 地址翻译设某系统满足以下条件：\n\n有一个TLB与一个data Cache\n存储器以字节为编址单位\n虚拟地址14位\n物理地址12位\n页面大小为64B\nTLB为四路组相联，共有16个条目\ndata Cache是物理寻址、直接映射的，行大小为4B，共有16组\n\n写出访问地址为0x03d4, 0x00f1和0x0229的过程。\n写出其地址结构\n\n\n根据页面大小求页内偏移量与页号长度\n本系统以字节编址，页面大小为64B，则页内偏移量为$log_2(64B/1B)=6位$，所以虚拟页号为$14-6=8位$，物理页号为$12-6=6位$。\n\n根据TLB结构求虚拟页号地址结构\n因为TLB为四路组相联，共有16个条目，则TLB有16/4=4组，因此虚拟页号低$log_24=2位$就为组索引，高6位为TLB标记。\n\n根据Cache机构求物理页号地址结构\n因为Cache行大小为4B，因此物理地址中低$log_24=2位$为块索引，Cache共有16组，可知接下来$log_216=4位$为组索引，剩下高6位作为标记。\n\n\n根据TLB、页表寻找物理页号\n\n先把十六进制的虚拟地址0x03d4, 0x00f1和0x0229转化为二进制形式，如下表所示。\n\n得到每个地址的组索引和TLB标记，接下来就要找出每个地址的页面在不在主存中，若在主存中，则还要找出物理地址。\n\n查TLB得到物理块号\n对于0x03d4，组索引为3，TLB标记为0x03。\n查TLB表，第3组中有标记为03的项，且有效位为1，找到物理块0D。\n拼接页内地址（010100），得到物理地址为0x354。\n\n\n查TLB未得到物理块号，查页表得到物理块号\n对于0x00f1，组索引为3，TLB标记为0x00。\n查TLB表，第3组未找到有标记为00的项。\n访存查页表，根据虚拟页号0x03，找到物理块号02，且有有效位为1。\n拼接页内地址（110001），得到物理地址为0x0b1。\n\n\n查TLB未得到物理块号，查页表也未得到物理块号\n对于0x0229，组索引为0，TLB标记为0x02。\n查TLB表，第0组未找到有标记为02的项。\n访存查页表，根据虚拟页号0x08，页表08项有效位为0，页面不在主存中，产生缺页中断。\n\n\n\n根据Cache寻找内存地址\n\n 找出在主存中的页面的物理地址后，就要通过物理地址访问数据，接下来要找该物理地址的 内容在不在Cache中，物理地址结构如下表所示。\n)\n\nCache块命中\n对于0x354，Cache索引为5，Cache标记为0x0d。\n查询Cache索引为5的行，标记为0d，有效位为1，则该块在Cache中。\n偏移为0，即块0，可得虚拟地址0x03d4的内容为36H。\n\n\nCache块未命中\n对于0x0b1，Cache索引为C，Cache标记为0x02。\n查询Cache索引为C的行，标记为02，有效位为0，则该块不在Cache中。\n需去访问主存查找，物理页号为2、偏移为0x31的内容。\n\n\n\n虚拟地址寻址总流程\n\n\n四、文件管理4.1 文件系统基础4.1.1 文件的基本概念定义\n文件是以计算机硬盘为载体的存储在计算机上的信息集合，在用户进行的输入、输出中，以文件位基本单位。\n文件管理系统是实现的文件的访问、修改和保存，对文件维护管理的系统。\n文件的组成\n\n存储空间：用于存储数据\n标签：便于对数据的分类和索引\n访问权限：不同用户对数据有不同的访问权限\n\n文件的结构\n\n数据项：是文件系统中最低级的数据组织形式，可分为以下两种类型：\n基本数据项：用于描述一个对象的某种属性的一个值，是数据中的最小逻辑单位。\n组合数据项：由多个基本数据项组成。\n\n\n记录：是一组相关的数据项的集合，用于描述一个对象在某方面的属性。\n文件：是指由创建者所定义的、具有文件名的一组相关元素的集合，分为有结构文件和无结构文件两种。\n在有结构的文件中，文件由若干个相似的记录组成，如一个班的学生记录；\n无结构文件则被视为一个字符流，比如一个二进制文件或字符文件。\n\n\n\n文件的访问\n早期的操作系统只有一种访问方式：序列访问(sequential access)。在这些系统中，进程可以按照顺序读取所有的字节或文件中的记录，但是不能跳过并乱序执行它们。顺序访问文件是可以返回到起点的，需要时可以多次读取该文件。当存储介质是磁带而不是磁盘时，顺序访问文件很方便。\n在使用磁盘来存储文件时，可以不按照顺序读取文件中的字节或者记录，或者按照关键字而不是位置来访问记录。这种能够以任意次序进行读取的称为随机访问文件(random access file)。许多应用程序都需要这种方式。\n随机访问文件对许多应用程序来说都必不可少，例如，数据库系统。如果乘客打电话预定某航班机票，订票程序必须能够直接访问航班记录，而不必先读取其他航班的成千上万条记录。\n有两种方法可以指示从何处开始读取文件。第一种方法是直接使用 read 从头开始读取。另一种是用一个特殊的 seek 操作设置当前位置，在 seek 操作后，从这个当前位置顺序地开始读文件。UNIX 和 Windows 使用的是后面一种方式。\n4.1.2 文件控制块和索引结点文件的属性\n\n文件名：由创建文件的用户决定文件名，主要是为了方便用户找到文件，同一目录下不允许有重名文件\n标识符：一个系统内的各文件标识符唯一，对用户来说毫无可读性，因此标识符只是操作系统用于区分各个文件的一种内部名称。\n类型：指明文件的类型\n位置：文件存放的路径（让用户使用）、在外存中的地址（操作系统使用，对用户不可见）\n大小：指明文件大小\n保护信息：对文件进行保护的访问控制信息\n创建时间、最后一次修改时间和最后一次存取时间：文件创建、上次修改和上次访问的相关信息，用于保护和跟踪文件的使用。\n\n文件的属性只有两种状态：设置(set) 和 清除(clear)。\n文件控制块FCB\n文件控制块（FCB）是用来存放控制文件需要的各种信息的数据结构，以实现“按名存取”。\n操作系统通过文件控制块（FCB）来维护文件元数据。FCB的有序集合称为文件目录，一个FCB就是一个文件目录项。下图为一个典型的FCB。\n\nFCB包含以下信息：\n\n基本信息：如文件名、文件的物理位置、文件的逻辑结构、文件的物理结构等。\n存取控制信息：包括文件主的存取权限、核准用户的存取权限以及一般用户的存取权限。\n使用信息：如文件建立时间、上次修改时间等。\n\n\n一个文件目录也被视为一个文件，称为目录文件。\n\n索引结点\n 在检索目录时，只用到了文件名，因此有的系统采用文件名与文件描述分开的方法，使文件描述信息单独形成一个称为索引结点的数据结构，简称 i 结点（inode)。\n 在文件目录中的每个目录项仅由文件名和指向该文件所对应的i结点的指针构成。\n\n假设一个FCB为64B，盘块大小是1KB，则每个盘块中可以存放16个FCB（FCB必须连续存放），若一个文件目录共有640个FCB，则查找文件平均需要启动磁盘20次。\n而在UNIX系统中，一个目录项仅占16B，其中14B是文件名，2B是 i 结点指针。在1KB的盘块中可存放64个目录项。这样，可使查找文件的平均启动磁盘次数减少到原来的1/4，大大节省了系统开销。\n\n\n磁盘索引结点\n它是指存放在磁盘上的索引结点。每个文件有一个唯一的磁盘索引结点，主要包括以下内容：\n\n文件主标识符，拥有该文件的个人或小组的标识符。\n文件类型，包括普通文件、目录文件或特别文件。\n文件存取权限，各类用户对该文件的存取权限。\n文件物理地址，每个索引结点中含有13个地址项，即iaddr(0)～iaddr(12)，它们以直接或间接方式给出数据文件所在盘块的编号。\n文件长度，指以字节为单位的文件长度。\n文件链接计教，在本文件系统中所有指向该文件的文件名的指针计数。\n文件存取时间，本文件最近被进程存取的时间、最近被修改的时间及索引结点最近被修改的时间。\n\n\n内存索引结点\n它是指存放在内存中的索引结点。当文件被打开时，要将磁盘索引结点复制到内存的索引结点中，便于以后使用。在内存索引结点中增加了以下内容：\n\n索引结点编号，用于标识内存索引结点。\n\n状态，指示 i 结点是否上锁或被修改。\n\n访问计数，每当有一进程要访问此 i 结点时，计数加1；访问结束减1。\n\n逻辑设备号，文件所属文件系统的逻辑设备号。\n\n链接指针，设置分别指向空闲链表和散列队列的指针。\n\n\n\n\n4.1.3 文件的操作文件的基本操作\n 文件属于抽象数据类型。为了正确地定义文件，需要考虑可以对文件执行的操作。操作系统提供系统调用，它对文件进行创建、写、读、重定位、删除和截断等操作。\n\n创建文件（create系统调用）\n\n为新文件分配必要的外存空间；\n在目录 中为之创建一个目录项，目录项记录了新文件名、在外存中的地址及其他可能的信息。\n\n\n删除文件（delete系统调用）\n\n先从目录中检索指定文件名的目录项\n然后释放该文件所占的存储空间，以便可被其他文件重复使用，并删除目录条目。\n\n\n读文件（read系统调用）\n\n对于给定文件名，搜索目录以查找文件位置。\n系统维护一个读位置的指针。\n每当发生读操作时，更新读指针。\n\n\n写文件（write系统调用）\n\n对于给定文件名，搜索目录以查找文件位置。\n系统必须为该文件维护一个写位置的指针。\n每当发生写操作时，便更新写指针。\n\n\n一个进程通常只对一个文件读或写，因此当前操作位置可作为每个进程当前文件位置的指针。\n由于读和写操作都使用同一指针，因此节省了空间，也降低了系统复杂度。\n\n\n重新定位文件\n也称文件定位。搜索目录以找到适当的条目，并将当前文件位置指针重新定位到给定值。\n重新定位文件不涉及读、写文件。\n\n截断文件\n允许文件所有属性不变，并删除文件内容，将其长度置为0并释放其空间。\n\n\n这6个基本操作可以组合起来执行其他文件操作。例如，一个文件的复制，可以创建新文件、从旧文件读出并写入新文件。\n文件的打开与关闭\n\n打开文件（open系统调用）\n\n过程：调用open根据文件名搜索目录，将指明文件的属性（包括该文件在外存上的物理位置)，从外存复制到内存打开文件表的一个表目中，并将该表目的编号（也称索引）返回给用户。\n\n\n打开文件时并不会把文件数据直接读入内存。“索引号”也称“文件描述符”。\n\n打开文件之后，对文件的操作不再需要每次都查询目录，可以根据内存中的打开文件表进行操作。\n\n如上图所示，在多个不同进程同时打开文件的操作系统中，通常采用两级表：整个系统表和每个进程表。\n\n整个系统的打开文件表包含FCB的副本及其他信息。\n每个进程的打开文件表根据它打开的所有文件，包含指向系统表中适当条目的指针。\n\n一旦有进程打开了一个文件，系统表就包含该文件的条目。当另一个进程执行调用open时，只不过是在其文件打开表中增加一个条目，并指向系统表的相应条目。\n\n关闭文件（close系统调用）\n\n1.将进程的打开文件表相应表项删除\n2.回收分配给该文件的内存空间等资源\n3.系统打开文件表的打开计数器count减1，若count=0，则删除对应表项。\n\n\n\n系统打开文件表为每个文件关联一个打开计数器（OpenCount)，以记录多少进程打开了该文件。\n\n文件名不必是打开文件表的一部分，因为一且完成对FCB在磁盘上的定位，系统就不再使用文件名。对于访问打开文件表的索引，UNIX称之为文件描述符，而Windows称之为文件句柄。因此，只要文件未被关闭，所有文件操作就通过打开文件表来进行。\n\n\n打开文件信息\n文件指针。系统跟踪上次的读写位置作为当前文件位置的指针，这种指针对打开文件的某个进程来说是唯一的，因此必须与磁盘文件属性分开保存。\n文件打开计数。计数器跟踪当前文件打开和关闭的数量。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件。\n文件磁盘位置。大多数文件操作要求系统修改文件数据。查找磁盘上的文件所需的信息保存在内存中，以便系统不必为每个操作都从磁盘上读取该信息。\n访问权限。每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等）。该信息保存在进程的打开文件表中，以便操作系统能够允许或拒绝后续的I/O请求。\n\n\n\n以下是与文件有关的最常用的一些系统调用：\n\nCreate，创建不包含任何数据的文件。调用的目的是表示文件即将建立，并对文件设置一些属性。\nDelete，当文件不再需要，必须删除它以释放内存空间。为此总会有一个系统调用来删除文件。\nOpen，在使用文件之前，必须先打开文件。这个调用的目的是允许系统将属性和磁盘地址列表保存到主存中，用来以后的快速访问。\nClose，当所有进程完成时，属性和磁盘地址不再需要，因此应关闭文件以释放表空间。很多系统限制进程打开文件的个数，以此达到鼓励用户关闭不再使用的文件。磁盘以块为单位写入，关闭文件时会强制写入最后一块，即使这个块空间内部还不满。\nRead，数据从文件中读取。通常情况下，读取的数据来自文件的当前位置。调用者必须指定需要读取多少数据，并且提供存放这些数据的缓冲区。\nWrite，向文件写数据，写操作一般也是从文件的当前位置开始进行。如果当前位置是文件的末尾，则会直接追加进行写入。如果当前位置在文件中，则现有数据被覆盖，并且永远消失。\nappend，使用 append 只能向文件末尾添加数据。\nseek，对于随机访问的文件，要指定从何处开始获取数据。通常的方法是用 seek 系统调用把当前位置指针指向文件中的特定位置。seek 调用结束后，就可以从指定位置开始读写数据了。\nget attributes，进程运行时通常需要读取文件属性。\nset attributes，用户可以自己设置一些文件属性，甚至是在文件创建之后，实现该功能的是 set attributes 系统调用。\nrename，用户可以自己更改已有文件的名字，rename 系统调用用于这一目的。\n\n4.1.4 文件保护 文件保护通过口令保护、加密保护和访问控制等方式实现。其中，口令和加密是为了防止用户文件被他人存取或窃取，而访问控制则用于控制用户对文件的访问方式。\n口令保护\n为文件设置一个“口令”，用户想要访问文件时需要提供口令，由系统验证口令是否正确。\n实现开销小，但“口令”一般存放在FCB或索引结点中（也就是存放在系统中）因此不太安全\n加密保护\n用一个“密码“对文件加密，用户想要访问文件时，需要提供相同的“密码“才能正确的解密\n安全性高，但加密解密需要耗费一定的时间（Eg：异或加密）\n访问控制\n\n访问类型\n对文件的保护可从限制对文件的访问类型中出发。可加以控制的访问类型主要有以下几种。\n\n读。从文件中读。\n写。向文件中写。\n执行。将文件装入内存并执行。\n添加。将新信息添加到文件结尾部分。\n删除。删除文件，释放空间。\n列表清单。列出文件名和文件属性。\n\n此外还可以对文件的重命名、复制、编辑等加以控制。这些高层的功能可以通过系统程序调用低层系统调用来实现。保护可以只在低层提供。\n\n访问控制\n 解决访问控制最常用的方法是根据用户身份进行控制。而实现基于身份访问的最为普通的方法是，为每个文件和目录增加一个访问控制列表（Access-Control List，ACL），以规定每个用户名及其所允许的访问类型。\n\n优点：可以使用复杂的访问方法，\n缺点：长度无法预计并且可能导致复杂的空间管理，\n\n使用精简的访问列表可以解决这个问题，精简的访问列表采用拥有者、组和其他三种用户类型。\n\n拥有者。创建文件的用户。\n组。一组需要共享文件且具有类似访问的用户。\n其他。系统内的所有其他用户。\n\n文件主在创建文件时，说明创建者用户名及所在的组名，系统在创建文件时也将文件主的名字、所属组名列在该文件的FCB中。用户访问该文件时，\n\n若用户是文件主，按照文件主所拥有的权限访问文件；\n若用户和文件主在同一个用户组，则按照同组权限访问，\n否则只能按其他用户权限访问。\n\n\n\n4.1.5 文件的逻辑结构 文件的逻辑结构是从用户观点出发看到的文件的组织形式。文件的物理结构（存储结构）是从实现观点出发看到的文件在外存上的存储组织形式。\n 文件的逻辑结构与存储介质特性无关，它实际上是指在文件的内部，数据逻辑上是如何组织起来的。\n无结构文件（流式文件）\n无结构文件将数据按顺序组织成记录并积累、保存，它是有序相关信息项的集合，以字节（Byte）为单位。\n\n只能通过穷举搜索的方式访问记录。\n其管理简单，用户操作方便。\n对基本信息单位操作不多的文件适于采用字符流的无结构文件。例如源程序文件、目标代码文件等。\n\n有结构文件（记录式文件）\n\n顺序文件\n文件中的记录一个接一个地顺序排列（逻辑上），记录可以是定长的或可变长的。\n各个记录在物理上可以顺序存储或链式存储。\n\n链式存储：无论是定长何变长记录，都无法实现随机存取，每次只能从第一个记录开始依次往后查找\n\n顺序存储：\n可实现随机存取，记录长度为L，则第ⅰ个记录存放的相对位置是i*L\n若采用串结构，记录之间的顺序与关键字无关，无法快速找到某关键字对应的记录\n若采用顺序结构，可以快速找到某关键字对应的记录（如折半查找）\n\n\n\n定长记录的顺序文件，若物理上采用顺序存储，则可实现随机存取：若能再保证记录的顺序结构，则可实现快速检索（即根据关键字快速找到对应记录）\n\n优点：读写一大批文件时，效率最高。适用于顺序存储设备（磁带）\n缺点：不方便增加、删除记录\n\n索引文件\n\n索引表：高效查询变长记录文件。索引表本身是定长记录的顺序文件，因此可以快速找到第ⅰ个记录对应的索引项。\n\n\n方式：可将关键字作为索引号内容，若按关键字顺序排列，则还可以支持按照关键字折半查找\n每当要增加/删除一个记录时，需要对索引表进行修改。由于索引文件有很快的检索速度，因此主要用于对信息处理的及时性要求比较高的场合。\n\n\n\n索引顺序文件\n索引顺序文件是索引文件和顺序文件思想的结合。索引顺序文件中，同样会为文件建立一张索引表，但不同的是：并不是每个记录对应一个索引表项，而是一组记录对应一个索引表项。\n\n将记录分组，每组对应一个素引表项\n检素记录时先顺序查索引表，找到分组，再顺序查找分组\n当记录过多时，可建立多级素引表\n\n\n如上图所示，主文件名包含姓名和其他数据项。\n\n姓名为关键字，索引表中为每组的第一条记录（不是每条记录）的关键字值，用指针指向主文件中该记录的起始位置。\n索引表只包含关键字和指针两个数据项，所有姓名关键字递增排列。\n主文件中记录分组排列，同一个组中的关键字可以无序，但组与组之间的关键字必须有序。\n查找一条记录时，首先通过索引表找到其所在的组，然后在该组中使用顺序查找，就能很快地找到记录。\n\n\n直接文件或散列文件（Hash File）\n 给定记录的键值或通过散列函数转换的键值直接决定记录的物理地址。这种映射结构不同于顺序文件或索引文件，没有顺序的特性。\n 散列文件有很高的存取速度，但是会引起冲突，即不同关键字的散列函数值相同。\n\n\n4.1.6 文件的物理结构（分配） 文件的物理结构就是研究文件的实现，即文件数据在物理存储设备上是如何分布和组织的。\n 文件分配对应于文件的物理结构，是指如何为文件分配磁盘块。常用的磁盘空间分配方法有三种：连续分配、链接分配和索引分配。\n连续分配\n最简单的分配方案是把每个文件作为一连串连续数据块存储在磁盘上。因此，在具有 1KB 块的磁盘上，将为 50 KB 文件分配 50 个连续块。\n\n 使用连续空间存储文件\n上面展示了 40 个连续的内存块。从最左侧的 0 块开始。初始状态下，还没有装载文件，因此磁盘是空的。接着，从磁盘开始处（块 0 ）处开始写入占用 4 块长度的内存 A 。然后是一个占用 6 块长度的内存 B，会直接在 A 的末尾开始写。\n注意每个文件都会在新的文件块开始写，所以如果文件 A 只占用了 3 又 1/2 个块，那么最后一个块的部分内存会被浪费。在上面这幅图中，总共展示了 7 个文件，每个文件都会从上个文件的末尾块开始写新的文件块。\n连续的磁盘空间分配有两个优点。\n\n第一，连续文件存储实现起来比较简单，只需要记住两个数字就可以：一个是第一个块的文件地址和文件的块数量。给定第一个块的编号，可以通过简单的加法找到任何其他块的编号。\n第二点是读取性能比较强，可以通过一次操作从文件中读取整个文件。只需要一次寻找第一个块。后面就不再需要寻道时间和旋转延迟，所以数据会以全带宽进入磁盘。\n\n因此，连续的空间分配具有实现简单、高性能的特点。\n不幸的是，连续空间分配也有很明显的不足。随着时间的推移，磁盘会变得很零碎。下图解释了这种现象\n\n这里有两个文件 D 和 F 被删除了。当删除一个文件时，此文件所占用的块也随之释放，就会在磁盘空间中留下一些空闲块。磁盘并不会在这个位置挤压掉空闲块，因为这会复制空闲块之后的所有文件，可能会有上百万的块，这个量级就太大了。\n连续分配方法要求每个文件在磁盘上占有一组连续的块。磁盘地址定义了磁盘上的一个线性排序，这种排序使作业访问磁盘时需要的寻道数和寻道时间最小。\n\n\n物理块号=起始块号+逻辑块号\n优点：支持顺序访问和直接访问（即随机访问）；连续分配的文件在顺序访问时速度最快。\n缺点：不方便文件拓展、存储空间利用率低、会产生磁盘碎片（外部碎片）。\n文件长度不宜动态增加，因为一个文件末尾后的盘块可能已分配给其他文件，一旦需要增加，就需要大量移动盘块。\n为保持文件的有序性，删除和插入记录时，需要对相邻的记录做物理上的移动，还会动态改变文件的长度。\n反复增删文件后会产生外部碎片（与内存管理分配方式中的碎片相似）。\n很难确定一个文件需要的空间大小，因而只适用于长度固定的文件。\n\n\n访存次数：访问第n条记录需访问磁盘1次\n\n链接分配\n第二种存储文件的方式是为每个文件构造磁盘块链表，每个文件都是磁盘块的链接列表，就像下面所示\n\n 以磁盘块的链表形式存储文件\n每个块的第一个字作为指向下一块的指针，块的其他部分存放数据。如果上面这张图你看的不是很清楚的话，可以看看整个的链表分配方案\n\n与连续分配方案不同，这一方法可以充分利用每个磁盘块。除了最后一个磁盘块外，不会因为磁盘碎片而浪费存储空间。同样，在目录项中，只要存储了第一个文件块，那么其他文件块也能够被找到。\n另一方面，在链表的分配方案中，尽管顺序读取非常方便，但是随机访问却很困难（这也是数组和链表数据结构的一大区别）。\n还有一个问题是，由于指针会占用一些字节，每个磁盘块实际存储数据的字节数并不再是 2 的整数次幂。虽然这个问题并不会很严重，但是这种方式降低了程序运行效率。许多程序都是以长度为 2 的整数次幂来读写磁盘，由于每个块的前几个字节被指针所使用，所以要读出一个完成的块大小信息，就需要当前块的信息和下一块的信息拼凑而成，因此就引发了查找和拼接的开销。\n链接分配采取离散分配的方式，可以为文件分配离散的磁盘块。分为隐式链接和显式链接两种。\n访问第n条记录需访问磁盘n次\n\n隐式链接\n除文件的最后一个盘块之外，每个盘块中都存有指向下一个盘块的指针。文件目录包括文件第一块的指针和最后一块的指针。\n\n\n优点：很方便文件拓展，不会有碎片问题，外存利用率高。\n缺点：只支持顺序访问，不支持随机访问，查找效率低，指向下一个盘块的指针也需要耗费少量的存储空间。\n结论：采用隐式链接的链接分配方式，很方便文件拓展。另外，所有的空闲磁盘块都可以被利用，不会有碎片问题，外存利用率高\n\n\n\n\n显式链接\n由于连续分配和链表分配都有其不可忽视的缺点。所以提出了使用内存中的表来解决分配问题。取出每个磁盘块的指针字，把它们放在内存的一个表中，就可以解决上述链表的两个不足之处。下面是一个例子\n\n上图表示了链表形成的磁盘块的内容。这两个图中都有两个文件，文件 A 依次使用了磁盘块地址 4、7、 2、 10、 12，文件 B 使用了6、3、11 和 14。也就是说，文件 A 从地址 4 处开始，顺着链表走就能找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找到文件 B 的全部磁盘块。你会发现，这两个链表都以不属于有效磁盘编号的特殊标记（-1）结束。内存中的这种表格称为 文件分配表(File Application Table,FAT)。\n把用于链接文件各物理块的指针显式地存放在文件分配表（FAT）中。一个磁盘只会建立一张文件分配表。开机时文件分配表放入内存，并常驻内存。\n\n\n优点：很方便文件拓展，不会有碎片问题，外存利用率高，并且支持随机访问。相比于隐式链接来说，地址转换时不需要访问磁盘，因此文件的访问效率更高。\n缺点：文件分配表的需要占用一定的存储空间。\n结论：采用链式分配（显式链接）方式的文件，支持顺序访问，也支持随机访问（想访问ⅰ号逻辑块时，并不需要依次访问之前的0~ｉ-1号逻辑块），由于块号转换的过程不需要访问磁盘，因此相比于隐式链接来说，访问速度快很多。\n\n\n文件分配表：FAT不仅记录了文件分配信息（显示链接），还“兼职”做了空闲块管理\n\n\n\n索引分配\n 索引分配允许文件离散地分配在各个磁盘块中，系统会为每个文件建立一张索引表，索引表中记录了文件的各个逻辑块对应的物理块。索引表存放的磁盘块称为索引块。文件数据存放的磁盘块称为数据块。\n\n\n索引表的 逻辑块号 可以是隐含的，进一步节约空间；\n\n\n链接方案\n如果索引表太大，一个索引块装不下，那么可以将多个索引块链接起来存放。\n\n缺点：需要顺序访问，当文件很大时，查我效率低下\n\n多层索引\n建立多层索引（原理类似于多级页表）。使第一层索引块指向第二层的索引块。还可根据文件大小的要求再建立第三层、第四层索引块。\n\n采用K层索引结构，且顶级索引表未调入内存，则访问一个数据块只需要K+1次读磁盘操作\n缺点：即使是小文件，访问数据块也需受K+1次读磁盘\n\n混合索引\n多种索引分配方式的结合。例如，一个文件的顶级索引表中，既包含直接地址索引（直接指向数据块），又包含一级间接索引（指向单层索引表）、还包含两级间接索引（指向两层索引表）。\n\n所允许的文件最大长度：设有N0个直接地址项；N1个一次间接地址项；N2个二次间接地址项；每个盘块大小M字节；盘块号占m个字节，公式如下：\n\n文件最大长度=(N_0 + N_1·\\frac{M}{m}+N_2·(\\frac{M}{m})^2)·M优点：对于小文件，只需较少的读磁盘次数就可以访问目标数据块。（一般计算机中小文件更多）\n\n总结\n\n\n\n4.2 目录4.2.1 目录的基本概念 文件目录指FCB的有序集合，一个FCB就是一个文件的目录项。与文件管理系统和文件集合相关联的是文件目录，它包含有关文件的属性、位置和所有权等。\n\n目录管理的基本要求：\n\n从用户的角度看，目录在用户（应用程序）所需要的文件名和文件之间提供一种映射，所以目录管理要实现“按名存取”；\n\n目录存取的效率直接影响到系统的性能，所以要提高对目录的检索速度；\n\n在多用户系统中，应允许多个用户共享一个文件，因此目录还需要提供用于控制访问文件的信息。\n\n此外，应允许不同用户对不同文件采用相同的名字，以便于用户按自己的习惯给文件命名，目录管理通过树形结构来解决和实现。\n\n\n\n\n4.2.2 目录结构单级目录结构\n在整个文件系统中只建立一张目录表，每个文件占一个目录项，不允许文件重名，如下图所示。\n\n当访问一个文件时，先按文件名在该目录中查找到相应的FCB，经合法性检查后执行相应的操作。\n当建立一个新文件时，必须先检索所有目录项，以确保没有“重名”的情况，然后在该目录中增设一项，把新文件的属性信息填入到该项中。\n两级目录结构\n 将文件目录分成主文件目录（MFD）和用户文件目录（UFD）两级，不同用户的文件可以重名，但不能对文件分类。\n\n主文件目录项记录用户名及相应用户文件目录所在的存储位置。\n用户文件目录项记录该用户文件的FCB信息。\n树形目录结构\n不同目录下的文件可以重名，可以对文件进行分类，不方便共享。\n\n根据“文件路径”找到目标文件。用户（或用户进程）要访问某个文件时要用文件路径名标识文件，文件路径名是个字符串。各级日录之间用“/”隔开。从根目录出发的路径称为绝对路径(absolute path name)。\n\n例如：自拍.jpg的绝对路径是“/照片/2015-08/自拍jpg”\n系统根据绝对路径一层一层地找到下一级目录。刚开始从外存读入根目录的目录表；找到“照片”目录的存放位置后，从外存读入对应的目录表；再找到“2015-08”目录的存放位置，再从外存读入对应目录表；最后才找到文件“自拍Jpg”的存放位置。整个过程需要3次读磁盘I/O操作。\n\n另外一种指定文件名的方法是 相对路径名(relative path name)。它常常和 工作目录(working directory) ，也称作当前目录(current directory），一起使用。用户可以指定一个目录作为当前工作目录。例如，如果当前目录是 /usr/ast，那么绝对路径 /usr/ast/mailbox可以直接使用 mailbox 来引用。\n引入“当前目录”和“相对路径”后，磁盘I/O的次数减少了。这就提升了访问文件的效率。\n从根目录出发是绝对路径；从当前目录出发是相对路径。\n无环图目录结构\n在树形目录的基础上，增加一些指向同一结点的有向边，使整个目录成为一个有向无环图，实现文件的共享。\n\n为共享结点设置一个共享计数器，计数器为0时才真正删除该结点。\n对于共享文件，只存在一个真正的文件，任何改变都会为其他用户所见。\n4.2.3 目录的操作\n搜索。当用户使用一个文件时，需要搜索目录，以找到该文件的对应目录项。\n创建文件。当创建一个新文件时，需要在目录中增加一个目录项。\n删除文件。当删除一个文件时，需要在目录中删除相应的目录项。\n创建目录。在树形目录结构中，用户可创建自己的用户文件目录，并可再创建子目录。\n删除目录。有两种方式：①不删除非空目录，删除时要先删除目录中的所有文件，并递归地删除子目录。②可删除非空目录，目录中的文件和子目录同时被删除。\n移动目录。将文件或子目录在不同的父目录之间移动，文件的路径名也会随之改变。\n显示目录。用户可以请求显示目录的内容，如显示该用户目录中的所有文件及属性。\n修改目录。某些文件属性保存在目录中，因而这些属性的变化需要改变相应的目录项。\n\n不同文件中管理目录的系统调用的差别比管理文件的系统调用差别大。为了了解这些系统调用有哪些以及它们怎样工作，下面给出一个例子（取自 UNIX）。\n\nCreate，创建目录，除了目录项 . 和 .. 外，目录内容为空。\nDelete，删除目录，只有空目录可以删除。只包含 . 和 .. 的目录被认为是空目录，这两个目录项通常不能删除\nopendir，目录内容可被读取。例如，未列出目录中的全部文件，程序必须先打开该目录，然后读其中全部文件的文件名。与打开和读文件相同，在读目录前，必须先打开文件。\nclosedir，读目录结束后，应该关闭目录用于释放内部表空间。\nreaddir，系统调用 readdir 返回打开目录的下一个目录项。以前也采用 read 系统调用来读取目录，但是这种方法有一个缺点：程序员必须了解和处理目录的内部结构。相反，不论采用哪一种目录结构，readdir 总是以标准格式返回一个目录项。\nrename，在很多方面目录和文件都相似。文件可以更换名称，目录也可以。\nlink，链接技术允许在多个目录中出现同一个文件。这个系统调用指定一个存在的文件和一个路径名，并建立从该文件到路径所指名字的链接。这样，可以在多个目录中出现同一个文件。有时也被称为硬链接(hard link)。\nunlink，删除目录项。如果被解除链接的文件只出现在一个目录中，则将它从文件中删除。如果它出现在多个目录中，则只删除指定路径名的链接，依然保留其他路径名的链接。在 UNIX 中，用于删除文件的系统调用就是 unlink。\n\n4.2.4 目录实现 目录实现有线性列表和哈希表两种方式，线性列表实现对应线性查找，哈希表的实现对应散列查找。\n线性列表\n最简单的目录实现方法是，采用文件名和数据块指针的线性列表。\n\n当创建新文件时，必须首先搜索目录以确定没有同名的文件存在，然后在目录中增加一个新的目录项。\n\n当删除文件时，则根据给定的文件名搜索目录，然后释放分配给它的空间。\n\n当要\n重用目录项\n时有许多种方法：\n\n可以将目录项标记为不再使用，或将它加到空闲目录项的列表上，\n还可以将目录的最后一个目录项复制到空闲位置，并减少目录的长度。\n\n\n\n采用链表结构可以减少删除文件的时间。\n线性列表的优点在于实现简单，不过由于线性表的特殊性，查我比较费时。\n哈希表\n哈希表根据文件名得到一个值，并返回一个指向线性列表中元素的指针。\n\n优点：查找非常迅速，插入和删除也较简单，\n问题：需要一些措施来避免冲突（两个文件名称哈希到同一位置）。\n为了减少I/O操作，把当前使用的文件目录复制到内存，以后要使用该文件时只需在内存中操作，因此降低了磁盘操作次数，提高了系统速度。\n\n\n4.2.5 文件共享 文件共享使多个用户共享同一个文件，系统中只需保留该文件的一个副本。\n基于索引结点的共享方式（硬链接）\n各个用户的目录项指向同一个索引结点，索引结点中需要链接计数count，用于表示链接到本索引结点上的用户目录项数。\n\n某用户删除文件只是删除该用户的目录项，count—\n只有count==0才能真正删除文件数据和索引结点。\n利用符号链实现文件共享（软链接）\n为使用户B能共享用户A的一个文件F,可以由系统创建一个LINK类型的新文件，也取名为F，并将该文件写入用户B的目录中，以实现用户B的目录与文件F的链接。\n\n在一个Link型的文件中记录共享文件的存放路径（Windows快捷方式），操作系统根据路径一层层查找目录，最终找到共享文件。\n\n当User3访问“ccc”时，操作系统判断文件“ccc”属于Link类型文件，于是会根据其中记录的路径层层查找目录，最终找到User1的目录表中的“aaa”表项，于是就找到了文件1的索引结点。\n\n即使软链接指向的共享文件已被删除，Link型文件依然存在，只是通过Link型文件中的路径去查找共享文件会失败（找不到对应目录项）。\n由于用软链接的方式访问共享文件时要查询多级目录，会有多次磁盘I/O，因此用软链接访问的速度要比硬链接更慢。\n 硬链接和软链接都是文件系统中的静态共享方法，在文件系统中还存在着另外的共享需求，即两个进程同时对同一个文件进行操作，这样的共享称为动态共享。\n4.3 文件系统4.3.1 文件系统结构 文件系统(File system)提供高效和便捷的磁盘访问，以便允许存储、定位、提取数据。\n\n用一个例子来辅助记忆文件系统的层次结构：假设某用户请求删除文件”D:/工作目录/学生信息.xIsx”的最后100条记录。\n\n用户需要通过操作系统提供的接口发出上述请求一一用户接口\n由于用户提供的是文件的存放路径，因此需要操作系统一层一层地查找目录，找到对应的目录项一一文件目录系统\n不同的用户对文件有不同的操作权限，因此为了保证安全，需要检查用户是否有访问权限一一存取控制模块（存取控制验证层）\n验证了用户的访问权限之后，需要把用户提供的“记录号”转变为对应的逻辑地址一一逻辑文件系统与文件信息缓冲区\n知道了标记录对应的逻辑地址后，还需要转换成实际的物理地址一一物理文件系统\n要删除这条记录，必定要对磁盘设备发出请求一一设备管理程序模块\n删除这些记录后，会有一些盘块空闲，因此要将这些空闲盘块回收一一辅助分配模块\n\n4.3.2 文件系统布局文件系统在磁盘中的结构\n文件系统存储在磁盘中。大部分的磁盘能够划分出一到多个分区，叫做磁盘分区(disk partitioning)或者是磁盘分片(disk slicing)。每个分区都有独立的文件系统，每块分区的文件系统可以不同。磁盘的 0 号分区称为 主引导记录(Master Boot Record, MBR)，用来引导(boot)计算机。在 MBR 的结尾是分区表(partition table)。每个分区表给出每个分区由开始到结束的地址。\n当计算机开始引 boot 时，BIOS 读入并执行 MBR。\n文件系统可能包括如下信息：启动存储在那里的操作系统的方式、总的块数、空闲块的数量和位置、目录结构以及各个具体文件等。如下图所示。\n\n\n主引导记录（MasterBootRecord，MBR)，位于磁盘的0号扇区，用来引导计算机，MBR后面是分区表，该表给出每个分区的起始和结束地址。表中的一个分区被标记为活动分区，当计算机启动时，BIOS读入并执行MBR。MBR做的第一件事是确定活动分区，读入它的第一块，即引导块。\n\n引导块（bootblock)，MBR执行引导块中的程序后，该程序负责启动该分区中的操作系统。为统一起见，每个分区都从一个引导块开始，即使它不含有一个可启动的操作系统，也不排除以后会在该分区安装一个操作系统。Windows系统称之为分区引导扇区。\n\n除了从引导块开始，磁盘分区的布局是随着文件系统的不同而变化的。文件系统经常包含有如上图所列的一些项目。\n\n超级块（superblock)，超级块 的大小为 4096 字节，从磁盘上的字节偏移 4096 开始。包含文件系统的所有关键信息，在计算机启动时，或者在该文件系统首次使用时，超级块会被读入内存。超级块中的典型信息包括分区的块的数量、块的大小、空闲块的数量和指针、空闲的FCB数量和FCB指针等。\n\n文件系统中空闲块的信息，可以使用位示图或指针链接的形式给出。\n\nBitMap 位图或者 Bit vector 位向量\n位图或位向量是一系列位或位的集合，其中每个位对应一个磁盘块，该位可以采用两个值：0和1，0表示已分配该块，而1表示一个空闲块。下图中的磁盘上给定的磁盘块实例（分配了绿色块）可以用16位的位图表示为：0000111000000110。\n\n\n\n  使用链表进行管理\n  在这种方法中，空闲磁盘块链接在一起，即一个空闲块包含指向下一个空闲块的指针。第一个磁盘块的块号存储在磁盘上的单独位置，也缓存在内存中。\n  \n\n碎片\n这里不得不提一个叫做碎片(fragment)的概念，也称为片段。一般零散的单个数据通常称为片段。 磁盘块可以进一步分为固定大小的分配单元，片段只是在驱动器上彼此不相邻的文件片段。\n\n\n\ninode\n然后在后面是一个 inode(index node)，也称作索引节点。它是一个数组的结构，每个文件有一个 inode，inode 非常重要，它说明了文件的方方面面。每个索引节点都存储对象数据的属性和磁盘块位置\n有一种简单的方法可以找到它们 ls -lai 命令。让我们看一下根文件系统：\n\ninode 节点主要包括了以下信息\n\n模式/权限（保护）\n所有者 ID\n组 ID\n文件大小\n文件的硬链接数\n上次访问时间\n最后修改时间\ninode 上次修改时间\n\n文件分为两部分，索引节点和块。一旦创建后，每种类型的块数是固定的。你不能增加分区上 inode 的数量，也不能增加磁盘块的数量。\n紧跟在 inode 后面的是根目录，它存放的是文件系统目录树的根部。最后，磁盘的其他部分存放了其他所有的目录和文件。\n\n\n文件系统在内存中的结构\n 内存中的信息用于管理文件系统并通过缓存来提高性能。这些数据在安装文件系统时被加载，在文件系统操作期间被更新，在卸载时被丢弃。这些结构的类型可能包括：\n\n内存中的安装表(mount table)，包含每个己安装文件系统分区的有关信息。\n内存中的目录结构的缓存，包含最近访问目录的信息。对安装分区的目录，它可以包括一个指向分区表的指针。\n整个系统的打开文件表，包含每个打开文件的FCB副本及其他信息。\n每个进程的打开文件表，包含一个指向整个系统的打开文件表中的适当条目的指针，以及其他信息。\n\n进程创建：\n 为了创建新的文件，应用程序调用逻辑文件系统。逻辑文件系统知道目录结构的格式，它将为文件分配一个新的FCB。然后，系统将相应的目录读入内存，使用新的文件名和FCB进行更新，并将它写回磁盘。\n一旦文件被创建，它就能用于I/O，不过，首先要打开文件。系统调用open()将文件名传递给逻辑文件系统。\n\n\n调用open()首先\n搜索整个系统的打开文件表，以确定这个文件是否已被其他进程使用。\n\n如果已被使用，则在单个进程的打开文件表中创建一个条目，让其指向现有整个系统的打开文件表的相应条目。该算法在文件已打开时，能节省大量开销。\n如果这个文件尚未打开，则根据给定文件名来搜索目录结构。部分目录结构通常缓存在内存中，以加快目录操作。\n\n\n找到文件后，它的FCB会复制到整个系统的打开文件表中；该表不但存储FCB，而且跟踪打开该文件的进程的数量。\n\n然后，在单个进程的打开文件表中创建一个条目，并且通过指针将整个系统打开文件表的条目与其他域（如文件当前位置的指针和文件访问模式等）相连。\n\n调用open()返回的是一个指向单个进程的打开文件表中的适当条自的指针。以后，所有文件操作都通过该指针执行。\n\n一旦文件被打开，内核就不再使用文件名来访问文件，而使用文件描述符（Windows称之为文件句柄）\n\n\n进程关闭：\n 当进程关闭一个文件时，就会删除单个进程打开文件表中的相应条目，整个系统的打开文件表的文件打开数量也会递减。当所有打开某个文件的用户都关闭该文件后，任何更新的元数据将复制到磁盘的目录结构中，并且整个系统的打开文件表的对应条目也会被删除。\n4.3.3 外存空闲空间管理 一个存储设备可以按整体用于文件系统，也可以细分。例如，一个磁盘可以划分为4个分区，每个分区都可以有单独的文件系统。包含文件系统的分区通常称为卷（volumme)。卷可以是磁盘的一部分，也可以是整个磁盘，还可以是多个磁盘组成RAID集，如图所示。\n\n 在一个卷中，存放文件数据的空间（文件区）和FCB的空间（目录区）是分离的。\n 文件存储设备分成许多大小相同的物理块，并以块为单位交换信息，因此，文件存储设备的管理实质上是对空闲块的组织和管理，它包括空闲块的组织、分配与回收等问题。\n空闲表法\n空闲表法属于连续分配方式，它与内存的动态分配方式类似，为每个文件分配一块连续的存储空间。\n\n\n盘块的分配\n与内存管理中的动态分区分配很类似，为一个文件分配连续的存储空间。\n同样可采用首次适应、最佳适应、最坏适应等算法来决定要为文件分配哪个区间。\n\n盘块的回收\n与内存管理中的动态分区分配很类似，当回收某个存储区时需要有四种情况：\n\n①回收区的前后都没有相邻空闲区\n②回收区的前后都是空闲区\n③回收区前面是空闲区\n④回收区后面是空闲区\n\n总之，回收时需要注意表项的合并问题\n\n\n空闲链表法\n将所有空闲盘区拉成一条空闲链。根据构成链所用基本元素的不同，分为两种形式：\n\n空闲盘块链：将磁盘上的所有空闲空间以盘块为单位拉成一条链。\n\n操作系统保存着链头、链尾指针。\n\n如何分配：若某文件申请K个盘块，则从链头开始依次摘下K个盘块分配，并修改空闲链的链头指针。\n如何回收：回收的盘块依次挂到链尾，并修改空闲链的链尾指针。\n\n适用于离散分配的物理结构。为文件分配多个盘块时可能要重复多次操作\n\n空闲盘区链：将磁盘上的所有空闲盘区（每个盘区可包含若干个盘块）拉成一条链。\n\n操作系统保存着链头、链尾指针。\n\n如何分配：\n若某文件申请K个盘块，则可以采用首次适应、最佳适应等算法，从链头开始检索，按照算法规则找到一个大小符合要求的空闲盘区，分配给文件。\n若没有合适的连续空闲块，也可以将不同盘区的盘块同时分配给一个文件，注意分配后可能要修改相应的链指针、盘区大小等数据。\n\n如何回收：若回收区和某个空闲盘区相邻，则需要将回收区合并到空闲盘区中。若回收区没有和任何空闲区相邻，将回收区作为单独的一个空闲盘区挂到链尾。\n\n\n离散分配、连续分配都适用。为一个文件分配多个盘块时效率更高。\n\n\n位示图法\n 位示图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。当其值为“0”时，表示对应的盘块空闲；为“1”时，表示已分配。\n 这样，一个m×n位组成的位示图就可用来表示m×n个盘块的使用情况，如图所示。行为位号，列为字号\n\n\n注意：盘块号、字号、位号到底是从0开始，还是从1开始。两者计算盘块号方式不同。\n\n\n盘块的分配\n\n顺序扫描位示图，从中找出一个或一组其值为“0”的二进制位。\n\n将找到的一个或一组二进制位，转换成与之对应的盘块号。若找到的其值为“0”的二进制位位于位示图的第i行、第j列，则其相应的盘块号应按下式计算（n为每行位数）：\n\n\n修改位示图，令\n\nmap[i,j]=1\n\n\n\n\n盘块的回收\n\n将回收盘块的盘块号转换成位示图中的行号和列号。转换公式为\n\n\n修改位示图，令\n\nmap[i,j]=0\n\n\n\n空闲表法和空闲链表法都不适用于大型文件系统，因为这会使空闲表或空闲链表太大。\n成组链接法\n 在UNIX系统中采用的是成组链接法，这种方法结合了空闲表和空闲链表两种方法，它具有上述两种方法的优点，克服了两种方法均有的表太长的缺点。\n 文件卷的目录区中专门用一个磁盘块作为“超级块”，当系统启动时需要将超级块读入内存。并且要保证内存与外存中的“超级块”数据一致。\n\n用来存放一组空闲盘块号（空闲盘块的块号）的盘块称为成组链块。\n\n成组链接思想：把顺序的n个空闲盘块号保存在第一个成组链块中，其最后一个空闲盘块（作为成组链块）则用于保存另一组空闲盘块号，如此继续，直至所有空闲盘块均予以链接。\n\n盘块的分配\n分配1个空闲块：\n\n①检查第一个分组的块数是否足够。1＜100，是足够的。\n②分配第一个分组中的1个空闲块，并修改相应数据\n\n\n分配100个空闲块：\n\n①检查第一个分组的块数是否足够。100=100，是足够的。\n②分配第一个分组中的100个空闲块。但是由于300号块内存放了再下一组的信息，因此300号块的数据需要复制到超级块中\n\n\n\n\n盘块的回收\n需要将超级块中的数据复制到新回收的块中，并修改超级块的内容，让新回收的块成为第一个分组。\n\n\n\n\n4.3.4 虚拟文件系统 虚拟文件系统（VFS）为用户程序提供了文件系统操作的统一接口，屏蔽了不同文件系统的差异和操作细节。\n普通文件系统\n 下图普通的文件系统，不同的外部存储设备，它的文件系统可能是不相同的，对于同一个操作的函数方法定义也许也各不相同；\n\n虚拟文件系统\n 下图为虚拟文件系统，用户程序可以通过VFS提供的统一调用函数（如open()等）来操作不同文件系统的文件，而无须考虑具体的文件系统和实际的存储介质。\n\n 虚拟文件系统采用了面向对象的思想，它抽象出一个通用的文件系统模型，定义了通用文件系统都支持的接口。新的文件系统只要支持并实现这些接口，即可安装和使用。\n 为了实现VFS，Linux主要抽象了四种对象类型。每个VFS对象都存放在一个适当的数据结构中，其中包括对象的属性和指向对象方法（函数）表的指针。\n\n超级块对象：表示一个已安装（或称挂载）的特定文件系统。\n索引结点对象：表示一个特定的文件。\n目录项对象：表示一个特定的目录项。\n文件对象：表示一个与进程相关的已打开文件。\n\n进程与VFS对象之间的交互如下图所示。\n\n\n三个不同的进程已打开了同一个文件，其中两个进程使用同一个硬链接。\n在这种情况下，每个进程都使用自己的文件对象，但只需要两个目录项对象，每个硬链接对应一个目录项对象。这两个目录项对象指向同一个索引结点对象, 这个索引结点对象标识的是超级块对象及随后的普通磁盘文件。\n\n 对于不同文件系统的数据结构，VFS 在每打开一个文件，就在主存建立一个vnode，用统一的数据结构表示文件；\n打开文件后，创建vnode，并将文件信息复制到vnode中，vnode的功能指针指向具体文件系统的函数功能\n\n\nvnode只存在于主存中，而inode既会被调入主存，也会在外存中存储\n\n特点：\n\n向上层用户进程提供统一标准的系统调用接口，屏蔽底层具体文件系统的实现差异\nVFS要求下层的文件系统必须实现某些规定的函数功能，如：open/read/write。一个新的文件系统想要在某操作系统上被使用，就必须满足该操作系统VFS的要求\n每打开一个文件，VFS就在主存中新建一个vnode，用统一的数据结构表示文件，无论该文件存储在哪个文件系统\n\n4.3.5 日志结构文件系统技术的改变会给当前的文件系统带来压力。这种情况下，CPU 会变得越来越快，磁盘会变得越来越大并且越来越便宜（但不会越来越快）。内存容量也是以指数级增长。但是磁盘的寻道时间（除了固态盘，因为固态盘没有寻道时间）并没有获得提高。\n为此，Berkeley 设计了一种全新的文件系统，试图缓解这个问题，这个文件系统就是 日志结构文件系统(Log-structured File System, LFS)。旨在解决以下问题。\n\n不断增长的系统内存\n顺序 I/O 性能胜过随机 I/O 性能\n现有低效率的文件系统\n文件系统不支持 RAID（虚拟化）\n\n另一方面，当时的文件系统不论是 UNIX 还是 FFS，都有大量的随机读写（在 FFS 中创建一个新文件至少需要5次随机写），因此成为整个系统的性能瓶颈。同时因为 Page cache的存在，作者认为随机读不是主要问题：随着越来越大的内存，大部分的读操作都能被 cache，因此 LFS 主要要解决的是减少对硬盘的随机写操作。\n在这种设计中，inode 甚至具有与 UNIX 中相同的结构，但是现在它们分散在整个日志中，而不是位于磁盘上的固定位置。所以，inode 很定位。为了能够找到 inode ，维护了一个由 inode 索引的 inode map(inode 映射)。表项 i 指向磁盘中的第 i 个 inode 。这个映射保存在磁盘中，但是也保存在缓存中，因此，使用最频繁的部分大部分时间都在内存中。\n\n到目前为止，所有写入最初都缓存在内存中，并且追加在日志末尾，所有缓存的写入都定期在单个段中写入磁盘。所以，现在打开文件也就意味着用映射定位文件的索引节点。一旦 inode 被定位后，磁盘块的地址就能够被找到。所有这些块本身都将位于日志中某处的分段中。\n真实情况下的磁盘容量是有限的，所以最终日志会占满整个磁盘空间，这种情况下就会出现没有新的磁盘块被写入到日志中。幸运的是，许多现有段可能具有不再需要的块。例如，如果一个文件被覆盖了，那么它的 inode 将被指向新的块，但是旧的磁盘块仍在先前写入的段中占据着空间。\n为了处理这个问题，LFS 有一个清理(clean)线程，它会循环扫描日志并对日志进行压缩。首先，通过查看日志中第一部分的信息来查看其中存在哪些索引节点和文件。它会检查当前 inode 的映射来查看 inode 否在在当前块中，是否仍在被使用。如果不是，该信息将被丢弃。如果仍然在使用，那么 inode 和块就会进入内存等待写回到下一个段中。然后原来的段被标记为空闲，以便日志可以用来存放新的数据。用这种方法，清理线程遍历日志，从后面移走旧的段，然后将有效的数据放入内存等待写到下一个段中。由此一来整个磁盘会形成一个大的环形缓冲区，写线程将新的段写在前面，而清理线程则清理后面的段。\n\n日志文件系统\n虽然日志结构系统的设计很优雅，但是由于它们和现有的文件系统不相匹配，因此还没有广泛使用。不过，从日志文件结构系统衍生出来一种新的日志系统，叫做日志文件系统，它会记录系统下一步将要做什么的日志。微软的 NTFS 文件系统、Linux 的 ext3 就使用了此日志。 OS X 将日志系统作为可供选项。为了看清它是如何工作的，我们下面讨论一个例子，比如 移除文件 ，这个操作在 UNIX 中需要三个步骤完成：\n\n在目录中删除文件\n释放 inode 到空闲 inode 池\n将所有磁盘块归还给空闲磁盘池。\n\n4.3.6 分区和安装分区\n\n物理格式化（低级格式化）：划分扇区、检测坏扇区、用备用扇区替换坏扇区；当要访问某一块坏扇区时，会使用备用扇区，默默完成替换工作；\n\n\n逻辑格式化（高级格式化）：磁盘分区；每个区的大小、地址范围等信息，会使用 分区表 来记录；\n在每个区里可以建立各自独立的文件系统 ，例如在C盘里建立UNIZX文件系统；\n分区的第一部分是引导块，里面存储着引导信息，它有自身的格式，因为在引导时系统并未 加载文件系统代码，因此不能解释文件系统的格式。下图为一个典型的Linux分区。\n安装\n\n\n文件系统在进程使用前必须先安装，也称挂载，任务是将一个文件系统挂载到操作系统中。\n功能：\n\n在VFS中注册新挂载的文件系统。内存中的挂载表（mount table）包含每个文件系统的相关信息，包括文件系统类型、容量大小等。\n新挂载的文件系统，要向VFS提供一个函数地址列表\n将新文件系统加到挂载点（mountpoint），也就是将新文件系统挂载在某个父目录下\n\n\nUNIX本身是一个固定的目录树，只要安装就有，但是如果不给它分配存储空间，就不能对它进行操作，所以首先要给根目录分配空间，这样才能操作这个目录树。\n\n4.4 文件系统的管理和优化能够使文件系统工作是一回事，能够使文件系统高效、稳定的工作是另一回事，下面我们就来探讨一下文件系统的管理和优化。\n4.4.1 磁盘空间管理文件通常存在磁盘中，所以如何管理磁盘空间是一个操作系统的设计者需要考虑的问题。在文件上进行存有两种策略：分配 n 个字节的连续磁盘空间；或者把文件拆分成多个并不一定连续的块。在存储管理系统中，主要有分段管理和 分页管理 两种方式。\n正如我们所看到的，按连续字节序列存储文件有一个明显的问题，当文件扩大时，有可能需要在磁盘上移动文件。内存中分段也有同样的问题。不同的是，相对于把文件从磁盘的一个位置移动到另一个位置，内存中段的移动操作要快很多。因此，几乎所有的文件系统都把文件分割成固定大小的块来存储。\n块大小\n一旦把文件分为固定大小的块来存储，就会出现问题，块的大小是多少？按照磁盘组织方式，扇区、磁道和柱面显然都可以作为分配单位。在分页系统中，分页大小也是主要因素。\n拥有大的块尺寸意味着每个文件，甚至 1 字节文件，都要占用一个柱面空间，也就是说小文件浪费了大量的磁盘空间。另一方面，小块意味着大部分文件将会跨越多个块，因此需要多次搜索和旋转延迟才能读取它们，从而降低了性能。因此，如果分配的块太大会浪费空间；分配的块太小会浪费时间。\n记录空闲块\n一旦指定了块大小，下一个问题就是怎样跟踪空闲块。有两种方法被广泛采用，如下图所示\n\n第一种方法是采用磁盘块链表，链表的每个块中包含极可能多的空闲磁盘块号。对于 1 KB 的块和 32 位的磁盘块号，空闲表中每个块包含有 255 个空闲的块号。考虑 1 TB 的硬盘，拥有大概十亿个磁盘块。为了存储全部地址块号，如果每块可以保存 255 个块号，则需要将近 400 万个块。通常，空闲块用于保存空闲列表，因此存储基本上是空闲的。\n另一种空闲空间管理的技术是位图(bitmap)，n 个块的磁盘需要 n 位位图。在位图中，空闲块用 1 表示，已分配的块用 0 表示。对于 1 TB 硬盘的例子，需要 10 亿位表示，即需要大约 130 000 个 1 KB 块存储。很明显，和 32 位链表模型相比，位图需要的空间更少，因为每个块使用 1 位。只有当磁盘快满的时候，链表需要的块才会比位图少。\n4.4.2 磁盘配额为了防止一些用户占用太多的磁盘空间，多用户操作通常提供一种磁盘配额(enforcing disk quotas)的机制。系统管理员为每个用户分配最大的文件和块分配，并且操作系统确保用户不会超过其配额。我们下面会谈到这一机制。\n在用户打开一个文件时，操作系统会找到文件属性和磁盘地址，并把它们送入内存中的打开文件表。其中一个属性告诉文件所有者是谁。任何有关文件的增加都会记到所有者的配额中。\n\n 配额表中记录了每个用户的配额\n第二张表包含了每个用户当前打开文件的配额记录，即使是其他人打开该文件也一样。如上图所示，该表的内容是从被打开文件的所有者的磁盘配额文件中提取出来的。当所有文件关闭时，该记录被写回配额文件。\n当在打开文件表中建立一新表项时，会产生一个指向所有者配额记录的指针。每次向文件中添加一个块时，文件所有者所用数据块的总数也随之增加，并会同时增加硬限制和软限制的检查。可以超出软限制，但硬限制不可以超出。当已达到硬限制时，再往文件中添加内容将引发错误。同样，对文件数目也存在类似的检查。\n4.4.3 文件系统备份做文件备份很耗费时间而且也很浪费空间，这会引起下面几个问题。首先，是要备份整个文件还是仅备份一部分呢？一般来说，只是备份特定目录及其下的全部文件，而不是备份整个文件系统。\n其次，对上次未修改过的文件再进行备份是一种浪费，因而产生了一种增量转储(incremental dumps) 的思想。最简单的增量转储的形式就是周期性的做全面的备份，而每天只对增量转储完成后发生变化的文件做单个备份。\n稍微好一点的方式是只备份最近一次转储以来更改过的文件。当然，这种做法极大的缩减了转储时间，但恢复起来却更复杂，因为最近的全面转储先要全部恢复，随后按逆序进行增量转储。为了方便恢复，人们往往使用更复杂的转储模式。\n第三，既然待转储的往往是海量数据，那么在将其写入磁带之前对文件进行压缩就很有必要。但是，如果在备份过程中出现了文件损坏的情况，就会导致破坏压缩算法，从而使整个磁带无法读取。所以在备份前是否进行文件压缩需慎重考虑。\n第四，对正在使用的文件系统做备份是很难的。如果在转储过程中要添加，删除和修改文件和目录，则转储结果可能不一致。因此，因为转储过程中需要花费数个小时的时间，所以有必要在晚上将系统脱机进行备份，然而这种方式的接受程度并不高。所以，人们修改了转储算法，记下文件系统的瞬时快照，即复制关键的数据结构，然后需要把将来对文件和目录所做的修改复制到块中，而不是到处更新他们。\n磁盘转储到备份磁盘上有两种方案：物理转储和逻辑转储。物理转储(physical dump) 是从磁盘的 0 块开始，依次将所有磁盘块按照顺序写入到输出磁盘，并在复制最后一个磁盘时停止。这种程序的万无一失性是其他程序所不具备的。\n第二个需要考虑的是坏块的转储。制造大型磁盘而没有瑕疵是不可能的，所以也会存在一些坏块(bad blocks)。有时进行低级格式化后，坏块会被检测出来并进行标记，这种情况的解决办法是用磁盘末尾的一些空闲块所替换。\n然而，一些块在格式化后会变坏，在这种情况下操作系统可以检测到它们。通常情况下，它可以通过创建一个由所有坏块组成的文件来解决问题，确保它们不会出现在空闲池中并且永远不会被分配。那么此文件是完全不可读的。如果磁盘控制器将所有的坏块重新映射，物理转储还是能够正常工作的。\nWindows 系统有分页文件(paging files) 和 休眠文件(hibernation files) 。它们在文件还原时不发挥作用，同时也不应该在第一时间进行备份。\n4.4.4 文件系统的一致性影响可靠性的一个因素是文件系统的一致性。许多文件系统读取磁盘块、修改磁盘块、再把它们写回磁盘。如果系统在所有块写入之前崩溃，文件系统就会处于一种不一致(inconsistent)的状态。如果某些尚未写回的块是索引节点块，目录块或包含空闲列表的块，则此问题是很严重的。\n为了处理文件系统一致性问题，大部分计算机都会有应用程序来检查文件系统的一致性。例如，UNIX 有 fsck；Windows 有 sfc，每当引导系统时（尤其是在崩溃后），都可以运行该程序。\n可以进行两种一致性检查：块的一致性检查和文件的一致性检查。为了检查块的一致性，应用程序会建立两张表，每个包含一个计数器的块，最初设置为 0 。第一个表中的计数器跟踪该块在文件中出现的次数，第二张表中的计数器记录每个块在空闲列表、空闲位图中出现的频率。\n4.4.5文件系统性能访问磁盘的效率要比内存满的多，是时候又祭出这张图了\n\n从内存读一个 32 位字大概是 10ns，从硬盘上读的速率大概是 100MB/S，对每个 32 位字来说，效率会慢了四倍，另外，还要加上 5 - 10 ms 的寻道时间等其他损耗，如果只访问一个字，内存要比磁盘快百万数量级。所以磁盘优化是很有必要的，下面我们会讨论几种优化方式\n高速缓存\n最常用的减少磁盘访问次数的技术是使用 块高速缓存(block cache) 或者 缓冲区高速缓存(buffer cache)。高速缓存指的是一系列的块，它们在逻辑上属于磁盘，但实际上基于性能的考虑被保存在内存中。\n管理高速缓存有不同的算法，常用的算法是：检查全部的读请求，查看在高速缓存中是否有所需要的块。如果存在，可执行读操作而无须访问磁盘。如果检查块不再高速缓存中，那么首先把它读入高速缓存，再复制到所需的地方。之后，对同一个块的请求都通过高速缓存来完成。\n高速缓存的操作如下图所示\n\n由于在高速缓存中有许多块，所以需要某种方法快速确定所需的块是否存在。常用方法是将设备和磁盘地址进行散列操作，然后，在散列表中查找结果。具有相同散列值的块在一个链表中连接在一起（这个数据结构是不是很像 HashMap?），这样就可以沿着冲突链查找其他块。\n如果高速缓存已满，此时需要调入新的块，则要把原来的某一块调出高速缓存，如果要调出的块在上次调入后已经被修改过，则需要把它写回磁盘。\n块提前读\n第二个明显提高文件系统的性能是，在需要用到块之前，试图提前将其写入高速缓存，从而提高命中率。许多文件都是顺序读取。如果请求文件系统在某个文件中生成块 k，文件系统执行相关操作并且在完成之后，会检查高速缓存，以便确定块 k + 1 是否已经在高速缓存。如果不在，文件系统会为 k + 1 安排一个预读取，因为文件希望在用到该块的时候能够直接从高速缓存中读取。\n当然，块提前读取策略只适用于实际顺序读取的文件。对随机访问的文件，提前读丝毫不起作用。甚至还会造成阻碍。\n减少磁盘臂运动\n高速缓存和块提前读并不是提高文件系统性能的唯一方法。另一种重要的技术是把有可能顺序访问的块放在一起，当然最好是在同一个柱面上，从而减少磁盘臂的移动次数。当写一个输出文件时，文件系统就必须按照要求一次一次地分配磁盘块。如果用位图来记录空闲块，并且整个位图在内存中，那么选择与前一块最近的空闲块是很容易的。如果用空闲表，并且链表的一部分存在磁盘上，要分配紧邻的空闲块就会困难很多。\n磁盘碎片整理\n在初始安装操作系统后，文件就会被不断的创建和清除，于是磁盘会产生很多的碎片，在创建一个文件时，它使用的块会散布在整个磁盘上，降低性能。删除文件后，回收磁盘块，可能会造成空穴。\n磁盘性能可以通过如下方式恢复：移动文件使它们相互挨着，并把所有的至少是大部分的空闲空间放在一个或多个大的连续区域内。Windows 有一个程序 defrag 就是做这个事儿的。Windows 用户会经常使用它，SSD 除外。\n磁盘碎片整理程序会在让文件系统上很好地运行。Linux 文件系统（特别是 ext2 和 ext3）由于其选择磁盘块的方式，在磁盘碎片整理上一般不会像 Windows 一样困难，因此很少需要手动的磁盘碎片整理。而且，固态硬盘并不受磁盘碎片的影响，事实上，在固态硬盘上做磁盘碎片整理反倒是多此一举，不仅没有提高性能，反而磨损了固态硬盘。所以碎片整理只会缩短固态硬盘的寿命。\n\n五、输入/输出（I/O）管理5.1 I/O管理概述5.1.1 I/O设备 I/O设备是将数据输入到计算机中，或者可以接收计算机输出数据的外部设备，属于计算机中的硬件部件；\n设备的分类\n\n按使用特性分类：\n人机交互类外部设备：鼠标、键盘、打印机等，用于人机交互。数据传输速度慢。\n存储设备：移动硬盘、光盘等，用于数据存储。数据传输速度快。\n网络通信设备：调制解调器等，用于网络通信。数据传输速度介于上述二者之间。\n\n\n按信息交换的单位分类：\n块设备。信息交换以数据块为单位。它属于有结构设备，如磁盘等。磁盘设备的基本特征是传输速率较高、可寻址，即对它可随机地读/写任意一块。\n字符设备。信息交换以字符为单位。它属于无结构类型，如交互式终端机、打印机等。\n\n\n按传输速率分类：\n低速设备。传输速率仅为每秒几字节到数百字节的一类设备，如键盘、鼠标等。\n中速设备。传输速率为每秒数千字节至数万字节的一类设备，如激光打印机等。\n高速设备。传输速率在数百千字节至千兆字节的一类设备，如磁盘机、光盘机等。\n\n\n\nI/O接口\n I/O接口（设备控制器）位于CPU与设备之间，它既要与CPU通信，又要与设备通信，还要具有按CPU发来的命令去控制设备工作的功能，主要由三部分组成，如下图所示。\n\n\n组成部分：\n\n设备控制器与CPU的接口：实现控制器与CPU之间的通信\n该接口有三类信号线：数据线、地址线和控制线。\n数据线与两类寄存器相连：数据寄存器（存放从设备送来的输入数据或从CPU送来的输出数据）和控制/状态寄存器（存放从CPU送来的控制信息或设备的状态信息）。\n\n设备控制器与设备的接口：实现控制器与设备之间的通信\n一个设备控制器可以连接一个或多个设备，因此控制器中有一个或多个设备接口。\n每个接口中都存在数据、控制和状态三种类型的信号。\n\nI/O逻辑：负责识别CPU发出的命令，并向设备发出命令\n用于实现对设备的控制。它通过一组控制线与CPU交互，对从CPU收到的I/O命令进行译码。\nCPU启动设备时，将启动命令发送给控制器，同时通过地址线把地址发送给控制器，由控制器的I/O逻辑对地址进行译码，并相应地对所选设备进行控制。\n\n\n\n主要功能：\n\n接受和识别CPU发出的指令（控制寄存器）\n向CPU报告设备的状态（状态寄存器）\n数据交换（数据寄存器暂存数据）\n地址识别（由I/O逻辑实现）\n数据缓冲\n差错控制\n\n\n\nI/O端口\nI/O端口是指设备控制器中可被CPU直接访问的寄存器，主要有以下三类寄存器。\n\n寄存器类型：\n\n数据寄存器：实现CPU和外设之间的数据缓冲。\n状态寄存器：获取执行结果和设备的状态信息，以让CPU知道是否准备好。\n控制寄存器：由CPU写入，以便启动命令或更改设备模式。\n\n\n实现I/O端口通信，有两种编址方法：\n\n\n独立编址。为每个端口分配一个I/O端口号，所有I/O端口形成I/O端口空间，普通用户程序不能对其进行访问，只有操作系统使用特殊的I/O指令才能访问端口。\n统一编址。又称内存映射I/O，每个端口被分配唯一的内存地址，且不会有内存被分配这地址，通常分配给端口的地址靠近地址空间的顶端。\n\n\n\n内存映射 I/O\n每个控制器都会有几个寄存器用来和 CPU 进行通信。通过写入这些寄存器，操作系统可以命令设备发送数据，接收数据、开启或者关闭设备等。通过从这些寄存器中读取信息，操作系统能够知道设备的状态，是否准备接受一个新命令等。\n为了控制寄存器，许多设备都会有数据缓冲区(data buffer)，来供系统进行读写。\n那么问题来了，CPU 如何与设备寄存器和设备数据缓冲区进行通信呢？存在两个可选的方式。第一种方法是，每个控制寄存器都被分配一个 I/O 端口(I/O port)号，这是一个 8 位或 16 位的整数。所有 I/O 端口的集合形成了受保护的 I/O 端口空间，以便普通用户程序无法访问它（只有操作系统可以访问）。使用特殊的 I/O 指令像是\n1IN REG,PORT\nCPU 可以读取控制寄存器 PORT 的内容并将结果放在 CPU 寄存器 REG 中。类似的，使用\n1OUT PORT,REG\nCPU 可以将 REG 的内容写到控制寄存器中。大多数早期计算机，包括几乎所有大型主机，如 IBM 360 及其所有后续机型，都是以这种方式工作的。\n第二个方法是 PDP-11 引入的，它将所有控制寄存器映射到内存空间中。\n5.1.2 I/O控制方式设备管理的主要任务之一是控制设备和内存或CPU之间的数据传送。外围设备与内存之间的输入/输出控制方式有以下4种。\n1.程序直接控制方式\n 如下图所示，计算机从外部设备读取的每个字，CPU需要对外设状态进行循环检查，直到确定该字已经在I/O控制器的数据寄存器中。\n\n\n工作流程：\n\n\n\nCPU干预的频率：很频繁，I/O操作开始之前、完成之后需要CPU介入，并且在等待I/O完成的过程中CPU需要不断地轮询检查。\n\n数据传送的单位：每次读/写一个字\n\n数据的流向：\n\n读操作（数据输入）：I/O设备→CPU→内存\n写操作（数据输出）：内存→CPU→I/O设备\n每个字的读/写都需要CPU的帮助\n\n\n优点：实现简单。在读/写指令之后，加上实现循环检查的一系列指令即可\n\n缺点：CPU和I/O设备只能串行工作，CPU需要一直轮询检查，长期处于“忙等”状态，CPU利用率低。\n\n\n2.中断驱动方式\n 中断驱动方式的思想是，允许I/O设备主动打断CPU的运行并请求服务，从而”解放” CPU，使得其向I/O控制器发送读命令后可以继续做其他有用的工作。\n\n\n\n工作流程：\n引入中断机制。由于I/O设备速度很慢，因此在CPU发出读/写命令后，可将等待I/O的进程阻塞，先切换到别的进程执行。\n当I/O完成后，控制器会向CPU发出一个中断信号，CPU检测到中断信号后，会保存当前进程的运行环境信息，转去执行中断处理程序处理该中断。\n处理中断的过程中，CPU从I/O控制器读一个字的数据传送到CPU寄存器，再写入主存。接着，CPU恢复等待I/O的进程（或其他进程）的运行环境，然后继续执行。\n\n\n\n\n①CPU会在每个指令周期的末尾检查中断：②中断处理过程中需要保存、恢复进程的运行环境，这个过程是需要一定时间开销的。可见，如果中断发生的须率太高，也会降低系统性能。\n\n\nCPU干预的频率：每次I/O操作开始之前、完成之后需要CPU介入。等待I/O完成的过程中CPU可以切换到别的进程执行。\n数据传送单位：每次读/写一个字\n数据流向：\n读操作（数据输入）：I/O设备→CPU→内存\n写操作（数据输出）：内存→CPU→I/O设备\n\n\n优点：与“程序直接控制方式”相比，在“中断驱动方式”中，I/O控制器会通过中断信号主动报告I/O已完成，CPU不再需要不停地轮询。CPU和I/O设备可并行工作，CPU利用率得到明显提升。\n缺点：每个字在I/O设备与内存之间的传输，都需要经过CPU。而频繁的中断处理会消耗较多的CPU时间。\n\n3.DMA方式\n DMA（直接存储器存取）方式的基本思想是在I/O设备和内存之间开辟直接的数据交换通路，彻底“解放”CPU。下图为DMA工作流程。\n\n\n与“中断驱动方式”相比，DMA方式有这样几个改进：\n\n①数据的传送单位是“块”。不再是一个字、一个字的传送：\n②数据的流向是从设备直接放入内存，或者从内存直接到设备。不再需要CPU作为“快递小哥”。\n③仅在传送一个或多个数据块的开始和结束时，才需要CPU干预，数据传送通过DMA控制器完成。\n\n\nDMA控制器组成：\n下图为DMA控制器的组成。\n\nDMA控制器中设置如下4类寄存器：\n\n数据寄存器（DR）。暂存从设备到内存或从内存到设备的数据。\n内存地址寄存器（MAR）。在输入时，它存放把数据从设备传送到内存的起始目标地址；在输出时，它存放由内存到设备的内存源地址。\n数据计数器（DC）。存放本次要传送的字（节）数，剩余要读/写的字节数。\n命令/状态寄存器（CR）。接收从CPU发来的I/O命令、有关控制信息，或设备的状态。\n\n\nCPU干预的频率：仅在传送一个或多个数据块的开始和结束时，才需要CPU干预。\n\n数据传送单位：每次读/写一个或多个块（注意：每次读写的只能是连续的多个块，且这些块读入内存后在内存中也必须是连续的）\n\n数据流向：\n\n读操作（数据输入）：I/O设备→内存\n写操作（数据输出）：内存→I/O设备\n\n\n优点：数据传输以“块”为单位，CPU介入频率进一步降低。数据的传输不再需要先经过CPU再写入内存，数据传输效率进一步增加。CPU和I/O设备的并行性得到提升。\n\n缺点：CPU每发出一条I/O指令，只能读/写一个或多个连续的数据块。如果要读/写多个离散存储的数据块，或者要将数据分别写到不同的内存区域时，CPU要发出多条I/O指令，进行多次中断处理才能完成。\n\n\n4.通道控制方式\n通道：I/O通道是指专门负责输入输出的处理机。是一种硬件，可以理解为是“弱鸡版的CPU”。通道可以识别并执行一系列通道指令。\n\n与CPU相比，通道可以执行的指令很单一，并且通道程序是放在主机内存中的，也就是说通道与CPU共享内存\n\n\n完成一次读写流程：\n\n\nCPU干预的频率：极低，通道会根据CPU的指示执行相应的通道程序，只有完成一组数据块的读/写后才需要发出中断信号，请求CPU干预。\n\n数据传送的单位：每次读/写一组数据块\n\n数据的流向（在通道的控制下进行）\n\n读操作（数据输入）：I/O设备→内存\n写操作（数据输出）：内存→I/O设备\n\n\n缺点：实现复杂，需要专门的通道硬件支持\n\n优点：CPU、通道、I/O设备可并行工作，资源利用率很高。\n\n\n5.1.3 IO 软件原理与目标设备独立性\nI/O 软件设计一个很重要的目标就是设备独立性(device independence)。这意味着我们能够编写访问任何设备的应用程序，而不用事先指定特定的设备。\n\n错误处理\n除了设备独立性外，I/O 软件实现的第二个重要的目标就是错误处理(error handling)。通常情况下来说，错误应该交给硬件层面去处理。如果设备控制器发现了读错误的话，它会尽可能的去修复这个错误。如果设备控制器处理不了这个问题，那么设备驱动程序应该进行处理，设备驱动程序会再次尝试读取操作，很多错误都是偶然性的，如果设备驱动程序无法处理这个错误，才会把错误向上抛到硬件层面（上层）进行处理，很多时候，上层并不需要知道下层是如何解决错误的。\n一般坏块有两种处理办法，一种是在控制器中进行处理；一种是在操作系统层面进行处理。\n这两种方法经常替换使用，比如一个具有 30 个数据扇区和两个备用扇区的磁盘，其中扇区 4 是有瑕疵的。\n\n控制器能做的事情就是将备用扇区之一重新映射。\n\n还有一种处理方式是将所有的扇区都向上移动一个扇区\n\n上面这这两种情况下控制器都必须知道哪个扇区，可以通过内部的表来跟踪这一信息，或者通过重写前导码来给出重新映射的扇区号。如果是重写前导码，那么涉及移动的方式必须重写后面所有的前导码，但是最终会提供良好的性能。\n同步和异步传输\nI/O 软件实现的第三个目标就是 同步(synchronous) 和 异步(asynchronous，即中断驱动)传输。这里先说一下同步和异步是怎么回事吧。\n同步传输中数据通常以块或帧的形式发送。发送方和接收方在数据传输之前应该具有同步时钟。而在异步传输中，数据通常以字节或者字符的形式发送，异步传输则不需要同步时钟，但是会在传输之前向数据添加奇偶校验位。大部分物理IO(physical I/O) 是异步的。物理 I/O 中的 CPU 是很聪明的，CPU 传输完成后会转而做其他事情，它和中断心灵相通，等到中断发生后，CPU 才会回到传输这件事情上来。\n缓冲\nI/O 软件的最后一个问题是缓冲(buffering)。通常情况下，从一个设备发出的数据不会直接到达最后的设备。其间会经过一系列的校验、检查、缓冲等操作才能到达。\n共享和独占\nI/O 软件引起的最后一个问题就是共享设备和独占设备的问题。有些 I/O 设备能够被许多用户共同使用。一些设备比如磁盘，让多个用户使用一般不会产生什么问题，但是某些设备必须具有独占性，即只允许单个用户使用完成后才能让其他用户使用。\n一共有三种控制 I/O 设备的方法\n\n使用程序控制 I/O\n使用中断驱动 I/O\n使用 DMA 驱动 I/O\n\n5.1.4 I/O软件层次结构 为了更好地设计 I/O 软件，采用 层次式结构 的 I/O 软件；\n 一个比较合理的层次划分如上图所示。整个I/O软件可以视为具有4个层次的系统结构，各层次功能如下：\n\n用户层软件\n 实现了与用户交互的接口，用户可直接使用该层提供的、与I/O操作相关的库函数对设备进行操作。\n 用户层软件将用户请求翻译成格式化的I/O请求，并通过“系统调用”请求操作系统内核的服务\n设备独立性软件\n 设备独立性软件，又称设备无关性软件。与设备的硬件特性无关的功能几乎都在这一层实现。\n\n功能：\n\n向上层提供统一的调用接口（如read/write系统调用）\n设备的保护\n差错处理\n设备的分配与可收\n数据缓冲区管理\n建立逻辑设备名到物理设备名的映射关系；根据设备类型选择调用相应的驱动程序\n\n\n逻辑设备\n 为实现设备独立性而引入了逻辑设备和物理设备这两个概念。在应用程序中，使用逻辑设备名来请求使用某类设备；而在系统实际执行时，必须将逻辑设备名映射成物理设备名使用。\n使用逻辑设备名好处：\n\n①增加设备分配的灵活性\n②易于实现I/O重定向，指用于I/O操作的设备可以更换(即重定向)，而不必改变应用程序。\n设备独立性软件需要通过“逻辑设备表（LUT，Logical Unit Table）”来确定逻辑设备对应的物理设备，并找到该设备对应的设备驱动程序。\n\n\n\n\nI/O设备被当做一种特殊的文件；不同类型的I/O设备需要有不同的驱动程序处理\n\n操作系统系统可以采用两种方式管理逻辑设备表（LUT）：\n\n第一种方式，整个系统只设置一张LUT，这就意味着所有用户不能使用相同的逻辑设备名，因此这种方式只适用于单用户操作系统。\n第二种方式，为每个用户设置一张LUT，各个用户使用的逻辑设备名可以重复，适用于多用户操作系统。系统会在用户登录时为其建立一个用户管理进程，而LUT就存放在用户管理进程的PCB中。\n\n\n\n设备驱动程序\n 与硬件直接相关，负责具体实现系统对设备发出的操作指令，驱动I/O设备工作的驱动程序。 将上层发出的一系列命令（如read/write）转化成特定设备“能听得懂”的一系列操作。\n 不同设备的内部硬件特性也不同，这些特性只有厂家才知道，因此厂家须提供与设备相对应的驱动程序，CPU执行驱动程序的指令序列，来完成设置设备寄存器，检查设备状态等工作。\n 为I/O内核子系统隐藏设备控制器之间的差异。\n设备控制器的主要功能有下面这些\n\n接收和识别命令：设备控制器可以接受来自 CPU 的指令，并进行识别。设备控制器内部也会有寄存器，用来存放指令和参数\n进行数据交换：CPU、控制器和设备之间会进行数据的交换，CPU 通过总线把指令发送给控制器，或从控制器中并行地读出数据；控制器将数据写入指定设备。\n地址识别：每个硬件设备都有自己的地址，设备控制器能够识别这些不同的地址，来达到控制硬件的目的，此外，为使 CPU 能向寄存器中写入或者读取数据，这些寄存器都应具有唯一的地址。\n差错检测：设备控制器还具有对设备传递过来的数据进行检测的功能。\n\n在这种情况下，设备控制器会阻塞，直到中断来解除阻塞状态。还有一种情况是操作是可以无延迟的完成，所以驱动程序不需要阻塞。在第一种情况下，操作系统可能被中断唤醒；第二种情况下操作系统不会被休眠。\n设备驱动程序必须是可重入的，因为设备驱动程序会阻塞和唤醒然后再次阻塞。驱动程序不允许进行系统调用，但是它们通常需要与内核的其余部分进行交互。\n中断处理程序\n 当I/O任务完成时，I/O控制器会发送一个中断信号，系统会根据中断信号类型找到相应的中断处理程序并执行。\n\n中断处理程序的处理流程如下：\n\n\n中断处理层的任务：\n\n进行进程上下文的切换，\n对处理中断信号源进行测试，\n读取设备状态和修改进程状态等。\n\n\n\n\n由于中断处理与硬件紧密相关，对用户而言，应尽量加以屏蔽，因此应放在操作系统的底层，系统的其余部分尽可能少地与之发生联系。\n\n总结\n说明用户对设备的一次命令过程如下所示：\n\n①当用户要读取某设备的内容时，通过操作系统提供的read命令接口，这就经过了用户层。\n②操作系统提供给用户使用的接口，一般是统一的通用接口，也就是几乎每个设备都可以响应的统一命令，如read命令，用户发出的read命令，首先经过设备独立层进行解析，然后交往下层。\n③接下来，不同类型的设备对read命令的行为会有所不同，如磁盘接收read命令后的行为与打印机接收read命令后的行为是不同的。因此，需要针对不同的设备，把read命令解析成不同的指令，这就经过了设备驱动层。\n④命令解析完毕后，需要中断正在运行的进程，转而执行read命令，这就需要中断处理程序。\n⑤最后，命令真正抵达硬件设备，硬件设备的控制器按照上层传达的命令操控硬件设备，完成相应的功能。\n\n直接涉及到硬件其体细节、且与中断无关的操作肯定是在设备驱动程序层完成的；\n没有涉及硬件的、对各种设备都需要进行的管理工作都是在设备独立性软件层完成的。\n5.1.5 应用程序I/O接口 在I/O系统与高层之间的接口中，根据设备类型的不同，又进一步分为若干接口。\n字符设备接口\n 字符设备是指数据的存取和传输是以字符为单位的设备，如键盘、打印机等。基本特征是传输速率较低、不可寻址，并且在输入输出时通常采用中断驱动方式。\n\n字符设备的操作\n\nget和put操作。由于字符设备不可寻址，只能采取顺序存取方式，通常为字符设备建立一个字符缓冲区，用户程序通过get操作从缓冲区获取字符，通过put操作将字符输出到缓冲区。\nin-control指令。字符设备类型繁多，差异甚大，因此在接口中提供一种通用的in-control指令来处理它们（包含了许多参数，每个参数表示一个与具体设备相关的特定功能）。\n\n字符设备都属于独占设备，为此接口中还需要提供打开和关闭操作，以实现互斥共享。\n\n\n块设备接口\n 块设备是指数据的存取和传输是以数据块为单位的设备，典型的块设备是磁盘。基本特征是传输速率较高、可寻址。磁盘设备的I/O常采用DMA方式。\n\n隐藏了磁盘的二维结构：在二维结构中，每个扇区的地址需要用磁道号和扇区号来表示。块设备接口将磁盘的所有扇区从0到n-1依次编号，这样，就将二维结构变为一种线性序列。\n将抽象命令映射为低层操作：块设备接口支持上层发来的对文件或设备的打开、读、写和关闭等抽象命令，该接口将上述命令映射为设备能识别的较低层的具体操作。\n内存映射接口：内存映射接口通过内存的字节数组来访问磁盘，而不提供读/写磁盘操作。映射文件到内存的系统调用返回包含文件副本的一个虚拟内存地址。只在需要访问内存映像时，才由虚拟存储器实际调页。内存映射文件的访问如同内存读写一样简单，极大地方便了程序员。\n\n网络设备接口\n 许多操作系统提供的网络I/O接口为网络套接字接口，套接字接口的系统调用使应用程序创建的本地套接字连接到远程应用程序创建的套接字，通过此连接发送和接收数据。\n\n阻塞/非阻塞I/O\n\n阻塞I/O：当用户进程调用I/O操作时，进程就被阻塞，需要等待I/O操作完成，进程才被唤醒继续执行。\neg：字符设备接口一一从键盘读一个字符get\n\n非阻塞I/O：用户进程调用I/O操作时，不阻塞该进程，该I/O调用返回一个错误返回值，通常，进程需要通过轮询的方式来查询I/O操作是否完成。\neg：块设备接口一一往磁盘写数据write\n\n\n5.2 设备独立性软件5.2.1 与设备无关的软件 与设备无关的软件是I/O系统的最高层软件，它的下层是设备驱动程序。\n\n设备保护：\n操作系统需要实现文件保护功能，不同的用户对各个文件有不同的访问权限（如：只读、读和写等）\n在UNIX系统中，设备被看做是一种特殊的文件，每个设备也会有对应的FCB。当用户请求访问某个设备时，系统根据FCB中记录的信息来判断该用户是否有相应的访问权限，以此实现“设备保护”的功能。\n\n\n\n5.2.2 高速缓存与缓冲区磁盘高速缓存（Disk Cache）\n操作系统中使用磁盘高速缓存技术来提高磁盘的I/O速度，对访问高速缓存要比访问原始磁盘数据更为高效。\n 磁盘高速缓存技术不同于通常意义下的介于CPU与内存之间的小容量高速存储器，而是指利用内存中的存储空间来暂存从磁盘中读出的一系列盘块中的信息。因此，磁盘高速缓存逻辑上属于磁盘，物理上则是驻留在内存中的盘块。\n\n高速缓存在内存中分为两种形式：\n\n一种是在内存中开辟一个单独的空间作为磁盘高速缓存，大小固定\n另一种是把未利用的内存空间作为一个缓冲池，供请求分页系统和磁盘I/O时共享。\n\n\n\n缓冲区（Buffer）\n\n概念：缓冲区是一个存储区域，可以由专门的硬件寄存器组成，也可利用内存作为缓冲区。\n\n硬件做缓冲区：使用硬件作为缓冲区的成本较高，容量也较小，一般仅用在对速度要求非常高的场合（如存储器管理中所用的联想寄存器，由于对页表的访问频率极高，因此使用速度很快的联想寄存器来存放页表项的副本）\n内存做缓冲区：一般情况下，更多的是利用内存作为缓冲区，“设备独立性软件”的缓冲区管理就是要组织管理好这些缓冲区\n\n\n作用\n\n缓和CPU与I/O设备之间速度不匹配的矛盾\n减少对CPU的中断频率，放宽对CPU中断相应时间的限制\n解决数据粒度不匹配的问题\n提高CPU与I/O设备之间的并行性\n\n\n单缓冲\n若采用单缓冲的策略，操作系统会在主存中为其分配一个缓冲区（若题目中没有特别说明，一个缓冲区的大小就是一个块）。\n\n\n\n注意：当缓冲区数据非空时，不能往缓冲区冲入数据，只能从缓冲区把数据传出；当缓冲区为空时，可以往缓冲区冲入数据，但必须把缓冲区充满以后，才能从缓冲区把数据传出。\n\n设块设备输入缓冲区时间为T，缓冲区传送至工作区时间为M，CPU处理工作区时间为C：\n\n若T&gt;C，从初始状态到下一个开始状态，整个过程用时为M+T\n若T&lt;C，从初始状态到下一个开始状态，整个过程用时为M+C\n\n结论：故单缓冲区处理每块数据的用时为$max(C,T)+M$。\n若两个相互通信的机器只设置单缓冲区，在任一时刻只能实现数据的单向传输。\n\n双缓冲\n若采用双缓冲的策略，操作系统会在主存中为其分配两个缓冲区。\n\n\n双缓冲题目中，假设初始状态为；工作区空，其中一个缓冲区满，另一个缓冲区空\n\n若T&gt;C+M，则处理一块数据的平均用时为T\n若T&lt;C+M，意味着输入数据块速度要比处理机处理数据块速度更快，处理一个数据块的平均耗时为C+M\n\n结论：采用双缓冲策略，处理一个数据块的平均耗时为$Max(T,C+M)$\n\n\n若两个相互通信的机器只设置单缓冲区，在任一时刻只能实现数据的单向传输。\n若两个相互通信的机器设置双缓冲区，则同一时刻可以实现双向的数据传输。\n\n\n循环缓冲\n将多个大小相等的缓冲区链接成一个循环队列。\n\n注：上图中，橙色表示已充满数据的缓冲区，绿色表示空缓冲区。\n\n缓冲池\n 缓冲池由系统中共用的缓冲区组成。\n这些缓冲区按使用状况可以分为：\n\n空缓冲队列\n装满输入数据的缓冲队列（输入队列）\n装满输出数据的缓冲队列（输出队列）\n\n\n根据一个缓冲区在实际运算中扮演的功能不同，又设置了四种工作缓冲区：\n\n用于收容输入数据的工作缓冲区（hin）\n用于提取输入数据的工作缓冲区（sin）\n用于收容输出教据的工作绥冲区（hout）\n用于提取输出数据的工作缓冲区（sout）\n\n工作过程：\n\n输入进程请求输入数据\n从空缓冲队列中取出一块作为收容输入数据的工作缓冲区（hin）。冲满数据后将缓冲区挂到输入队列队尾\n\n计算进程想要取得一块输入数据\n从输入队列中取得一块冲满输入数据的缓冲区作为“提取输入数据的工作缓冲区（sin）”。缓冲区读空后挂到空缓冲区队列\n\n计算进程想要将准备好的数据冲入缓冲区\n从空缓冲队列中取出一块作为“收容输出数据的工作缓冲区（hout）”，数据冲满后将缓冲区挂到输出队列队尾\n\n输出进程请求输出数据\n从输出队列中取得一块冲满输出数据的缓冲区作为“提取输出数据的工作缓冲区（sout）”。缓冲区读空后挂到空缓冲区队列\n\n\n\n\n5.2.3 设备的分配与回收设备分配概述\n 设备分配是指根据用户的I/O请求分配所需的设备。分配的总原则是充分发挥设备的使用效率，尽可能地让设备忙碌，又要避免由于不合理的分配方法造成进程死锁。\n 从设备特性来看，分为以下三种设备：\n\n独占式使用设备。进程分配到独占设备后，便由其独占，直至该进程释放该设备。\n分时式共享使用设备。对于共享设备，可同时分配给多个进程，通过分时共享使用。\n以SPOOLing方式使用外部设备。SPOOLing技术实现了虚拟设备功能，可以将设备同时分配给多个进程。这种技术实质上就是实现了对设备的I/O操作的批处理。\n\n设备分配的数据结构\n 设备分配依据的主要数据结构有设备控制表(DCT)、控制器控制表(COCT)、通道控制表 (CHCT)和系统设备表(SDT)，各数据结构功能如下。\n\n设备控制表（DCT）：系统为每个设备配置一张DCT，用于记录设备情况\n\n\n控制器控制表（COCT）：每个设备控制器都会对应一张COCT。操作系统根据COCT的信息对控制器进行操作和管理。\n\n\n通道控制表（CHCT）：每个通道都会对应一张CHCT。操作系统根据CHCT的信息对通道进行操作和管理。\n\n\n系统设备表（SDT）：记录了系统中全部设备的情况，每个设备对应一个表目。\n\n\n\n设备分配的策略\n\n设备分配原则。设备分配应根据设备特性、用户要求和系统配置情况。既要充分发挥设备的使用效率，又要避免造成进程死锁，还要将用户程序和具体设备隔离开。\n\n设备分配方式。设备分配方式有静态分配和动态分配两种。\n\n①静态分配：进程运行前为其分配全部所需资源，运行结束后归还资源。主要用于对独占设备的分配，它在用户作业开始执行前，由系统一次性分配该作业所要求的全部设备、控制器。\n静态分配方式不会出现死锁，但设备的使用效率低。\n\n②动态分配：进程运行过程中动态申请设备资源。在进程执行过程中根据执行需要进行。\n这种方式有利于提高设备利用率，但若分配算法使用不当，则有可能造成进程死锁。\n\n\n\n设备分配算法。常用的动态设备分配算法有先请求先分配、优先级高者优先等。\n\n\n设备分配的安全性\n设备分配的安全性是指设备分配中应防止发生进程死锁。\n\n安全分配方式。\n\n为进程分配一个设备后就将进程阻塞，本次I/O完成后才将进程唤醒。一个时段内每个进程只能使用一个设备。\n优点：破坏了“请求和保持”条件，不会死锁\n缺点：对于一个进程来说，CPU和I/O设备只能串行工作\n\n\n不安全分配方式。\n进程发出I/O请求后，系统为其分配I/O设备，进程可继续执行，之后还可以发出新的I/O请求。只有某个I/O请求得不到满足时才将进程阻塞。一个进程可以同时使用多个设备。\n\n优点：进程的计算任务和I/O任务可以并行处理，使进程迅速推进\n缺点：有可能发生死锁（死锁避免、死锁的检测和解除）\n\n\n\n设备分配的步骤\n\n步骤\n\n① 根据进程请求的物理设备名查找SDT（注：物理设备名是进程请求分配设备时提供的参数）\n② 根据SDT找到DCT，若设备忙碌则将进程PCB挂到设备等待队列中，不忙碌则将设备分配给进程。\n③ 根据DCT找到COCT，若控制器忙碌则将进程PCB挂到控制器等待队列中，不忙碌则将控制器分配给进程。\n④ 根据COCT找到CHCT，若通道忙碌则将进程PCB挂到通道等待队列中，不忙碌则将通道分配给进程。\n\n\n注：只有设备、控制器、通道三者都分配成功时，这次设备分配才算成功，之后便可启动I/O设备进行数据传送\n\n\n缺点\n\n①用户编程时必须使用“物理设备名”，底层细节对用户不透明，不方便编程\n②若换了一个物理设备，则程序无法运行\n③若进程请求的物理设备正在忙碌，则即使系统中还有同类型的设备，进程也必须阻塞等待\n\n\n\n设备分配步骤的改进\n改进方法：建立逻辑设备名与物理设备名的映射机制，用户编程时只需提供逻辑设备名。\n\n逻辑设备表（LUT）\n逻辑设备表（LUT）建立了逻辑设备名与物理设备名之间的映射关系。\n\n某用户进程第一次使用设备时使用逻辑设备名向操作系统发出请求，操作系统根据用户进程指定的设备类型（逻辑设备名）查找系统设备表，找到一个空闲设备分配给进程，并在LUT中增加相应表项。\n如果之后用户进程再次通过相同的逻辑设备名请求使用设备，则操作系统通过LUT表即可知道用户进程实际要使用的是哪个物理设备了，并且也能知道该设备的驱动程序入口地址。\n\n整个系统只有一张LUT：各用户所用的逻辑设备名不允许重复，适用于单用户操作系统\n每个用户一张LUT：不同用户的逻辑设备名可重复，适用于多用户操作系统\n\n\n步骤\n\n根据进程请求的逻辑设备名查找SDT（注：用户编程时提供的逻辑设备名其实就是“设备类型”）\n查找SDT，找到用户进程指定类型的、并且空闲的设备，将其分配给该进程。操作系统在逻辑设备表（LUT）中新增一个表项。\n根据DCT找到COCT，若控制器忙碌则将进程PCB挂到控制器等待队列中，不忙碌则将控制器分配给进程。\n根据COCT找到CHCT，若通道忙碌则将进程PCB挂到通道等待队列中，不忙碌则将通道分配给进程\n\n\n\n5.2.4 SPOOLing技术（假脱机技术）概念\n\n脱机技术：脱离主机的控制进行的输入/输出操作。\n批处理阶段引入了脱机输入/输出技术（用磁带完成），在外围控制机的控制下，慢速输入设备的数据先被输入到更快速的磁带上。之后主机可以从快速的磁带上读入数据，从而缓解了速度矛盾。\n\nSPOOLing技术：“假脱机技术”，又称“SPOOLing技术”，用软件的方式模拟脱机技术。\n\n要实现SPOOLing技术，必须要有多道程序技术的支持。系统会建立“输入进程”和“输出进程”\n\n\n原理\n\n输入井和输出井\n在磁盘上开辟出两个存储区域一一“输入井”和“输出井”\n”输入井”模拟脱机输入时的磁带，用于收容I/O设备输入的数据\n“输出井”模拟脱机输出时的磁带，用于收容用户进程输出的数据\n\n\n输入进程和输出进程\n“输入进程”模拟脱机输入时的外围控制机\n“输出进程”模拟脱机输出时的外围控制机\n\n\n输入缓冲区和输出缓冲区\n在输入进程的控制下，“输入缓冲区”用于暂存从输入设备输入的数据，之后再转存到输入井中\n在输出进程的控制下，“输出缓冲区”用于暂存从输出井送来的数据，之后再传送到输出设备上\n注意，输入缓冲区和输出缓冲区是在内存中的缓冲区\n\n\n\n预读和滞后写\n\n预读（提前读）：\n如果采用的顺序访问方式对文件进行访问，便可预知下一次要读的盘块。此时可采用预读策略，即在读当前块的同时，也将下一个盘块提前读入内存缓冲区，这样在访问下一个盘块时就不需要再启动磁盘，从而提升磁盘I/O速度。\n\n滞后写（延迟写）\n滞后写是指缓冲区A中的数据本应立即写回磁盘，但考虑到其中的数据在不久之后有可能再次被访问，因此并不会立即把A中的数据写回磁盘，而是将缓冲区A挂到空闲缓冲区队列。如果有别的进程申请使用该缓冲区时，才把A中的数据写回磁盘。这样做的好处是，只要缓冲区A仍在队列中，任何访问该数据的进程，都可以直接读出其中的数据而不必访问磁盘。因而这种方式也可以减少磁盘I/O次数，改善性能。\n\n\n共享打印机的原理分析\nSPOOLing技术可以把一台物理设备虚拟成逻辑上的多台设备，可将独占式设备改造成共享设备。\n\n\n当多个用户进程提出输出打印的请求时，系统会答应它们的请求，但是并不是真正把打印机分配给他们，而是由假脱机管理进程为每个进程做两件事：\n1）在磁盘输出井中为进程申请一个空闲缓冲区（也就是说，这个缓冲区是在磁盘上的），并将要打印的数据送入其中\n2）为用户进程申请一张空白的打印请求表，并将用户的打印请求填入表中（其实就是用来说明用户的打印数据存放位置等信息的），再将该表挂到假脱机文件队列上。\n\n\n\n当打印机空闲时，输出进程会从文件队列的队头取出一张打印请求表，并根据表中的要求将要打印的数据从输出井传送到输出缓冲区，再输出到打印机进行打印。用这种方式可依次处理完全部的打印任务。\n\n虽然系统中只有一个台打印机，但每个进程提出打印请求时，系统都会为在输出井中为其分配一个存储区（相当于分配了一个逻辑设备），使每个用户进程都觉得自己在独占一台打印机，从而实现对打印机的共享。\n\n5.2.5 设备驱动程序接口 要求每个设备驱动程序与操作系统之间都有着相同或相近的接口。这样会使得添加一个新设备驱动程序变得很容易，同时也便于开发人员编制设备驱动程序。\n 对于每种设备类型，例如磁盘，操作系统都要定义一组驱动程序必须支持的函数。\n 与设备无关的软件还要负责将符号化的设备名映射到适当的驱动程序上。\n 在UNIX和Windows中，设备是作为命名对象出现在文件系统中的，因此针对文件的常规保护规则也适用于I/O设备。系统管理员可以为每个设备设置适当的访问权限。\n5.3 磁盘和固态硬盘5.3.1 磁盘磁盘结构\n\n\n磁盘：磁盘的表面由一些磁性物质组成，可以用这些磁性物质来记录二进制数据\n\n磁道：磁盘的盘面被划分成一个个磁道。这样的一个“圈”就是一个磁道\n\n扇区：一个磁道又被划分成一个个扇区，每个扇区就是一个“磁盘块”各个扇区存放的数据量相同\n最内侧磁道上的扇区面积最小，因此数据密度最大\n\n盘面：磁盘有多个盘片”摞”起来，每个盘片有两个盘面。\n\n柱面：所有盘面中相对位置相同的磁道组成柱面。\n\n\n如何在磁盘中读/写数据\n需要把“磁头”移动到想要读/写的扇区所在的磁道。磁盘会转起来，让目标扇区从磁头下面划过，才能完成对扇区的读/写操作。\n磁盘的物理地址：磁盘地址用“柱面号•盘面号•扇区号”表示，可根据该地址读取一个“块”\n\n①根据“柱面号”移动磁臂，让磁头指向指定柱面；\n②激活指定盘面对应的磁头；\n③磁盘旋转的过程中，指定的扇区会从磁头下面划过，这样就完成了对指定扇区的读/写。\n\n磁盘读写时间\n\n寻找时间Ts：在读/写数据前，将磁头移动到指定磁道所花的时间。\n\n①启动磁头臂是需要时间的。假设耗时为s；\n\n②移动磁头也是需要时间的。假设磁头匀速移动，每跨越一个磁道耗时为m，总共需要跨越n条磁道。则：寻道时间\n\nTs=s+ m*n\n\n\n\n\n延迟时间TR：通过旋转磁盘，使磁头定位到目标扇区所需要的时间。\n\n设磁盘转速为r（单位：转/秒，或转/分），则平均所需的延迟时间\n\nTR=(1/2)*(1/r)= 1/2r\n硬盘的典型转速为5400 转/分，或7200转/分\n\n\n\n传输时间Tt：从磁盘读出或向磁盘写入数据所经历的时间\n\n假设磁盘转速为r，此次读/写的字节数为b，每个磁道上的字节数为N。\n\n则：传输时间\n\nTt=(1/r)*(b/N)= b/(rN)\n\n\n\n\n平均存取时间\n\n总的平均存取时间\nT=Ts + 1/2r + b/(rN)\n\n\n延迟时间和传输时间都与磁盘转速相关，且为线性相关而转速是硬件的固有属性，因此操作系统也无法优化延迟时间和传输时间\n\n\n磁盘的分类\n\n磁头是否移动\n磁头可以移动的称为活动头磁盘。磁臂可以来回伸缩来带动磁头定位磁道\n磁头不可移动的称为固定头磁盘。这种磁盘中每个磁道有一个磁头\n\n\n根据盘片是否可更换\n固定盘磁盘\n可换盘磁盘\n\n\n\nRAID\nRAID 称为 磁盘冗余阵列，简称 磁盘阵列。利用虚拟化技术把多个硬盘结合在一起，成为一个或多个磁盘阵列组，目的是提升性能或数据冗余。\nRAID 有不同的级别\n\nRAID 0 - 无容错的条带化磁盘阵列\nRAID 1 - 镜像和双工\nRAID 2 - 内存式纠错码\nRAID 3 - 比特交错奇偶校验\nRAID 4 - 块交错奇偶校验\nRAID 5 - 块交错分布式奇偶校验\nRAID 6 - P + Q冗余\n\n磁盘格式化\n磁盘由一堆铝的、合金或玻璃的盘片组成，磁盘刚被创建出来后，没有任何信息。磁盘在使用前必须经过低级格式化(low-levvel format)，下面是一个扇区的格式\n\n前导码相当于是标示扇区的开始位置，通常以位模式开始，前导码还包括柱面号、扇区号等一些其他信息。紧随前导码后面的是数据区，数据部分的大小由低级格式化程序来确定。大部分磁盘使用 512 字节的扇区。数据区后面是 ECC，ECC 的全称是 error correction code ，数据纠错码，它与普通的错误检测不同，ECC 还可以用于恢复读错误。ECC 阶段的大小由不同的磁盘制造商实现。ECC 大小的设计标准取决于设计者愿意牺牲多少磁盘空间来提高可靠性，以及程序可以处理的 ECC 的复杂程度。通常情况下 ECC 是 16 位，除此之外，硬盘一般具有一定数量的备用扇区，用于替换制造缺陷的扇区。\n5.3.2 磁盘管理磁盘初始化\n 一个新的磁盘只是一个磁性记录材料的空白盘。在磁盘可以存储数据之前，必须将它分成扇区，以便磁盘控制器能够进行读写操作，这个过程称为低级格式化（或称物理格式化）。\n 低级格式化为每个扇区使用特殊的数据结构，填充磁盘。每个扇区的数据结构通常由头部、数据区域（通常为512B大小）和尾部组成。头部和尾部包含了一些磁盘控制器的使用信息。\n分区\n在可以使用磁盘存储文件之前，操作系统还要将自己的数据结构记录到磁盘上，分为两步：\n\n第一步是，将磁盘分为由一个或多个柱面组成的分区（即我们熟悉的C盘、D盘等形式的分区），每个分区的起始扇区和大小都记录在磁盘主引导记录的分区表中\n第二步是，对物理分区进行逻辑格式化（创建文件系统），操作系统将初始的文件系统数据结构存储到磁盘上，这些数据结构包括空闲空间和已分配的空间以及一个初始为空的目录。\n\n因扇区的单位太小，为了提高效率，操作系统将多个相邻的扇区组合在一起，形成一簇（在Linux中称为块）。为了更高效地管理磁盘，一簇只能存放一个文件的内容，文件所占用的空间只能是簇的整数倍；如果文件大小小于一簇（甚至是0字节），也要占用一簇的空间。\n引导块\n 计算机启动时需要运行一个初始化程序（自举程序），它初始化CPU、寄存器、设备控制器和内存等，接着启动操作系统。为此，自举程序找到磁盘上的操作系统内核，将它加载到内存， 并转到起始地址，从而开始操作系统的运行。\n 自举程序通常存放在ROM中，为了避免改变自举代码而需要改变ROM硬件的问题，通常只在ROM中保留很小的自举装入程序，而将完整功能的引导程序保存在磁盘的启动块上，启动块位于磁盘的固定位置。具有启动分区的磁盘称为启动磁盘或系统磁盘。\n\nWindows允许将磁盘分为多个分区，有一个分区为引导分区，它包含操作系统和设备驱动程序。\n\nWindows系统将引导代码存储在磁盘的第0号扇区，它称为主引导记录（MBR）。\n\n引导首先运行ROM中的代码，这个代码指示系统从MBR中读取引导代码。\n除了包含引导代码，MBR还包含：一个磁盘分区表和一个标志（以指示从哪个分区引导系统）\n\n当系统找到引导分区时，读取分区的第一个扇区，称为引导扇区，并继续余下的引导过程，包括加载各种系统服务。\n\n\n坏块\n由于磁盘有移动部件且容错能力弱，因此容易导致一个或多个扇区损坏。\n\n对于简单磁盘，如采用IDE控制器的磁盘，坏块可手动处理，如MS-DOS的Format命令执行逻辑格式化时会扫描磁盘以检查坏块。坏块在FAT表上会标明，因此程序不会使用它们。\n对于复杂的磁盘，控制器维护磁盘内的坏块列表。这个列表在出厂低级格式化时就已初始化，并在磁盘的使用过程中不断更新。低级格式化将一些块保留作为备用，操作系统看不到这些块。控制器可以采用备用块来逻辑地替代坏块，这种方案称为扇区备用。\n\n对坏块的处理实质上就是用某种机制使系统不去使用坏块。\n减少磁盘延迟时间的方法\n\n磁盘地址结构的设计：\n\n为什么磁盘的物理地址是（柱面号，盘面号，扇区号）而不是（盘面号，柱面号，扇区号）？\n答：读取地址连续的磁盘块时，采用这样的的地址结构可以减少磁头移动消耗的时间\n\n\n方法\n\n交替编号\n\n具体做法：让编号相邻的扇区在物理上不相邻\n原理：磁头读入一个扇区数据后需要一小段时间处理，如果逻辑上相邻的扇区在物理上也相邻，则读入几个连续的逻辑扇区，可能需要很长的“延迟时间”\n\n\n错位命名\n\n具体做法：让相邻盘面的扇区编号”错位”\n\n原理：与”交替编号”的原理相同。“错位命名法”可降低延迟时间\n\n\n\n\n\n\n5.3.3 磁盘调度算法先来先服务（FCFS）\n根据进程请求访问磁盘的先后顺序进行调度。\n\n\n优点：公平；如果请求访问的磁道比较集中的话，算法性能还算过的去\n缺点：如果有大量进程竞争使用磁盘，请求访问的磁道很分散，则FCFS在性能上很差，寻道时间长。\n\n最短路径优先(SSF) 算法\n假如我们在对磁道 6 号进行寻址时，同时发生了对 11 , 2 , 4, 14, 8, 15, 3 的请求，如果采用先来先服务的原则，如下图所示\n\n我们可以计算一下磁盘臂所跨越的磁盘数量为 5 + 9 + 2 + 10 + 6 + 7 + 12 = 51，相当于是跨越了 51 次盘面，如果使用最短路径优先，我们来计算一下跨越的盘面\n\n跨越的磁盘数量为 4 + 1 + 1 + 4 + 3 + 3 + 1 = 17 ，相比 51 足足省了两倍的时间。\n但是，最短路径优先的算法也不是完美无缺的，这种算法照样存在问题，那就是优先级 问题，\n这里有一个原型可以参考就是我们日常生活中的电梯，电梯使用一种电梯算法(elevator algorithm) 来进行调度，从而满足协调效率和公平性这两个相互冲突的目标。电梯一般会保持向一个方向移动，直到在那个方向上没有请求为止，然后改变方向。\n电梯算法需要维护一个二进制位，也就是当前的方向位：UP(向上)或者是 DOWN(向下)。当一个请求处理完成后，磁盘或电梯的驱动程序会检查该位，如果此位是 UP 位，磁盘臂或者电梯仓移到下一个更高跌未完成的请求。如果高位没有未完成的请求，则取相反方向。当方向位是 DOWN时，同时存在一个低位的请求，磁盘臂会转向该点。如果不存在的话，那么它只是停止并等待。\n我们举个例子来描述一下电梯算法，比如各个柱面得到服务的顺序是 4，7，10，14，9，6，3，1 ，那么它的流程图如下\n\n所以电梯算法需要跨越的盘面数量是 3 + 3 + 4 + 5 + 3 + 3 + 1 = 22\n电梯算法通常情况下不如 SSF 算法。\n最短寻找时间优先（SSTF）\nSSTF算法会优先处理的磁道是与当前磁头最近的磁道。可以保证每次的寻道时间最短，但是并不能保证总的寻道时间最短。（其实就是贪心算法的思想，只是选择眼前最优，但是总体未必最优）\n\n\n优点：性能较好，平均寻道时间短\n\n缺点：可能产生“饥饿”现象，磁头有可能在一个小区域内来回来去地移动。\n\nEg：本例中，如果在处理18号磁道的访问请求时又来了一个38号磁道的访问请求，处理38号磁道的访问请求时又来了一个18号磁道的访问请求。如果有源源不断的18号、38号磁道的访问请求到来的话，150、160、184号磁道的访问请求就永远得不到满足，从而产生“饥饿”现象。\n\n\n\n扫描算法（SCAN）\n又称电梯算法，只有磁头移动到最外侧磁道的时候才能往内移动，移动到最内侧磁道的时候才能往外移动。\n\n\n优点：性能较好，平均寻道时间较短，不会产生饥饿现象\n缺点：\n①只有到达最边上的磁道时才能改变磁头移动方向，事实上，处理了184号磁道的访问请求之后就不需要再往右移动磁头了。\n②SCAN算法对于各个位置磁道的响应频率不平均（如：假设此时磁头正在往右移动，且刚处理过90号磁道，那么下次处理90号磁道的请求就需要等磁头移动很长一段距离；而响应了184号磁道的请求之后，很快又可以再次响应184号磁道的请求了）\n\n\n\nLOOK调度算法\n如果在磁头移动方向上已经没有别的请求，就可以立即改变磁头移动方向。（边移动边观察，因此叫 LOOK）\n\n\n优点：比起SCAN算法来，不需要每次都移动到最外侧或最内侧才改变磁头方向，使寻道时间进一步缩短\n\n循环扫描算法（C-SCAN）\n只有磁头朝某个特定方向移动时才处理磁道访问请求，而返回时直接快速移动至起始端而不处理任何请求。\n\n\n优点：比起SCAN来，对于各个位置磁道的响应频率很平均。\n缺点：只有到达最边上的磁道时才能改变磁头移动方向，事实上，处理了184号磁道的访问请求之后就不需要再往右移动磁头了；并且，磁头返回时其实只需要返回到18号磁道即可，不需要返回到最边缘的磁道。另外，比起SCAN算法来，平均寻道时间更长。\n\nC-LOOK 调度算法\n如果磁头移动的方向上已经没有磁道访问请求了，就可以立即让磁头返回，并且磁头只需要返回到有磁道访问请求的位置即可。\n\n\n优点：比起C-SCAN算法，不需要每次都移动到最外侧或最内侧才改变磁头方向，使寻道时间进一步缩短\n\n5.3.4 固态硬盘固态硬盘的特性\n\n原理：固态硬盘(SSD)是基于闪存技术Flash Memory，属于电可擦除ROM，即EEPROM\n\n组成：\n\n\n闪存翻译层：负责翻译逻辑块号，找到对应页（Page）\n存储介质：多个闪存芯片（Flash Chip）；每个芯片包含多个块（block）；每个块包含多个页（page）。\n\n\n读写性能特性：\n\n数据是以页为单位读写的。相当于磁盘的“扇区”\n以块（bock）为单位“擦除“，擦干净的块，其中的每页都可以写一次，读无限次。\n支持随机访问，系统给定一个逻辑地址，闪存翻译层可通过电路迅速定位到对应的物理地址\n读快、写慢。要写的页如果有数据，则不能写入，需要将块内其他页全部复制到一个新的（擦除过的）块中，再写入新的页\n\n\n与机械硬盘对比\n\nSSD读写速度快，随机访问性能高，用电路控制访问位置；机诚硬盘通过移动磁臂旋转磁盘控制访问位置，有寻道时间和旋转延迟\nSSD安静无噪音、耐摔抗震、能耗低、造价更贵\nSSD的一个”块”被擦除次数过多（重复写同一个块）可能会坏掉，而机械硬盘的扇区不会因为写的次数太多而坏掉\n\n\n\n磨碎均衡\n思想：将“擦除”平均分布在各个块上，以提升使用寿命\n\n动态磨损均衡：写入数据时，优先选择累计擦除次数少的新闪存块。\n静态磨损均衡：SSD监测并自动进行数据分配、迁移，让老旧的闪存块承担以读为主的储存任务，让较新的闪存块承担更多的写任务\n\n\n例：某固态硬盘采用磨损均衡技术，大小为240B=1TB，闪存块的擦写寿命只有210=1K次。某男子平均每天会对该固态硬盘写237B=128GB数据。在最理想的情况下，这个固态硬盘可以用多久？\nSSD采用磨损均衡技术，最理想情况下，SSD中每个块被擦除的次数都是完全均衡的。\n\n1T/128G=8因此，平均8天，每个闪存块需要擦除一次。\n每个闪存块可以被擦除1K次，因此经过8K天，约23年，固态使用到寿命。\n\n","slug":"操作系统","date":"2024-09-01T00:00:00.000Z","categories_index":"理论","tags_index":"","author_index":"Gueason"},{"id":"0bba57f59c9514b7a259aa10ab4307d3","title":"虚拟机相关问题","content":"一、VirtualBox 虚拟机 ping 不通sudo vim /etc/resolv.conf\n在里面输入 nameserver 8.8.8.8保存\n二、mysql密码重置1.首先输入“service mysqld status”查看当前mysql服务状态\n2.输入“killall -TERM mysqld”命令停止所有的mysqld进程。\n3.输入“service mysqld stop”命令停止mysqld服务。\n4.输入“mysqld_safe —skip-grant-tables &amp;”命令以无密码方式进入MySQL安全模式。\n5.输入“mysql -u root”并按回车键即可。\n6.输入“use mysql;”挂载数据库。\n7.输入”update user set password=password(“Newpassword”) where user=’root’;”将Root密码修改为Newpassword。\n8.输入”flush privileges;”更新权限。\n9.输入“quit”并按回车键退出。\n10.输入”service mysqld restart”重启mysqld服务。\n11.通过命令”pgrep -l mysqld” 查看进程\n12.”service mysqld status”查看状态\n13.输入“mysql -u root -p”并按回车键提示输入密码。\n在Linux上跳过密码登录MySQL报错“-bash: mysqld_safe: command not found“\nmysqld_safe --skip-grant-tables &amp; mysql -uroot -p报错\n解决\n执行命令mysqld --user=mysql --skip-grant-tables --skip-networking &amp;\n通过mysql -uroot -p命令直接跳过密码登录，不需要输入密码。\nmysql退出三种方法：mysql &gt; exit;mysql &gt; quit;mysql &gt; q;\n","slug":"虚拟机相关问题","date":"2024-08-01T00:00:00.000Z","categories_index":"技术栈","tags_index":"","author_index":"Gueason"},{"id":"c8b121ec7894987ab65ba8cb5cccd987","title":"机器学习初级（搬运）","content":"相关学习链接谷歌机器学习教育课程\n线性回归  | Machine Learning  | Google for Developers\n动手学深度学习\nhttps://zh.d2l.ai/index.html\n李沐学AI\nhttps://courses.d2l.ai/zh-v2/\nhttps://space.bilibili.com/1567748478/channel/series\n实用机器学习\nhttps://c.d2l.ai/stanford-cs329p/index.html\n一、绪论1.1 Machine Learning definitionArthur Samuel (1959). Machine Learning: “Field of study that gives computers the ability to learn without being explicitly programmed.”在没有明确设置的情况下，使计算机具有学习能力的研究领域。\nTom Mitchell (1998) Well-posed Learning Problem: “A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.”字幕翻译：计算机程序从经验E中学习解决某一任务T进行某一性能度量P，通过P测定在T上的表现因经验E而提高。\nDeepL：如果一个计算机程序在某个任务T和某个性能指标P方面的性能随着经验E的增加而提高，那么它就被称为从经验E中学习。\n样本(sample)、特征(feature)、标签(label)样本（Sample）是用于描述一个事件或一个对象的记录的集合，也可以理解为模型训练和学习的基础数据单元。\n特征（Feature）是描述一个实例的属性或特点，也可以称为自变量（Independent Variable）或输入变量（Input Variable）。特征构成了机器学习模型的输入部分，用于描述样本或数据点。模型通过学习样本的特征与其对应的标签（或输出）之间的关系来做出预测或分类。\n特征可以是任何类型的数据，包括数字、文本、图像和音频等。根据取值不同，特征可以分为离散特征和连续特征。\n标签（Label）是指与样本相关联的目标值或预期输出。它代表了我们希望模型从输入数据中学习并预测的结果。\n标签可以是离散的类别值，也可以是连续的数值。\n在分类问题中，标签通常表示样本所属的类别或分类结果。例如，在图像分类任务中，标签可以是图片中物体的类别（如“猫”、“狗”等）。\n在回归问题中，标签是连续的数值，代表了某种度量或预测结果，如房价、股票价格或温度预测等。\n标签为机器学习模型提供了明确的训练目标和方向。\n在监督学习任务中，模型需要通过学习从输入数据中提取特征，并将这些特征映射到正确的标签上。\n标签为模型提供了一个明确的标准，使模型能够评估其预测的准确性，并据此调整其参数和结构，以优化其性能。\n每个样本通常由一组特征（Feature）组成，这些特征可以是数值、文本、图像等各种形式的数据。\n样本可以看作是机器学习模型的输入，用于训练和优化模型。\n具体来说，样本是数据集中的一个元素，用于表示一个独立的数据点或实例。\n样本通常包括一个标签（Label），即与样本对应的预期输出或结果。\n模型通过比较预测结果与样本标签之间的差异，来优化自身的参数和结构，从而提高预测性能。\n举一个例子，假设我们正在构建一个用于识别手写数字的图像分类模型。\n在这个例子中，每一张手写数字的图片都可以看作是一个样本。\n每张图片中的像素值、颜色、纹理等信息都可以提取为特征。\n同时，我们还知道每张图片对应的真实数字（如0-9），这就是样本的标签。\n1.2 Supervised Learning1.2.1 Definition\n根据已有的数据集，知道输入和输出结果之间的关系。根据这种已知的关系，训练得到一个最优的模型。\n在监督学习中训练数据既有特征(feature)又有标签(label)，通过训练，让机器可以自己找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标签。\n简单理解：可以把监督学习理解为我们教机器如何做事情。\n\n1.2.2 Classification\n监督学习任务主要包括分类和回归两种类型，在监督学习中，数据集中的样本被称为“训练样本”，并且每个样本都有一个输入特征和相应的标签（分类任务）或目标值（回归任务）。\n分类（Classification）： 在分类任务中，目标是将输入数据分到预定义的类别中。每个类别都有一个唯一的标签。算法在训练阶段通过学习数据的特征和标签之间的关系来构建一个模型。然后，在测试阶段，模型用于预测未见过的数据的类别标签。例如，将电子邮件标记为“垃圾邮件”或“非垃圾邮件”，将图像识别为“猫”或“狗”。\n回归（Regression）： 在回归任务中，目标是预测连续数值的输出。与分类不同，输出标签在回归任务中是连续的。算法在训练阶段通过学习输入特征和相应的连续输出之间的关系来构建模型。在测试阶段，模型用于预测未见过的数据的输出值。例如，预测房屋的售价、预测销售量等。\n1.2.3 Common Algorithms\n监督学习算法种类众多，有着极其广泛的应用，下面是一些常见的监督学习算法：\n支持向量机（Support Vector Machine，SVM）：SVM是一种用于二分类和多分类任务的强大算法。它通过找到一个最优的超平面来将不同类别的数据分隔开。SVM在高维空间中表现良好，并且可以应用于线性和非线性分类问题。\n决策树（Decision Trees）：决策树是一种基于树结构的分类和回归算法。它通过在特征上进行递归的二分决策来进行分类或预测。决策树易于理解和解释，并且对于数据的处理具有良好的适应性。\n逻辑回归（Logistic Regression）：逻辑回归是一种广泛应用于二分类问题的线性模型。尽管名字中带有”回归”，但它主要用于分类任务。逻辑回归输出预测的概率，并使用逻辑函数将连续输出映射到[0, 1]的范围内。\nK近邻算法（K-Nearest Neighbors，KNN）：KNN是一种基于实例的学习方法。它根据距离度量来对新样本进行分类或回归预测。KNN使用最接近的K个训练样本的标签来决定新样本的类别。\n1.2.4 Applications\n图像识别：监督学习在图像识别任务中非常常见。例如，将图像分类为不同的物体、场景或动作，或者进行目标检测，找出图像中特定对象的位置。\n自然语言处理：在自然语言处理任务中，监督学习用于文本分类、情感分析、机器翻译、命名实体识别等。\n语音识别：监督学习在语音识别领域被广泛应用，例如将语音转换为文本、说话者识别等。\n医学诊断：在医学领域，监督学习可以用于疾病诊断、影像分析、药物发现等。\n1.3 Unsupervised Learning1.3.1 Definition\n我们不知道数据集中数据、特征之间的关系，而是要根据聚类或一定的模型得到数据之间的关系。\n在无监督学习中数据只有特征(feature)无标签(label)，是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。\n简单理解：比起监督学习，无监督学习更像是自学，让机器学会自己做事情。\n1.3.2 Classification\n聚类（Clustering）：聚类是将数据样本分成相似的组别或簇的过程。它通过计算样本之间的相似性度量来将相似的样本聚集在一起。聚类是无监督学习中最常见的任务之一，常用于数据分析、市场细分、图像分割等。\n降维（Dimensionality Reduction）：降维是将高维数据转换为低维表示的过程，同时尽可能地保留数据的特征。降维技术可以减少数据的复杂性、去除冗余信息，并可用于可视化数据、特征提取等。常见的降维方法有主成分分析（PCA）和t-SNE等。\n关联规则挖掘（Association Rule Mining）：关联规则挖掘用于发现数据集中项之间的关联和频繁项集。这些规则描述了数据集中不同项之间的关联性，通常在市场篮子分析、购物推荐等方面应用广泛。\n异常检测（Anomaly Detection）：异常检测用于识别与大多数样本不同的罕见或异常数据点。它在检测异常事件、欺诈检测、故障检测等领域有着重要的应用。\n1.3.3 Common Algorithms\nK均值聚类（K-Means Clustering）：K均值聚类是一种常用的聚类算法，它将数据样本分成K个簇，使得每个样本与所属簇中心的距离最小化。\n主成分分析（Principal Component Analysis，PCA）：PCA是一种常用的降维算法，它通过线性变换将高维数据投影到低维空间，以保留最重要的特征。\n关联规则挖掘（Association Rule Mining）：关联规则挖掘是一种发现数据集中项之间关联性的方法，它常用于市场篮子分析、购物推荐等领域。\n异常检测（Anomaly Detection）：异常检测算法用于识别与大多数样本不同的罕见或异常数据点。常见的方法包括基于统计的方法、基于聚类的方法和基于生成模型的方法等。\n1.3.4 Applications\n无监督学习在数据挖掘、模式识别、特征学习等应用场景发挥着重要作用。通过无监督学习，我们可以从未标记的数据中获得有用的信息和洞察力，为其他任务提供有益的预处理步骤，并且有助于更好地理解和利用数据。：\n聚类与分组：无监督学习中的聚类算法可以帮助将数据样本分成相似的组别或簇，例如在市场细分中将顾客分成不同的群体、在图像分割中将图像区域分割成不同的物体等。\n特征学习与降维：无监督学习的降维算法如PCA和t-SNE可以用于特征学习和可视化高维数据，例如在图像、音频和自然语言处理中，以及用于数据压缩和可视化。\n异常检测：无监督学习中的异常检测算法可用于发现与大多数数据样本不同的罕见或异常数据点。这在欺诈检测、故障检测和异常事件监测等场景中具有重要应用。\n关联规则挖掘：无监督学习的关联规则挖掘算法可用于发现数据集中项之间的关联性，常应用于市场篮子分析、购物推荐等领域。\n1.4 Semi-supervised Learning1.4.1Definition\n半监督学习的目标是利用同时包含有标签和无标签的数据来构建一个模型，使得模型能够在测试阶段更好地泛化到新的、未见过的数据。\n半监督学习介于监督学习和无监督学习之间。在半监督学习中，训练数据同时包含有标签的数据和无标签的数据。\n与监督学习不同的是，半监督学习的训练数据中只有一小部分样本是带有标签的，而大部分样本是没有标签的。通常情况下，获取带有标签的数据可能会比较昂贵或耗费大量的时间，而采集无标签的数据则相对容易和便宜。\n在半监督学习中，无标签的数据可以起到两个重要作用：\n利用未标记数据的信息：未标记数据可能包含对数据分布、结构和隐含特征的有用信息，这些信息可以帮助模型更好地进行泛化。\n利用标记数据的传播效应：通过利用标记数据与无标签数据之间的数据分布相似性，可以通过传播标签信息到无标签样本，进而增强模型的性能。\n\n1.4.2 Classification\n半监督分类（Semi-supervised Classification）：在半监督分类中，训练数据中同时包含带有标签的样本和无标签的样本。模型的目标是利用这些标签信息和无标签数据的分布信息来提高分类性能。半监督分类算法可以在分类任务中利用未标记数据来扩展有标签数据集，从而提高模型的准确性。\n半监督回归（Semi-supervised Regression）：半监督回归任务与半监督分类类似，但应用于回归问题。模型通过有标签的数据和无标签数据进行训练，以提高对未标记数据的回归预测准确性。\n半监督聚类（Semi-supervised Clustering）：半监督聚类算法将有标签数据和无标签数据同时用于聚类任务。它们可以通过结合数据的相似性信息和标签信息，来更好地识别潜在的簇结构。\n半监督异常检测（Semi-supervised Anomaly Detection）：半监督异常检测任务旨在从同时包含正常样本和异常样本的数据中，利用有限的标签信息来检测异常。这在异常样本较少的情况下特别有用。\n生成对抗网络（GANs）中的半监督学习：GANs可以被用于实现半监督学习。在这种情况下，生成器和判别器网络可以使用有标签和无标签的样本，以提高生成模型的性能。\n1.4.3 Common Algorithms\n半监督学习算法可以在不同的问题和数据集上发挥作用。选择合适的半监督学习算法取决于问题的特性、可用的有标签和无标签数据量，以及算法的性能和复杂度要求。半监督学习在处理数据有限或数据标记成本高昂的场景下具有重要的应用价值。以下是一些常见的半监督学习算法：\n自训练（Self-Training）：自训练是一种简单的半监督学习方法。它通过使用有标签数据训练一个初始模型，然后使用该模型对未标记数据进行预测，并将置信度较高的预测结果作为伪标签，将未标记数据添加到有标签数据中，然后重新训练模型。\n协作训练（Co-Training）：协作训练是一种使用多个视图或特征的半监督学习方法。它通过将数据划分为两个或多个视图，并在每个视图上独立训练模型。然后，模型之间相互交互并使用对方的预测结果来增强训练。\n半监督支持向量机（Semi-Supervised Support Vector Machines）：半监督支持向量机是基于支持向量机的半监督学习方法。它利用有标签数据和未标记数据之间的关系来学习一个更好的分类器。\n生成式半监督学习（Generative Semi-Supervised Learning）：这类方法尝试使用生成模型来建模数据的分布，并利用有标签和无标签数据共同训练生成模型，以提高对未标记数据的预测。\n半监督深度学习：近年来，许多深度学习方法已经扩展到半监督学习。这些方法通过在深度神经网络中引入半监督性质，如半监督自编码器（Semi-Supervised Autoencoders）等，来利用未标记数据的信息。\n图半监督学习（Graph-based Semi-Supervised Learning）：图半监督学习方法利用数据样本之间的关系来辅助半监督学习。这些方法通常利用图模型或图卷积神经网络（GCN）来利用数据的拓扑结构。\n1.4.4 Applications\n半监督学习在许多实际应用场景中具有重要的应用价值，尤其在数据有限或数据标记成本高昂的情况下。以下是一些半监督学习的应用场景：\n自然语言处理：在自然语言处理任务中，很多时候获取大规模的标记数据是非常昂贵和耗时的。半监督学习可以利用少量有标签的文本数据和大量未标签的文本数据来提高文本分类、情感分析、命名实体识别等任务的性能。\n图像识别和计算机视觉：在图像识别和计算机视觉领域，获取大规模的标记图像数据也可能是困难的。半监督学习可以在少量有标签图像和大量未标签图像上进行训练，以提高图像分类、目标检测等任务的准确性。\n数据聚类：在聚类任务中，半监督学习可以将有标签和未标签数据结合起来进行聚类，从而提高聚类结果的准确性和稳定性。\n医学图像和诊断：在医学图像分析和诊断中，获取大量标记的医学图像数据可能是困难的。半监督学习可以在少量有标签医学图像和大量未标签医学图像上进行训练，提高医学图像分割、病变检测等任务的性能。\n机器人控制：在机器人控制领域，半监督学习可以帮助机器人在未知环境中进行自主决策和学习，从而提高其任务执行能力。\n图像生成和数据增强：在生成式模型中，半监督学习可以结合有标签和未标签数据来训练模型，以提高生成模型的质量和多样性。\n1.5 Reinforcement Learning1.5.1Definition\n强化学习是让一个智能体（agent）在环境中通过尝试和错误来学习行为策略。智能体通过与环境进行交互，根据奖励信号来调整其行为策略，以达到最大化累积奖励的目标。\n在强化学习中，智能体不需要明确地告诉如何执行任务，而是通过尝试和错误的方式进行学习。当智能体在环境中采取某个动作时，环境会返回一个奖励信号，表示该动作的好坏程度。智能体的目标是通过与环境交互，学习到一种最优策略，使其在长期累积的奖励最大化。\n1.5.2 Classification\n基于值的强化学习（Value-Based Reinforcement Learning）：基于值的强化学习方法旨在学习价值函数，即给定状态或状态-动作对的值，代表了智能体在该状态或状态-动作对上能够获得的累积奖励的估计值。这些方法通常通过使用贝尔曼方程或其变种来更新价值函数，并使用它来选择动作。\n基于策略的强化学习（Policy-Based Reinforcement Learning）：基于策略的强化学习方法直接学习策略函数，即将状态映射到动作的映射。策略可以是确定性的（对于每个状态只输出一个动作）或是概率性的（对于每个状态输出动作的概率分布）。这些方法通常通过梯度上升方法来更新策略参数，以最大化累积奖励。\n基于模型的强化学习（Model-Based Reinforcement Learning）：基于模型的强化学习方法学习环境的模型，即从状态和动作预测下一个状态和奖励。然后，它可以使用学到的模型进行规划和决策，而无需真实地与环境进行交互。这样可以提高样本效率和规划效率。\n深度强化学习（Deep Reinforcement Learning）：深度强化学习将深度神经网络与强化学习相结合。它通常使用深度神经网络来近似值函数或策略函数。深度强化学习在处理高维状态空间和动作空间的任务时表现出色。\n多智能体强化学习（Multi-Agent Reinforcement Learning）：多智能体强化学习研究多个智能体在相互作用环境中的学习问题。在这种情况下，每个智能体的策略和动作会影响其他智能体的状态和奖励，因此学习变得更加复杂。\n1.5.3 Common Algorithms\nQ-Learning：Q-Learning是一种基于值的强化学习算法。它通过学习一个值函数（Q函数）来表示在给定状态下采取某个动作的累积奖励。Q-Learning使用贝尔曼方程更新Q值，并使用贪心策略来选择动作。\nSARSA：SARSA是另一种基于值的强化学习算法。它与Q-Learning类似，但不同之处在于它在学习和决策阶段都使用当前策略的动作来更新Q值。\nDQN（Deep Q Network）：DQN是一种深度强化学习算法，结合了深度神经网络和Q-Learning。它使用深度神经网络来近似Q函数，通过经验回放和目标网络来稳定训练。\nA3C（Asynchronous Advantage Actor-Critic）：A3C是一种基于策略的强化学习算法，它结合了Actor-Critic方法和异步训练。A3C使用多个智能体并行地训练，以提高样本效率。\nPPO（Proximal Policy Optimization）：PPO是一种基于策略的强化学习算法，它通过限制更新幅度来稳定训练。PPO在深度强化学习中表现出色，并被广泛应用于各种任务。\nTRPO（Trust Region Policy Optimization）：TRPO是另一种基于策略的强化学习算法，它使用限制步长的方法来保证更新策略时不会使性能变差。\n1.5.4 Applications\n强化学习在许多实际应用场景中具有广泛的应用，尤其是那些需要自主决策和学习的任务。强化学习能够使智能体从与环境的交互中学习，并根据学到的知识做出适当的决策，以达到预定的目标或最大化累积奖励。由于强化学习的自主学习和决策特性，它在许多自主系统和智能系统中都有重要的应用潜力。以下是一些强化学习的应用场景：\n自动驾驶：强化学习可以应用于自动驾驶领域，使车辆能够根据环境和交通状况做出决策，例如规划路径、避免障碍物和遵守交通规则。\n机器人控制：强化学习可以帮助机器人在未知环境中进行自主探索和学习，以完成复杂的任务，例如导航、抓取物体和人机交互。\n游戏：强化学习在游戏玩法中有广泛的应用。例如，使用强化学习训练智能体来玩电子游戏、围棋、扑克等，使其能够与人类玩家媲美甚至超越。\n医疗治疗：强化学习可以在医疗领域中应用于个性化治疗和药物治疗决策，根据患者的情况和病情做出合适的治疗计划。\n语音识别和自然语言处理：强化学习可以应用于语音识别和自然语言处理任务，使智能体能够更好地理解和生成自然语言。\n\n二、单变量线性回归（Linear Regression with One Variable）用人话讲明白线性回归LinearRegression - 知乎 (zhihu.com)\n2.1 参数(Parameter)模型参数：模型内部的配置变量，可以通过数据估计（学习）模型参数的值超参数：模型的外部配置变量，需要手动设置，好的超参数可以进一步提高模型的性能\n模型参数的特点：1.进行模型预测时需要模型参数；2.可定义模型功能；3.可使用数据估计/学习得到；4.一般不用手动设置；5.作为学习模型的一部分保留比如：神经网络中的权重，线性回归中的系数\n超参数的特点：1.常应用于估计模型参数的过程中，在开始学习之前就要设置好；训练过程不影响超参数2.通常要手动设置3.通常不能从数据中估计（学习）得到，一般需要通过经验设定；4.定义关于模型的更高层次的概念，如复杂性或学习能力比如：学习率，K邻域中的K\n2.2 训练集（train set）、验证集（validation set）、验证集（validation set）训练集（train set） —— 用于训练模型（拟合参数）：即模型拟合的数据样本集合，如通过训练拟合一些参数来建立一个分类器。\n验证集（validation set）—— 用于确定网络结构或者控制模型复杂程度的超参数（拟合超参数）：是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估。 通常用来在模型迭代训练时，用以验证当前模型泛化能力（准确率，召回率等），防止过拟合的现象出现，以决定如何调整超参数。\n测试集（test set） —— 用来评估模最终模型的性能如何（评价模型好坏）：测试集没有参与训练，主要是测试训练完成的模型的准确性，但不能作为调参、选择特征等算法相关的选择的依据。\n基于数据集是否参与了训练过程，可通过下图来理解，即测试集完全没参与训练，它只是用于测试，评估模型到底性能如何\n\n\n在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。\n如果数据集仅仅分为训练集和测试集，那么通过修改一些超参数（不能通过学习来自动调整的参数）来降低误差，但是这种方法在实际中的应用效果却并没有想象的那么好。这是因为超参数都是基于测试集来调整的，就相当于把测试集当成了训练超参数的数据。这样对于新的数据效果不一定会更好。\n于是就想出一种解决办法，即保留一个数据集作为验证集，在这些步骤做完之后再进行最终的验证。\n\n2.3 交叉验证（Cross-Validation）     交叉验证是在机器学习建立模型和验证模型参数时常用的办法，一般被用于评估一个机器学习模型的表现。更多的情况下，我们也用交叉验证来进行模型选择(model selection)。\n     交叉验证把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本。\n     交叉验证用在数据不是很充足的时候。如果数据样本量小于一万条，我们就会采用交叉验证来训练优化选择模型。如果样本大于一万条的话，我们一般随机的把数据分成三份，一份为训练集（Training Set），一份为验证集（Validation Set），最后一份为测试集（Test Set）。用训练集来训练模型，用验证集来评估模型预测的好坏和选择模型及其对应的参数。把最终得到的模型再用于测试集，最终决定使用哪个模型以及对应参数。\n留出法 （holdout cross validation）\n随机的将样本数据分为两部分（比如： 70%的训练集，30%的测试集），然后用训练集来训练模型，在测试集上验证模型及参数。接着，我们再把样本打乱，重新选择训练集和测试集，继续训练数据和检验模型。最后选择损失函数评估最优的模型和参数。\nk 折交叉验证（k-fold cross validation）\nk 折交叉验证通过对 k 个不同分组训练的结果进行平均来减少方差， 因此模型的性能对数据的划分就不那么敏感。\n\n第一步，不重复抽样将原始数据随机分为 k 份。\n第二步，每一次挑选其中 1 份作为测试集，剩余 k-1 份作为训练集用于模型训练。\n第三步，重复第二步 k 次，这样每个子集都有一次机会作为测试集，其余机会作为训练集。在每个训练集上训练后得到一个模型，用这个模型在相应的测试集上测试，计算并保存模型的评估指标，\n第四步，计算 k 组测试结果的平均值作为模型精度的估计，并作为当前 k 折交叉验证下模型的性能指标。k 一般取 10， 数据量小的时候，k 可以设大一点，这样训练集占整体比例就比较大，不过同时训练的模型个数也增多。 数据量大的时候，k 可以设小一点。\n\n\n\n留一法（Leave one out cross validation）\n当 k＝m 即样本总数时， 每次的测试集都只有一个样本，要进行 m 次训练和预测。\n这个方法用于训练的数据只比整体数据集少了一个样本，因此最接近原始样本的分布。 但是训练复杂度增加了，因为模型的数量与原始数据样本数量相同。 一般在数据缺乏时使用。 样本数很多的话，这种方法开销很大。\nBootstrap\n\n数据假设要分成10组，则先设置一个采样比例，比如采样比例70%。则10组数据是每次从原始数据集中随机采样总数70%的数据构成训练集1，没有选中的样本作为测试集1；然后把数据放回，再随机采样总数70%的数据构成训练集2，没选中的作为测试集2……以此类推，放回式采样10组。\n训练生成10个模型\n计算平均测试误差来评估当前参数下的模型性能\n\n2.4 损失函数/代价函数（Loss/Cost Function）在机器学习中，损失函数是代价函数的一部分，而代价函数则是目标函数的一种类型。\n损失函数(Loss Function)： 用于定义单个训练样本与真实值之间的误差，也就是就算一个样本的误差，比如我们想要分类，就是预测的类别和实际类别的区别，是一个样本的哦，用L表示。代价函数(Cost Function)： 用于定义单个批次/整个训练集样本与真实值之间的误差，也就是所有样本的误差的总和的平均，也就是损失函数的总和的平均，有没有这个平均其实不会影响最后的参数的求解结果。目标函数(Objective Function)： 泛指任意可以被优化的函数。\n在机器学习中，我们想让预测值无限接近于真实值，所以需要将差值降到最低（在这个过程中就需要引入损失函数）。而在此过程中损失函数的选择是十分关键的，在具体的项目中，有些损失函数计算的差值梯度下降的快，而有些下降的慢，所以选择合适的损失函数也是十分关键的。\n每一个样本经过模型后会得到一个预测值，然后得到的预测值和真实值的差值就成为损失（当然损失值越小证明模型越是成功），我们知道有许多不同种类的损失函数，这些函数本质上就是计算预测值和真实值的差距的一类型函数，然后经过库（如pytorch，tensorflow等）的封装形成了有具体名字的函数。\n输入的feature（或称为x）需要通过模型（model）预测出y，此过程称为向前传播（forward pass），而要将预测与真实值的差值减小需要更新模型中的参数，这个过程称为向后传播（backward pass），其中我们损失函数（lossfunction）就基于这两种传播之间，起到一种有点像承上启下的作用，承上指：接収模型的预测值，启下指：计算预测值和真实值的差值，为下面反向传播提供输入数据。\n2.4.1回归损失(Regression Loss)L1 Loss也称Mean Absolute Error，即平均绝对误差(MAE)，计算预测值与真实值的差的绝对值，衡量预测值与真实值之间距离的平均误差幅度，范围为0到正无穷。\n\n\nHypotheis:\n\n\n\nParameters:\n​                                                                          θ0,θ1\n\n\n\nCost function:\n\n\n\nGoal:\n\n\n优点：\nL1损失函数对离群点（Outliers）或者异常值更具有鲁棒性。\n缺点：\n在零处不可导，求解效率低，收敛速度慢；梯度始终相同，即使很小的损失值，梯度也很大，这样不利于模型的收敛。针对它的收敛问题，一般的解决办法是在优化算法中使用变化的学习率，在损失接近最小值时降低学习率。\nL2 Loss也称为Mean Squred Error，即均方差(MSE)，计算的是预测值与真实值之间距离的平方和，范围同为0到正无穷。\n\n优点：\n收敛速度快，能够对梯度给予合适的惩罚权重，而不是“一视同仁”，使梯度更新的方向可以更加精确。缺点：\n对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性。\n2.4.2分类损失(Classification Loss)Entropy\n熵（信息熵），它可以很好地描述事件的不确定性，衡量的是得到的信息量的期望。同时也反映了在给定概率分布p中样本值的平均信息量的条件下，它的概率分布有多不可预测。一个事件的不确定性就越大，其信息量越大，它的信息熵就越高。\n交叉熵损失（Cross-entropy loss）衡量了模型预测结果与实际结果之间的差距，是优化模型参数的关键指标之一。交叉熵损失的公式如下： \n\n\nK-L DivergenceKL散度。对于交叉熵损失，除了我们在这里使用预测概率的对数（log(q(i))）外，它看起来与上面熵的方程非常相似。 如果我们的预测是完美的，那就是预测分布等于真实分布，此时交叉熵就等于熵。 但是，如果分布不同，则交叉熵将比熵大一些位数。交叉熵超过熵的量称为相对熵，或更普遍地称为库尔贝克-莱布里埃发散度（KL Divergence）。总结如下：\n\n2.5 梯度下降(Gradient Descent)梯度下降通过不断迭代计算函数的梯度，判断该点的某一方向和目标之间的距离，最终求得最小的损失函数和相关参数，为建立线性模型提供支持。\n梯度下降是一种广泛用于求解线性和非线性模型最优解的迭代算法，它的中心思想在于通过迭代次数的递增，调整使得损失函数最小化的权重。\n它的作用是用于优化一个目标函数，如果要最小化一个损失函数，使用的就是梯度下降法，如果要最大化一个效用函数，使用的是梯度上升法。\n作用：\n\n梯度下降梯度下降的目的就是求函数的极小值点，最小化损失函数。\n\n\n损失函数就是一个自变量为算法的参数，函数值为误差值的函数。所以梯度下降就是找让误差值最小时候算法取的参数。\n\n找到 J（θ）的最小值：\n\nJ(θ)的真正图形类似是一个凸函数，只有一个全局最优解，所以不必担心像上图一样找到局部最优解\n\n思想：\n先任取点（x0,f(x0))，求f(x)在该点x0的导数f’(x0),在用x0减去导数值f’(x0),计算所得就是新的点x1,然后再用x1减去f’(x1)得x2…。\n以此类推，循环多次，改变x的值使得导数的绝对值变小，无限接近极小值点。\n当导数小于0时候，让目前x值大一点点，再看它导数值。\n当导数大于0时候，让目前x值减小一点点，再看它导数值。\n当导数接近0时候，就得到想要的自变量x了。也就是说找到这个算法最佳参数，使得拟合曲线与真实值误差最小。\n官方点讲大概意思就是：梯度下降就是找让误差值最小时的算法取的参数\n\n损失函数的梯度（即偏导数）为\n\n按参数 θ 的梯度负方向，来更新θ，即梯度下降算法为\nrepeat until convergence {\n\n}\nα在梯度下降算法中被称作为学习率或者步长，a的大小，如果a太小，则迭代很多次才找到最优解，若a太大，可能跳过最优解。\n\n另外，在不断迭代的过程中，梯度值会不断变小，所以θ1的变化速度也会越来越慢，所以不需要使速率a的值越来越小。它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，所以在下图的梯度下降过程中可以看到，它是一个近乎直线的下降过程，直接前往最低点。\n\n当梯度下降到一定数值后，每次迭代的变化很小，这时可以设定一个阈值，只要变化小鱼该阈值，就停止迭代，而得到的结果也近似于最优解。\n\n算法求解过程：\n1）确定当前位置的损失函数的梯度，对于θi,其梯度表达式如下：\n\n2）用步长乘以损失函数的梯度，得到当前位置下降的距离\n3）确定是否所有的θi,梯度下降的距离都小于ε，如果小于ε则算法终止，当前所有的θi(i=0,1,…n)即为最终结果。否则进入步骤4\n4）更新所有的θ，对于θi，其更新表达式如下。更新完毕后继续转入步骤1.\n\n注意：\n\n更新方程时需要同时更新θ\n\n先同时计算右边部分，然后同时更新θ\n\n\n下面用线性回归的例子来具体描述梯度下降\n假设我们的样本是\n\n损失函数如前面先决条件所述：\n\n则在算法过程步骤1中对于θi 的偏导数计算如下：\n!\n由于样本中没有x0上式中令所有的xj0为1.\n步骤4中θi的更新表达式如下：\n\n2.6 梯度消失、爆炸详解机器学习中的梯度消失、爆炸原因及其解决方法_梯度消失的原因及解决方法-CSDN博客\n\n\n\n\n三、多变量线性回归(Linear Regression with Multiple Variables)3.1 向量化（vectorization）向量化就是非常简单的去除for循环，在代码中使用for循环会让算法变得非常低效。在深度学习的领域中，我们的数据集是非常非常庞大的，如果使用for循环的话，那么代码的运行会花费很长很长的时间。由于numpy中的dot函数通过计算机硬件实现向量化，所以计算机可以在t0时得到向量w和x的所有值，并且同时将w和x相乘，然后在t1，计算机调用专门的硬件去计算这16个数字的和，而不需要一个接一个的做加法运算了，在大型数据集上使用向量可以更快的更高效的运行算法。\n\n举例\n在线性回归的初级版本中，只有一个特征x（房子的大小），可以预测y（房子的价格），所以该模型是fw,b(x)=wx+b，现在不仅考虑这一个特征，还知道其他特征，比如卧室数量、楼层数量以及房子年代来预测房子的价格使用x1 x2 x3 x4来表示这4个特征。\n\n\n\nw和b两个向量之间点积运算，就是w1x1 +w2x2 +w3x3 +……+wnxn下面的表达式和上面的表达式是一样的，点积表示法可以让我们用更少的字符，更紧凑的表示模型。这种具有多个输入特征的线性回归称为多元线性回归，这与只有一个特征变量的回归形成对比，将这个模型称为多元线性回归。\n3.2 用于多元线性回归的梯度下降法\n\n3.3 特征缩放（Feature Scaling）特征向量（Eigenvector）特征向量：一个向量经过线性变换，仍留在它所张成的空间中\n特征值：描述特征向量经过线性变换后的缩放程度\n大部分的向量经过线性变换都离开了它所张成的空间。 如图中黄色的向量经过线性变换 [3102] ,离开了它所张成的通过原点的直线张成的空间。\n\n\n但是也有一部分向量留在了它所张成的空间，这样线性变换起到的作用就是压缩或者拉升如：\n特别的, 对于 [−11] 经过线性变换 [3102] ，相当于将该向量拉升了两倍。\n对于处在它所在的对角线上的任何向量也仅被拉伸了2倍。\n他们的共同点就是线性变换后留在了原来向量所张成的线性空间里边， 对于其他的向量则离开他原来张成的向量空间\n\n\n因此将这些特殊的向量称为“特征向量” ， 每个特征向量都有一个对应的值，称为“特征值”，目的是衡量特征向量经过变换后的压缩或者拉伸因子。\n\n特征缩放意义在机器学习算法处理数据时通常是忽略数据的单位信息的，或者说不知道这些数据代表什么。比如50kg和50公里代表着两个不同的度量，人类是很容易区分它们的。但是对于机器来说，这两个数据是一样的。\n特征矩阵各个维度的取值通常是不一样的，此时如果采用欧几里得距离来衡量两个特征的距离，那么最终的距离将严重取决于取值范围跨度大的特征维度，比如说在代表人属性的特征向量有两个维度，分别是年龄和身高，其中年龄的取值范围可以是[1, 100]，身高的取值范围是[0.4, 2.5]（单位：米），那么两个特征向量的距离将严重取决于年龄这个特征，身高基本上对两者的距离没有太大的影响。但通常我们不希望我们的算法一开始就偏向于某个特征。\n\n数量级的差异将导致量级较大的属性占据主导地位\n数量级的差异将导致迭代收敛速度减慢\n依赖于样本距离的算法对于数据的数量级非常敏感\n\n好处：\n\n缩放后的特征矩阵，各个维度都具有相同的重要性\n\n加快梯度下降，同时防止梯度爆炸(消除过大值的影响)\n\n提升模型的精度：在机器学习算法的目标函数中使用的许多元素（例如支持向量机的 RBF 内核或线性模型的 l1 和 l2 正则化)，都是假设所有的特征都是零均值并且具有同一阶级上的方差。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们期望的那样，从其他特征中学习\n\n提升收敛速度：对于线性模型来说，数据归一化后，寻找最优解的过程明显会变得平缓，更容易正确地收敛到最优解\n\n标准化（Standardization/Z-Score Normalization）将数据变换为均值为0，标准差为1的分布切记，并非一定是正态的；                                                            \nStandardization会改变数据的均值、标准差(严格的说，均值和标准差变了，分布也是变了，但分布种类依然没变，原来是啥类型，现在就是啥类型)，实际数据大部分都是正态分布或近似正态，但本质上的分布并不一定是标准正态，完全取决于原始数据是什么分布。\n归一化（Rescaling/Normalization）将一列数据变化到某个固定区间(范围)中，通常，这个区间是[0, 1]，广义上，可以是各种区间，比如映射到[0，1]一样可以继续映射到其他范围，图像中可能会映射到[0,255]，其他情况可能映射到[-1,1]；\n\n均值归一化(Mean Normalization)将数值范围缩放到[ − 1 , 1 ]区间里，数据的均值变为0 \n​                                                         \n差异第一点：显而易见，Normalization会严格的限定变换后数据的范围，比如按之前最大最小值处理的Normalization，它的范围严格在[ 0 , 1 ]之间；而Standardization就没有严格的区间，变换后的数据没有范围，只是其均值是0，标准差为1。\n第二点：归一化(Normalization)对数据的缩放比例仅仅和极值有关，就是说比如100个数，你除去极大值和极小值其他数据都更换掉，缩放比例α=Xmax-Xmin是不变的；反观，对于标准化(Standardization)而言，它的α=σ，β = μ，如果除去极大值和极小值其他数据都更换掉，那么均值和标准差大概率会改变，这时候，缩放比例自然也改变了。\n如果你对处理后的数据范围有严格要求，那肯定是归一化，个人经验，标准化是ML中更通用的手段，如果你无从下手，可以直接使用标准化；如果数据不为稳定，存在极端的最大最小值，不要用归一化。在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，标准化表现更好；在不涉及距离度量、协方差计算的时候，可以使用归一化方法。\n3.4 多项式回归 多项式回归（Polynomial Regression）是线性回归（Linear Regression）的一种扩展形式。它通过在输入变量上添加高次项来拟合非线性关系。虽然多项式回归本质上还是线性模型，但它允许模型在输入特征的多项式基础上进行线性拟合，从而捕捉复杂的非线性关系。\n\n​              \n\n多项式回归的步骤\n选择多项式的阶数：选择合适的多项式阶数 n 是模型拟合的关键。阶数过低可能会导致欠拟合，阶数过高则可能导致过拟合。\n构建多项式特征：将输入特征扩展为多项式特征。例如，对于一个一维特征 x，构建的特征矩阵为\n\n拟合模型：使用线性回归方法在多项式特征上进行拟合。\n评估模型：通过均方误差（MSE）等指标评估模型的性能。\n3.5 正规方程正规方程公式：\n\n y，其中 是特征值矩阵，y是目标值矩阵。\n优点：正规方程可以直接求出最好结果（即最小损失）\n缺点：由于涉及到矩阵运算，当特征过多时矩阵运算变得很复杂，求解速度会很慢。所以只适用于小数据量。\n\n\n\n四、逻辑回归（Logistic Regression）4.1 理论推导线性回归拟合的是数值，并不符合我们预测类别的需求。但数值与类别也是有关联的。例如，天色越黑，下雨概率就越大。即值越大，属于某类别的概率也越大。值与概率之间可以互转。\n1、问题描述和转化\n\n一个二分类问题给的条件：\n\n分类标签Y {0，1}，特征自变量X{x1，x2，……，xn}\n如何根据我们现在手头上有的特征X来判别它应该是属于哪个类别（0还是1）\n\n问题的求解转化为：\n\n我们如何找一个模型，即一个关于X的函数来得出分类结果（0或1）\n2、初步思路：找一个线性模型来由X预测Y\n\n但是很明显，这样的函数图像是类似一条斜线，难以达到我们想要的（0或1）的取值\n所以我们引入了一个特殊的函数：\n3、Sigmoid函数（逻辑函数）公式\n\n函数图像\n\n4、刚刚的线性模型与Sigmoid函数合体\n\n这样我们就把取值控制在了0或1上，初步达成了我们的目标。\n5、条件概率\n我们令 ，则可得 \n若我们将y视为样本x作为正例的概率，那么1-y则为样本x作为反例的概率，二者的比值为，\n因此  被称为对数几率。\n因此有：\n所以推出了：\n\n可化简为：\n\n\n假设每个样本是独立事件，则总评估正确的概率为所有样本评估正确的积：\n\n\n 损失函数\n\n\n求最小值时的w\n梯度下降法（一阶收敛）\n通过 J(w) 对 w 的一阶导数来找下降方向，并以迭代的方式来更新参数\n\n(这里的k代表的是第k次迭代；是我们设定的学习率；就是我们上面所说的 )\n4.2 决策边界（Decision Boundary）利用训练好的模型对样本空间所有的坐标点进行预测，然后观察样本空间所有点的不同类别之间的边界，最终就是模型的决策边界。\n在二分类问题中，决策边界或决策表面是超曲面，其将基础向量空间划分为两个集合，一个集合。 分类器将决策边界一侧的所有点分类为属于一个类，而将另一侧的所有点分类为属于另一个类。可以通过绘制模型决策边界，来辅助判别分类模型的模型性能。\n不同模型的决策边界并不相同，逻辑回归在二维样本空间中的决策边界是一条直线，KNN模型决策边界实际上是一个个圆圈叠加而成的拥有一定幅度的边界，而对于决策树模型来说，其决策边界实际上是一条条折线。\n基于反向传播的人工神经网络或感知器的情况下，网络可以学习的决策边界的类型由网络具有的隐藏层的数量来确定。如果它没有隐藏层，那么它只能学习线性问题。如果它有一个隐藏层，则它可以学习Rn的紧致子集上的任何连续函数 ，如通用近似定理所示，因此它可以具有任意的决策边界。\n神经网络试图学习决策边界，最小化经验误差，而支持向量机试图学习决策边界，最大化决策边界和数据点之间的经验边际。\n阈值\n阈值的理解可以浅显的认为，相当于一个边界线，大于这个边界线是一个结果，小于这个边界线则是另一个结果。在逻辑回归中，大于这个边界线就是1，小于这个边界线则就是0。\n拿 sigmoid 函数举例\n当 x = 0 时为 sigmoid function 的决策边界。即所有 x &gt; 0 的部分 g ( x ) = 1，所有 x &lt; 0的部分 g ( x ) = 0。\n引用到 逻辑回归\ndecision boundary为 z = 0 z=0z=0 的时候，即 0.5 x + 1 = 0 0.5x+1=00.5x+1=0 时。\n五、正则化（Regularization）5.1 过拟合（overfitting）、欠拟合（underfitting）回归模型：\n\n分类模型：\n\n\n欠拟合\n泛化能力差，拟合程度比较低，训练样本集准确率低，测试样本集准确率低。\n欠拟合原因：\n\n训练样本数量少\n模型复杂度过低\n参数还未收敛就停止循环\n\n欠拟合的解决办法：\n\n增加样本数量\n增加模型参数，提高模型复杂度\n增加循环次数\n查看是否是学习率过高导致模型无法收敛\n\n过拟合\n泛化能力差，拟合程度非常高，训练样本集准确率高，测试样本集准确率低。\n过拟合原因：\n\n样本量少、样本噪声大（质量不好）\n特征太多\n模型太复杂或模型本身就不适用\n网络层数过多，导致后面学习得到的特征不够具有代表性\n\n过拟合的表现（判定方法）：\n\n训练集的正确率不增反减\n\n验证集的正确率不再发生变化\n\n训练集的error一直下降，但是验证集的error不减反增\n\n\n过拟合的解决办法：\n\n清洗数据\n减少模型参数，降低模型复杂度，包括greedy constructive learning、剪枝和权重共享等\n增加惩罚因子（正则化），保留所有的特征，但是减少参数的大小（magnitude）。\n\n5.2 正则化方法5.2.1 L1 正则化也称为 Lasso 正则化，是指权值向量w中各个元素的绝对值之和。比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|。它通过在模型的损失函数中增加权重的 L1 范数（权重向量的绝对值之和）来实现正则化。L1正则化可以让一部分特征的系数缩小到0，所以L1适用于特征之间有关联的情况可以产生稀疏权值矩阵（很多权重为0，则一些特征被过滤掉），即产生一个稀疏模型，可以用于特征选择。\nL1 正则化是一种通过在模型的损失函数中增加权重的 L1 范数作为惩罚项来控制模型复杂度的技术。L1 范数是向量中各个元素的绝对值之和，其数学表示如下：\n\n惩罚项可以写为权重的 L1 范数：\n\n其中 w是模型的权重向量，n是权重向量的长度，即权重的数量， λ是正则化参数，用于控制正则化的强度。\n线性回归L1正则化损失函数：\n\n正则化是权值的绝对值之和，所以L1是带有绝对值符号的函数，因此是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数后添加L1正则化项时，相当于对损失函数做了一个约束。\nL1正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择。\n稀疏性，就是模型的很多参数是0。通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，很多参数是0，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，即使去掉对模型也没有什么影响，此时我们就可以只关注系数是非零值的特征。这相当于对模型进行了一次特征选择，只留下一些比较重要的特征，提高模型的泛化能力，降低过拟合的可能。\n5.2.2 L2 正则化也称为 Ridge 正则化，它通过在模型的损失函数中增加权重的 L2 范数（权重向量的平方和）来实现正则化。L2 正则化会使权重值变得较小，但不会直接导致权重稀疏，因此不具有特征选择的作用，但可以有效地控制模型的复杂度。\nL2正则化通过向模型的损失函数添加一个权重参数的 L2 范数的惩罚项来实现。用 L2 正则化的损失函数时，优化算法在优化过程中会同时考虑数据损失和正则化项，从而在保持对训练数据的拟合能力的同时，尽可能减小模型参数的大小，降低模型的复杂度。\n线性回归L2正则化损失函数：\n\n拟合过程中通常都倾向于让权值尽可能小，数据偏移得多一点也不会对结果造成什么影响，抗扰动能力强。\nL2正则化获得更小参数过程：\n(1) 以线性回归中的梯度下降法为例。假设要求的参数为θ，hθ(x)是假设函数，线性回归的代价函数如下：\n\n(2)在梯度下降中θ的迭代公式为：\n\n(3)在原始代价函数之后添加L2正则化，则迭代公式为：\n\n从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代，θj都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的。\n调参经验从0开始，逐渐增大λ。在训练集上学习到参数，然后在测试集上验证误差。反复进行这个过程，直到测试集上的误差最小。一般的说，随着λ从0开始增大，测试集的误分类率应该是先减小后增大，交叉验证的目的，就是为了找到误分类率最小的那个位置。建议一开始将正则项系数λ设置为0，先确定一个比较好的learning rate。然后固定该learning rate，给λ一个值（比如1.0），然后根据validation accuracy，将λ增大或者减小10倍，增减10倍是粗调节，当你确定了λ的合适的数量级后，比如λ=0.01，再进一步地细调节，比如调节为0.02，0.03，0.009之类。\n5.2.3 Dropout可先跳至神经网络一章\n正常神经网络需要对每一个节点进行学习，而添加了DropOut的神经网络先随机选择中的一些神经元并将其临时隐藏(丢弃)，即暂时将其从网络中移除，以及它的所有传入和传出连接。在下一次迭代中，继续随机隐藏一些神经元，如此直至训练结束。由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。\n将DropOut应用于神经网络相当于从神经网络中采样了一个“更薄的”网络，即单元个数较少。（如下图所示，DropOut是从左图采样了一个右图这样更薄的网络）在正反向传播的过程中，采样了多个稀薄网络，即Dropout可以解释为模型平均的一种形式。\n\n　在训练时，每个神经单元以概率pp被保留(Dropout丢弃率为1−p)；在预测阶段（测试阶段），每个神经单元都是存在的，权重参数w要乘以p，输出是：pw。\n模型描述\n前提：带有L层隐藏层的神经网络。\nl：第几层。z： 代表输入向量y： 代表输出向量W：代表权重b：偏差f： 激活函数\n没有DropOut的神经网络前向传播计算公式可以被描述为：l+1层的输入向量是l+1的权重乘以l层的输出向量加l+1层的偏差。l+1层的输出向量为经过激活函数的l+1层的输入向量。\n\n添加DropOut的神经网络前向传播计算公式可以被描述为：相比于之前输出向量经过了伯努利分布，类似于经过一个门筛选了一下。*代表的是点积。\n\n神经网络图描述\n神经网络图示如下所示：上图为标准的神经网络，下图为添加了Dropout的神经网络，相比于标准的神经网络，添加了Dropout的神经网络相当于为前一层的输出向量添加了一道概率流程，即是否经过筛选。\n\n\n防止过拟合原因\n（1）取平均的作用 \n　　先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。\n（2）减少神经元之间复杂的共适应关系\n　　用作者原话是“在标准神经网络中，每个参数接收的导数表明其应该如何变化才能使最终损失函数降低，并给定所有其它神经网络单元的状态。因此神经单元可能以一种可以修正其它神经网络单元的错误的方式进行改变。而这就可能导致复杂的共适应(co-adaptations)。由于这些共适应现象没有推广到未见的数据，将导致过拟合。我们假设对每个隐藏层的神经网络单元，Dropout通过使其它隐藏层神经网络单元不可靠从而阻止了共适应的发生。因此，一个隐藏层神经元不能依赖其它特定神经元去纠正其错误。”\n　　 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。\n（3）Dropout类似于性别在生物进化中的角色\n　　物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。\n\n六、神经网络（Neural Network）6.1 前言神经网络主要由：输入层，隐藏层，输出层构成。当隐藏层只有一层时，该网络为两层神经网络，由于输入层未做任何变换，可以不看做单独的一层。实际中，网络输入层的每个神经元代表了一个特征，输出层个数代表了分类标签的个数（在做二分类时，如果采用sigmoid分类器，输出层的神经元个数为1个；如果采用softmax分类器，输出层神经元个数为2个；如果是多分类问题，即输出类别&gt;=3时，输出层神经元为类别个数），而隐藏层层数以及隐藏层神经元是由人工设定。一个基本的两层神经网络可见下图（注意：说神经网络多少层数的时候一般不包括输入层。 在神经网络中的激活主要讲的是梯度的更新的激活）：\n\n神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。\n　　下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。\n　　注意中间的箭头线。这些线称为“连接”。每个上有一个“权值”。\n\n我们使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成a * w。\n\n在MP模型里，函数g是sgn函数，也就是取符号函数。这个函数当输入大于0时，输出1，否则输出0。\n　　下面对神经元模型的图进行一些扩展。首先将sum函数与sgn函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入a与输出z写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。\n　　神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。\n\n当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“单元”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“节点”（node）来表达同样的意思。 \n6.2 单层神经网络（感知器 Perception）在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。\n在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：从本图开始，我们将权值w1, w2, w3写到“连接线”的中间。\n\n下图显示了带有两个输出单元的单层神经网络\n\n\n\n输入的变量是[a1，a2，a3]T（代表由a1，a2，a3组成的列向量），用向量a来表示。方程的左边是[z1，z2]T，用向量z来表示。\n系数则是矩阵W（2行3列的矩阵，排列形式与公式中的一样）。\n于是，输出公式可以改写成： g(W  a) = *z;\n6.3 两层神经网络（多层感知器MLP Multilayer Perceptron）两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。\n\n\n矩阵计算公式：\n g(W(1)  a(1)) = *a(2)\ng(W(2)  a(2)) = *z\n偏置节点（bias unit）是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。\n偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量b\n\n矩阵运算公式：\ng(W(1)  a(1) + b(1)) = *a(2)\ng(W(2)  a(2) + b(2)) = *z\n在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g\n6.4 普通多层神经网络在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。\n依照这样的方式不断添加，我们可以得到更多层的多层神经网络。公式推导的话其实跟两层神经网络类似，使用矩阵运算的话就仅仅是加一个公式而已。\n在已知输入a(1)，参数W(1)，W(2)，W(3)的情况下，输出z的推导公式如下：\ng(W(1)  a(1)) = *a(2); \ng(W(2)  a(2)) = *a(3);\ng(W(3)  a(3)) = *z;\n\nW(1)中有6个参数，W(2)中有4个参数，W(3)中有6个参数，所以整个神经网络中的参数有16个（这里我们不考虑偏置节点，下同）。\n假设我们将中间层的节点数做一下调整。第一个中间层改为3个单元，第二个中间层改为4个单元。\n经过调整以后，整个网络的参数变成了33个。\n\n\n6.5 激活函数（Activation Function）神经网络中每一层的输入输出都是一个线性求和的过程，下一层的输出只是承接了上一层输入函数的线性变换，所以如果没有激活函数，那么无论你构造的神经网络多么复杂，有多少层，最后的输出都是输入的线性组合，纯粹的线性组合并不能够解决更为复杂的问题。而引入激活函数之后，我们会发现常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。\n神经元中使用的函数，在术语上通常叫做激活函数。主要的激活函数有step,sigmoid,softmax,tanh和ReLU等\n每一个隐藏层可以有一个不同的激活函数，例如，在同一个神经网络中，隐藏层layer1可能使用sigmoid函数，隐藏层layer2可能使用ReLU，后续的隐藏层layer3使用Tanh。激活函数的选择取决于待解决的问题以及使用的数据的类型。\n阶跃函数\n\n其中，如果x的值大于等于零，则输出为1；如果x的值小于零，则输出为0。我们可以看到阶跃函数在零点是不可微的。目前，神经网络采用反向传播法和梯度下降法来计算不同层的权重。由于阶跃函数在零处是不可微的，因此它并不适用于梯度下降法，并且也不能应用在更新权重的任务上。 为了克服这个问题，我们引入了sigmoid函数。\nSigmoid函数\n\nSigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。\n当z或自变量趋于负无穷大时，函数的值趋于零；当z趋于正无穷大时，函数的值趋于1。该函数表示因变量行为的近似值，并且是一个假设。\n在什么情况下适合使用 Sigmoid 激活函数呢？\n\nSigmoid 函数的输出范围是 0 到 1。由于输出值限定在 0 到1，因此它对每个神经元的输出进行了归一化；\n用于将预测概率作为输出的模型。由于概率的取值范围是 0 到 1，因此 Sigmoid 函数非常合适；\n梯度平滑，避免「跳跃」的输出值；\n函数是可微的。这意味着可以找到任意两个点的 sigmoid 曲线的斜率；\n明确的预测，即非常接近 1 或 0。\n\nSigmoid 激活函数存在的不足：\n\n梯度消失：注意：Sigmoid 函数趋近 0 和 1 的时候变化率会变得平坦，也就是说，Sigmoid 的梯度趋近于 0。神经网络使用 Sigmoid 激活函数进行反向传播时，输出接近 0 或 1 的神经元其梯度趋近于 0。这些神经元叫作饱和神经元。因此，这些神经元的权重不会更新。此外，与此类神经元相连的神经元的权重也更新得很慢。该问题叫作梯度消失。因此，想象一下，如果一个大型神经网络包含 Sigmoid 神经元，而其中很多个都处于饱和状态，那么该网络无法执行反向传播。\n不以零为中心：Sigmoid 输出不以零为中心的,，输出恒大于0，非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢。\n计算成本高昂：exp() 函数与其他非线性激活函数相比，计算成本高昂，计算机运行起来速度较慢。\n\nSoftmax函数\nsoftmax函数是用于多类分类问题的激活函数，在多类分类问题中，超过两个类标签则需要类成员关系。对于长度为K的任意实向量，Softmax函数可以将其压缩为长度为K，值在[ 0 , 1 ] 范围内，并且向量中元素的总和为1的实向量。 softmax 对向量进行操作，而 sigmoid 则采用标量。                                                                  \nSoftmax函数与正常的max函数不同：max函数仅输出最大值，但Softmax函数确保较小的值具有较小的概率，并且不会直接丢弃。\nTanh函数\nTanh 激活函数又叫作双曲正切激活函数（hyperbolic tangent activation function）。与 Sigmoid 函数类似，Tanh 函数也使用真值，但 Tanh 函数将其压缩至-1 到 1 的区间内。与 Sigmoid 不同，Tanh 函数的输出以零为中心，因为区间在-1 到 1 之间。\n\n\n可以将 Tanh 函数想象成两个 Sigmoid 函数放在一起。在实践中，Tanh 函数的使用优先性高于 Sigmoid 函数。负数输入被当作负值，零输入值的映射接近零，正数输入被当作正值：\n\n当输入较大或较小时，输出几乎是平滑的并且梯度较小，这不利于权重更新。二者的区别在于输出间隔，tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；\n在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。\n\ntanh存在的不足：\n\n与sigmoid类似，Tanh 函数也会有梯度消失的问题，因此在饱和时（x很大或很小时）也会「杀死」梯度。\n\n注意：在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。\nReLU函数\n在深度学习模型中，修正线性单元(ReLU)是最常用的激活函数。ReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题，在目前的深度神经网络中被广泛使用。ReLU函数本质上是一个斜坡（ramp）函数\n\n\nReLU 函数是深度学习中较为流行的一种激活函数，相比于 sigmoid 函数和 tanh 函数，它具有如下优点：\n\n当输入为正时，导数为1，一定程度上改善了梯度消失问题，加速梯度下降的收敛速度；\n计算速度快得多。ReLU 函数中只存在线性关系，因此它的计算速度比 sigmoid 和 tanh 更快。\n被认为具有生物学合理性（Biological Plausibility）,比如单侧抑制、宽兴奋边界（即兴奋程度可以非常高）\n\nReLU函数的不足：\n\nDead ReLU 问题。当输入为负时，ReLU 完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零；\n\n\n【Dead ReLU问题】ReLU神经元在训练时比较容易“死亡”。在训练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个ReLU 神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活。这种现象称为死亡ReLU问题，并且也有可能会发生在其他隐藏层。\n\n\n不以零为中心：和 Sigmoid 激活函数类似，ReLU 函数的输出不以零为中心，ReLU 函数的输出为 0 或正数,给后一层的神经网络引入偏置偏移，会影响梯度下降的效率。\n\nLeaky ReLU\n为了解决 ReLU 激活函数中的梯度消失问题，当 x &lt; 0 时，我们使用 Leaky ReLU——该函数试图修复 dead ReLU 问题。它是一个其中最出名的一种变形，对于正数输入，其输出和ReLU一样，但是对于所有负数输出，不再是0，而是具有一个常数斜率（小于1）.\n\n为什么使用Leaky ReLU会比ReLU效果要好呢？\n\nLeaky ReLU 通过把 x 的非常小的线性分量给予负输入（0.01x）来调整负值的零梯度（zero gradients）问题，当 x &lt; 0 时，它得到 0.1 的正梯度。该函数一定程度上缓解了 dead ReLU 问题，\nleak 有助于扩大 ReLU 函数的范围，通常 a 的值为 0.01 左右；\nLeaky ReLU 的函数范围是（负无穷到正无穷）\n\n尽管Leaky ReLU具备 ReLU 激活函数的所有特征（如计算高效、快速收敛、在正区域内不会饱和），但并不能完全证明在实际操作中Leaky ReLU 总是比 ReLU 更好。\n6.6 代价函数6.6.1 交叉熵代价函数（Cross-entropy Loss Function）当输出只有两种可能，即输出层神经元只有一个节点时，代价函数如下：\n\n当输出有三种及以上种可能时，即输出层神经元有三个及以上节点时，（多分类问题），代价函数如下：\n\n\nh(x)是一个K维向量\nK为输出的类别数，即输出层神经元的个数\nh(x)_i表示第i个输出 →i表示选择输出神经网络输出向量中的第i个元素\n正则化项分别对 j i l 求和，除去了偏置单元（i=0)\n\n\n6.6.2 交叉熵代价函数+ softmax函数\n代价函数公式：\n\n6.7 前向传播（Forward Propagation）与反向传播（Back Propagation，BP）BP神经网络是一种多层的前馈神经网络，其主要的特点是：信号是前向传播的，而误差是反向传播的。\n正向传播数据（信息、信号）从输入端输入后，沿着网络的指向，乘以对应的权重后再加和，再将结果作为输入在激活函数中计算，将计算的结果作为输入传递给下一个节点。依次计算，直到得到最终结果。通过每一层的感知器，层层计算，得到输出，每个节点的输出作为下一个节点的输入。这个过程就是正向传播。                  \n反向传播将输出的结果与期望的输出结果进行比较，将比较产生的误差利用网络进行反向传播，本质是一个“负反馈”的过程。通过多次迭代，不断地对网络上的各个节点间权重进行调整（更新），权重的调整（更新）采用梯度下降法。\n\n6.7.1 BP算法的推导一文彻底搞懂BP算法：原理推导+数据演示+项目实战（上篇） - 人工智能遇见磐创 - 博客园 (cnblogs.com)\n6.8 独热编码（One-Hot Encoding）one-hot编码，又称独热编码、一位有效编码。其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时间只有一位有效（标记为1），而其他所有特征都被标记为0。举个例子，假设我们有四个样本（行），每个样本有三个特征（列），如下图：\n\n我们拿feature2来说明：这里feature2有4种取值（状态），我们就用4个状态位来表示这个特征，one-hot编码就是保证每个样本中的单个特征只有1位处于状态1,其他的都是0。\n\n对于2种状态、3种状态、甚至更多状态都可以这样表示，所以我们可以得到这些样本特征的新表示：\n\none-hot编码将每个状态位都看成一个特征。对于前两个样本我们可以得到它的特征向量分别为\n12Sample_1---&gt;[0,1,1,0,0,0,1,0,0]Sample_2---&gt;[1,0,0,1,0,0,0,1,0]\none-hot在提取文本特征上的应用\none hot在特征提取上属于词袋模型(bag of words)。关于如何使用one-hot抽取文本特征向量我们通过以下例子来说明。\n性别特征：[“男”,”女”]（N=2）：\n男  =&gt;  10\n女  =&gt;  01\n祖国特征：[“中国”，”美国，”法国”]（这里N=3）：\n中国  =&gt;  100\n美国  =&gt;  010\n法国  =&gt;  001\n运动特征：[“足球”，”篮球”，”羽毛球”，”乒乓球”]（这里N=4）：\n足球  =&gt;  1000\n篮球  =&gt;  0100\n羽毛球  =&gt;  0010\n乒乓球  =&gt;  0001\n所以，当一个样本为[“男”,”中国”,”乒乓球”]的时候，完整的特征数字化的结果为：\n[1，0，1，0，0，0，0，0，1]\n独热编码的意义\n在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。\n为了使非偏序关系的变量取值不具有偏序性，并且到圆点是等距的。使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1。\n比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。\n但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。 有些基于树的算法在处理变量时，并不是基于向量空间度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码。 Tree Model不太需要one-hot编码： 对于决策树来说，one-hot的本质是增加树的深度。\n\n七、误差分析与偏斜类的误差度量（Error Analysis and Error Metrics for Skewed Classes）所谓的偏斜类（Skewed Class）的问题，对于二元分类来说，其实就是一种分类的数据量远远大于另外一种分类。\n以是否恶性肿瘤（癌症）的分类为例，我们希望能根据病人的一些特征判断病人是否患有癌症（y=1表示有癌症，y=0表示没有癌症）。\n我们用逻辑回归算法来解决问题，发现在测试集有99%的正确率，这个结果看上去很完美。但是，你要知道患有癌症的毕竟是少数，可能在我们的测试集中只有0.5%的人真的患有癌症。这样的话，全部给预测为y=0（没有癌症），那也只有0.5%的错误。\n混淆矩阵：\n\n　　\n我们将算法预测的结果分成四种情况：\n\n正确肯定（True Positive,TP）：预测为真，实际为真\n正确否定（True Negative,TN）：预测为假，实际为假\n错误肯定（False Positive,FP）：预测为真，实际为假\n错误否定（False Negative,FN）：预测为假，实际为真\n\n\n准确率（accuracy）\n所有预测正确的样本（包含正例或负例均预测正确，即正例预测为正TP或负例预测为负TN）占总样本的比例。\n准确率=(TP+TN)/(FP+FN+TN+TP)\n查准率/精确率（Precision）\n查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。\n查全率/召回率（Recall）\n查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。\nP-R曲线：\n\n\n曲线越靠近右上方，性能越好。（例如上图黑色曲线）\n\n当一个曲线被另一个曲线完全包含了，则后者性能优于前者。（例如橙蓝曲线，橙色优于蓝色）\n\n如果曲线发生交叉（黑橙曲线），判断依据：\n\n\n\n根据曲线下方面积大小判断，面积更大的更优于面积小的。\n\n根据平衡点F判断：平衡点是查准率与查重率相等时的点。F计算公式为F = 2  P  R ／( P +R )，F值    越大，性能越好。\n\n\nF1-score\n精确率和召回率互相影响，理想状态下肯定追求两个都高，但是实际情况是两者相互“制约”：追求精确率高，则召回率就低；追求召回率高，则通常会影响精确率。我们当然希望预测的结果精确率越高越好，召回率越高越好， 但事实上这两者在某些情况下是矛盾的。这样就需要综合考虑它们，最常见的方法就是F-score。 也可以绘制出P-R曲线图，观察它们的分布情况。\nF1值为算数平均数除以几何平均数，且越大越好，将Precision和Recall的上述公式带入会发现，当F1值小时，True Positive相对增加，而false相对减少，即Precision和Recall都相对增加，即F1对Precision和Recall都进行了加权。\n\nROC曲线\nReceiver Operating Characteristic，“受试者工作特征曲线”，其主要分析工具是一个画在二维平面上的曲线——ROC 曲线。平面的横坐标是false positive rate(FPR)，纵坐标是true positive rate(TPR)。对某个分类器而言，我们可以根据其在测试样本上的表现得到一个TPR和FPR点对。这样，此分类器就可以映射成ROC平面上的一个点。调整这个分类器分类时候使用的阈值，我们就可以得到一个经过(0, 0)，(1, 1)的曲线，这就是此分类器的ROC曲线。     ROC 曲线距离基准线越远，则说明该模型的预测效果越好。\n\n一般情况下，这个曲线都应该处于(0, 0)和(1, 1)连线的上方。因为(0, 0)和(1, 1)连线形成的ROC曲线实际上代表的是一个随机分类器。如果很不幸，你得到一个位于此直线下方的分类器的话，一个直观的补救办法就是把所有的预测结果反向，即：分类器输出结果为正类，则最终分类的结果为负类，反之，则为正类。虽然，用ROC曲线来表示分类器的性能很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。于是Area Under roc Curve(AUC)就出现了。\nAUC的值就是处于ROC曲线下方的那部分面积的大小。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的性能。AUC（Area Under roc Curve）是一种用来度量分类模型好坏的一个标准。\n从AUC判断分类器（预测模型）优劣的标准\n· AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。\n· 0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。\n· AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。\n· AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。\n\n八、决策树（Decision Tree）决策树是一种以树形数据结构来展示决策规则和分类结果的模型，又称为判定树，是数据挖掘技术中的一种重要的分类与回归方法。其重点是将看似无序、杂乱的已知数据，通过某种技术手段将它们转化成可以预测未知数据的树状模型。\n其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。每个内部结点视为一个条件，每对结点之间的有向边视为一个选项，每一条从根结点（对最终分类结果贡献最大的属性）到叶子结点（最终分类结果）的路径都代表一条决策的规则。\n\n决策树的组成\n1、决策节点通过条件判断而进行分支选择的节点。如：将某个样本中的属性值(特征值)与决策节点上的值进行比较，从而判断它的流向。\n2、叶子节点没有子节点的节点，表示最终的决策结果。\n决策树通常有三个步骤：\n特征选择：选取有较强分类能力的特征。\n决策树生成：典型的算法有 ID3 和 C4.5， 它们生成决策树过程相似， ID3 是采用信息增益作为特征选择度量， 而 C4.5 采用信息增益比率。\n决策树剪枝：剪枝原因是决策树生成算法生成的树对训练数据的预测很准确， 但是对于未知数据分类很差， 这就产生了过拟合的现象。涉及算法有CART算法。\n\n决策树学习的目标：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。并在损失函数的意义下，选择最优决策树的问题。\n\n决策树学习的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。\n\n决策树学习的损失函数：正则化的极大似然函数\n\n决策树学习的测试：最小化损失函数\n\n\n\n8.1 熵熵的作用熵（Entropy)是表示随机变量不确定性的度量。说简单点就是物体内部的混乱程度。比如下边的两幅图中，从 图1 到 图2 表示了熵增的过程。对于决策树的某个结点而言，它在对样本数据进行分类后，我们当然希望分类后的结果能使得整个样本集在各自的类别中尽可能有序，即希望某个特征在被用于分类后，能最大程度地降低样本数据的熵。\n\n现在假设有这样一个待分类数据（如下图所示），若分类器 1 选择特征 𝑥1、分类器 2 选择特征 𝑥2 分别为根构建了一棵决策树，其效果如下：\n\n则根据以上结果，可以很直观地认为，决策树 2 的分类效果优于决策树 1 。从熵的角度看，决策树 2 在通过特征 𝑥2 进行分类后，整个样本被划分为两个分别有序的类簇；而决策树 1 在通过特征 𝑥1 进行分类后，得到的分类结果依然混乱（甚至有熵增的情况），因此这个特征在现阶段被认为是无效特征。\n熵的定义\n构建决策树的实质是对特征进行层次选择，而衡量特征选择的合理性指标，则是熵。为便于说明，下面先给出熵的定义：设 𝑋 是取值在有限范围内的一个离散随机变量，其概率密度为：\n\n\n\n条件熵的引入\n在构建决策树时我们可采用一种很简单的思路来进行“熵减”：每当要选出一个内部结点时，考虑样本中的所有“尚未被使用”特征，并基于该特征的取值对样本数据进行划分。即有：\n\n对于每个特征，都可以算出“该特征各项取值对运动会举办与否”的影响（而衡量各特征谁最合适的准则，即是熵）。为此，引入条件熵。首先看原始数据集 𝐷 （共14天）的熵，该数据的标签只有两个（“举办”与“不举办”），且各占一半，故可算出该数据集的初始熵为：\n\n分析天气特征：\n\n\n条件熵的计算\n\n8.2 决策树的划分选择信息增益（ID3决策树）表示某特征 𝑋 使得数据集 𝐷 的不确定性减少程度，定义为集合 𝐷 的熵与在给定特征 𝑋 的条件下 𝐷 的条件熵 𝐻(𝐷 | 𝑋) 之差\n\n信息增益率（C4.5决策树）\n基尼指数（CART决策树）\n三种算法的对比适用范围：\nID3算法只能处理离散特征的分类问题，C4.5能够处理离散特征和连续特征的分类问题，CART算法可以处理离散和连续特征的分类与回归问题。\n假设空间：\nID3和C4.5算法使用的决策树可以是多分叉的，而CART算法的决策树必须是二叉树。\n优化算法：\nID3算法没有剪枝策略，当叶子节点上的样本都属于同一个类别或者所有特征都使用过了的情况下决策树停止生长。\nC4.5算法使用预剪枝策略，当分裂后的增益小于给定阈值或者叶子上的样本数量小于某个阈值或者叶子节点数量达到限定值或者树的深度达到限定值，决策树停止生长。\nCART决策树主要使用后剪枝策略。\n8.3 剪枝处理决策树会无休止的生长，直到训练样本中所有样本都被划分到正确的分类。实际上训练样本中含有异常点，当决策树节点样本越少的时候，异常点就可能使得该结点划分错误。另外，我们的样本属性并不一定能完全代表分类的标准，可能有漏掉的特征，也可能有不准确的特征。这样就会导致决策树在训练集上准确率超高，但是在测试集上效果不好，模型过拟合，泛化能力弱。因此我们需要适当控制决策树的生长。剪枝处理是防止决策树过拟合的有效手段。剪枝，其实就是把决策树里不该生长的枝叶剪掉，也就是不该划分的节点就不要继续划分了。剪枝分为“预剪枝”和“后剪枝”。\n\n预剪枝\n在决策树生成过程中，对每个结点在划分前先进性估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。它的位置在每一次生成分支节点前，先判断有没有必要生成，如没有必要，则停止划分。\n预剪枝方法有：（1）当叶节点的实例个数小于某个阈值时停止生长；（2）当决策树达到预定高度时停止生长；（3）当每次拓展对系统性能的增益小于某个阈值时停止生长；\n限制决策树的深度\n下图展示了通过限制树的深度以防止决策树出现过拟合风险的情况。\n\n限制决策树中叶子结点的个数\n下图展示了通过限制决策树中叶子结点的个数以防止决策树出现过拟合风险的情况。\n\n限制决策树中叶子结点包含的样本个数\n下图展示了通过限制决策树中叶子结点包含的样本个数以防止决策树出现过拟合风险的情况。\n\n限制决策树的最低信息增益\n下图展示了通过限制决策树中叶子结点包含的样本个数以防止决策树出现过拟合风险的情况。\n\n后剪枝\n先从训练集生成一棵完整的决策树（相当于结束位置），然后自底向上的对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点，相当于将子树剪去。如果剪掉该节点，带来的验证集中准确性差别不大或有明显提升，则可以对它进行剪枝，用叶子节点来代填该节点。\n值得注意的是，后剪枝时要用到一个测试数据集合，如果存在某个叶子剪去后能使得在测试集上的准确度或其他测度不降低（不变得更坏），则剪去该叶子。\n后剪枝决策树通常比预剪枝决策树保留了更多的分枝，一般情形下，后剪枝决策树的欠拟合风险很小，泛化能力往往优于预剪枝决策树。但后剪枝决策树是在生产完全决策树之后进行的，并且要自底向上地对所有非叶子节点进行逐一考察，因此其训练时间开销比未剪枝的决策树和预剪枝的决策树都要大很多。\n8.4 连续与缺失值连续值处理\n（1）提出原因\n看看我们上面的例子，有些属性取值是离散的，有些是连续的。连续属性的可取值数目不是有限的，所以不能直接根据连续属性的可取值来对节点进行划分。\n（2）做法——连续属性离散化技术\n最简单的方法是二分法。\n\n缺失值处理\n（1）提出原因\n在样本获得的过程中，难免会因某些原因致使最后拿到的样本集出现某些属性数据的缺失。\n（2）做法\n当缺失的数据非常少时，一般直接舍弃掉那些缺失的数据；而当缺失的数据较多时，简单舍弃则是对样本的极大浪费，则按照一定的方法进行处理。\n当缺失的数据较多时，对信息增益的计算公式进行修改：\n\n\n8.5 回归树（regression tree）常用的决策树有 ID3、C4.5、CART 等，其中 CART 就可以用来做回归问题，CART 全称就是 Classification And Regression Tree（分类和回归树）。\n回归树（regression tree）就是用树模型做回归问题，每一片叶子都输出一个预测值。预测值一般是该片叶子所含训练集元素输出的均值，即\n\nCART 在分类问题和回归问题中的相同和差异：\n\n相同：\n在分类问题和回归问题中，CART 都是一棵二叉树，除叶子节点外的所有节点都有且仅有两个子节点；\n所有落在同一片叶子中的输入都有同样的输出。\n\n\n差异：\n在分类问题中，CART 使用基尼指数（Gini index）作为选择特征（feature）和划分（split）的依据；在回归问题中，CART 使用 mse（mean square error）或者 mae（mean absolute error）作为选择 feature 和 split 的 criteria。\n在分类问题中，CART 的每一片叶子都代表的是一个 class；在回归问题中，CART 的每一片叶子表示的是一个预测值，取值是连续的。\n\n\n\n下面以 criteria = ‘mse’ 为例，介绍 CART 回归树。\n\n\n8.6 随机森林（Random Forest，RF）随机森林是一个由许多决策树组成的集成模型。它的核心思路是，当训练数据被输入模型时，随机森林并不是用整个训练数据集建立一个大的决策树，而是采用不同的子集和特征属性建立多个小的决策树，然后将它们合并成一个更强大的模型。通过对多个决策树的结果进行组合，随机森林可以增强模型的效果。\n另一个随机森林的重要特点是，每个子集都是通过随机选择的样本和随机选择的特征属性建立的。这种随机化可以减少决策树对训练数据的敏感性，从而防止过拟合。\n随机森林的可视化结构图如下：\n\n除叶子节点外，所有节点都有5个部分：\n ● 基于某个特征的一个值对数据进行的提问，每个提问都有一个真或假的答案可以分裂节点。根据答案，数据点相应地向下移动。 ● gini： 节点的Gini不纯度。当我们沿着树向下移动时，平均加权基尼不纯度会减少。 ● samples ：节点中的观测数据数量。 ● value： 每个类中的样本数。例如，根节点中有2个样本属于类0，有4个样本属于类1。 ● class： 该节点中大多数点的分类。在叶节点中，即是对节点中所有样本的预测。\n叶节点中不再提问，因为这里已经产生了最终的预测。要对某个新数据点进行分类，只需沿着树向下移动，使用新点的特征来回答问题，直到到达某个叶节点，该叶节点对应的分类就是最终的预测。\n8.6.1 转换器（transformer）和估计器（estimator）Scikit-learn (sklearn)中两个重要的概念是转换器（transformer）和估计器（estimator）\n转换器\n转换器是将数据集从一种形式转换为另一种形式的工具。例如，将原始数据进行标准化处理，将文本数据转换为数值特征等。在sklearn中，转换器类的名称以Transformer结尾。转换器通常有一个fit_transform()方法，可以在训练集上拟合模型并将其应用于测试集。\n特征工程的步骤\n\n实例化（实例化是一个转换器类（Transformer））\n\n调用fit_tranformer(对于文档建立分类词频矩阵)\n\n\n我们把特征工程的接口称之为转换器，其中转换器调用有这么几种形式：\n\nfit_transform\nfit 计算每列的平均值与标准差\ntransform 进行最终的标准化\n\n常见的转换器：\nStandardScaler：用于标准化数值特征。OneHotEncoder：用于将分类变量转换为数值特征。CountVectorizer：用于将文本数据转换为数值特征。PCA：用于将高维数据集降低维度。\n估计器在sklearn中，估计器（estimator）是一个重要角色，分类器和回归器都属于estimator，是一类实现了算法的API。\n估计器是一种从数据集中学习模型的工具。估计器的任务是使用拟合模型对新数据进行预测。在sklearn中，估计器类的名称以Estimator结尾。估计器有两个基本方法，fit()方法和predict()方法。fit()方法在训练集上训练模型，而predict()方法用于在新数据上进行预测。\n需要注意的是，某些转换器也可以作为估计器使用，这意味着它们可以使用fit()方法在训练集上拟合模型，并使用predict()方法对新数据进行预测。这些转换器估计器也被称为“带监督的转换器”。\n用于分类的估计器\nsklearn.neighbors                                        k-近邻算法sklearn.native_bayes                                   贝叶斯sklearn.linear_model.LogisticRegression 逻辑回归sklearn.tree                                                   决策树与随机森林\n用于回归估计器\nsklearn.linear_model.LinearRegression    线性回归sklearn.linear_model.Ridge                         岭回归\n用于无监督学习的估计器\nsklearn.cluster.KMeans 聚类\n估计器的流程\n数据划分为训练集和测试集，我们建立模型的时候只需要把训练集输入进去就可以。训练集包括x_train、y_train,调用fit传入x_train、y_train,这样此算法就能利用这个模型进行计算，模型建立好之后要预测数据，看模型预测的数据准确与否，输入测试集的数据x_test、y_test,调用predict(x_test)，把测试集的特征值输入进去，来预测此测试集的目标值是什么（就像把特征值输入进去预测房价走向），每个算法应该都包括score这个方法，查看预测的准确性score(x_test,y_test)，真实值是1类别，预测值是2类别，就视为不准确。预测的时候应该有预测的类别和真实的类别。\n1.实例化一个estimator类\n2.estimator.fit(x_train,y_train) 计算\n ——调用完毕，模型生成\n3.模型评估\n1）直接比对真实值和预测值\n y_predict = estimator.predict(x_test)\n y_test == y_predict\n2）计算准确率\n accuracy = estimator.score(x_test,y_test)\n\n8.6.2 集成学习集成学习通过训练学习出多个估计器，当需要预测时通过结合器将多个估计器的结果整合起来当作最后的结果输出。集成学习的优势是提升了单个估计器的通用性与鲁棒性，比单个估计器拥有更好的预测性能。集成学习的另一个特点是能方便的进行并行化操作。\n\nBagging算法  Bagging 算法是一种集成学习算法，其全称为自助聚集算法（Bootstrap aggregating），顾名思义算法由 Bootstrap 与 Aggregating 两部分组成。  图展示了Bagging 算法使用自助取样（Bootstrapping4）生成多个子数据的示例\n具体生成规则：\n在随机森林算法中，为了增强模型的多样性和避免过拟合，每棵决策树在拆分节点时只使用所有特征的一个随机子集。对于分类任务，通常会选择特征总数的平方根（sqrt(n_features)）作为子集的大小。例如，如果有16个特征，每个节点的拆分将只考虑4个特征。这个设置可以通过Scikit-Learn中的参数来调整，也可以选择使用全部特征进行拆分，特别是在回归问题中。\n假设有一个大小为 N 的训练数据集，每次从该数据集中有放回的取选出大小为 M 的子数据集，一共选 K 次，根据这 K 个子数据集，训练学习出 K 个模型。当要预测的时候，使用这 K 个模型进行预测，再通过取平均值或者多数分类的方式，得到最后的预测结果。\n　　1）如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；每棵树的训练集都是不同的，而且里面包含重复的训练样本。\n在训练时，随机森林中的每棵树都会从数据点的随机样本中学习。样本被有放回的抽样，称为自助抽样法（bootstrapping），这意味着一些样本将在一棵树中被多次使用。背后的想法是在不同样本上训练每棵树，尽管每棵树相对于特定训练数据集可能具有高方差，但总体而言，整个森林将具有较低的方差，同时不以增加偏差为代价。\n　　2）如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；\n　　3）每棵树都尽最大程度的生长，并且没有剪枝过程。\n　随机森林中的“随机”就是指的这里的随机性。随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。\n随机森林算法将多个决策树结合在一起，每次数据集是随机有放回的选出，同时随机选出部分特征作为输入，所以该算法被称为随机森林算法。可以看到随机森林算法是以决策树为估计器的Bagging算法。\n\n结合器在分类问题中，选择多数分类结果作为最后的结果，在回归问题中，对多个回归结果取平均值作为最后的结果。使用Bagging算法能降低过拟合的情况，从而带来了更好的性能。单个决策树对训练集的噪声非常敏感，但通过Bagging算法降低了训练出的多颗决策树之间关联性，有效缓解了上述问题。\n随机森林分类效果（错误率）与两个因素有关：\n森林中任意两棵树的相关性：相关性越大，错误率越大；森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。\nRF在可解释性上不能同传统决策树相提并论，但是它的一大优势是不用太关注选择好的超参数。RF相比单个决策树，对噪声或者异常值十分具有鲁棒性，因而RF通常亦不需要剪枝。在实践中唯一需要关心的参数就是the number of trees: n。n越大，RF分类器的性能越好，同时其计算代价也越大。\n当然，如果需要，像bootstrap sample的采样数m，以及再每次split时随机选取的属性子集的数目d，这些超参数也是可以优化的。可以通过这些参数来控制RF的bias-variance trade-off。\n\n属性子集d，越小，个体树的性能有所降低，但整个RF模型会越健壮。d越大，属性随机性会降低，容易过拟合。\n减小采样数m，能增加个体树之间的多样性，因为一个样本被包含在bootstrap sample里的概率变小了。所以减小m可以增加RF的随机性，这将有助于降低过拟合风险。然而，更小的m将会使得RF的整体性能降低，使得training和test 性能之间的有小的gap，有更低的test performance。相反的，增大m会引起一定程度的过拟合，因为bootstrap sample以及后续的个体决策树会变得彼此更相似，他们会更加接近的学习拟合原始训练数据集。\n\n8.7 XGBoost（eXtreme Gradient Boosting）XGBoost全称为eXtreme Gradient Boosting，即极致梯度提升树。\nXGBoost是Boosting算法的其中一种，Boosting算法的思想是将许多弱分类器集成在一起，形成一个强分类器（个体学习器间存在强依赖关系，必须串行生成的序列化方法）。XGBoost是由多棵CART(Classification And Regression Tree)，即分类回归树组成，因此他可以处理分类回归等问题。\n整体思路\n（1）训练过程——构建XGBoost模型       \n​       从目标函数出发，可以推导出“每个叶子节点应该赋予的权值”，”分裂节点后的信息增益“，以及”特征值重要性排序函数“。\n  与决策树的建立方法类似。当前决策树的建立首先根据贪心算法进行划分，通过计算目标函数增益（及上面所说的”分裂节点后的信息增益“），选择该结点使用哪个特征。\n   选择好哪个特征后，就要确定分左右子树的条件了（比如选择特征A，条件是A&lt;7）：为了提高算法效率（不用一个一个特征值去试），使用“加权分位法”，计算分裂点（这里由”特征值重要性排序函数“得出分裂点）。\n  并且对应叶子节点的权值就由上述的“每个叶子节点应该赋予的权值”给出。\n  不断进行上述算法，直至所有特征都被使用或者已经达到限定的层数，则完整的决策树构建完成。\n（2）测试过程      将输入的特征，依次输入进XGBoost的每棵决策树。每棵决策树的相应节点都有对应的预测权值w，将“在每一棵决策树中的预测权值”全部相加，即得到最后预测结果，看谁大，谁大谁是最后的预测结果。\nXGBoost模型详解-CSDN博客\n机器学习算法（十五）：XGBoost_xgboost回归-CSDN博客\n\n九、支持向量机（SVM，support vector machines）超平面的数学表示在二维空间中，一个直线可以表示为 ( ax + by + c = 0 )。其中，向量 (a, b) 就是这条直线的法向量，指向垂直于直线的方向。\n在更高维度，比如三维空间，一个平面可以用方程 ( ax + by + cz + d = 0 ) 来描述，这里 ( a, b, c ) 是这个平面的法向量。\n神经网络试图学习决策边界，最小化经验误差，而支持向量机试图学习决策边界，最大化决策边界和数据点之间的经验边际。\n与Logistic Regression相比，SVM是一种优化的分类算法，其动机是寻找一个最佳的决策边界（超平面），能够将两个不同类别的样本划分开来，使得从决策边界与各组数据之间存在margin，并且需要使各侧的margin最大化。比较容易理解的是，从决策边界到各个training example的距离越大，在分类操作的差错率就会越小。因此，SVM也叫作Large Margin Classifier。\n\n\n分割超平面：将上述数据集分隔开来的直线成为分隔超平面。对于二维平面来说，分隔超平面就是一条直线。对于三维及三维以上的数据来说，分隔数据的是个平面，称为超平面，也就是分类的决策边界。\n间隔：点到分割面的距离，称为点相对于分割面的间隔。数据集所有点到分隔面的最小间隔的2倍，称为分类器或数据集的间隔。论文中提到的间隔多指这个间隔。SVM分类器就是要找最大的数据集间隔。\n支持向量：离分隔超平面最近的那些点。\n\n目标函数：\n\n支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局\n线性SVM不直接依赖于数据分布，分类平面不受一类点影响，影响SVM决策面的样本点只有少数的结构支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果如果数据不同类别strongly unbalance，一般需要先对数据做balancing。用下图进行说明：\n支持向量机改变非支持向量样本并不会引起决策面的变化：\n逻辑回归中改变任何样本都会引起决策面的变化：\n机器学习之支持向量机（SVM）-CSDN博客\n支持向量机通俗导论（理解SVM的三层境界）-CSDN博客\n机器学习：深入解析SVM的核心概念【一、间隔与支持向量】_svm最小间隔是什么意思-CSDN博客\n核函数\n核函数详解-CSDN博客\n统计学习方法：核函数（Kernel function） - LeeLIn。 - 博客园 (cnblogs.com)\n核函数 高斯核函数，线性核函数，多项式核函数-CSDN博客\n十、无监督学习\n监督学习是一种目的明确的训练方式，你知道得到的是什么；而无监督学习则是没有明确目的的训练方式，你无法提前知道结果是什么。\n监督学习需要给数据打标签；而无监督学习不需要给数据打标签。\n监督学习由于目标明确，所以可以衡量效果；而无监督学习几乎无法量化效果如何。\n\n\n\n\n\n聚类方法\n适用场景\n代表算法\n优点\n缺陷\n延伸\n\n\n\n\n层次聚类\n小样本数据\n\n可以形成类相似度层次图谱，便于直观的确定类之间的划分。\n难以处理大量样本\n\n\n\n基于划分的聚类\n大样本数据\nK-means算法\n-是解决聚类问题的一种经典算法，简单、快速，复杂度为O(N)                      -对处理大数据集，该算法保持可伸缩性和高效率                                    -当簇近似为高斯分布时，它的效果较好\n-在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用                                 -必须事先给出k(要生成的簇的数目)，而且对初值敏感，对于不同的初始值，可能会导致不同结果。       -不适合于发现非凸形状的簇或者大小差别很大的簇                                     -对躁声和孤立点数据敏感\n-可作为其他聚类方法的基础算法，如谱聚类                     -k值可以通过其他的算法来估计，如：BIC(Bayesian information criterion)、MDL(minimum description length)\n\n\n两步法聚类\n大样本数据\nBIRCH算法\n层次法和k-means法的结合，具有运算速度快、不需要大量递归运算、节省存储空间的优点\n\n\n\n\n基于密度的聚类\n大样本数据\nDBSCAN算法\n基于密度定义，相对抗噪音，能处理任意形状和大小的簇。 无需指定聚类数量，对数据的先验要求不高。\n当簇的密度变化太大时，会有麻烦对于高维问题，密度定义是个比较麻烦的问题\n\n\n\n\n10.1 k-means聚类算法(K-means clustering)与分类、序列标注等任务不同，聚类是在事先并不知道任何样本标签的情况下，通过数据之间的内在关系把样本划分为若干类别，使得同类别样本之间的相似度高，不同类别之间的样本相似度低（即增大类内聚，减少类间距）。\n K-means 的著名解释：牧师—村民模型（1）有四个牧师去郊区布道，一开始牧师们随意选了几个布道点，并且把这几个布道点的情况公告给了郊区所有的村民，于是每个村民到离自己家最近的布道点去听课。\n（2）听课之后，大家觉得距离太远了，于是每个牧师统计了一下自己的课上所有的村民的地址，搬到了所有地址的中心地带，并且在海报上更新了自己的布道点的位置。\n（3）牧师每一次移动不可能离所有人都更近，有的人发现A牧师移动以后自己还不如去B牧师处听课更近，于是每个村民又去了离自己最近的布道点……\n（4）就这样，牧师每个礼拜更新自己的位置，村民根据自己的情况选择布道点，最终稳定了下来。\nK-Means算法的思想\n对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。\n\n\n上图a表达了初始的数据集，假设k=2。在图b中，我们随机选择了两个k类所对应的类别质心，即图中的红色质心和蓝色质心，然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图c所示，经过计算样本和红色质心和蓝色质心的距离，我们得到了所有样本点的第一轮迭代后的类别。此时我们对我们当前标记为红色和蓝色的点分别求其新的质心，如图4所示，新的红色质心和蓝色质心的位置已经发生了变动。图e和图f重复了我们在图c和图d的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。最终我们得到的两个类别如图f。\n当然在实际K-Mean算法中，我们一般会多次运行图c和图d，才能达到最终的比较优的类别。\nK-Means 算法 步骤\n给定数据集  X , 该数据集有  n 个样本 , 将其分成  K 个聚类 ;\n① 中心点初始化 : 为 K 个聚类分组选择初始的中心点 , 这些中心点称为 Means ; 可以依据经验 , 也可以随意选择 ;\n② 计算距离 : 计算  n 个对象与  K 个中心点 的距离 ; ( 共计算  n×K 次 )\n③ 聚类分组 : 每个对象与 K 个中心点的值已计算出 , 将每个对象分配给距离其最近的中心点对应的聚类 ;\n④ 计算中心点 : 根据聚类分组中的样本 , 计算每个聚类的中心点 ;\n⑤ 迭代直至收敛 : 迭代执行 ② ③ ④ 步骤 , 直到新计算出来的质心和原来的质心之间的距离小于某一个设置的阈值（聚类算法收敛）, 即中心点和分组经过多少次迭代都不再改变 , 也就是本次计算的中心点与上一次的中心点一样 \n\n【海量数据挖掘/数据分析】之 K-Means 算法（K-Means算法、K-Means 中心值计算、K-Means 距离计算公式、K-Means 算法迭代步骤、K-Means算法实例）_kmeans聚类算法-CSDN博客\nK-Means的K值选择\nK 值的选取对 K-means 影响很大，这也是 K-means 最大的缺点，常见的选取 K 值的方法有：手肘法、Gap statistic 方法。\n（1）手肘法\n\n核心指标：SSE(sum of the squared errors，误差平方和), SSE值越小表示数据点越接近他们的质心，聚类效果也最好。因为对误差取了平方，因此更加重视远离中心的点。\n核心思想\n\n随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数\n显然，肘部对于的k值为3(曲率最高)，故对于这个数据集的聚类而言，最佳聚类数应该选3。\n（2）轮廓系数\n结合内聚度和分离度两种因素。可以用来在相同原始数据的基础上用来评价不同算法、或者算法不同运行方式对聚类结果所产生的影响。\n\n轮廓系数的范围为[−1,1]，越趋近于1代表内聚度和分离度都相对较优。\n所以可以在k-means算法开始的时候，先设置k值的范围k∈[2,n]，从而计算k取每一个值的轮廓系数，轮廓系数最小的那个k值就是最优的分类总数。\nK-Means初始化优化K-Means++\nk个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心。如果仅仅是完全随机的选择，有可能导致算法收敛很慢。K-Means++算法就是对K-Means随机初始化质心的方法的优化。\n\nK-Means距离计算优化elkan K-Means\n\n大样本优化Mini Batch K-Means\n传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。因此有了一种分批处理的改进算法Mini Batch K-Means。\nMini Batch K-Means算法是K-Means算法的变种，采用小批量的数据子集减小计算时间，同时仍试图优化目标函数，这里所谓的小批量是指每次训练算法时所随机抽取的数据子集，采用这些随机产生的子集进行训练算法，大大减小了计算时间，与其他算法相比，减少了k-均值的收敛时间，小批量k-均值产生的结果，一般只略差于标准算法。\n该算法的迭代步骤有两步：\n\n从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心\n更新质心与K均值算法相比，数据的更新是在每一个小的样本集上。对于每一个小批量，通过计算平均值得到更新质心，并把小批量里的数据分配给该质心，随着迭代次数的增加，这些质心的变化是逐渐减小的，直到质心稳定或者达到指定的迭代次数，停止计算。Mini Batch K-Means比K-Means有更快的 收敛速度，但同时也降低了聚类的效果，但是在实际项目中却表现得不明显，有差异的基本都是聚类边界上的点。\n\n　K-Means的主要优点有：\n　　　　1）原理比较简单，实现也是很容易，收敛速度快。\n　　　　2）聚类效果较优。\n　　　　3）算法的可解释度比较强。\n　　　　4）主要需要调参的参数仅仅是簇数k。\n　K-Means的主要缺点有：\n　　　　1）K值的选取不好把握\n　　　　2）对于不是凸的数据集比较难收敛\n　　　　3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则          聚类效果不佳。\n　　　　4） 采用迭代方法，得到的结果只是局部最优。\n　　　　5） 对噪音和异常点比较的敏感。\n10.2 异常检测（anomaly detection）异常检测是通过数据挖掘方法发现与数据集分布不一致的异常数据，也被称为离群点、异常值检测等等。在异常检测中，我们通常处理的是未标记的数据，即没有明确的标签指示哪些样本是异常的。相反，算法需要根据数据本身的特征来确定异常。这使得异常检测成为一项挑战，因为异常通常是稀有事件，不易获取大量标记的异常数据以进行训练，我们可以将其视为一种“半监督”学习，因为我们通常有一些正常样本，但没有足够的异常样本。这种情况下，我们可以利用正常样本来构建模型，然后将其应用于整个数据集以检测异常。\n异常检测算法适用的场景特点有：（1）无标签或者类别极不均衡；（2）异常数据跟样本中大多数数据的差异性较大；（3）异常数据在总体数据样本中所占的比例很低。\n【机器学习】李宏毅——Anomaly Detection（异常检测） - FavoriteStar - 博客园 (cnblogs.com)\n异常检测(Anomaly Detection)方法与Python实现 - 郝hai - 博客园 (cnblogs.com)\n一文详解8种异常检测算法（附Python代码）-CSDN博客\n异常检测方法总结_s.quantile(.25), s.quantile(.75)-CSDN博客\n10.3 推荐系统(Recommender System)（1）推荐系统的定义推荐系统（Recommendation System, RS）是一种自动联系用户和物品的工具，它能够帮助用户在信息过载的环境中发现令他们感兴趣的信息。它通常由前台的展示页面、后台的日志系统、推荐算法系统三个部分组成。\n10.3.1协同过滤（Collaborative Filtering）算法篇—协同过滤_协同过滤算法-CSDN博客\n推荐系统之协同过滤算法_协同过滤推荐算法-CSDN博客\n一文入门推荐系统——推荐系统实践读书笔记_推荐系统入门教程-CSDN博客\n10.3.2 PCA主成分分析法（PCA）-CSDN博客\n一文入门推荐系统——推荐系统实践读书笔记_推荐系统入门教程-CSDN博客\n降维算法之PCA：从原理到应用，8000多字，助你彻底理解！_pca降维-CSDN博客\n一文读懂PCA分析 （原理、算法、解释和可视化）-腾讯云开发者社区-腾讯云 (tencent.com)\n","slug":"机器学习初级","date":"2024-08-01T00:00:00.000Z","categories_index":"Deep Learning","tags_index":"","author_index":"Gueason"},{"id":"aba5b7476e687cdc475e58c292a78a0a","title":"引擎搜索","content":"一、搜索语法\n限定关键词”XXX”(ctrl+F：快捷在PDF中搜索关键词）\n限定标题 intitle:xxx\n限定标题多个关键词 allintitle:xxx（空格）xxx\n限定文章内容关键词 intext:xxx\n限定网址关键词 inurl:xxx(【例】inurl:CCTV)\n限定网站来源site:xxx(【例】site:pexels.com)\n限制图片尺寸xxx imagesize:xxx(【例】elon musk imagesize: 5760x3840)\n限制文件格式 xxx filetype:xxx(【例】大模型filetype:PDF)\n\n特定搜索\n采用格式：keyword + format\n集成电路 + site: 1991T.com :专门从一个报告网站里查找集成电路有关的资源，会有很多行业分析报告文档\n最短路径+ site: zhihu.com: 可以专门从知乎查找有关的知识问答\n2023考研复试 site: fudan.edu.cn可以从官网筛选到需要的信息，权威\ntiger site: pexels.com 从一个知名素材图片视频网站查找。或者另外一个素材网站 pixabay.com,gettyimages.com\n二、知识技能1.搜化妆品成分：国家普通化妆品备案信息2.搜日本产品功能参数：亚马逊、乐天3.ChatGPT最新账号：X(twitter/Youtube)4.程序员：GltHub、 StackOverflow、CSDN5.产品经理导航网址、设计导航、新媒体导航网址6.国际论文：谷歌学术、Sci-Hub（免费、只能搜完整题目）、谷粉学术\n谷粉学术 (99lb.net)\n7.下载电子书：a．中文：鸠摩搜书b．英文：https://z-lib.id/、https://manybooks.net、www.pdfbooksworld.com\n8.公众号：quaro:国外知乎redit：国外论坛\n9．百度网盘搜索引擎（自己百度）找出类似网站：SimilarSites\n三、素材文件视频：best sites for free stock videos【例】pexels、pixabay\n1．特殊技巧，【例】youtube视频：在www．后面加上9x2．下载网站／插件：save.tube3．高分辨率：直接搜4K的频道\n音频：royalty free BGM sites【例】pixabay\n图片：1．静态图片：a．谷歌搜（imagesize)b．有版权的（gettyimages复制网址到下载器gettyimages downloader)\n2.GIF动图：best gif sites【例】GIPHY\n3.icon图标a．谷歌搜【例】火箭 icon filetype:PNGb.iconfont.cn\n文件：PPT模板：1.islide插件2．在线设计网站canva.com\n四、工具软件在线工具：(xxx需求＋Online)\n\n设计网站canva.com\n在线抠图remove.bg\n文字转语音 腾讯智影／Azure\n画脑图miro/canva \n找台词找台词网\n在线剪辑视频Clipchamp/FLexclip\n测网速 fast.com\n检测陌生链接安全性virustotal.com\n\n寻找生产力软件Plugin:1．谷歌／Youtube搜：2023 best APPs/Top10 best Mac APPs productivity/best chorme plugin2．找替代软件：alternativeto.net\n寻找生产力插件Extension:1．插件商店2．谷歌搜best chorme extension productivity\n五、网址整理Ladderhttps://binghe.gitbook.io/quan-ping-tai-fan-qiang-gong-ju\nhttps://fnyun.gitbook.io/fabu\nPPTPPT模板_PPT模版免费下载_免费PPT模板下载 -【第一PPT】 (1ppt.com)\nhttps://www.ypppt.com/\nhttp://www.pptbz.com/pptmoban/jianyuejianjie/\nhttps://www.canva.cn/create/ppt-slides/\n音频处理https://audio-extractor.net/cn/\nhttps://vocalremover.org/zh/cutter\n熊猫无损音乐官网-全网无损音乐mp3歌曲免费下载网站 (xmwav.com)\n六、计算机技术https://github.com/\nCSDN - 专业开发者社区\n博客园 - 开发者的网上家园 (cnblogs.com)\n通义灵码安装教程-阿里云 (aliyun.com)\n整理来源\nhttps://github.com/tuteng/Best-websites-a-programmer-should-visit-zh?tab=readme-ov-file#when-you-get-stuck\n当你遇到问题时\nCodementor: 国际版在行，更侧重于开发者之间的帮助，可以从某些方面的专家那里获取帮助\ndevRant: 供你吐槽和释放压力的地方\nLearn Anything: 帮助你找到学习任何知识时的最佳路径\nQuora: 一个共享知识和更好的理解世界的地方\nStack Overflow: 订阅他们的每周新闻和任何你感兴趣的主题\n\n针对初学者的代码练习\nCave of programming : 学习编程，提升技巧\nCodeacademy : 交互式编码学习，免费\nCodeAbbey - 一个任何人都能精通编程的地方 : 从菜鸟到专家进阶之地\nExercism.io : 用超过30种不同的语言解决实践问题，与其他人分享你的解决方案\nfreeCodeCamp : 学习编码并且为非营利性组织构建项目。打造你的全栈开发技能\nkaran/Projects-Solutions 用不同语言实现的项目\nLod - Cloud : 开放数据云图\nProgramming by Doing : 编程进阶站点\nReddit.com/r/dailyprogrammer : 有趣的编程挑战，在这里你能学习其他人的编码，对于你解决不了的编码你能看看其他人是怎么解决的\nPeople Can Program :互联网上最友好的编程学习应用\nProgramming Tasks : 小程序汇总，不是微信的小程序\nVim adventures :边玩边学vim\nPramp : 尝试面试官角色，结束后，可以点击左边的按钮交换角色\nPaqmind :学习编程的指南和挑战\ntreehouse :快速容易和经济实惠的方法来提升技巧\n\n区块链基础 :区块链介绍\n\n\n小项目\nfreeCodeCamp | React project ideas :学习react的27个有趣的想法\nmartyr2s-mega-project-ideas-list : 包含125个项目，从初级到中级\nkaran/Projects : 针对新手的小项目的汇总\nWrong “big projects” for beginners : 怎样选择，从哪里开始\nvicky002/1000-Projects : 一个能够用任何编程语言解决的巨型项目清单\n\n通用编码建议\n成为一名优秀开发者的10种方法\n代码审查最佳实践: Kevin London的博客\nDieter Rams : 好的产品设计的10个原则\n设计模式 : 用例子详细说明设计模式\n如何成为一个程序员或谷歌搜索的艺术\njs项目指南:javascript项目最佳实践集合\nLearn to Code With Me : 一个资源非常全的网站，旨在为科技界的开发人员提供资源\n一个程序员一生应遵循的原则\n编程规范 :程序设计原理与模式分类综述\n软件设计模式 : 设计模式的汇总\n良好的编程原则\n我希望在我一开始学习编码的时候就能得到的一些建议 — Free Code Camp : 从教别人处学到的东西\n在计算机科学专业应该知道什么 :良好的编程规范\n软件开发者的工作 : Henrik Warne的博客\n\n编码风格\nAirbnb JS 风格指南 : 写JavaScript最好的方式\nAirbnb Ruby 风格指南 : Airbnb的ruby风格指南\nRuby 编码风格指南 : 社区驱动的Ruby编码风格指南\nAngular 1 风格指南 : 官方认可的风格指南，作者John Pappa\nCS 106B 编码风格指南 : 代码臃肿者必看\n调试问题 :检查如何调试程序\nCS课程目录(许多在线讲座) : 另一个在线CS课程\n在线CS课程目录 : 免费的在线CS课程\n优秀的 C 编码习惯 • /r/C_Programming\ngoogle C++ 编码风格\n怎样高效的报告Bug\n建议初学者避免使用的坏的编码习惯?\nPEP8 - Python编码风格指南\n标准 JS 风格指南 : JavaScript风格指南\nGoogle Python 风格指南\nAurelia 风格指南\n\n通用工具\nCodePad : 远程面试工具\nCodePen : 基于浏览器共享前端代码的地方\nDevicons : 免费图标汇总\nregex101 : 在线正则表达式测试和调试，国内也有不少\nregexr : 另一个在线学习、测试正则表达式的工具\nWit AI :帮助开发者灵活的控制应用，适合没有移动端开发经验的用户\n\nbash和shell脚本\nAdvanced Bash-Scripting Guide :对shell脚本艺术的深入挖掘\nBash Guide for Beginners :对新手的bash使用指南\nBash编程\nBash参考手册\nBash指南\nConquering the Command Line : 针对开发者的Unix和Linux命令\n\n面试准备\n/r/cscareerquestions : 这是一个非常大的面试问题列表是我在为4大面试时用到的，我认为对大家也是有用的\n10个最频繁的SQL查询面试问题\n逻辑题答案汇总\n算法设计流程\n资质问题与答案 : Quant and aptitude preparation\n一个技术面试题，逻辑题等的站点 : 各种让你思考的东西\nBeehYve :各种学生需要的资源\n面试准备时的链接的汇总 • /r/cscareerquestions\n算法复杂度笔记\n大家的误解\n位操作技巧\nChiperSoft/InterviewThis : 在面试期间问到对公司的了解的问题\nCode Project : 面向开发人员\nJava核心面试问题 - 每个主题的面试题\n一些有趣的C问题\nCS9: CS技术面试中的问题解决\n优秀的逻辑题\n确定不同循环的时间复杂度? :在stackoverflow上的一些好问题包括计算时间复杂度.\n5个基本的电话面试问题 - steveyegge2\n新人面试\nGeeksforGeeks | 针对geek的计算机科学入门 : 订阅他们获取新文章\n准备学习SQL篇：用语言描述一下数据库规范化 - Essential SQL\nhttps://github.com/odino/interviews : 面试重要问题列表\n让你的简历改头换面\n技术面试指南\n如何为技术面试做准备• /r/cscareerquestions\n怎样进行算法面试\n怎样进行自我介绍 | The Art of Manliness\n如何回答最棘手的40个面试问题| ICS Job Portal\n如何进入科技企业-求职和面试指南\n怎样面试\n怎样准备一个面试 - 1\nIIT Delhi 实习经验 :\nsamwincott/Internship-Guide : 与实习相关的链接汇总\n华尔街面试\n面试架构 - Java Honk\n工作面试：如何做好求职面试|展示个人魅力\n求职面试新闻、视频、评论和八卦 - Lifehacker\n求职面试问题和最佳答案\nkimberli/interviews : 面试学习表格\nMission-peace/interview problems : 一个大的编码面试的问题的汇总\n软件工程师面试实践平台 :与真正的同行进行编程交流\nProblems | LeetCode OJ : 针对面试的编码训练\n程序员和软件面试问答\nReddit.com/user/ashish2199/m/puzzles : Reddit逻辑题\nSQL面试问题 : 好的SQL测试\n使用韦恩图进行SQL连接\nsvozniuk/java-interviews : Java面试题\n25个最难的HR问题\n面试10大算法\nUnix／Linux笔记\n为求职者和雇主提供的建议\n通过动画可视化数据结构和算法\nWe Help Coders Get Hired : 提供系统设计，面试策略，软件技能等的web站点\n你的问题是什么?\n为何你会赚钱少 • /r/cscareerquestions\nwu :: riddles(hard) : 逻辑题\n夏季实习：最终篇\n求职面试问题和最好的答案\nJava面试问题与答案\n编程语言概念 课堂笔记 : 面向对象概念和编程语言概念\n系统设计面试\n一个站点提供技术面试题，逻辑题或者你想的其他内容\n前端常见面试题汇总\n\n学习新知识\n课程中心 : 超过10万名学生评价的目录\n计算机科学资源 : 针对自学者的mooc列表\nCoursera.org :在网上学习全世界最好的课程\nCS50\nedX : 免费在线课程，帮助你进步，改善你的生活\nKadenze | Creative Programming: 注重艺术和创造力的编程课\n麻省理工学院电气工程和计算机科学\nMOOC.fi : 赫尔辛基大学在线课程\nNPTEL Vidoes COMP_SCI_ENGG\nprakhar1989/awesome-CS-courses : CS课程大汇总\nUdacity\nUCBerkeley\n网上CS课程的收集\n极客教程\n\n编程语言相关的网站\n学习java的最好的书 : Java基础\nBjarne Stroustrup’s C++风格和技术问答 : C++问答\nBjarne Stroustrup’s FAQ : The C++ FAQ\nC++11 -新的ISO C++ 标准 : C++11问答\n编译器(视频)\n深入Java：垃圾回收!\n免费在线的章节用来学习java虚拟机\n垃圾搜集如何工作\n算法、数据结构、面试问题和答案的实现\nIntelliJ键盘快捷键 :使用IntelliJ的键盘快捷键用来提高生产力\nJava Corner at Artima.com\nJava课程笔记\nJava堆\nJava-source : Java开源软件\nJava Visualizer : 帮助将引用，值，变量可视化\nJournalDev - Java, Java EE, Android, Web Development Tutorials\nLearning Java:一个免费的在线学习Java语言的教材\nNetbeans的键盘快捷键 : 键盘快捷键能够使你在工作时提高生产力\n搜索开源的Java API : 浏览Java库的源代码，并且学习如何实现\nC++编程语言 : C++编程语言\nJava内存模型\nThe Java™ Tutorials : 最好的Java手册\n理解JVM\n垃圾回收是什么 : 动态垃圾搜集\nWelcome to JavaWorld.com\nXyzWs Java FAQs : Java面试题汇总\n\nAI学习\nfast.ai : 针对没有研究生水准的数学的开发者的免费实用的深度学习课程\ngrakn.ai : 数据库AI\nRobots that learn : 机器人学习\n无监督神经元\nAI、机器学习、深度学习的不同\nTensorFlow : 一个针对机器智能的软件开源库\nScikit-learn : 一个针对机器学习的Python模块\nDeepLearning.ai : 深度学习课程，作者coursera的创建者，吴恩达\n深度学习的历史\nSerpent AI : 游戏代理框架\n\n研讨会、研究性写作、会谈等\n关于研究和写作的建议\n在会议上讨论的实用技巧\n研讨会和报告\nLatex参考\nBegin Latex in minutes : 针对初学者的Latex的简单介绍帮助你轻松掌握Latex\nLshort : 对Latex2的详细介绍\n\n知识汇总\nreddit.com/user/ashish2199/m/cs_student_subs : 在reddits中计算机科学和编程相关的主题\n一个程序员应该访问的web站点 :发表在Quora上，作者ashish2199\nMoocha : 从edx、coursera、udacity等搜索在线课程\nRico’s cheatsheets : 常用知识备忘集合\nAPI Documentation : 一个众所周知的带有搜索接口的API文档，类似于dash\n\nyoutube频道\nC++Now (BoostCon) :当前的C++会议\ncode::dive conference : 有诺基亚技术中心组织的会议\nCoding Blocks : 指南，技巧\nComputerphile : 每个CS学生都应该看\nComputerHistory : 针对那些想知道如何达成目标的人\nCppCon : C++会议\nFacebook Developers\nFun Fun Function : 每周一次的对编程主题的分类，也包括一些与编码不直接相关的内容\nGoogle Developers\nGoogleTechTalks : 关于热门话题的视频和在技术界的一些趣事\nGynvael Coldwin :逆向工程和黑客（CTF）博客，每周三都有新的直播\nHowToBecomeTV : 与技术界相关的开发人员的好的面试\nJava : 有关Java的会谈\nJavaOne : Java会议\nMeeting C++ YT Kanalseite : C++会议\nNetflix UI Engineering : web开发者，移动开发者和对Netflix技术栈感兴趣的视频\nO’Reilly : 世界最佳技术作家访谈录\nPlacement Grid : 面试及校园实习经验\nScott Meyers: Past Talks\nSiraj Raval : AI和深度学习指南视频\nThinMatrix : 开发者在java中使用OpenGL制作的一个3D游戏的博客和指南\nthoughtbot : 谈论各种话题\nyegor256\n\n文章\n白话40个关键的计算机科学概念\n关于图论的一个介绍\n一个对程序员友好的语言\n软件开发者的阅读列表 : 这是一些好书和链接\n关于TCP/IP协议栈的代码: 编写一个TCP/IP的协议栈，5:TCP重传\n代码的转换:语言的选择\n数据结构与算法:一些算法和数据结构的解决方案\n深入字节码\n初级开发者的想法\nLinux内核\n算法清单\n提升编码技巧的关键\nUnicode编码\n我们正在通过创新重塑零售业\n每个程序员都需要知道的关于在文本中设置编码和字符的内容\n每个程序员都应该了解的关于内存的内容-pdf\n为什么网页的快速加载是很重要的:为什么一个APP的速度决定了其收入\nqotoqot-提升技巧:我是如何一个月工作200个小时的\nPixel Beat - Unix:使用unix工具并行处理\n学习Vim:我希望我应该知道什么\n\n播客\nCoding Blocks:一个播客，涵盖最佳编程实践，设计模式，性能编码，面向对象编码，数据库设计和实现、提示、技巧和许多其他主题的内容\n水深火热的开发者:一个播客，分享开发人员的人性，并讲述了由Dave Rael主持的一些令人惊叹的软件人物的故事\n前端的欢乐时光:来自Netflix，Evernote，Atlassian＆LinkedIn的软件工程师小组，讨论关于前端开发的所有事情的一个博客。\n无线电的所有流程:包括从产品设计和用户体验到单元测试和系统管理。\nJavaScript 论坛:每周一次关于JavaScript，前端开发，社区，职业和框架的讨论。\n用我的播客学习代码:以一个季为一集的科技播客，是由劳伦斯·布拉德福德（Laurence Bradford）主持，从科技职业到科技业务的课程\nMS示例展示:Jason Young和Carl Schweitzer谈到有关Azure云，Windows，Windows Phone，Visual Studio以及使用Microsoft平台的跨平台开发等最新的开发者新闻\nReact Native Radio:每周讨论使用JavaScript和React构建移动应用程序的工具，技术和技术\n软件工程日报:关于软件主题的日常技术面试\n软件工程无线电:针对专业软件开发人员的播客，目标是成为一个持久的教育资源而不是新闻\n语法 :对Wes Bos＆Scott Tolinski和Web开发人员来说是一个比较有趣的博客\nThe Bike Shed :参与者讨论他们的开发经验和Ruby，Rails，JavaScript等的挑战\nThe Change log : 每周一次的谈话，参与者都是开源技术的核心人员和创建者\n愤世嫉俗的开发者 :一个播客，旨在通过解释最新和最伟大的开发技术，为您提供开发知识和职业生涯，并提供您作为开发人员取得成功的需要。覆盖桌面，网络和移动开发，主要围绕.Net堆栈，也经常研究其他软件和框架\n\n教程\nA Hacker’s Guide to Git : 为那些想学习git的人打下坚实的基础\nBest Of - Gustavo Duarte : 包含各种主题的文章\nCMSI 281: Data Structures : 针对数据结构的轻量级的说明\nCollecting all the cheat sheets : 大量编程语言的接口表\nC Programming\nC编程语言常见问题\nData Structures and Algorithms by John Morris : 代码和其分析的另一个好的来源\n深入学习C语言 : 关于C语言的一个很好的演示\n设计模式：可重用面向对象的软件的基础 : aka the “Gang Of Four” book, or GOF\nDynamic programming - PrismoSkills : 学习如何解决动态规划问题\nGit从入门到精通\nHead First Design Patterns\nHow to Program in C++ : 怎样学习C++编程，关于学习C++和STL的好资源\nhttp://www.mysqltutorial.org/\nindradhanush tutotials : 写一个Unix核\nIntroduction to C Programming\n10分钟搞定UNIX\nLearning the shell.\nLinux Journey : 学习Linux的好站点\nLinux Tutorial : 学习Linux的好资源\nMore about Github-flavored markdown\nMySQL Essentials\nOpen Data Structures : 用来学习数据结构和算法的优秀的资源提供了C++ , Java等各种语言的伪代码\nOS Course Notes : Galvin’s书的章节笔记\n从新手到专家-编程、web开发、DevOps新闻，教程和工具\nSQL (Structured Query Language) in one page : SQL.SU : 结构化查询语言，一个非常好的SQL记录表\nSubtle | Poor Man’s CI : 了解连续集成平台如何工作，通过使用Node.js构建您自己的git之一\nTCP/IP图解\nThe Bash Guide : 学习Bash Shell的优秀教程\nThe Descent to C : 针对那些从高级编程语言像java或者python迁移到C的人\nThe Linux 命令行: yige完整介绍\nThe Unix环境编程\nTopCoder Tutorials\nTutorialspoint : UPSC，IAS，PCS，公务员，银行，能力，问题，答案，解释，面试，入学考试，解决方案的文本和视频教程\nUNIX和Linux系统管理员手册 第四版\nVimTutor+ : 从浏览器上学习VIM\nW3Schools在线Web教程\nUnix Shell : ksh / bash的Unix shell脚本\nSnap SVG : 现代Web的JavaScript SVG库\nvim.rtorr : Vim Cheat Sheet\nOpen Vim : 交互式Vim教程\nAlgorithm Using Dynamic Programming and A : 使用动态编程和A*设计树差分算法\nLearn Python : 免费的交互式Python教程\nC++17 : C ++指南17\n\n一个程序员应该知道的东西\nGitHub.com Build software better, together : 向其他人展示你的项目和与人协作的地方（为了更高效的使用它必须知道Git）\nGitlab提供无限制的私有库，和无限制的组织\n程序员能力矩阵 : 一篇用来了解我们作为程序员的水平的文章\n\n编程比赛\nArchived Problems - Project Euler : 问题归档\nArt of Problem Solving : 数学课对你来说太容易了吗？你来到正确的地方！\nCodeChef : 唯一的编程比赛Web 2.0平台\nCodefights : 测试你的编码技巧\nCodeforces : 编程竞赛，在线计算机编程\nCodewars : 根据完成代码排名\nCodility : 验证和提高编码技能\nCodingame : 通过游戏和挑战学习编码！\nGoogle Code Jam Practice and : 过去比赛的练习题\nHackerEarth - Programming challenges and Developer jobs\nHackerRank : 实践编码、竞争、找工作\nPKU ACM ICPC Practice problems : ACMACPC 在线审核\nSphere Online Judge (SPOJ) : 成为一名真正的编程硕士学习如何编写和构建高效的算法\nTopcoder : 通过更多的包为代码提供更快的速度\nUVa Online Judge : 支持多种语言的数百种问题\nWakaTime : 通过编辑器插件收集编码度量的排行榜\n\n计算机书籍\nBecome a Programmer, Motherfucker (list of books) :来自Zed A. Shaw的书籍详尽清单\ncses.fi/book.html\ngithub.com/vhf/free-programming-books : 超过500本免费电子书几乎包括你能想到的所有语言\nGitBook : GitBook 可以帮助您的团队在线编写，协作和发布内容\nData Science course : Python数据科学手册\n\n视频指南\ncodedamn : 前端Web开发教程\n代码学院 : PluralSight公司和一个有抱负和有经验的开发人员的互动学习的地方\nCodingMadeEasy : C ++教程\nCS1: Higher Computing - Richard Buckland UNSW : 一个很好的介绍性的CS课程\nDerek Banas : 优质的教程\n算法的设计与分析\nDevTips : web开发教程\nKathryn Hodge : 为初学者提供好视频\nmycodeschool : 数据结构和算法教程\nPluralsight :通过多个短期课程学习软件开发，DevOps和数据科学\nthenewboston : 课程很好但是与实际内容相比有点啰嗦\nTushar Roy : 印度Youtuber的算法和数据结构教程\nVim Tutorial Videos - Flarfnoogins : 用于学习Vim好的视频教程\nXDA-University - Helping You Learn Android Development:帮助您学习Android开发\nKhan Academy : 免费学习计算机科学\nFunctional programming : John Carmack功能编程（2013）\nVideo about vims : 关于Vim的一系列教程\n\n在线编译和共享代码片段\nCodePad : 支持代码编写，测试和运行，支持超过25种语言\nCodesandbox.io : CodeSandbox可以更轻松地创建，与他人共享和重用React项目\nGodbolt.org : 一款非常优秀的工具用来探索在不同编译器没有优化时的输出\nIdeone.com : 用于60多种编程语言的在线编译和调试工具\nJSFiddle : 在线测试你的javascript，CSS，HTML或者CoffeeScript\nPastebin.com\nC9.io : 在云端的开发环境\nGithub Gist : 即时分享代码，笔记和片段\n\n开发者博客\nAlgo-Geeks : 编程拼图，数学技巧，算法等\nAntirez - Redis Creator’s blog : Antirez的博客\nAntonio081014’s Algorithms Codes : RULE下的世界\nArchives — Ask a Manager : 人力资源相关的东西\nArmin Ronacher’s Thoughts and Writings : 关于Python和开源的博客\nblog.might.net : might dot net的博客\nBrendon Gregg - Linux Kernel Dev : Brendon D. Gregg博客\nClean Coder Blog : 《代码整洁之道》一书作者的博客\nCodeAhoy : 软件和人为因素的博客，100％测试人类\nCoderGears Blog Insights from : the CoderGears Team\nCoding Geek - A blog about IT, programming and Java : 关于IT，编程和Java的博客\nCoding Horror : 一个最好的编码博客\nCSE Blog : 量子，数学，计算机科学难题\nDaedtech.com : 有关软件的故事\nDan Dreams of Coding\nDaniel Lemire’s Blog : Daniel Lemire的博客\nEli Bendersky : 从Python到LLVM的一切\nGeek Land : 我珍贵的收藏品\nHackerEarth Blog : 黑客博客\nIT Enthusiast : IT热衷者\nJoel on Software : StackOverflow首席执行官的博客\nLate Developer : 一个老C ++人的随机想法\n1ucasvb’s laboriginal math and physics visualization : Lucas Vieira Barbosa的实验室原始数学和物理可视化\nMath ∩ Programming : Math ∩ Programming\nMy Tech Interviews : 准备技术访谈\nPaul Graham Essays : Paul Grahan Essays\nProgramming Blog : Yegor Bugayenko的编程博客\nProgramming in the 21st Century : 二十一世纪的编程\nrudhakar Rayavaram : Sudhakar Rayavaram博客\nRunhe Tian Coding Practice : 苹果，谷歌，Facebook，亚马逊和微软的技术面试问题\nSmall Programming Challenges and Puzzles : Nayuki项目\nStephen Haunts { Coding in the Trenches } : 软件开发，架构和技术领导力\nstevehanov.ca : 我知道如何在线制作和销售软件，我可以和你分享我的提示\nTakipi Blog : 主要关注Java和JVM语言\nWildMl : 机器学习博客\nXDA - Android Developer Forum : Android开源开发者论坛\n\n英语\nEnglishclub.com/learn-english\n语法和写作指南 : 对于那些想提高英语语言能力的人\n标点符号和字母大小写规则\nPPurdue大学在线写作实验室\nQuia - English\n\n","slug":"引擎搜索","date":"2024-07-01T00:00:00.000Z","categories_index":"技能","tags_index":"","author_index":"Gueason"},{"id":"c346e5cb761ad8572ad1cf2d93ff167b","title":"服务器基础知识（搬运）","content":"一、服务器概述1.1概念服务器是计算机的一种，是网络中为客户端计算机提供各种服务的高性能的计算机；\n服务器在网络操作系统的控制下，将与其相连的硬盘、磁带、打印机及昂贵的专用通讯设备提供给网络上的客户站点共享，在网络中为其它客户机（如PC机、智能手机、ATM等终端）提供集中计算、信息发布及数据管理或者应用设备。 服务器具有高速的CPU运算能力、长时间的可靠运行、强大的I/O外部数据吞吐能力以及更好的扩展性。\n（1）服务器硬件：是指在互联网上具有独立IP地址的计算机，比如我们自己用的计算机也可以作为服务器使用。\n（2）服务器软件：就是一个计算机程序。比如Mysql服务器软件，tomcat服务器软件。服务器软件分为很多类型，比如：ftp服务器，数据库服务器，web服务器软件，邮件服务器等。\n1.2作用\n通俗的说，服务器主要是用来响应终端的服务请求，并进行处理。\n我们在上网的时候是不可能直接将网络接入互联网的，我们都需要通过服务器来连接网络，只有服务器响应你的联网请求，并且进行处理以后才可以联网\n存储的功能，服务器的存储空间一般比较充足，可以存储非常多的信息。\n\n1.3分类按物理形态 ECS服务器云服务器(Elastic Compute Service)，通常用户可以根据自己的需要选定主机容量、CPU能力、内存大小、带宽及购买时长等，因此也称之为弹性计算服务器。\nECS在使用上和独立的服务器没有区别，且可以让企业节省自行购买和维护服务器硬件的成本，ECS已被中小企业广泛使用。\nVPS服务器虚拟专用服务器（Virtual Private Server),即将一台独立服务器通过虚拟技术分割为若干个虚拟服务器，每个VPS可以独立安装系统，拥有独立的IP，实现不同VPS间磁盘空间、内存、CPU资源、进程和系统配置的隔离，为用户和应用程序模拟出“独占”使用计算资源的体验。\nVPS可以像独立服务器一样，重装操作系统，安装程序，单独重启服务器，形式上和ECS没有区别，但总体性能和付费灵活性不如ECS。\n虚拟主机即将一台已安装操作系统和安全防范的服务器通过技术手段分割为若干个独立的空间，分配给用户独立使用，用户只需要上传网站程序，解析和绑定域名即可使用。\n虚拟主机不同于服务器，用户无法安装操作系统和操作软件，只能运行网站脚本语言、html文件、图片及其他静态文件等。\n虚拟主机相当于若干个用户一起租用一台服务器，价格实惠，简单易用，因此大多数企业网站选择了虚拟主机，而自助建站及云建站使用的也是虚拟主机。\n按物理位置国内主机\n国外主机（美国、德国等地区的主机)\n区别：\n备案方面\n国内主机必须备案，国外主机不需要提交备案资料的。\n访问速度\n国内主机国内访问快，海外主机海外访问快。\n线路\n国内主机有线路限制，海外主机没有，国内主机线路以联通和电信为主，相同的线路访问不存在速度限制，但相互访问就有点问题\n按处理器架构分类1）X86架构服务器（CISC架构服务器） \nIA-32、x86-32、x86-64都属于x86，即英特尔的32位x86架构，x86-64是AMD在其最新的Athlon 64处理器系列中采用的新架构，但这一处理器基础架构还是IA-32（因英特尔的x86架构并未申请专利保护，所以绝大多数处理器厂商为了保持与Intel的主流处理器兼容，都不得不采用这一x86架构），只是在此架构基础之上作了一些扩展，以支持64位程序的应用，进一步提高处理器的运算性能。\n2）RISC架构服务器\nRISC的英文全称为“Reduced Instruction Set Computing”（减少的指令集计算），中文即“精简指令集”，它的指令系统相对简单，它只要求硬件执行很有限且最常用的那部分执令，大部分复杂的操作则使用成熟的编译技术，由简单指令合成。目前在中高档服务器中普遍采用这一指令系统的CPU，特别是高档服务器全都采用RISC指令系统的CPU，并且此类服务器都采用UNIX操作系统。在中高档服务器中采用RISC指令的CPU主要有Compaq（康柏，即新惠普）公司的Alpha、HP公司的PA-RISC、IBM公司的Power PC、SGI公司的MIPS、SUN公司的Sparc、华为基于ARM架构级研发的鲲鹏920。\n3）IA-64\nEPIC（Explicitly Parallel InstructionComputers，精确并行指令计算机）。Intel采用EPIC技术的服务器CPU是安腾Itanium。它是64位处理器，也是IA-64系列中的第一款。在Intel采用了X86指令集之后，它又转而寻求更先进的64-bit微处理器，Intel这样做的原因是，它们想摆脱容量巨大的x86架构，从而引入精力充沛而又功能强大的指令集，于是采用EPIC指令集的IA-64架构便诞生了。IA-64在很多方面来说，都比x86有了长足的进步。突破了传统IA32架构的许多限制，在数据的处理能力，系统的稳定性、安全性、可用性、可观理性等方面获得了突破性的提高。IA-64微处理器最大的缺陷是它们缺乏与x86的兼容。\n\n按功能应用分类域控制服务器（Domain Server）\n文件服务器（File Server）\n打印服务器（Print Server）\n数据库服务器（Database Server）\n邮件服务器（E-mail Server）\nWeb服务器（Web Server）\n多媒体服务器（MultimediaServer）\n通讯服务器（Communication Server）\n终端服务器（Terminal Server）\n基础架构服务器（Infrastructure Server）\n虚拟化服务器（Virtualization Server）\n目前的技术来说，这些功能划分为逻辑形态。可以把多个功能把多个功能部署在一台服务器上面。从物理形态上来说，可以是一台服务器完成多个功能。\n按产品形态分类1U服务器、2U服务器、4U服务器\n服务器规定的尺寸是服务器的宽（48.26cm=19英寸）与高（4.445cm的倍数），厚度（高度））以4.445cm为基本单位。\n在机架式服务器尺寸当中，常见的就是1U服务器、2U服务器、4U服务器，这些服务器的尺寸是：1U=4.445厘米，2U=4.4452=8.89厘米，4U=4.4454=17.78 厘米。在实际使用当中，1U或者2U服务器是最经常使用的。因为服务商是根据服务器占用空间来计算费用的，所以采用1U服务器是最节省空间的和价格最低的，但是1U服务器的扩展性不如2U服务器的好。1U的硬盘数最多可以插4个，2U可以插8个，另外PCI的插槽数目也不同，1U最多2个，2U的可以到6个。\n标准机柜的结构比较简单，主要包括基本框架、内部支撑系统、布线系统、通风系统。19寸标准机柜外型有宽度、高度、深度三个常规指标。\n塔式服务器\n塔式服务器是最基本的服务器类型，通常被误认为台式计算机的传统CPU。在外部，塔式服务器的外观和感觉非常类似于传统的塔式PC。这些服务器旨在提供基本的性能水平，因此即使在价格方面也处于较低端。但是，当前有许多塔式服务器，它们成本很高，并且可以处理大量和多项任务。\n塔式服务器会占用大量要安装和使用的物理空间。由于它们体积大（大多数情况下），因此对其进行物理管理变得困难。而且，由于尺寸的原因，很难将它们堆叠在一起或将它们从一个地方重新布置到另一个地方。\n每个塔式服务器都占用大量办公空间，并且还需要一个单独的KVM（键盘，视频和鼠标，keyboard，Video，Mouse）开关才能进行管理。\n\n机架服务器\n机架服务器比塔式服务器小，安装在机架内部。这些机架与普通机架类似，我们使用它们来堆叠一组文件和文件夹。通过将服务器与其他设备（例如存储单元，冷却系统，SAN设备，网络外围设备和电池）垂直堆叠在一起，可以将机架服务器设计为位于机架中。\n用于安装这些机架服务器的机架符合IEEE标准，通常以机架单位或“ U”进行测量。每个U宽约19英寸，高约1.5-1.75英寸。使用这些机架的优点是它允许用户将其他电子设备与服务器一起堆叠。单个机架可以包含多个服务器以及上述其他设备。因此，与塔式服务器相比，这些机架式服务器使用起来非常方便，并且占用的空间更少。\n\n优点\n\n故障抑制：在机架式服务器中，只需花费很少的精力就可以识别，卸下和更换故障服务器。\n简化的电缆管理：机架中的管理工具可轻松有效地组织电缆。\n经济高效：它们以相对较低的成本提供了大量的计算能力和效率。\n\n缺点\n功耗：机架服务器由于总体组件密度高而常常需要具有附加的冷却系统，从而消耗更多的功率。\n维护：由于将多个设备一起放置在机架中，因此随着机架数量的增加，维护它们变得非常困难。\n刀片服务器\n刀片服务器是市场上最新，最先进的服务器。它们可以称为混合机架服务器，其中服务器被放置在刀片机箱内，形成刀片系统。刀片服务器的最大优势在于，这些服务器是目前可用的最小类型的服务器，非常适合节省空间。\n\n二、服务器技术2.1核心数/线程数多内核是指在一枚处理器中集成两个或多个完整的计算引擎（内核）。多核处理器是单枚芯片（也称为“硅核”），能够直接插入单一的处理器插槽中，但操作系统会利用所有相关的资源，将它的每个执行内核作为分立的逻辑处理器。通过在两个执行内核之间划分任务，多核处理器可在特定的时钟周期内执行更多任务。\n多线程是指从软件或者硬件上实现多个线程并发执行的技术。线程是操作系统能够进行运算调度的最小单位；它被包含在进程之中，是进程中的实际运作单位。具有多线程能力的计算机因有硬件支持而能够在同一时间执行多于一个线程，进而提升整体处理性能。\n多核心技术需要系统和软件的支持。windows2000以后的系统提供了多核心的支持，而之前的win me和win98等则仅支持单核。现阶段大部分程序都只是不超过4核心的优化支持，超过4核后性能提升不明显。\n一般来说，线程数等于核心数。但Intel为了更充分的利用CPU资源，开发了超线程技术。\nHT超线程技术，也就是Hyper-Threading，是Intel早在2001年就提出的一种技术。尽管提高时钟频率和缓存容量可以改善CPU的性能，但是受到工艺和成本的限制，CPU无法无限的提升参数来提升性能，实际上在应用中基于很多原因，CPU的执行单元都没有被充分使用。\n为此，Intel则采用另一个思路去提高CPU的性能，让CPU可以同时执行多重线程，就能够让CPU发挥更大效率，即所谓“超线程（Hyper-Threading，简称“HT”）”技术。超线程技术就是利用特殊的硬件指令，把两个逻辑内核模拟成两个物理芯片，让单个处理器都能使用线程级并行计算，进而兼容多线程操作系统和软件，减少了CPU的闲置时间，提高的CPU的运行效率。目前的多线程技术一般采用多个微处理器即多处理器结构，线程与处理器形成一一对应关系。而英特尔Hyper-Threading技术的特点是:\n（1）物理上用一个处理器处理多个线程\n（2）多线程的分配采用根据计数器的空闲状态进行线程处理的SMT（simultaneous multi-threading）方式。\nHT技术最早出现在2002年的Pentium4上，它是利用特殊的硬件指令，把两个逻辑内核模拟成两个物理芯片，让单个处理器都能使用线程级并行计算，进而兼容多线程操作系统和软件，减少了CPU的闲置时间，提高CPU的运行效率。但是，由于这个设计太过超前，奔腾4并没有借助HT大放光彩，在之后的酷睿架构中，Intel也再没有使用这个技术。然而，基于Nehalem架构的Core i7再次引入超线程技术，使四核的Corei7可同时处理八个线程操作，大幅增强其多线程性能。\n现在的HT技术很成熟，超线程技术带来的效率提升可达30%之多。不过对于一般的程序来说，超线程带来的提升或许很小，尤其是超过了四线程之后。\n4个物理CPU的对称多处理器结构：\n\n4个物理CPU，8个逻辑CPU的超线程结构：\n\n4个CPU，8个核的多核结构:\n在一个CPU里面布置两个执行核，即两套执行单元，如ALU、FPU和L2缓存等。而其他部分则两个核共享。这样，由于使用的是一个CPU，其功耗和单CPU一样。由于布置了多个核，其指令级并行将是真正的并行，而不是超线程结构的半并行。\n\n4个CPU，8个核，16个逻辑的多核超线程结构:\n即每个物理执行核里面又分解为两个或多个逻辑执行单元\n\n2.2内存服务器内存，可以看做普通内存进行了技术升级，使之更加安全稳定。服务器内存上拥有的技术ECC、Chipkill、热插拔、register寄存器等功能。\nECC服务器内存中，ECC可以被视为核心技术之一，ECC是 Error Checking and Correcting(错误检查和纠正)的简写，ECC和奇偶校验(Parity)类似，然而，在那些Parity只能检测到错误的地方，ECC实际上可以纠正绝大多数错误。经过内存的纠错，计算机的操作指令才可以继续执行。绝大多数常见的内存出错都是：单位错，多位错，列错，行错。它们都比较相似。单位错大多发生在读一个完整的比特或词的时候有一位比特出错。当读相同的比特和词时总是同一位数据出错，则称为多位错。单位错发生在很多词中，就称列错或行错。\n\nECC内存使用额外的比特(bit)存储一个用数据加密的代码。当数据被写入内存，相应的ECC代码与此同时也被保存下来。当重新读回刚才存储的数据时，保存下来的ECC代码就会和读数据时产生的ECC代码做比较。如果两个代码不相同，他们则会被解码，以确定数据中的那一位是不正确的。然后这一错误位会被抛弃，内存控制器则会释放出正确的数据。 被纠正的数据很少会被放回内存。假如相同的错误数据再次被读出，则纠正过程再次被执行。重写数据会增加处理过程的开销，这样则会导致系统性能的明显降低。如果是随机事件而非内存的缺点产生的错误，则这一内存地址的错误数据会被再次写入的其他数据所取代。\n而因为服务器内存使用了这些技术，导致必须有对应的服务器主板配合才能使用，也就是说服务器内存放在普通PC机上是认不到的。而且也无法对服务器内存进行颗粒拆除，自制内存，因为内存颗粒中的数据位宽都不一样。\nDDR SDRAMDDR SDRAM英文全称：Double Data Rate Synchronous Dynamic Random Access Memory.\n即为双倍速率的同步动态随机存取存储器，就是我们平时说的内存颗粒，也就是内存芯片。\nDDR SDRAM是传统动态随机存取存储器（DRAM）的一种改进版本。与传统的SDRAM相比，DDR SDRAM在同样的工作频率下能够实现双倍的数据传输速率，从而大幅提升了数据传输效率。这是通过在每个时钟周期内进行两次数据传输来实现的，因此称为“双倍数据率”。\n随着技术的发展，DDR经历了多轮技术迭代，发展出了DDR2、DDR3、DDR4、DDR5，从DDR到DDR5主要的区别是在于传输速率的不同，随着时钟周期的不断降低，传输速率也不断提高。\nDDR4 vs DDR5DDR5 内存在物理上与现有的 DDR4 极为相似。与从 DDR3 过渡到 DDR4时不同（240 引角增加到 288），新的 DDR5 与其前身相比，没有新增引脚数，它仍然保留了 288 引脚的布局，但引脚防呆缺口位置却略有不同。\n两者之间真正的区别在于架构设计层面，DDR4 采用单个 64 位通道，而 DDR5 则具有两个独立的 32 位子（双）通道。传输长度也从 8 字节增加到了 16 字节。\n\n外观和接口\n每一代新的 DDR SDRAM 都比旧一代拥有更先进的功能和更好的性能。它们乍一看外观几乎相同，但实际上存在一些细微差别。例如，DDR4 比 DDR3 略厚。此外，在从 DDR3 到 DDR4 的迭代过程中，插槽防呆口趋向于靠近中间；在 DDR5 时代，虽然防呆口仍然略偏向一侧，但更接近中间了。\n初始频率\nDDR5 内存最大的优势之一是更高的频率。随着多核心处理器的不断推出和更新，内存频率升级变得至关重要。例如，Intel Core i9-13900K 共有 24 个核心，即使是相对低功率的 Core i5-13400 也有十个核心。未来，主流 PC 市场只会提供强大的 CPU，核心数量也会更多，因此提高内存频率是非常有必要的。\nJEDEC（联合电子器件工程理事会）规定：DDR4 的数据速率（或频率）范围从 DDR4-1600 到DDR4-3200；对于 DDR5，规定的频率范围为 DDR5-3200 到 DDR5-6400。但可以肯定的是，DDR5-4800 将会成为初始基线，标准会随着时间的推移而不断拉高。DDR5 更高的频率成为了它比 DDR4 有更优秀带宽的原因。\n单条内存最大容量\nDDR5 标准带来的另一个好处是更高容量的内存。在 DDR4 时代，单条内存条的最大容量可达 32GB。基于支持 2DPC（每通道 2 个 DIMM）的主流主板，DDR4 平台上最常见的总内存容量限制为 128GB；而在 DDR5 时代，单条内存已经完全跨越到了一个新水平，可以达到 128GB。如果使用相同的 2DPC 主板进行比较，则总内存容量可达 512GB。\n工作电压\n电源效率也是 DDR5 内存的主要亮点之一。从表面上看，DDR5 内存的工作电压为 1.1V，低于 DDR4 的 1.2V。但已经有 DDR4 内存可以扩展到 1.6V，可以肯定地说，DDR5 也将进一步增加到更高电压。随着对更快内存速度的市场需求增加，DDR5 的工作电压也将随之攀升。\n\nDIMMDIMM（Dual Inline Memory Module）双列直插内存模块，也就是我们常见的内存条。将若干个内存颗粒，单独焊接在一块独立的电路板上，方便模块化和安装。\nUDIMM：无缓冲双信道内存模块 (Unbuffered Dual In-Line Memory Modules)，这是我们平时所用到的标准台式电脑DIMM，也就是通常意义上的RAM或DIMM和SODIMM；\nRDIMM：registered DIMM（Registered Dual In-line Memory Module），带寄存器的双线内存模块，就是我们所说的服务器内存了。为了增加内存的容量和稳定性分有ECC和无ECC两种，但市场上几乎都是ECC的。服务器内存较为常见的是4G内存条和8G内存条，偶尔有16G内存条，不过因为成本较高所以很少大规模使用，另外2G内存条基本已淘汰，还能看到的都是老旧机器上拆下来的，俗称拆机条。\nSO-DIMM：全称（Small Outline DIMM），小外型DIMM，笔记本电脑中所使用的DIMM，分ECC和无ECC两种。\nMini-DIMM：DDR2时代新出现的模组类型，它是Registered DIMM的缩小版本，用于刀片式服务器等对体积要求苛刻的高端领域。\nLRDIMM：全称Load Reduced DIMM，低负载双列直插内存模块。相比RDIMM，LRDIMM并未使用复杂寄存器，只是简单缓冲，缓冲降低了下层主板上的电力负载，但对内存性能几乎无影响。LRDIMM内存将RDIMM内存上的Register芯片改为iMB（isolation Memory Buffer）内存隔离缓冲芯片，直接好处就是降低了内存总线负载，进一步提升内存支持容量。\n2.3存储硬盘硬盘种类1、固态硬盘（SSD），采用闪存颗粒来储存；2、机械硬盘（HDD），采用磁性碟片来储存；3、混合硬盘（HHD），是把磁性硬盘和闪存集成到一起的一种硬盘。\n\n接口种类IDE/ATA\nIDE(Integrated Drive Electronics), 本意是指把控制器与盘体集成在一起的硬盘驱动器，是一种硬盘的传输接口, 有另一个名称叫做ATA（Advanced Technology Attachment），指的是相同的东西。\nATA 全称 Advanced Technology Attachment，是用传统的40-pin 并口数据线连接主板与硬盘的，外部接口速度最大为133MB/s，因为并口线的抗干扰性太差，且排线占空间，不利计算机散热，将慢慢被SATA 所取代。\nSATA\nSATA（Serial ATA）口的硬盘又叫串口硬盘. SATA以它串行的数据发送方式得名。在数据传输的过程中，数据线和信号线独立使用，并且传输的时钟频率保持独立，因此同以往的PATA相比，SATA的传输速率可以达到并行的30倍。可以说:SATA技术并不是简单意义上的PATA技术的改进，而是一种全新的总线架构，串行ATA总线运用嵌入式时钟信号，具备了更强的纠错能力。\nSCSI\nSCSI英文全称：Small Computer System Interface(小型计算机系统接口)，是同IDE(ATA)完全不一样的接口，IDE接口是普通PC的标准接口，而SCSI并不是专门为硬盘规划的接口，是一种广泛使用于小型机上的高速数据传输技术。它出现的原因主要是因为原来的IDE接口的硬盘转速太慢，传输速率太低，因此高速的SCSI硬盘出现。其实SCSI并不是专为硬盘设计的，实际上它是一种总线型接口。独立于系统总线工作.\nSCSI接口具有使用范围广、多任务、带宽大、处理器占用率低，以及热插拔等优点，但较高的价格使得它很难如IDE硬盘般普及，因此SCSI硬盘主要使用于中、高端服务器和高档工作站中。\nSAS\nSAS(Serial Attached SCSI)即串行连接SCSI，是新一代的SCSI技术。和现在流行的Serial ATA(SATA)硬盘相同，都是采用串行技术以获得更高的传输速度，并通过缩短连结线改善内部空间等。SAS是并行SCSI接口之后开发出的全新接口。此接口的设计是为了改善存储系统的效能、可用性和扩充性，并且提供与SATA硬盘的兼容性。\nSAS的接口技术可以向下兼容SATA。具体来说，二者的兼容性主要体现在物理层和协议层的兼容。\n光纤通道\n光纤通道的英文拼写是Fibre Channel，和SCSI接口一样光纤通道最初也不是为硬盘规划开发的接口技术，是专门为网络系统规划的，但随着储存系统对速度的需要，才慢慢使用到硬盘系统中。光纤通道硬盘是为提升多硬盘储存系统的速度和灵活性才开发的，它的出现大大提升了多硬盘系统的通信速度。光纤通道的主要特点有：热插拔性、高速带宽、远程连接、连接设备数量大等。\n光纤通道是为在像服务器这样的多硬盘系统环境而规划的，能满足高端工作站、服务器、海量储存子网络、外设间通过集线器、交换机和点对点连接进行双向、串行数据通讯等系统对高数据传输率的要求。\n固态硬盘固态驱动器（Solid State Drive），俗称固态硬盘，固态硬盘是用固态电子存储芯片阵列而制成的硬盘，通常使用 NAND 闪存来保存持久数据，为“闪存介质+主控”的半导体存储芯片结构。SSD由控制单元和存储单元（FLASH芯片、DRAM芯片）组成。\n每个 NAND 闪存芯片均由一个块阵列（也称为网格）构成，且每个块内均有一个存储单元阵列（称为页面或扇区）。每个单元中存储的位数可能有所不同，而这些单元也通常被归类为单位单元（即“单级单元”或“SLC”）、2 位单元和 3 位单元（即“多级单元/MLC”和“三级单元/TLC”）或 4 位单元（“QLC”）。每种单元类型也有其优点和缺点。SLC 以其可靠性、高速度和价格而著称，而 QLC 的优势则在于价格更低廉。每个网格均可存储 256 KB 到 4 MB 之间的数据。中央处理单元 (CPU) 会充当所有内存读取或写入作业的控制器。由于它们尺寸很小且功率要求低，因而非常适合笔记本电脑、平板电脑和智能手机。\n固态硬盘 (SSD) 试图通过使用非易失性固态内存来模仿 HDD，但它们的速度却比传统硬盘驱动器或软盘要快得多。HDD 具有固有的延迟和访问时间，而这源于盘片旋转和读/写磁头移动所产生的机械延迟。由于固态硬盘 (SSD) 不含移动部件，因此访问和存储数据的延迟和时间均会大幅缩短。\n新一代的固态硬盘普遍采用SATA-2接口、SATA-3接口、SAS接口、MSATA接口、PCI-E接口、NGFF接口、CFast接口、SFF-8639接口和M.2 NVME/SATA协议。\n目前固态硬盘应用最多的硬盘接口为SATA 3.0。普通2.5英寸SSD以及HDD硬盘都使用这种接口，理论传输带宽6Gbps，由于理论带宽的限制读写速度在600MB/s。\n\nFlash和EEPROM都属于非易失性存储器，但它们在一些方面有一些关键的区别。\nFlash和EEPROM之间的主要区别：\n擦除和写入操作：\nFlash: Flash存储器通常以较大的块（通常为扇区或页）进行擦除，然后再写入新的数据。这意味着在更新或修改存储的数据时，必须擦除整个块，而不仅仅是修改的位置。EEPROM: EEPROM支持单个字节的擦除和写入，允许更灵活地修改存储的数据，而无需擦除整个块。写入速度：\nFlash: Flash存储器通常具有较高的写入速度，尤其是在大容量存储中。写入时，整个块需要擦除，但这也可以增加写入的效率。【CPU系统】EEPROM: EEPROM的写入速度相对较慢，因为它允许逐个字节的擦除和写入。这在某些应用中可能会导致较长的写入时间。\n容量和应用：\nFlash: Flash存储器通常具有较大的存储容量，广泛用于嵌入式系统、移动设备、固态硬盘（SSD）等需要大容量存储的场合。EEPROM: EEPROM通常用于存储小量的关键数据，如配置设置、校准参数等，而不太适用于需要大容量存储的场景。\n寿命和耐用性：\nFlash: Flash存储器的擦写寿命通常较高，适合用于频繁擦写的应用，如操作系统和应用程序存储。EEPROM: EEPROM的擦写寿命相对较低，因为它支持单字节擦写，但这使其更适合于相对较小规模的数据存储和较少的擦写操作。\n使用场景：\nFlash: Flash广泛用于存储操作系统、应用程序代码、大型文件等。EEPROM: EEPROM适用于存储小型数据、设备配置信息、参数设置等。\nPCIePCI Express (peripheral component interconnect express) 简称 PCIe，是一种高速串行计算机扩展总线标准。是一种全双工总线，使用高速串行传送方式，能够支持更高的频率，连接的设备不再像 PCI 总线那样共享总线带宽。PCIe目前发布了4个版本——PCIe1.0、PCIe2.0、PCIe3.0、PCIe4.0\nPCIe传输的数据从上到下，都是以packet的形式传输的，每个packet都是有其固定的格式的。\n事务层的主要职责是创建（发送）或者解析（接收）TLP (Transaction Layer packet)，流量控制，QoS，事务排序等。\n数据链路层的主要职责是创建（发送）或者解析（接收）DLLP(Data Link Layer packet)，Ack/Nak协议（链路层检错和纠错），流控，电源管理等。\n物理层的主要职责是处理所有的Packet数据物理传输，发送端数据分发到各个Lane传输（stripe），接收端把各个Lane上的数据汇总起来（De-stripe），每个Lane上加扰（Scramble，目的是让0和1分布均匀，去除信道的电磁干扰EMI）去扰（De-scramble)，以及8/10或者128/130编码解码，等等。\nM.2(NVME)ssd是固态硬盘，普通的ssd配的是SATA口（AHCI协议），nvme ssd配的是PCIe口（nvme传输协议）\nmSATA接口，全称迷你版SATA接口（mini-SATA）。是早期为了更适应于超级本这类超薄设备的使用环境，针对便携设备开发的mSATA接口应运而生。可以把它看作标准SATA接口的mini版，而在物理接口上（也就是接口类型）是跟mini PCI-E接口是一样的。\nM.2接口是Intel推出的一种替代mSATA的新的接口规范，也就是我们以前经常提到的NGFF，即Next Generation Form Factor。M.2接口能够同时支持PCI-E通道以及SATA，其中前者在提高速度方面更轻松一些。最开始该接口所采用的是PCI-E 2.0 x2通道，理论带宽10Gbps，可以说它也突破了SATA接口理论传输瓶颈。\n如今的M.2接口全面转向PCI-E 3.0 x4通道，理论带宽达到了32Gbps，相比以往水准提升了很多，这也让SSD性能潜力大幅提升。\nNVM Express（NVMe），或称非易失性内存主机控制器接口规范(Non-Volatile Memory express),是一个逻辑设备接口规范。他是与AHCI类似的、基于设备逻辑接口的总线传输协议规范（相当于通讯协议中的应用层），运行在PCIe 接口之上，用于访问通过PCI-Express（PCIe）总线附加的非易失性内存介质，虽然理论上不一定要求 PCIe 总线协议。\n此规范目的在于充分利用PCI-E通道的低延时以及并行性，还有当代处理器、平台与应用的并行性，在可控制的存储成本下，极大的提升固态硬盘的读写性能，降低由于AHCI接口带来的高延时，彻底解放SATA时代固态硬盘的极致性能。\nRAID独立磁盘冗余阵列(Redundant Array of Independent Disks)\n简单的说，RAID是一种把多块独立的硬盘（物理硬盘）按不同的方式组合起来形成一个硬盘组（逻辑硬盘），从而提供比单个硬盘更高的存储性能和提供数据备份技术。\n软件RAID\n优点1、成本低，无需购置硬件;2、允许用户重新配置磁盘阵列，不受硬件限制。缺点1、读写性能差;2、软件RAID会占用系统资源。\n硬件RAID\n优点:1、不消耗硬盘性能及存储空间;·相对于操作系统独立;2、磁盘故障易更换。缺点:1、相比较软件RAID成本高;2、硬件RAID卡故障，必须更换同型号或与故障卡相互兼容的型号。\nRAID保护方式有两种镜像与校验镜像就是克隆出来一个副本，数据相同校验就是使用奇偶校验法逻辑算法为异或\n组成磁盘阵列的不同方式称为RAID级别（RAID Levels）\n常用的RAID级别:RAID0、RAID1、RAID3、RAID5、RAID6、RAID10、RAID50\n\n2.4热插拔热插拔（hot-plugging或Hot Swap）即带电插拔，是指将设备板卡或模块等带电接入或移出正在工作的系统，而不影响系统工作的技术。我们日常最常用的应用就是USB热插拔。\n2.5BIOSBIOS是英文“Basic Input Output System”的缩略词，直译过来后中文名称就是”基本输入输出系统“。是刻在主板 ROM 芯片上不可篡改的启动程序，BIOS 负责计算系统自检程序（POST，Power On Self Test）和系统自启动程序，因此是计算机系统启动后的第一道程式。由于不可篡改性，故程序存储在 ROM 芯片中，并且在断电后，依然可以维持原有设置。\n它是一组固化到计算机内主板上一个ROM芯片上的程序，它保存着计算机最重要的基本输入输出的程序、系统设置信息、开机后自检程序和系统自启动程序。 其主要功能是为计算机提供最底层的、最直接的硬件设置和控制。\nBIOS是连接软件程序与硬件设备的一座”桥梁”，负责解决硬件的即时要求。 一块主板性能优越与否，很大程度上就取决于BIOS程序的管理功能是否合理、先进。主板上的BIOS芯片或许是主板上唯一贴有标签的芯片，一般它是一块32针的双列直插式的集成电路，上面印有”BIOS”字样。586以前的BIOS多为可重写EPROM芯片，上面的标签起着保护BIOS内容的作用(紫外线照射会使EPROM内容丢失)，不能随便撕下。586以后的ROM BIOS多采用EEPROM(Electrically Erasable Programmable Read-Only Memory)带电可擦可编程只读存储器，通过跳线开关和系统配带的驱动程序盘，可以对EEPROM进行重写，方便地实现BIOS升级。 常见的BIOS芯片有Award、AMI、Phoenix、MR等，在芯片上都能见到厂商的标记。\nBIOS的作用 \n自检及初始化程序：计算机电源接通后，系统将有一个对内部各个设备进行检查的过程，这是由一个通常称之为POST（Power On Self Test/上电自检）的程序来完成，这也是BIOS程序的一个功能。\n完整的自检包括了对CPU、640K基本内存、1M以上的扩展内存、ROM、主板、CMOS存贮器、串并口、显示卡、软硬盘子系统及键盘的测试。\n在自检过程中若发现问题，系统将给出提示信息或鸣笛警告。如果没有任何问题，完成自检后BIOS将按照系统CMOS设置中的启动顺序搜寻软、硬盘驱动器及CDROM、网络服务器等有效的启动驱动器 ，读入操作系统引导记录，然后将系统控制权交给引导记录，由引导记录完成系统的启动，你就可以放心地使用你的宝贝了。\n硬件中断处理：计算机开机的时候，BIOS会告诉CPU等硬件设备的中断号，当你操作时输入了使用某个硬件的命令后，它就会根据中断号使用相应的硬件来完成命令的工作，最后根据其中断号跳会原来的状态。\n程序服务请求：从BIOS的定义可以知道它总是和计算机的输入输出设备打交道，它通过最特定的数据端口发出指令，发送或接收各类外部设备的数据，从而实现软件应用程序对硬件的操作。 \n\nCMOS是”Complementary Metal Oxide Semiconductor”的缩写，翻译出来的本意是互补金属氧化物半导体存储器，指一种大规模应用于集成电路芯片制造的原料。\n但在这里CMOS的准确含义是指目前绝大多数计算机中都使用的一种用电池供电的可读写的RAM芯片。作用是具有数据保存功能，但它也只能起到存储的作用，而不能对存储于其中的数据进行设置，要对CMOS中各项参数的设置就要通过专门的设置程序。现在多数厂家将CMOS的参数设置程序做到了BIOS芯片中，在计算机打开电源时按特殊的按键进入设置程序就可以方便地对系统进行设置。也就是说BIOS中的系统设置程序是完成CMOS参数设置的手段，而CMOS RAM是存放设置好的数据的场所\n\n宏观上整个从CPU加电到操作系统启动的过程：\nBIOS 程序首先将存储设备的引导记录（Boot Record）载入内存，并执行引导记录中的引导程序（Boot）；引导程序会将存储设备中的操作系统内核载入内存，并进入内核的入口点开始执行后操作系统内核完成系统的初始化，并允许用户与操作系统进行交互\n三、服务器硬件\n\n\n硬盘背板接口\n\n\n","slug":"服务器基础知识","date":"2024-07-01T00:00:00.000Z","categories_index":"理论","tags_index":"","author_index":"Gueason"},{"id":"aa511ac4879771de2733384acfc7e52f","title":"Linux操作系统（搬运）","content":"一、简史UNIX\n1970年，美国贝尔实验室的 Ken Thompson，以 BCPL语言 为基础，设计出很简单且很接近硬件的 B语言（取BCPL的首字母），并且他用B语言写了第一个UNIX操作系统。因为B语言的跨平台性较差，为了能够在其他的电脑上也能够运行这个非常棒的Unix操作系统，Dennis Ritchie和Ken Thompson 从B语言的基础上准备研究一个更好的语言1972年，美国贝尔实验室的 Dennis Ritchie在B语言的基础上最终设计出了一种新的语言，他取了BCPL的第二个字母作为这种语言的名字，这就是C语言1973年初，C语言的主体完成。Thompson和Ritchie迫不及待地开始用它完全重写了Unix操作系统\nMINUX\n因为AT&amp;T(通用电气)的政策改变，在Version 7 Unix推出之后，发布新的使用条款，将UNIX源代码私有化，在大学中不再能使用UNIX源代码。Andrew S. Tanenbaum(塔能鲍姆)教授为了能在课堂上教授学生操作系统运作的实务细节，决定在不使用任何AT&amp;T的源代码前提下，自行开发与UNIX兼容的操作系统，以避免版权上的争议。他以小型UNIX（mini-UNIX）之意，将它称为MINIX。\nLINUX\nLinus Torvalds利用GNU的bash当做开发环境，gcc当做编译工具，编写了Linux内核-v0.02，但是一开始Linux并不能兼容Unix，即Unix上跑的应用程序不能在Linux上跑，即应用程序与内核之间的接口不一致，因为Unix是遵循POSIX规范的，因此Torvalds修改了Linux，并遵循POSIX（Portable Operating System Interface，他规范了应用程序与内核的接口规范）； 一开始Linux只适用于386，后来经过全世界的网友的帮助，最终能够兼容多种硬件\n1991年的10月5日，林纳斯·托瓦兹在comp.os.minix新闻组上发布消息，正式向外宣布Linux内核的诞生（Freeminix-likekernel sources for 386-AT）。\n1993年，大约有100余名程序员参与了Linux内核代码编写/修改工作，其中核心组由5人组成，此时Linux 0.99的代码大约有十万行，用户大约有10万左右。\n1994年3月，Linux1.0发布，代码量17万行，当时是按照完全自由免费的协议发布，随后正式采用GPL协议。\n二、简介2.1Linux内核版本内核(kernel)是系统的心脏，是运行程序和管理像磁盘和打印机等硬件设备的核心程序，它提供了一个在裸设备与应用程序间的抽象层。\nLinux内核版本又分为稳定版和开发版，两种版本是相互关联，相互循环：\n稳定版：具有工业级强度，可以广泛地应用和部署。新的稳定版相对于较旧的只是修正一些bug或加入一些新的驱动程序。开发版：由于要试验各种解决方案，所以变化很快。\n内核源码网址：http://www.kernel.org \n2.2Linux发行版本GNU 是一个类 Unix 操作系统。它是由多个应用程序、系统库、开发工具乃至游戏构成的程序集合。GNU 的开发始于 1984 年 1 月，称为 GNU 工程。GNU 的许多程序在 GNU 工程下发布；我们称之为 GNU 软件包。\n“GNU”这个名字是“GNU’s Not Unix”的递归首字母缩写词。“GNU”的发音为g’noo，只有一个音节，发音很像“grew”，但需要把其中的r音替换为n音。\n类 Unix 操作系统中用于资源分配和硬件管理的程序称为 “内核”。GNU 所用的典型内核是 Linux。该组合叫做 GNU/Linux 操作系统。GNU/Linux 为几百万用户所使用，然而许多人 错误地称之为 “Linux”。\nGNU/Linux 发行版 通常包含了包括桌面环境、办公套件、媒体播放器、数据库等应用软件。\n一类是商业公司维护的发行版本    如RedHat（RHEL）一类是社区组织维护的发行版本    如Debian\nRedHat\nRedhat（小红帽），应该称为Redhat系列，包括RHEL(Redhat Enterprise Linux，也就是所谓的Redhat Advance Server，收费版本)、Fedora Core(由原来的Redhat桌面版本发展而来，免费版本)、CentOS(RHEL的社区克隆版本，免费)。Redhat应该说是在国内使用人群最多 的Linux版本，甚至有人将Redhat等同于Linux，而有些老鸟更是只用这一个版本的Linux。所以这个版本的特点就是使用人群数量大，资料非 常多，言下之意就是如果你有什么不明白的地方，很容易找到人来问，而且网上的一般Linux教程都是以Redhat为例来讲解的。Redhat系列的包管 理方式采用的是基于RPM包的YUM包管理方式，包分发方式是编译好的二进制文件。稳定性方面RHEL和CentOS的稳定性非常好，适合于服务器使用， 但是Fedora Core的稳定性较差，最好只用于桌面应用。\n红帽 | Red Hat 企业开源技术领导者\nCentOS\nCentOS是一款企业级Linux发行版,它使用红帽企业级Linux中的免费源代码重新构建而成。这款重构版完全去掉了注册商标以及Binary程序包方面一个非常细微的变化。有些人不想支付一大笔钱,又能领略红帽企业级Linux;对他们来说,CentOS值得一试。此外,CentOS的外观和行为似乎与母发行版红帽企业级Linux如出一辙。 CentOS使用YUM来管理软件包。\nThe CentOS Project\nFedora\n小巧的Fedora适合那些人:想尝试最先进的技术,等不及程序的稳定版出来。其实,Fedora就是红帽公司的一个测试平台;产品在成为企业级发行版之前,在该平台上进行开发和测试。Fedora是一款非常好的发行版,有庞大的用户论坛,软件库中还有为数不少的软件包。Fedora同样使用YUM来管理软件包。\nFedora Linux | The Fedora Project\nDebian\nDebian运行起来极其稳定,这使得它非常适合用于服务器。Debian平时维护三套正式的软件库和一套非免费软件库,这给另外几款发行版(比如Ubuntu和Kali等)带来了灵感。Debian这款操作系统派生出了多个Linux发行版。它有37500多个软件包,这方面唯一胜过Debian的其他发行版只有Gentoo。Debian使用apt或aptitude来安装和更新软件。\nDebian — 通用操作系统\nUbuntu\nUbuntu是Debian的一款衍生版,也是当今最受欢迎的免费操作系统。Ubuntu侧重于它在这个市场的应用,在服务器、云计算、甚至一些运行Ubuntu Linux的移动设备上很常见。作为Debian Gnu Linux的一款衍生版,Ubuntu的进程、外观和感觉大多数仍然与Debian一样。它使用apt软件管理工具来安装和更新软件。它也是如今市面上用起来最容易的发行版之一。Ubuntu使用基于apt的程序包管理器。\nUbuntu系统下载 | Ubuntu\nGentoo\n与Debian一样,Gentoo这款操作系统也包含数量众多的软件包。Gentoo并非以预编译的形式出现,而是每次需要针对每个系统进行编译。连Gentoo社区都觉得Gentoo安装和使用起来很困难;不过它被认为是最佳学习对象,可以进而了解Linux操作系统的内部运作原理。提到Gentoo总有人这么说:”如果你要学用Linux发行版,那就学用该发行版吧;如果你学会了Gentoo,也就学会了Linux。”Gentoo使用portage来安装和更新软件。\nhttp://www.gentoo.org/main/en/where.xml\nOpenSuse\nOpenSuse这款Linux发行版是免费的,并不供商业用途使用,仍然供个人使用。OpenSuse的真正竞争对手是红帽企业级Linux。它使用Yast来管理软件包。有了Yast,使用和管理服务器应用程序就非常容易。此外,Yast安装向导程序可以配置电子邮件服务器、LDAP服务器、文件服务器或Web服务器,没有任何不必要的麻烦。它随带snapper快照管理工具,因而可以恢复或使用旧版的文件、更新和配置。由于让滚动发行版本成为可能的Tumbleweed,可将已安装的操作系统更新到最新版本,不需要任何的新发行版。\nopenSUSE - Linux OS. 桌面用户、开发者以及系统管理员的匠之所选。\nKali Linux\nKali Linux是Debian的一款衍生版。Kali旨在用于渗透测试。它大概在三个月前才发行。Kali的前身是Backtrack。用于Debian的所有Binary软件包都可以安装到Kali Linux上,而Kali的魅力或威力就来自于此。此外,支持Debian的用户论坛为Kali加分不少。Kali随带许多的渗透测试工具,无论是Wifi、数据库还是其他任何工具,都设计成立马可以使用。Kali使用APT来管理软件包。\nKali Linux | Penetration Testing and Ethical Hacking Linux Distribution\nArch Linux\nArch是一款采用滚动发行方式的操作系统:只要安装一次就够了;每当发行了某个新版本,就可以升级发行版,不需要重新安装。Pacman是Arch Linux的软件包管理器。Arch Linux既支持X86处理器架构,又支持X86_64架构,安装程序可以从光盘或U盘来运行。Arch旨在从开发者的角度而不是从用户的角度做到力求简单。Arch配置和安装起来超容易。\nArch Linux\n\n2.3Linux系统目录\nLinux是树型目录结构，只有一个根目录’/‘，其余各个目录都是基于这个根目录发散，就是树形结构。将某个分区挂到一个对应的目录上，例如/home对应一块分区,home目录就是这块分区的挂载点、/boot对应一块分区,boot目录就是这块分区的挂载点。可以形象的理解：一棵大树(根目录/)有很多树枝(目录，挂载点)，每个树枝上都挂着一个箱子(分区，存储空间)。\nLinux 中的每一个分区都是构成支持一组文件和目录所必需的贮存区的一部分。它是通过挂载(mounting)来实现的，挂载是将分区关联到某一目录的过程，在linux系统中，磁盘分区后，需要将其挂载到其它目录下，才可以进行访问。将设备文件中的顶级目录连接到 Linux 根目录下的某一目录（最好是空目录），访问此目录就等同于访问设备文件，如果不挂载，通过Linux系统中的图形界面系统可以查看找到硬件设备，但命令行方式无法找到。挂载分区使起始于这个指定目录(通称为挂载点，mount point)的贮存区能够被使用。\n总之，Linux 系统使用任何硬件设备，都必须将设备文件与已有目录文件进行挂载\n例如，如果分区 /dev/hda5 被 挂载在 /usr 上，这意味着所有在 /usr 之下的文件和目录在物理意义上位于 /dev/hda5 上。因此文件 /usr/share/doc/FAQ/txt/Linux-FAQ 被储存在 /dev/hda5上。\n/usr 之下的一个或多个目录还有可能是其它分区的挂载点。例如，某个分区(假设为，/dev/hda7)可以被挂载到 /usr/local 下，这意味着 /usr/local/man/whatis 将位于 /dev/hda7 上而不是 /dev/hda5 上。 \n/：根目录，一般根目录下只存放目录，在Linux下有且只有一个根目录。所有的东西都是从这里开始。当你在终端里输入“/home”，你其实是在告诉电脑，先从/（根目录）开始，再进入到home目录。\n/boot：放置linux系统启动时用到的一些文件，如Linux的内核文件：/boot/vmlinuz，系统引导管理器：/boot/grub。\n/bin: Binary的缩写, 这个目录存放着可执行二进制文件的目录，如常用的命令ls、tar、mv、cat等。\n/sbin: 放置系统管理员使用的可执行命令，如fdisk、shutdown、mount 等。与 /bin 不同的是，这几个目录是给系统管理员 root使用的命令，一般用户只能”查看”而不能设置和使用。\n文件夹下的箭头是软链接（快捷方式）\n/dev：Device(设备)的缩写，存放linux系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，常用的是挂载光驱 mount /dev/cdrom /mnt。\n/dev/console: 系统控制台，也就是直接和系统连接的监视器。\n/dev/sda:硬盘驱动程序接口。如：/dev/sda指的是第一个硬盘，sda1则是指/dev/sda的第一个分区。如系统中有其他的硬盘，则依次为/dev/sdb /dev/sdc\n/dev/fd: 软驱设备驱动程序。如： /dev/fd0指系统的第一个软盘，也就是通常所说的a：盘，/dev/fd1指第二个软盘.\n/dev/stscsi:磁带驱动器驱动程序。\n/dev/tty: 提供虚拟控制台支持。如:/dev/tty1指的是系统的第一个虚拟控制台， /dev/tty2则是系统的第二个虚拟控制台。\n/dev/pty: 提供远程登陆伪终端支持。在进行telnet登录时就要用到/dev/pty设备。\n/dev/ttys: 计算机串行接口，对于windows来说就是com1口。 9. /dev/cua 计算机串行接口，与调制解调器一起使用的设备。\n/dev/null: “黑洞”，所有写入该设备的信息都将消失。例如：当想要将屏幕上的输出信息隐藏起来时，只要将输出信息输出到/dev/null中即可。\n/etc：系统配置文件存放的目录，不建议在此目录下存放可执行文件，重要的配置文件有 /etc/inittab、/etc/fstab、/etc/init.d、/etc/X11、/etc/sysconfig、/etc/xinetd.d。\n/etc/rc或/etc/rc.d：启动、或改变运行级时运行的脚本或脚本的目录。\n/etc/passwd:用户数据库，其中的域给出了用户名、用户描述、用户起始目录、加密口令和用户的其他信息。\n/etc/group: 类似/etc/passwd ，但说明的不是用户信息而是组的信息。包括组的各种数据。\n/etc/fdprm: 软盘参数表，用以说明不同的软盘格式。可用setfdprm 进行设置。更多的信息见setfdprm的帮助页。\n/etc/fstab:指定启动时需要自动安装的文件系统列表。也包括用swapon -a启用的swap区的信息。\n/etc/inittab: init 的配置文件。\n/etc/issue: 包括用户在登录提示符前的输出信息。通常包括系统的一段短说明或欢迎信息。具体内容由系统管理员确定。\n/etc/magic: file的配置文件。包含不同文件格式的说明，file基于它猜测文件类型。\n/etc/motdmotd:是message of the day的缩写，用户成功登录后自动输出。内容由系统管理员确定。常用于通告信息，如计划关机时间的警告等。\n/etc/mtab: 当前安装的文件系统列表。由脚本( s c r i t p )初始化，并由mount 命令自动更新。当需要一个当前安装的文件系统的列表时使用(例如df 命令)。\n/etc/shadow: 在安装了影子( shadow)口令软件的系统上的影子口令文件。影子口令文件将/etc/passwd文件中的加密口令移动到/etc/shadow中，而后者只对超级用户(root)可读。这使破译口令更困难，以此增加系统的安全性。\n/etc/login.defs: login命令的配置文件。\n/etc/printcap: 类似/etc/termcap ，但针对打印机。语法不同。\n/etc/profile 、/ etc/csh.login、/etc/csh.cshrc: 登录或启动时bourne或cshells执行的文件。这允许系统管理员为所有用户建立全局缺省环境。\n/etc/securetty: 确认安全终端，即哪个终端允许超级用户(root)登录。一般只列出虚拟控制台，这样就不可能(至少很困难)通过调制解调器(modem )或网络闯入系统并得到超级用户特权。\n/etc/shells: 列出可以使用的shell。chsh 命令允许用户在本文件指定范围内改变登录的shell。提供一台机器f t p服务的服务进程ftpd 检查用户shell是否列在/etc/shells 文件中，如果不是，将不允许该用户登录。\n/etc/termcap: 终端性能数据库。说明不同的终端用什么“转义序列”控制。写程序时不直接输出转义序列(这样只能工作于特定品牌的终端)，而是从/etc/termcap 中查找要做的工作的正确序列。这样，多数的程序可以在多数终端上运行。\n/usr：可记为Unix Software Resource。应用程序存放目录，类似于windows下的program files目录。\n/usr/bin :存放应用程序。\n/usr/share: 存放共享数据。\n/usr/lib: 存放不能直接运行的，却是许多程序运行所必需的一些函数库文件。\n/usr/src：内核源代码默认的放置目录。\n/usr/local: 存放软件升级包。\n/usr/share/doc: 系统说明文件存放目录。\n/usr/share/man: 程序说明文件存放目录。\n/var：放置系统执行过程中经常变化的文件\n/var/log/message：所有的登录文件存放目录。\n/var/spool/mail：邮件存放的目录。\n/var/run:程序或服务启动后，其PID存放在该目录下。\n/var 包括系统一般运行时要改变的数据.每个系统是特定的，即不通过网络与其他计算机共享。\n/var/catman: 当要求格式化时的man页的cache.man页的源文件一般存在/usr/man/man 中；有些man页可能有预格式化的版本，存在/usr/man/cat 中.而其他的man页在第一次看时需要格式化，格式化完的版本存在/var/man 中，这样其他人再看相同的页时就无须等待格式化了. (/var/catman 经常被清除，就象清除临时目录一样.)\n/var/lib: 系统正常运行时要改变的文件。\n/var/local:/usr/local 中安装的程序的可变数据(即系统管理员安装的程序).注意，如果必要，即使本地安装的程序也会使用其他/var 目录，例如/var/lock 。\n/var/lock: 锁定文件.许多程序遵循在/var/lock 中产生一个锁定文件的约定，以支持他们正在使用某个特定的设备或文件.其他程序注意到这个锁定文件，将不试图使用这个设备或文件。\n/var/log: 各种程序的Log文件，特别是login (/var/log/wtmp log所有到系统的登录和注销) 和syslog (/var/log/messages 里存储所有核心和系统程序信息. /var/log 里的文件经常不确定地增长，应该定期清除。\n/var/run: 保存到下次引导前有效的关于系统的信息文件.例如， /var/run/utmp 包含当前登录的用户的信息。\n/var/spool，/var/mail, /var/news 打印队列和其他队列工作的目录.每个不同的spool在/var/spool 下有自己的子目录，例如，用户的邮箱在/var/spool/mail 中。\n/home：系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，表示当前用户的家目录，edu 表示用户 edu 的家目录。\n/lib: Library的缩写，系统使用的函数库（动态连接共享库）的目录，程序在执行过程中，需要调用一些额外的参数时需要函数库的协助。\n/lost+found：系统异常产生错误时，会将一些遗失的片段放置于此目录下。\n/media：linux系统自动识别设备，例如U盘、光驱并挂载到这个目录下。\n/mnt:光盘默认挂载点，通常光盘挂载于 /mnt/cdrom 下，也不一定，可以选择任意位置进行挂载。\n/opt：(optional)给主机额外安装软件所摆放的目录。\n/proc：这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。此目录的数据都在内存中，如系统核心，外部设备，网络状态，由于数据都存放于内存中，所以不占用磁盘空间，比较重要的目录有 /proc/cpuinfo、/proc/interrupts、/proc/dma、/proc/ioports、/proc/net/* 等。\n/root：系统管理员root的家目录。\n/tmp：一般用户或正在执行的程序临时存放文件的目录，任何人都可以访问，重要数据不可放置在此目录下。\n/srv：服务启动之后需要访问的数据目录，如 www 服务需要访问的网页数据存放在 /srv/www 内。\n/run：是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。\n/sys： 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。该文件系统是内核设备树的一个直观反映。当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。\n2.4网络连接模式\n桥接方式\n桥接方式下，虚拟机和宿主机处于同一网段，真实存在于网络中，像是一台真实的主机。虚拟机和宿主机彼此互通，且网络中的其他主机也可以互通。就像是连接在hub中的主机一样。\n\nNAT方式\nNAT方式全称Network Address Translation，即借助网络地址转换功能，通过宿主机所在的网络实现访问互联网。此种方式下，虚拟机并不真实的存在于网络中，所以宿主机无法ping通虚拟机，虚拟机彼此间也不通。但是通过nat虚拟机可以访问互联网，且可以访问宿主机以及宿主机同网络中的其他主机。\nhost-only方式\nhost-only方式下，虚拟机和真实的网络是互相隔离的，不过所有的虚拟机彼此是可以互通的，可是访问不了互联网。宿主机可以访问虚拟机。\n2.5运行级别1）CentOS6 的运行级别\n\n2）CentOS7 的运行级别简化为: \nmulti-user.target 等价于原运行级别 3\n graphical.target 等价于原运行级别 5\n 3） 查看当前运行级别:\n systemctl get-default\n 4）修改当前运行级别 \nsystemctl set-default TARGET.target （这里 TARGET 取 multi-user 或者 graph\n2.6网络管理手册与链接配置和管理网络 | Red Hat Product Documentation\n部分 II. 管理 IP 网络 | Red Hat Product Documentation\nCentos8的网络管理 - 运维小菜鸟 - 博客园 (cnblogs.com)\n使用nmcli/nmtui/ifcfg在RHEL8/CentOS8系统中配置静态IP地址 图文教程 - AndyX.Net - 安迪克斯\n2.7NetworkManager网络管理服务第 2 章 NetworkManager 入门 | Red Hat Product Documentation\n部分 II. 管理 IP 网络 | Red Hat Product Documentation\n2.8防火墙管理1:查看防火状态\nsystemctl status firewalld\n2:暂时关闭防火墙\nsystemctl stop firewalld\n3:永久关闭防火墙\nsystemctl disable firewalld\n4:重启防火墙\nsystemctl enable firewalld\nfirewalld\nCentos7默认安装了firewalld，如果没有安装的话，可以使用 yum install firewalld firewalld-config进行安装。\n1.启动防火墙\nsystemctl start firewalld2.禁用防火墙\nsystemctl stop firewalld3.设置开机启动\nsystemctl enable firewalld4.停止并禁用开机启动\nsytemctl disable firewalld5.重启防火墙\nfirewall-cmd —reload6.查看状态\nsystemctl status firewalld或者 firewall-cmd —state7.查看版本\nfirewall-cmd —version8.查看帮助\nfirewall-cmd —help9.查看区域信息\nfirewall-cmd —get-active-zones10.查看指定接口所属区域信息\nfirewall-cmd —get-zone-of-interface=eth011.拒绝所有包\nfirewall-cmd —panic-on12.取消拒绝状态\nfirewall-cmd —panic-off13.查看是否拒绝\nfirewall-cmd —query-panic14.将接口添加到区域(默认接口都在public)\nfirewall-cmd —zone=public —add-interface=eth0(永久生效再加上 —permanent 然后reload防火墙)15.设置默认接口区域\nfirewall-cmd —set-default-zone=public(立即生效，无需重启)16.更新防火墙规则\nfirewall-cmd —reload或firewall-cmd —complete-reload(两者的区别就是第一个无需断开连接，就是firewalld特性之一动态添加规则，第二个需要断开连接，类似重启服务)17.查看指定区域所有打开的端口\nfirewall-cmd —zone=public —list-ports18.在指定区域打开端口（记得重启防火墙）\nfirewall-cmd —zone=public —add-port=80/tcp(永久生效再加上 —permanent)\nfirewall-cmd —zone=public —add-port=80/tcp —permanent\n注：开启后需要重启防火墙才生效\n【重启命令】： firewall-cmd —reload\n其他常用命令：\nfirewall-cmd —state ##查看防火墙状态，是否是runningfirewall-cmd —reload ##重新载入配置，比如添加规则之后，需要执行此命令firewall-cmd —get-zones ##列出支持的zonefirewall-cmd —get-services ##列出支持的服务，在列表中的服务是放行的firewall-cmd —query-service ftp ##查看ftp服务是否支持，返回yes或者nofirewall-cmd —add-service=ftp ##临时开放ftp服务firewall-cmd —add-service=ftp —permanent ##永久开放ftp服务firewall-cmd —remove-service=ftp —permanent ##永久移除ftp服务firewall-cmd —add-port=80/tcp —permanent ##永久添加80端口firewall-cmd —remove-port=80/tcp —permanent ##永久移除80端口firewall-cmd —zone=public —list-ports ##查看已开放的端口\niptables -L -n ##查看规则，这个命令是和iptables的相同的man firewall-cmd\n1、开放端口\nfirewall-cmd —zone=public —add-port=5672/tcp —permanent # 开放5672端口\nfirewall-cmd —zone=public —remove-port=5672/tcp —permanent #关闭5672端口\nfirewall-cmd —reload # 配置立即生效\n2、查看防火墙所有开放的端口\nfirewall-cmd —zone=public —list-ports\n3.、关闭防火墙\n如果要开放的端口太多，嫌麻烦，可以关闭防火墙，安全性自行评估\nsystemctl stop firewalld.service\n4、查看防火墙状态\nfirewall-cmd —state\n5、查看监听的端口\nnetstat -lnpt\nPS:centos7默认没有 netstat 命令，需要安装 net-tools 工具，yum install -y net-tools\n6、检查端口被哪个进程占用\nnetstat -lnpt |grep 5672\n7、查看进程的详细信息\nps 6832\n8、中止进程\nkill -9 6832\n说明：–zone 作用域–add-port=8080/tcp 添加端口，格式为：端口/通讯协议–permanent #永久生效，没有此参数重启后失效\n2.9 RPMRPM（RedHat Package Manager），RedHat软件包管理工具，类似windows里面的setup.exe 是Linux这系列操作系统里面的打包安装工具\n它会建立统一的数据库文件，详细记录软件包安装 、卸载等变化信息，能够自动分析软件包依赖关系\n它最大的特点就是将你要安装的软件先编译过，并且打包成为 RPM 机制的文件，通过打包好的软件里面默认的数据库，记录这个软件要安装的时候必须具备的依赖属性软件。当在你的 Linux 主机安装时，RPM 会先依照软件里面的数据查询Linux 主机的依赖属性软件是否满足，若满足则子以安装，若不满足则不子安装。那么安装的时候就将该软件的信息整个写入 RPM 的数据库中，以便未来的查询、验证与反安装。\nRPM包的名称格式 \nApache-1.3.23-11.i386.rpm——\n\n“apache” 软件名称\n“1.3.23-11”软件的版本号，主版本和此版本\n“i386”是软件所运行的硬件平台，\nIntel32位处理器的统称 \n“rpm”文件扩展名，代表RPM包\n\n查询命令\n1）基本语法 \n123rpm -qa 软件名称rpm -qi 软件名称\n2）经验技巧 （功能描述：查询所安装的所有rpm软件包） 由于软件包比较多，一般都会采取过滤。\nrpm-qa|grep rpm软件包\n卸载命令\n1）基本语法 \n123rpm -e RPM软件包 rpm -e --nodeps 软件包 \n安装命令\n1）基本语法 \nrpm-ivh RPM包全名\n2）选项说明\n\n2.10 YUMYUM（全称为YellowdogUpdater, Modified）是一个在 Fedora和 RedHat以及 CentOS 中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包 并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次 次下载、安装\n1）基本语法 \n1yum [选项][参数] \n2）选项说明\n-y 同意\n3）参数说明\n显示：yum list installed\n\n\n三、Gnome下的快捷键Alt + F1：类似Windows下的Win键，在GNOME中打开”应用程序”菜单(Applications) （在KDE下同样适用。）\nAlt + F9：最小化窗口 Alt + F10：最大化窗口Alt + space：打开窗口的控制菜单 (点击窗口左上角图标出现的菜单)单击鼠标滚轮：当您选中一段文字后，按下鼠标滚轮键，可以将选中的文字复制到鼠标所指的位置，在中端中也有效。 （KDE中未测试。）Atl+单击鼠标拖动：移动窗口的位置。 （在KDE和Compiz中同样有效。）\nShift+Ctrl+N - 新建文件夹, 很有用\nAlt + Enter - 查看选择文件/文件夹的属性，代替单击右键选择属性\nCtrl + 1/2 - 改变文件夹视图查看方式，图标视图/列表视图\nCtrl + W - 关闭当前Nautilus窗口\nCtrl + Shift + W - 关闭所有Nautilus窗口\nCtrl+T - 在Nautilus中新建一个标签\nAlt + Up/Down Arrow - 移动到父文件夹/选择的文件夹\nAlt + Left/Right Arrow - 后退/前进\nAlt + Home -直接移动到主文件夹\nF9 - 开关显示Nautilus侧边栏\nCtrl + H -开关显示隐藏文件夹\n\n四、VIM编辑器vi概述vi（visual editor）编辑器通常被简称为vi，它是Linux和Unix系统上最基本的文本编辑器，类似于Windows 系统下的notepad（记事本）编辑器。\nvim编辑器Vim(Vi improved)是vi编辑器的加强版，比vi更容易使用。vi的命令几乎全部都可以在vim上使用。\n4.1 vim编辑器的四种模式命令模式使用VIM编辑器时，默认处于命令模式。在该模式下可以移动光标位置，可以通过快捷键对文件内容进行复制、粘贴、删除等操作。\n编辑模式或输入模式在命令模式下输入小写字母a或小写字母i即可进入编辑模式，在该模式下可以对文件的内容进行编辑\n末行模式在命令模式下输入冒号:即可进入末行模式，可以在末行输入命令来对文件进行查找、替换、保存、退出等操作\n可视化模式可以做一些列选操作（通过方向键选择某些列的内容,类似于Windows鼠标刷黑）\n\n模式间切换方法：（1）命令模式下，输入:后，进入末行模式（2）末行模式下，按esc慢退、按两次esc快退、或者删除所有命令，可以回到命令模式（3）命令模式下，按下i、a等键，可以计入编辑模式（4）编辑模式下，按下esc，可以回到命令模式\n4.2 VIM编辑器的使用\n使用vim打开文件基本语法：\n1vim  文件名称\n① 如果文件已存在，则直接打开\n② 如果文件不存在，则vim编辑器会自动在内存中创建一个新文件\n案例：使用vim命令打开readme.txt文件\n1vim readme.txt\n\n\n\n\nVi 使用的选项\n说 明\n\n\n\n\nvim filename\n打开或新建一个文件，并将光标置于第一行的首部\n\n\nvim -r filename\n恢复上次 vim 打开时崩溃的文件\n\n\nvim -R filename\n把指定的文件以只读方式放入 Vim 编辑器中\n\n\nvim + filename\n打开文件，并将光标置于最后一行的首部\n\n\nvi +n filename\n打开文件，并将光标置于第 n 行的首部\n\n\nvi +/pattern filename\n打幵文件，并将光标置于第一个与 pattern 匹配的位置\n\n\nvi -c command filename\n在对文件进行编辑前，先执行指定的命令\n\n\n\n\nvim编辑器保存文件在任何模式下，连续按两次Esc键，即可返回到命令模式。然后按冒号:，进入到末行模式，输入wq，代表保存并退出。\n\n建议使用:x：使用效果等同于wq，如果文件有改动则先保存后退出；但是如果文件没有做修改，会直接退出，不会修改文件更新时间，避免用户混淆文件的修改时间\n\nvim编辑器强制退出（不保存）在任何模式下，连续按两次Esc键，即可返回到命令模式。然后按冒号：，进入到末行模式，输入q!，代表强制退出但是不保存文件。\n\n命令模式下的相关操作如何进入命令模式\n在Linux操作系统中，当我们使用vim命令直接打开某个文件时，默认进入的就是命令模式。如果我们处于其他模式（编辑模式、可视化模式以及末行模式）可以连续按两次Esc键也可以返回命令模式\n1.光标移动\n\n\n\n快捷键\n功能描述\n\n\n\n\njkhl\n基本上下左右\n\n\ngg\n光标移动到文档首行\n\n\nG\n光标移动到文档尾行\n\n\n^或_\n光标移动到行首第一个非空字符\n\n\nhome键或0或者g0\n光标移动到行首第一个字符\n\n\ng_\n光标移动到行尾最后一个非空字符\n\n\nend或者 g\n光标移动到行尾最后一个字符\n\n\ngm\n光标移动到当前行中间处\n\n\nb/B\n光标向前移动一个单词（大写忽略/-等等特殊字符）\n\n\nw/W\n光标向后移动一个单词（大写忽略/-等等特殊字符）\n\n\ne/E\n移到单词结尾（大写忽略/-等等特殊字符）\n\n\nctrl+b或pageUp键\n翻屏操作，向上翻\n\n\nctrl+f或pageDn键\n翻屏操作，向下翻\n\n\n行号+G\n快速将光标移动到指定行\n\n\n`.\n移动到上次编辑处\n\n\n数字+上下方向键\n以当前光标为准，向上/下移动n行\n\n\n数字+左右方向键\n以当前光标为准，向左/右移动n个字符\n\n\nH\n移动到屏幕顶部\n\n\nM\n移动到屏幕中间\n\n\nL\n移动到屏幕尾部\n\n\nz+Enter键\n当前行在屏幕顶部\n\n\nz+ .\n当前行在屏幕中间\n\n\nz+ -\n当前行在屏幕底部\n\n\nshift+6\n光标移动到行首\n\n\nshift+4\n光标移动到行尾\n\n\n-\n移动到上一行第一个非空字符\n\n\n+\n移动到下一行第一个非空字符\n\n\n)\n向前移动一个句子\n\n\n(\n向后移动一个句子\n\n\n}\n向前移动一个段落\n\n\n{\n向前移动一个段落\n\n\ncount l\n移动到count 列\n\n\ncounth\n向左移动count 字符\n\n\ncountl\n向右移动count字符\n\n\ncountgo\n移动到count字符\n\n\n\n\n2.选中内容\n\n\n\n快捷键\n功能描述\n\n\n\n\nv\n进行字符选中\n\n\nV 或shift+v\n进行行选中\n\n\ngv\n选中上一次选择的内容\n\n\no\n光标移动到选中内容另一处结尾\n\n\nO\n光标移动到选中内容另一处角落\n\n\nctr + V\n进行块选中\n\n\n\n\n3.复制（配合粘贴命令p使用）\n\n\n\n快捷键\n功能描述\n\n\n\n\ny\n复制已选中的文本到剪贴板\n\n\nn+yy\n复制光标所在行，此命令前可以加数字 n，可复制多行\n\n\nyw\n复制光标位置的单词\n\n\nctrl+v + 方向键+yy\nctrl+v，并按方向键选中区块，按下yy复制\n\n\n\n\n4.剪切\n\n\n\n快捷键\n功能描述\n\n\n\n\ndd\n剪切光标所在行\n\n\n数字+dd\n以光标所在行为准（包含当前行），向下剪切指定行数\n\n\nD\n剪切光标所在行\n\n\n\n\n5.粘贴\n\n\n\n快捷键\n功能描述\n\n\n\n\np\n将剪贴板中的内容粘贴到光标后\n\n\nP（大写）\n将剪贴板中的内容粘贴到光标前\n\n\n\n\n6.删除\n\n\n\n快捷键\n功能描述\n\n\n\n\nx\n删除光标所在位置的字符\n\n\nX(大写)\n删除光标前一个字符\n\n\ndd\n删除光标所在行，删除之后，下一行上移\n\n\nD\n删除光标位置到行尾的内容，删除之后，下一行不上移\n\n\nndd\n删除当前行（包括此行）后 n 行文本\n\n\ndw\n移动光标到单词的开头以删除该单词\n\n\ndG\n删除光标所在行一直到文件末尾的所有内容\n\n\n:a1,a2d\n删除从 a1 行到 a2 行的文本内容\n\n\n\n\n7.撤销/恢复\n\n\n\n快捷键\n功能描述\n\n\n\n\nu\n撤销最近修改\n\n\nctrl+r\n恢复\n\n\nU(大写)\n撤销当前行所有编辑\n\n\n\n\n8.字符转换\n\n\n\n快捷键\n功能描述\n\n\n\n\n~\n转换大小写\n\n\nu\n变成小写\n\n\nU\n变成大写\n\n\n\n\n9.编辑命令的快捷键\n\n\n\n快捷键\n功能描述\n\n\n\n\n↑或ctr + p\n上一条命令\n\n\n↓或ctr + n\n下一条命令\n\n\nctr + b\n移动到命令行开头\n\n\nctr + e\n移动到命令行结尾\n\n\nctr + ←\n向左一个单词\n\n\nctr + →\n向右一个单词\n\n\n\n\n末行模式下的相关操作在命令模式下使用冒号：的方式进入\n1.保存/退出文件操作\n\n\n\n命令\n功能描述\n\n\n\n\n:wq\n保存并退出 Vim 编辑器\n\n\n:wq!\n保存并强制退出 Vim 编辑器\n\n\n:q\n不保存就退出 Vim 编辑器\n\n\n:q!\n不保存，且强制退出 Vim 编辑器\n\n\n:w\n保存但是不退出 Vim 编辑器\n\n\n:w!\n强制保存文本\n\n\n:w filename\n另存到 filename 文件\n\n\nx！\n保存文本，并退出 Vim 编辑器\n\n\nZZ\n直接退出 Vim 编辑器\n\n\n\n\n2.查找：“/关键词”在查找结果中，用N、n可以切换上下结果；输入nohl，可以取消高亮\n\n\n\n\n快捷键\n功能描述\n\n\n\n\n/abc\n从光标所在位置向前查找字符串 abc\n\n\n/^abc\n查找以 abc 为行首的行\n\n\n/abc$\n查找以 abc 为行尾的行\n\n\n?abc\n从光标所在位置向后查找字符串 abc\n\n\nn或；\n向同一方向重复上次的查找指令\n\n\nN或,\n向相反方向重复上次的查找指定\n\n\n\n\n3.替换\n\n\n\n快捷键\n功能描述\n\n\n\n\nr\n替换光标所在位置的字符\n\n\nR\n从光标所在位置开始替换字符，其输入内容会覆盖掉后面等长的文本内容，按“Esc”可以结束\n\n\n:s/a1/a2\n替换当前光标所在行第一处符合条件的内容\n\n\n:s/a1/a2/g\n替换当前光标所在行所有的 a1 都用 a2 替换\n\n\n:%s/a1/a2\n替换所有行中，第一处符合条件的内容\n\n\n:%s/a1/a2/g\n替换所有行中，所有符合条件的内容\n\n\n:n1,n2 s/a1/a2\n将文件中 n1 到 n2 行中第一处 a1 都用 a2 替换\n\n\n:n1,n2 s/a1/a2/g\n将文件中 n1 到 n2 行中所有 a1 都用 a2 替换\n\n\n\n\n4.行号显示：“: set nu”;\n行号显示:set nu\n取消行号显示：:set nonu\n\n5.文件切换使用vim打开多个文件后，在末行模式下可以进行切换。\n查看当前已经打开的所有文件：:files(%a表示激活状态，#表示上一个打开的文件)切换到指定文件：:open 文件名切换到上一个文(back previous)：:bp切换到下一个文件(back next)：:bn\n6.编辑模式下的相关操作\n\n\n\n快捷键\n功能描述\n\n\n\n\ni\n在当前光标所在位置插入，光标后的文本相应向右移动\n\n\nI\n在光标所在行的行首插入，行首是该行的第一个非空白字符，相当于光标移动到行首执行 i 命令\n\n\no\n在光标所在行的下插入新的一行。光标停在空行首，等待输入文本\n\n\nO（大写）\n在光标所在行的上插入新的一行。光标停在空行的行首，等待输入文本\n\n\na\n在当前光标所在位置之后插入\n\n\nA\n在光标所在行的行尾插入，相当于光标移动到行尾再执行 a 命令\n\n\nesc键\n退出编辑模式\n\n\n\n\n7.扩展代码颜色显示：“：syntax on/off”\nvim内置计算器：\na.进入编辑模式b.按“ctrl+r，光标变成引号，，输入=，光标转到最后一行c.输入需要计算的内容，按下enter后，计算结果回替代上一步中的引号，光标恢复\nvim的配置\na.文件打开时，末行模式下输入的配置为临时配置，关闭文件后配置无效b.修改个人配置文件，可以永久保存个人配置（~/.vimrc，如果没有可以自行创建）c.修改全局配置文件，对每个用户生效（vim自带，/etc/vimrc）\n注：个人配置文件优先级更高，当个人配置和全局配置发生冲突时，系统以当前用户的个人配置文件为准\n异常退出\n在编辑文件后，未正常保存退出时，会产生异常退出交换文件（.原文件名.swp）将交换文件删除后，再次打开文件时，无提示：“#rm -f .原文件名.swp”\n别名机制：自定义指令\nLinux中，存在一个别名映射文件： ~/.bashrc修改文件内容，可以自定义指令，重新登录账号后生效\n文件快捷方式\n对于深层文件，可以创建文件快捷方式，便于后续操作：#ln -s 源路径 新路径\n\n五、Shell命令解释器快捷键编辑命令Ctrl + a ：移到命令行首Ctrl + e ：移到命令行尾Ctrl + f ：按字符前移（右向）Ctrl + b ：按字符后移（左向）Alt + f ：按单词前移（右向）Alt + b ：按单词后移（左向）Ctrl + xx：在命令行首和光标之间移动Ctrl + u ：从光标处删除至命令行首Ctrl + k ：从光标处删除至命令行尾Ctrl + w ：从光标处删除至字首Alt + d ：从光标处删除至字尾Ctrl + d ：删除光标处的字符Ctrl + h ：删除光标前的字符Ctrl + y ：粘贴至光标后（主要针对CTRL+U或CTRL+W）Alt + c ：从光标处更改为首字母大写的单词Alt + u ：从光标处更改为全部大写的单词Alt + l ：从光标处更改为全部小写的单词Ctrl + t ：交换光标处和之前的字符Alt + t ：交换光标处和之前的单词Alt + Backspace：与 Ctrl + w 相同类似，分隔符有些差别\n搜索命令Ctrl + r：逆向搜索命令历史Ctrl + g：从历史搜索模式退出Ctrl + p：历史中的上一条命令Ctrl + n：历史中的下一条命令Alt + .：打印之前执行过的命令的最后一部分 以空格为分隔符\n控制命令Ctrl + l：清屏Ctrl + o：执行当前命令，并选择上一条命令Ctrl + s：阻止屏幕输出Ctrl + q：允许屏幕输出Ctrl + c：终止命令Ctrl + z：挂起命令ESC + .：自动补全最近的命令TAB：自动补全Ctrl + m：换行Ctrl + L：撤销操作\n其它!!                      – 执行上一条命令\n**!ifconfig     – 执行最近运行过的以ifconfig开头的命令\n!ifconfig:p      – 打印!ifconfig要执行的命令（并将其作为最后一条命令加入到命令历史中）\n!$                    – 上一条命令的最后一个单词 (等同于Alt + .)\n!:p                  – 打印!指代的单词\n!*                    – 上一条命令除最后一个词的部分\n\n六、命令行基本操作6.1 Linux终端命令格式命令提示符命令提示符解析：\n1[deng@localhost ~]$ \n[]：这是提示符的分隔符号，没有特殊含义。\ndeng：显示的是当前的登录用户\n@：分隔符号，没有特殊含义。\nlocalhost：当前系统的简写主机名（完整主机名是 localhost.localdomain）。\n~：代表用户当前所在的目录，此例中用户当前所在的目录是家目录。\n“#”：命令提示符，Linux 用这个符号标识登录的用户权限等级。如果是超级用户，提示符就是 #；如果是普通用户，提示符就是$。\n用户家目录Linux 系统是纯字符界面，用户登录后，要有一个初始登录的位置，这个初始登录位置就称为用户的家：主目录超级用户的家目录：/root。普通用户的家目录：/home/用户名。\n用户在自己的家目录中拥有完整权限，所以我们也建议操作实验可以放在家目录中进行。我们切换一下用户所在目录，看看有什么效果。\n123[deng@localhost ~]$ cd /usr/local/src[deng@localhost src]$ \n如果切换用户所在目录，那么命令提示符中的会变成用户当前所在目录的最后一个目录（不显示完整的所在目录 /usr/ local/src，只显示最后一个目录 src)。\n命令的基本格式格式：\n12命令 [选项] [参数]command [-options] [parameter]\n命令： 可执行文件\n选项：用于调整命令的功能。命令不同，选项的个数和内容会有所不同；要实现的命令功能不同，选项的个数和内容也会有所不同。\n参数：是命令处理的对象，通常情况可以是文件名、目录、或用户名。\n温馨提示：命令格式中的 [] 代表可选项，也就是有些命令可以不写选项或参数，也能执行。\n选项的作用选项的作用就是调整命令的功能。\nls 命令之后不加选项和参数也能执行，不过只能执行最基本的功能，即显示当前目录下的文件名。\n123[deng@localhost local]$ lsbin  etc  games  include  lib  lib64  libexec  sbin  share  src  ssl[deng@localhost local]$ \n如果加一个”-l”选项，则可以看到显示的内容明显增多了。”-l”是长格式（long list）的意思，也就是显示文件的详细信息。至于 “-l” 选项的具体含义，我们稍后再详细讲解。可以看到选项的作用是调整命令功能。如果没有选项，那么命令只能执行最基本的功能；而一旦有选项，则可以显示更加丰富的数据。\n1234567891011121314[deng@localhost local]$ ls -l总用量 0drwxr-xr-x. 2 root root  92 7月   4 16:19 bindrwxr-xr-x. 2 root root   6 4月  11 2018 etcdrwxr-xr-x. 2 root root   6 4月  11 2018 gamesdrwxr-xr-x. 3 root root  33 2月  21 11:36 includedrwxr-xr-x. 2 root root   6 4月  11 2018 libdrwxr-xr-x. 4 root root 159 7月   4 16:19 lib64drwxr-xr-x. 2 root root   6 4月  11 2018 libexecdrwxr-xr-x. 2 root root   6 4月  11 2018 sbindrwxr-xr-x. 6 root root  60 11月 14 2018 sharedrwxr-xr-x. 2 root root   6 4月  11 2018 srcdrwxr-xr-x  5 root root 140 7月   4 16:19 ssl[deng@localhost local]$ \nLinux 的选项又分为短格式选项（-l）和长格式选项（–all）。\n短格式选项是英文的简写，前面有一个-号。\n123[deng@localhost local]$ ls -a.  ..  bin  etc  games  include  lib  lib64  libexec  sbin  share  src  ssl[deng@localhost local]$ \n而长格式选项是英文完整单词，前面用两个-号\n123[deng@localhost local]$ ls --all.  ..  bin  etc  games  include  lib  lib64  libexec  sbin  share  src  ssl[deng@localhost local]$ \n一般情况下，短格式选项是长格式选项的缩写，也就是一个短格式选项会有对应的长格式选项。当然也有例外，比如 ls 命令的短格式选项 -l 就没有对应的长格式选项。所以具体的命令选项可以通过后面我们要学习的帮助命令来进行査询。\n参数的作用参数是命令的操作对象，一般文件、目录、用户和进程等可以作为参数被命令操作。\n123456789101112[itcast@localhost ~]$ ls -l /home/itcast/总用量 0drwxr-xr-x 2 itcast itcast 6 7月  12 17:34 公共drwxr-xr-x 2 itcast itcast 6 7月  12 17:34 模板drwxr-xr-x 2 itcast itcast 6 7月  12 17:34 视频drwxr-xr-x 2 itcast itcast 6 7月  12 17:34 图片drwxr-xr-x 2 itcast itcast 6 7月  12 17:34 文档drwxr-xr-x 2 itcast itcast 6 7月  12 17:34 下载drwxr-xr-x 2 itcast itcast 6 7月  12 17:34 音乐drwxr-xr-x 2 itcast itcast 6 7月  12 17:34 桌面[itcast@localhost ~]$ \n命令一般都需要加入参数，用于指定命令操作的对象是谁。如果可以省略参数，则一般都有默认参数。\n123[itcast@localhost ~]$ ls公共  模板  视频  图片  文档  下载  音乐  桌面[itcast@localhost ~]$ \n这个 ls 命令后面没有指定参数，默认参数是当前所在位置，所以会显示当前目录下的文件名。\n\n注意事项命令、命令选项、命令参数之间用空格隔开。\n\n123456[itcast@localhost ~]$ ls -a .              .bash_profile  .dbus          .local    视频  音乐..             .bashrc        .esd_auth      .mozilla  图片  桌面.bash_history  .cache         .ICEauthority  公共      文档.bash_logout   .config        .kshrc         模板      下载[itcast@localhost ~]$ \n当有多个命令选项时，可以进行合并。例如，可以将 ls -a / 和 ls -l / 合并为\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[itcast@localhost ~]$ ls -l -a总用量 28drwx------  15 itcast itcast 319 7月  12 17:34 .drwxr-xr-x.  5 root   root    46 7月  12 17:32 ..-rw-------   1 itcast itcast  36 7月  12 17:36 .bash_history-rw-r--r--   1 itcast itcast  18 4月  11 2018 .bash_logout-rw-r--r--   1 itcast itcast 193 4月  11 2018 .bash_profile-rw-r--r--   1 itcast itcast 231 4月  11 2018 .bashrcdrwxrwxr-x  13 itcast itcast 275 7月  12 17:36 .cachedrwxrwxr-x  14 itcast itcast 261 7月  12 17:36 .configdrwx------   3 itcast itcast  25 7月  12 17:34 .dbus-rw-------   1 itcast itcast  16 7月  12 17:34 .esd_auth-rw-------   1 itcast itcast 314 7月  12 17:34 .ICEauthority-rw-r--r--   1 itcast itcast 172 4月  11 2018 .kshrcdrwx------   3 itcast itcast  19 7月  12 17:34 .localdrwxr-xr-x   4 itcast itcast  39 11月  8 2018 .mozilladrwxr-xr-x   2 itcast itcast   6 7月  12 17:34 公共drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 模板drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 视频drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 图片drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 文档drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 下载drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 音乐drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 桌面[itcast@localhost ~]$ ls -al总用量 28drwx------  15 itcast itcast 319 7月  12 17:34 .drwxr-xr-x.  5 root   root    46 7月  12 17:32 ..-rw-------   1 itcast itcast  36 7月  12 17:36 .bash_history-rw-r--r--   1 itcast itcast  18 4月  11 2018 .bash_logout-rw-r--r--   1 itcast itcast 193 4月  11 2018 .bash_profile-rw-r--r--   1 itcast itcast 231 4月  11 2018 .bashrcdrwxrwxr-x  13 itcast itcast 275 7月  12 17:36 .cachedrwxrwxr-x  14 itcast itcast 261 7月  12 17:36 .configdrwx------   3 itcast itcast  25 7月  12 17:34 .dbus-rw-------   1 itcast itcast  16 7月  12 17:34 .esd_auth-rw-------   1 itcast itcast 314 7月  12 17:34 .ICEauthority-rw-r--r--   1 itcast itcast 172 4月  11 2018 .kshrcdrwx------   3 itcast itcast  19 7月  12 17:34 .localdrwxr-xr-x   4 itcast itcast  39 11月  8 2018 .mozilladrwxr-xr-x   2 itcast itcast   6 7月  12 17:34 公共drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 模板drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 视频drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 图片drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 文档drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 下载drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 音乐drwxr-xr-x   2 itcast itcast   6 7月  12 17:34 桌面[itcast@localhost ~]$ \n6.2 Linux文件类型\n\n\n\n前缀\n描述\n\n\n\n\n-\n普通文件。如文本文件、二进制可执行文件、源代码等。\n\n\nb\n块设备文件。硬盘可以使用块设备文件。\n\n\nc\n字符设备文件。硬盘也可以使用字符设备文件。\n\n\nd\n目录文件。目录可以包含文件和其他目录。\n\n\nl\n符号链接（软链接）。可以链接任何普通文件，类似于 Windows 中的快捷方式。\n\n\np\n具名管道。管道是进程间的一种通信机制。\n\n\ns\n用于进程间通信的套接字。\n\n\n\n\n普通文件\n我们用 ls -l 来查看某个文件的属性，可以看到有类似-rwxrwxrwx，值得注意的是第一个符号是 - ，这样的文件在Linux中就是普通文件。这些文件一般是用一些相关的应用程序创建，比如图像工具、文档工具、归档工具… … 或 cp工具等。这类文件的删除方式是用rm 命令。\n依照文件的内容，又大略可以分为：\n纯文本档(ASCII)\nLinux系统中最多的一种文件类型，称为纯文本档是因为内容为我们人类可以直接读到的数据，例如数字、字母等等。 几乎只要我们可以用来做为设定的文件都属于这一种文件类型。 举例来说，你可以用命令： cat ~/.bashrc 来看到该文件的内容。\n 二进制文件(binary)\nLinux系统其实仅认识且可以执行二进制文件(binary file)。Linux当中的可执行文件(scripts, 文字型批处理文件不算)就是这种格式的文件。常用的一些命令几乎都是二进制文件。\n数据格式文件(data)\n有些程序在运作的过程当中会读取某些特定格式的文件，那些特定格式的文件可以被称为数据文件 (data file)。举例来说，我们的Linux在使用者登录时，都会将登录的数据记录在 /var/log/wtmp那个文件内，该文件是一个数据文件，他能够通过last这个指令读出来。 但是使用cat时，会读出乱码，因为该文件是一种特殊格式的文件。\n1234[deng@localhost ~]$ ls -l /etc/bashrc -rw-r--r--. 1 root root 2853 4月  11 2018 /etc/bashrc[deng@localhost ~]$ \n目录文件\n当我们查看文件的详细信息的时候，看到有类似 drwxr-xr-x ，这样的文件就是目录。目录在Linux是一个比较特殊的文件。注意它的第一个字符是d。创建目录的命令可以用 mkdir 命令，cp可以把一个目录复制为另一个目录。删除用rm 或rmdir命令。\n123[deng@localhost ~]$ ls -ld /homedrwxr-xr-x. 5 root root 46 7月  12 17:32 /home[deng@localhost ~]$ \n字符设备\n字符设备是指在I/O传输过程中以字符为单位进行传输的设备，例如键盘，打印机等。在UNIX系统中，字符设备以特别文件方式在文件目录树中占据位置并拥有相应的结点。\n字符设备可以使用与普通文件相同的文件操作命令对字符设备文件进行操作，例如打开、关闭、读、写等。\n123[deng@localhost ~]$ ls -l /dev/input/mouse1crw-rw---- 1 root input 13, 33 7月  12 09:45 /dev/input/mouse1[deng@localhost ~]$\n我们看到/dev/input/mouse1的属性是crw-rw—— ，注意前面第一个字符是 c ，这表示字符设备文件。\n字符设备文件可以使用mknode来创建，用rm来删除。目前在最新的Linux发行版本中，我们一般不用自己来创建设备文件。因为这些文件是和内核相关联的。\n块设备\n块设备将信息存储在固定大小的块中，每个块都有自己的地址。数据块的大小通常在512字节到32768字节之间。块设备的基本特征是每个块都能独立于其它块而读写。磁盘是最常见的块设备。\n123[deng@localhost ~]$ ls -l /dev/sda1brw-rw---- 1 root disk 8, 1 7月  12 09:45 /dev/sda1[deng@localhost ~]$\n/dev/sda1 的属性是 brw-rw—— ，注意前面的第一个字符是b，这表示块设备，比如硬盘，光驱等设备。\n套接字\n套接字（socket）是一个抽象层，应用程序可以通过它发送或接收数据，可对其进行像对文件一样的打开、读写和关闭等操作。套接字允许应用程序将I/O插入到网络中，并与网络中的其他应用程序进行通信。网络套接字是IP地址与端口的组合。\n1[root@localhost ~]*# ls -l /var/lib/mysql/mysql.sock* srwxrwxrwx 1 mysql mysql 0 7月  12 09:45 /var/lib/mysql/mysql.sock [root@localhost ~]*#* \n注意这个文件的属性的第一个字符是 s。\n符号链接\n符号链接（软链接）是一类特殊的文件， 其包含有一条以绝对路径或者相对路径的形式指向其它文件或者目录的引用。今天POSIX操作系统标准、大多数类Unix系统、Windows Vista、Windows 7都支持符号链接。Windows 2000与Windows XP在某种程度上也支持符号链接。\n符号链接的操作是透明的：对符号链接文件进行读写的程序会表现得直接对目标文件进行操作。某些需要特别处理符号链接的程序（如备份程序）可能会识别并直接对其进行操作。\n一个符号链接文件仅包含有一个文本字符串，其被操作系统解释为一条指向另一个文件或者目录的路径。它是一个独立文件，其存在并不依赖于目标文件。如果删除一个符号链接，它指向的目标文件不受影响。如果目标文件被移动、重命名或者删除，任何指向它的符号链接仍然存在，但是它们将会指向一个不复存在的文件。这种情况被有时被称为被遗弃。\n123[root@localhost ~]# ls -l /bin/cclrwxrwxrwx. 1 root root 3 11月  8 2018 /bin/cc -&gt; gcc[root@localhost ~]# \n当我们查看文件属性时，会看到有类似 lrwxrwxrwx,注意第一个字符是l，这类文件是链接文件。Windows操作系统中的快捷方式有点相似。\n管道文件\n它是一种文件类型，在文件系统中可以看到。程序中可以查看文件stat结构中st_mode成员的值来判断文件是否是FIFO文件。创建一个FIFO文件类似于创建文件，FIFO文件就像普通文件一样。\nFIFO中可以很好地解决在无关进程间数据交换的要求，并且由于它们是存在于文件系统中的，这也提供了一种比匿名管道更持久稳定的通信办法。\nFIFO的通信方式类似于在进程中使用文件来传输数据，只不过FIFO类型文件同时具有管道的特性。在数据读出时，FIFO管道中同时清除数据。在shell中mkfifo命令可以建立有名管道，下面通过一个实例来帮助读者理解FIFO。\n123[root@localhost ~]# ls -l fifo prw-r--r-- 1 root root 0 7月  12 19:30 fifo[root@localhost ~]# \n查看文件属性时，第一个字符是p，就表示是管道文件。\n文件扩展名和文件名\n实际上，Linux的文件是没有所谓的扩展名的，一个Linux文件能不能被执行，与他的第一栏的十个属性有关， 与扩展名根本一点关系也没有。这个观念跟Windows的情况不相同喔！在Windows底下， 能被执行的文件扩展名通常是 .com .exe .bat等等，而在Linux底下，只要你的权限当中具有x的话，例如[ -rwx-r-xr-x ] 即代表这个文件可以被执行。\n不过，可以被执行跟可以执行成功是不一样的～举例来说，在root家目录下的install.log 是一个纯文本档，如果经由修改权限成为 -rwxrwxrwx 后，这个文件能够真的执行成功吗？ 当然不行～因为他的内容根本就没有可以执行的数据。所以说，这个x代表这个文件具有可执行的能力， 但是能不能执行成功，当然就得要看该文件的内容.\n不过我们仍然希望可以使用扩展名来了解该文件是什么东西，所以，通常我们还是会以适当的扩展名来表示该文件是什么种类的。Linux平台有以下常用的扩展名：\n*.sh ： 脚本或批处理文件 (scripts)，因为批处理文件为使用shell写成的，所以扩展名就编成 .sh\nZ, .tar, .tar.gz, .zip, *.tgz： 经过打包的压缩文件。这是因为压缩软件分别为 gunzip, tar 等等的，由于不同的压缩软件，而取其相关的扩展名！\n.html, .php：网页相关文件，分别代表 HTML 语法与 PHP 语法的网页文件。 .html 的文件可使用网页浏览器来直接开启，至于 .php 的文件， 则可以透过 client 端的浏览器来 server 端浏览，以得到运算后的网页结果。\n基本上，Linux系统上的文件名真的只是让你了解该文件可能的用途而已，真正的执行与否仍然需要权限的规范才行。例如虽然有一个文件为可执行文件，如常见的/bin/ls这个显示文件属性的指令，不过，如果这个文件的权限被修改成无法执行时，那么ls就变成不能执行。\n6.3 Linux帮助命令man查阅 command 命令的使用手册man 是 manual 的缩写，是 Linux 提供的一个 手册，包含了绝大部分的命令、函数的详细使用说明\n使用 man 时的操作键\n\n\n常用选项\n12345678910111213141516171819202122232425262728293031323334353637383940Usage: man [OPTION...] [章节] 手册页...  -a, --all                  寻找所有匹配的手册页  -d, --debug                输出调试信息   -D, --default              将所有选项都重置为默认值      --warnings[=警告]    开启 groff 的警告   -f, --whatis               等同于 whatis  -h     显示man的语法和参数说明，执行完成后退出程序。  -k, --apropos              等同于 apropos将搜索whatis数据库，模糊查找关键字    -S, -s, --sections=列表  使用以半角冒号分隔的章节列表  -t, --troff                使用 groff 对手册页排版  -w, --where, --path, --location                             输出手册页的物理位置  -W, --where-cat, --location-cat                             输出 cat 文件的物理位置    -c, --catman               由 catman 使用，用来对过时的 cat                             页重新排版     -C, --config-file=文件   使用该用户设置文件  -K, --global-apropos       search for text in all pages  -M, --manpath=路径       设置搜索手册页的路径为“路径”  -?, --help                 give this help list      --usage                give a short usage message  -V, --version              print program version  -R, --recode=编码        output source page encoded in ENCODING 寻找手册页：  -L, --locale=区域                             定义本次手册页搜索所采用的区域设置  -m, --systems=系统       use manual pages from other systems  -e, --extension=扩展                             将搜索限制在扩展类型为“扩展”的手册页之内  -i, --ignore-case          查找手册页时不区分大小写字母                             (默认)  -I, --match-case           查找手册页时区分大小写字母。      --regex                show all pages matching regex      --wildcard             show all pages matching wildcard      --names-only           make --regex and --wildcard match page names only,                             not descriptions\n\n相关描述\nman命令帮助信息的结构以及意义\n\n man对应的章节概述\n123456789101112131415-S 区段清单    该清单是一组用冒号分隔的欲查找的手册清单。此选项将覆盖      MANSECT     环境变量。    有些指令或程序可能有一个以上的主题，它们位于不同的区段中。因此，要查看较后的区    段，你可以在此指定 man 查找区段的顺序。具体区段划分如下所示：    区段1：用户指令    区段2：系统调用    区段3：程序库调用    区段4：设备    区段5：文件格式    区段6：游戏    区段7：杂项    区段8：系统指令    区段9：内核内部指令    区段n：Tcl或Tk指令\nhelp说明：\n显示 command 命令的帮助信息\n命令格式：help [参数] 内部命令\n查看外部命令的用法\n命令格式：命令 —help\n\n6.4 tab键自动补全在敲出 文件 ／ 目录 ／ 命令 的前几个字母之后，按下 tab 键\n如果输入的没有歧义，系统会自动补全如果还存在其他 文件 ／ 目录 ／ 命令 ，再按一下 tab 键，系统会提示可能存在的命令小技巧按 上 ／ 下 光标键可以在曾经使用过的命令之间来回切换如果想要退出选择，并且不想执行当前选中的命令，可以按 ctrl + c\n6.5 find查找文件\n\n1.搜索桌面目录下，文件名包含 1 的文件\n\n1find -name &quot;*1*&quot;\n\n2.搜索桌面目录下，所有以 .txt 为扩展名的文件\n\n1find -name &quot;*.txt&quot;\n\n3.搜索桌面目录下，以数字 1 开头的文件\n\n1find -name &quot;1*&quot;\n\n12345678910111213141516171819202122232425262728293031323334353637命令参数：    pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。     -print： find命令将匹配的文件输出到标准输出。     -exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为&#x27;command&#x27; &#123;  &#125; \\;，注意&#123;   &#125;和\\；之间的空格。     -ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。命令选项：    -name   按照文件名查找文件。    -perm   按照文件权限来查找文件。    -prune  使用这一选项可以使find命令不在当前指定的目录中查找，如果同时使用-depth选项，那么-prune将被find命令忽略。    -user   按照文件属主来查找文件。    -group  按照文件所属的组来查找文件。    -mtime -n +n  按照文件的更改时间来查找文件， - n表示文件更改时间距现在n天以内，+ n表示文件更改时间距现在n天以前。find命令还有-atime和-ctime 选项，但它们都和-m time选项。    -nogroup  查找无有效所属组的文件，即该文件所属的组在/etc/groups中不存在。    -nouser   查找无有效属主的文件，即该文件的属主在/etc/passwd中不存在。    -newer file1 ! file2  查找更改时间比文件file1新但比文件file2旧的文件。    -type  查找某一类型的文件，诸如：        b - 块设备文件。        d - 目录。        c - 字符设备文件。        p - 管道文件。        l - 符号链接文件。        f - 普通文件。    -size n：[c] 查找文件长度为n块的文件，带有c时表示文件长度以字节计。-depth：在查找文件时，首先查找当前目录中的文件，然后再在其子目录中查找。    -fstype：查找位于某一类型文件系统中的文件，这些文件系统类型通常可以在配置文件/etc/fstab中找到，该配置文件中包含了本系统中有关文件系统的信息。    -mount：在查找文件时不跨越文件系统mount点。    -follow：如果find命令遇到符号链接文件，就跟踪至链接所指向的文件。    -cpio：对匹配的文件使用cpio命令，将这些文件备份到磁带设备中。    另外,下面三个的区别:    -amin n   查找系统中最后N分钟访问的文件    -atime n  查找系统中最后n*24小时访问的文件    -cmin n   查找系统中最后N分钟被改变文件状态的文件    -ctime n  查找系统中最后n*24小时被改变文件状态的文件    -mmin n   查找系统中最后N分钟被改变文件数据的文件    -mtime n  查找系统中最后n*24小时被改变文件数据的文件\n6.6 history游览历史history内建命令用于显示用户以前执行过的历史命令，并且能对历史命令进行追加和删除等操作。\n该命令单独使用时，仅显示历史命令，在命令行中，可以使用符号!执行指定序号的历史命令。例如，要执行第2个历史命令，则输入!2。\n历史命令是被保存在内存中的，当退出或者登录shell时，会自动保存或读取。在内存中，历史命令仅能够存储1000条历史命令，该数量是由环境变量HISTSIZE进行控制。\n1格式: history [选项] [参数]\n123history N\t\t显示最近N条命令history -c\t\t清除所有的历史记录history -w  xxx.txt\t保存历史记录到文本xxx.txt\n12345678-a\t将当前shell会话的历史命令追加到命令历史文件中,命令历史文件是保存历史命令的配置文件-c\t清空当前历史命令列表-d\t删除历史命令列表中指定序号的命令-n\t从命令历史文件中读取本次Shell会话开始时没有读取的历史命令-r\t读取命令历史文件到当前的Shell历史命令内存缓冲区-s\t将指定的命令作为单独的条目加入命令历史内存缓冲区。\t在执行添加之前先删除命令历史内存缓冲区中最后一条命令-w\t把当前的shell历史命令内存缓冲区的内容写入命令历史文件\n\n七、系统管理命令1.shutdownshutdown命令用来系统关机命令。shutdown指令可以关闭所有程序，并依用户的需要，进行重新开机或关机的动作。\n1格式：shutdown [选项] [参数]\n12345678-c\t当执行“shutdown -h 11:50”指令时，只要按+键就可以中断关机的指令-f\t重新启动时不执行fsck-F\t重新启动时执行fsck-h\t将系统关机-k\t只是送出信息给所有用户，但不会实际关机-n\t不调用init程序进行关机，而由shutdown自己进行-r\tshutdown之后重新启动-t\t送出警告信息和删除信息之间要延迟多少秒\n2.poweroffpoweroff命令用来关闭计算机操作系统并且切断系统电源。如果确认系统中已经没有用户存在且所有数据都已保存，需要立即关闭系统，可以使用poweroff命令。\n1格式：poweroff [选项]\n123456-n\t关闭操作系统时不执行sync操作-w\t不真正关闭操作系统，仅在日志文件“/var/log/wtmp”中-d\t关闭操作系统时，不将操作写入日志文件“/var/log/wtmp”中添加相应的记录-f\t强制关闭操作系统-i\t关闭操作系统之前关闭所有的网络接口-h\t关闭操作系统之前将系统中所有的硬件设置为备用模式\n3.halthalt命令用来关闭正在运行的Linux操作系统，如果是线上跑了业务的服务器，执行此命令需谨慎。\nhalt命令会先检测系统的runlevel，若runlevel为0或6，则关闭系统，否则即调用shutdown来关闭系统。\n1格式：halt [选项]\n123456-n\t在关机或重启之前不对系统缓存进行同步。-w\t不真正重启或关机，而仅仅将关机信息写入wtmp（在/var/log/wtmp文件里）。-d\t不记录此次关机情况。当使用 -n 参数时隐含 -d。-f\t强制执行 halt 或 reboot 而不去调用 shutdown(8)。-i\t在关闭或重启系统之前关闭所有网络界面。-p\t当关闭系统时执行关闭电源操作。当以poweroff方式调用halt时，此为缺省参数。\n4.rebootreboot命令用于用来重新启动计算机。但是机器重启必须要root用户才有权限。\n1格式： reboot [选项]\n12345-n\t重开机之前不检查是否有未结束的程序-w \t并不会真的重开机，只是把记录写到 /var/log/wtmp 档案里-d\t不把记录写到 /var/log/wtmp 档案里（-n 这个参数包含了 -d）-f\t强迫重开机，不呼叫 shutdown 这个指令-i\t在重开机之前先把所有网络相关的装置先停止\n\n八、磁盘管理命令1.lsblk查看磁盘大小\n命令：lsblk\n命令：lsblk -a\n功能：查看挂载磁盘信息，磁盘名称、大小、挂载目录等。\n\n\nIDE磁盘的文件名为：/dev/hdxx\nSCSI/SATA/USB磁盘文件名为：/dev/sdxx\n\nVDA管理虚拟磁盘，SDA管理实体硬盘，而SDB是SDA的扩展，管理的是第二个物理磁盘设备。此外，虚拟磁盘是虚拟机中的设备，而物理磁盘则是实体服务器中的设备。\n2.dudu - 报告磁盘空间使用情况\ndu命令**的英文全称是“Disk Usage”，即用于查看磁盘占用空间的意思。但是与df命令不同的是du命令是对文件和目录磁盘使用的空间的查看，而不是某个分区。\n12用法：du [选项]... [文件]...　或：du [选项]... --files0-from=F\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657POSIX 选项       -a     显示对涉及到的所有文件的统计，而不只是包含子目录。       -k     用1024字节作为计数单位，替代缺省时512字节的计数单位。       -x     只输出指定参数的实际使用空间，而不包括其下的子目录。       -s     只统计指定参数的在同一设备上所使用的空间。GNU 选项       -a, --all              显示对所有文件的统计，而不只是包含子目录。       -b, --bytes              输出以字节为单位的大小，替代缺省时1024字节的计数单位。       --block-size=size              输出以块为单位的大小，块的大小为 size  字节。(  file-  utils-4.0              的新选项)       -c, --total              在处理完所有参数后给出所有这些参数的总计。这个选项被              用给出指定的一组文件或目录使用的空间的总和。       -D, --dereference-args              引用命令行参数的符号连接。但不影响其他的符号连接。    这对找出象              /usr/tmp          这样的目录的磁盘使用量有用，          /usr/tmp              等通常是符号连接。  译住：例如在  /var/tmp   下建立一个目录test,              而/usr/tmp  是指向  /var/tmp  的符号连接。du  /usr/tmp  返回一项              /usr/tmp , 而 du - D /usr/tmp 返回两项 /usr/tmp，/usr/tmp/test。       --exclude=pattern              在递归时，忽略与指定模式相匹配的文件或子目录。模式    可以是任何              Bourne shell 的文件 glob 模式。( file- utils-4.0 的新选项)       -h, --human-readable              为每个数附加一个表示大小单位的字母，象用M表示二进制 的兆字节。       -H, --si              与    -h    参数起同样的作用，只是使用法定的    SI    单位(   用              1000的幂而不是  1024  的幂，这样  M   代表的就是1000000   而不是              1048576)。(fileutils-4.0 的新选项)       -k, --kilobytes              输出以1024字节为计数单位的大小。       -l, --count-links              统计所有文件的大小，包括已经被统计过的(作为一个硬连接)。       -L, --dereference              引用符号连接(不是显示连接点本身而是连接指向的文件或              目录所使用的磁盘空间)。       -m, --megabytes              输出以兆字节的块为计数单位的大小(就是 1,048,576 字节)。       --max-depth=n              只输出命令行参数的小于等于第n层的目录的总计。--max-depth=0的作用同于-s选项。(fileutils-4.0的新选项)       -s, --summarize              对每个参数只显示总和。       -S, --separate-dirs              单独报告每一个目录的大小，不包括子目录的大小。       -x, --one-file-system              忽略与被处理的参数不在同一个文件系统的目录。       -X file, --exclude-from=file              除了从指定的文件中得到模式之外与         --exclude        一样。              模式以行的形式列出。如果指定的文件是&#x27;-&#x27;,那么从标准输              入中读出模式。(fileutils-4.0 的新选项) GNU 标准选项       --help 在标准输出上输出帮助信息后正常退出。       --version              在标准输出上输出版本信息后正常退出。       --     终结选项列表\n3.dfdf命令的英文全称即“Disk Free”，顾名思义功能是用于显示系统上可使用的磁盘空间。默认显示单位为KB，建议使用“df -h”的参数组合，根据磁盘容量自动变换合适的单位，更利于阅读。\n日常普遍用该命令可以查看磁盘被占用了多少空间、还剩多少空间等信息。\n1用法：df [选项]... [文件]...\n1234567891011121314151617181920212223242526272829303132GNU 参数说明\t   -a, --all              列出包括BLOCK为0的文件系统       --block-size=SIZE use SIZE-byte blocks              指定块的大小       -h,--huma-readable&quot;              用常见的格式显示出大小(例如:1K 234M 2G)       -H,--si&quot;              同上,但是这里的1k等于1000字节而不是1024字节       -i, --inodes              用信息索引点代替块表示使用状况       -k, --kilobytes              指定块大小等于1024字节来显示使用状况       -l, --local              只显示本地文件系统使用状况       -m, --megabytes              以指定块大小等于1048576字节(1M)来显示使用状况       --no-sync              在取得使用信息前禁止调用同步 (default)       -P, --portability              使用POSIX格式输出       --sync 在取得使用信息前调用同步       -t, --type=TYPE              只显示指定类型(TYPE)的文件系统       -T, --print-type              输出每个文件系统的类型       -x, --exclude-type=TYPE              只显示指定类型(TYPE)之外的文件系统.       -v (忽略)       --help 输出该命令的帮助信息并退出       --version              输出版本信息并退出             \n4.freefree - 显示系统中已用和未用的内存空间总和.\nfree 命令能够显示系统中物理上的空闲和已用内存，还有交换内存，同时，也能显示被内核使用的缓冲和缓存。这些信息是通过解析文件 /proc/meminfo 而收集到的。\n不带任何选项运行 free 命令会显示系统内存，包括空闲、已用、交换、缓冲、缓存和交换的内存总数。\n1free [参数]\n123456789-b 选项 以字节为单位显示内存总和; -k 选项(缺省的)以KB为单位显示; -m 选项以MB 为单位.-t 选项 显示 一个 总计行.-o  选项  禁止  &quot;buffer adjusted&quot; 行的显示. 除非 指定 free 从 (相应的)已用/未用的 内存 减去/加上 缓冲区内存.-s 使 free 以 delay 秒为间隔,  连续抽样显示.  delay  可以设置成浮点数,它用 usleep(3) 做 微秒级 延迟.-V 显示版本信息.\n5.fdiskfdisk 是 Linux 的磁盘分区表操作工具。\n1fdisk [-l] 装置名称\n1-l ：输出后面接的装置所有的分区内容。若仅有 fdisk -l 时， 则系统将会把整个系统内能够搜寻到的装置的分区均列出来。\n6.mkfs磁盘分割完毕后自然就是要进行文件系统的格式化，格式化的命令非常的简单，使用 mkfs（make filesystem） 命令。\n123mkfs [-t 文件系统格式] 装置文件名-t ：可以接文件系统格式，例如 ext3, ext2, vfat 等(系统有支持才会生效)\n7.fsckfsck（file system check）用来检查和维护不一致的文件系统。\n若系统掉电或磁盘发生问题，可利用fsck命令对文件系统进行检查。\n1fsck [-t 文件系统] [-ACay] 装置名称\n1234567891011-t : 给定档案系统的型式，若在 /etc/fstab 中已有定义或 kernel 本身已支援的则不需加上此参数-s : 依序一个一个地执行 fsck 的指令来检查-A : 对/etc/fstab 中所有列出来的 分区（partition）做检查-C : 显示完整的检查进度-d : 打印出 e2fsck 的 debug 结果-p : 同时有 -A 条件时，同时有多个 fsck 的检查一起执行-R : 同时有 -A 条件时，省略 / 不检查-V : 详细显示模式-a : 如果检查有错则自动修复-r : 如果检查有错则由使用者回答是否修复-y : 选项指定检测每个文件是自动输入yes，在不确定那些是不正常的时候，可以执行 # fsck -y 全部检查修复。\n8.mount/umountLinux 的磁盘挂载使用 mount 命令，卸载使用 umount 命令。\n1mount [-t 文件系统] [-L Label名] [-o 额外选项] [-n]  装置文件名  挂载点\n1234umount [-fn] 装置文件名或挂载点-f ：强制卸除！可用在类似网络文件系统 (NFS) 无法读取到的情况下；-n ：不升级 /etc/mtab 情况下卸除。\n\n九、文件管理命令1.pwdpwd命令是print working directory中每个单词的首字母缩写，其功能正如所示单词一样，为打印工作目录，即显示当前工作目录的绝对路径。\n1pwd [选项]\n1234567891011显示出 完整的 当前 活动目录 名称.   -L              \t打印 $PWD 变量的值，如果它命名了当前的工作目录   -P           \t\t打印当前的物理路径，不带有任何的符号链接    \t默认情况下，pwd 的行为和带 -L 选项一致\t--help \t\t显示 帮助 信息, 然后 退出\t--version \t\t显示 版本 信息, 然后 退出\n2.cdcd命令是”change directory”中单词的首字母缩写，其英文释义是改变目录，所以该命令的功能是从当前目录切换到指定目录。\n其中目录的路径可分为绝对路径和相对路径。若目录名称省略，则切换至使用者的用户目录(也就是刚登录时所在的目录)。\n另外，“~”也表示为用户目录的意思，“.”则是表示目前所在的目录，“…”则表示当前目录位置的上一级目录。\ncd 为最常用的命令，与 DOS 下的 cd 命令类似。\n1cd [选项] [目录名]\n12345678-p \t如果要切换到的目标目录是一个符号连接，直接切换到符号连接指向的目标目录-L \t如果要切换的目标目录是一个符号的连接，直接切换到字符连接名代表的目录，而非符号连接所指向的目标目录。-  \t当仅实用&quot;-&quot;一个选项时，当前工作目录将被切换到环境变量&quot;OLDPWD&quot;所表示的目录，即回到前一个目录。..    回到上一层目录\n3.lsls 命令是Linux下最常用的指令之一。ls命令为英文单词 list 的缩写，正如英文单词 list 的意思，其功能是列出指定目录下的内容及其相关属性信息。通过 ls 命令，不仅可以查看 Linux 文件夹包含的文件，而且可以查看文件的权限（包括：目录、文件权限）、查看目录信息等。\n1ls [选项]… [文件]…\n12345678910111213141516171819202122232425262728293031323334选项\t说明-a\t列出目录下所有文件，包括以 . 开头的隐藏文件-b\t把文件名中不可输出的字符用反斜杠加字符编号（就像 C 语言一样）的形式列出-c\t输出文件的 i 节点的修改时间，并以此排序-d\t将目录像文件一样显示，而不是显示其下的文件-e\t输出时间的全部信息，而不是输出简略信息-f -U\t对输出的文件不排序-i\t输出文件的 i 节点的索引信息-k\t以 k 字节的形式表示文件的大小-l\t列出文件的详细信息-m\t横向输出文件名，并以 , 作为分隔符-n\t用数字 UID、GID 代替名称-o\t显示文件除组信息外的详细信息-r\t对目录反向排序-s\t对每个文件名后输出该文件的大小-t\t以时间排序-u\t以文件上次被访问的时间排序-v\t根据版本进行排序-x\t按列排序，横向排序-A\t显示除 . 和 .. 外的所有文件-B\t不输出以 ~ 结尾的备份文件-C\t按列输出，纵向排序-G\t列出文件的组的信息-L\t列出链接文件名，而不是链接到的文件-N\t不限制文件长度-Q\t把输出的文件名用双引号扩起来-R\t列出所有子目录下的文件-S\t以文件大小排序-X\t以文件的扩展名（最后一个 . 后的字符）排序-1\t一行只输出一个文件-color=no\t不显示彩色文件名--help\t在标准输出上显示帮助信息并退出--version\t在标准输出上显示版本信息并退出\n4.mkdirmkdir命令是”make directories”的缩写，若指定目录不存在则创建目录。。\n注意：默认状态下，如果要创建的目录已经存在，则提示已存在，而不会继续创建目录。 所以在创建目录时，应保证新建的目录与它所在目录下的文件没有重名。\n要创建文件夹或目录的用户必须对所创建的文件夹的父文件夹具有写权限。并且，所创建的文件夹(目录)不能与其父目录(即父文件夹)中的文件名重名，即同一个目录下不能有同名的(区分大小写)。\n1mkdir [选项] 目录…\n12345678长选项必须使用的参数对于短选项时也是必需使用的。  -m, --mode=模式       设置权限模式(类似chmod)，而不是rwxrwxrwx 减umask  -p, --parents         需要时创建目标目录的上层目录，但即使这些目录已存在也不当作错误处理  -v, --verbose         每次创建新目录都显示信息  -Z, --context=CTX     将每个创建的目录的SELinux 安全环境设置为CTX      --help            显示此帮助信息并退出      --version         显示版本信息并退出\n5.rmdir英文全称：“remove directory”, rmdir命令作用是删除空的目录。\n注意：rmdir命令只能删除空目录。当要删除非空目录时，就要使用带有“-R”选项的rm命令。\nrmdir命令的“-p”参数可以递归删除指定的多级目录，但是要求每个目录也必须是空目录。\n1rmdir [选项]… 目录…\n12345678\t  --ignore-fail-on-non-empty                      忽略仅由目录非空产生的所有错误-p, --parents         删除指定目录及其上级文件夹，例如&quot;rmdir -p a/b/c&#x27;&quot;                      与&quot;rmdir a/b/c a/b a&#x27;&quot; 基本相同-v, --verbose         输出处理的目录详情    --help            显示此帮助信息并退出    --version         显示版本信息并退出\n6.treetree命令以树状图列出目录的内容。\n1tree [选项] [参数]\n12345678910111213141516171819202122-a 显示所有文件和目录。-A 使用ASNI绘图字符显示树状图而非以ASCII字符组合。-C 在文件和目录清单加上色彩，便于区分各种类型。-d 显示目录名称而非内容。-D 列出文件或目录的更改时间。-f 在每个文件或目录之前，显示完整的相对路径名称。-F 在执行文件，目录，Socket，符号连接，管道名称名称，各自加上&quot;*&quot;,&quot;/&quot;,&quot;=&quot;,&quot;@&quot;,&quot;|&quot;号。-g 列出文件或目录的所属群组名称，没有对应的名称时，则显示群组识别码。-i 不以阶梯状列出文件或目录名称。-I 不显示符合范本样式的文件或目录名称。-l 如遇到性质为符号连接的目录，直接列出该连接所指向的原始目录。-L 层级显示-n 不在文件和目录清单加上色彩。-N 直接列出文件和目录名称，包括控制字符。-p 列出权限标示。-P 只显示符合范本样式的文件或目录名称。-q 用&quot;?&quot;号取代控制字符，列出文件和目录名称。-s 列出文件或目录大小。-t 用文件和目录的更改时间排序。-u 列出文件或目录的拥有者名称，没有对应的名称时，则显示用户识别码。-x 将范围局限在现行的文件系统中，若指定目录下的某些子目录，其存放于另一个文件系统上，则将该子目录予以排除在寻找范围外。\n7.mvmv命令是“move”单词的缩写，其功能大致和英文含义一样，可以移动文件或重命名文件。经常用来备份文件或者目录。\n123mv [选项]... 源文件 目标文件mv [选项]... 源文件... 目录mv [选项]... --target-directory=DIRECTORY SOURCE...\n123456789101112131415161718192021222324252627282930将源文件重命名为目标文件，或将源文件移动至指定目录。长选项必须使用的参数对于短选项时也是必需使用的。      --backup[=CONTROL]       为每个已存在的目标文件创建备份  -b                           类似--backup 但不接受参数  -f, --force                  覆盖前不询问  -i, --interactive            覆盖前询问  -n, --no-clobber             不覆盖已存在文件如果您指定了-i、-f、-n 中的多个，仅最后一个生效。      --strip-trailing-slashes  去掉每个源文件参数尾部的斜线  -S, --suffix=SUFFIX           替换常用的备份文件后缀  -t, --target-directory=DIRECTORY      将所有参数指定的源文件或目录                                        移动至 指定目录  -T, --no-target-directory     将目标文件视作普通文件处理  -u, --update                  只在源文件文件比目标文件新，或目标文件                                不存在时才进行移动  -v, --verbose         详细显示进行的步骤      --help            显示此帮助信息并退出      --version         显示版本信息并退出      备份文件的后缀为&quot;~&quot;，除非以--suffix 选项或是SIMPLE_BACKUP_SUFFIX环境变量指定。版本控制的方式可通过--backup 选项或VERSION_CONTROL 环境变量来选择。以下是可用的变量值：  none, off       不进行备份(即使使用了--backup 选项)  numbered, t     备份文件加上数字进行排序  existing, nil   若有数字的备份文件已经存在则使用数字，否则使用普通方式备份  simple, never   永远使用普通方式备份\n8.cpcp命令可以理解为英文单词copy的缩写，其功能为复制文件或目录。\ncp命令可以将多个文件复制到一个具体的文件名或一个已经存在的目录下，也可以同时复制多个文件到一个指定的目录中。\n1234用法：cp [选项]... [-T] 源文件 目标文件　或：cp [选项]... 源文件... 目录　或：cp [选项]... -t 目录 源文件...Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667长选项必须使用的参数对于短选项时也是必需使用的。  -a, --archive\t\t\t等于-dR --preserve=all      --attributes-only\t\t\t只拷贝文件属性，不拷贝文件内容      --backup[=CONTROL\t\t为每个已存在的目标文件创建备份  -b\t\t\t\t类似--backup 但不接受参数      --copy-contents\t\t在递归处理是复制特殊文件内容  -d\t\t\t\t等于--no-dereference --preserve=links  -f, --force\t\t\t如果目标文件无法打开则将其移除并重试(当 -n 选项\t\t\t\t\t存在时则不需再选此项)  -i, --interactive\t\t覆盖前询问(使前面的 -n 选项失效)  -H\t\t\t\t跟随源文件中的命令行符号链接  -l, --link\t\t\t链接文件而不复制  -L, --dereference\t\t总是跟随符号链接  -n, --no-clobber\t\t不要覆盖已存在的文件(使前面的 -i 选项失效)  -P, --no-dereference\t\t不跟随源文件中的符号链接  -p\t\t\t\t等于--preserve=模式,所有权,时间戳      --preserve[=属性列表\t保持指定的属性(默认：模式,所有权,时间戳)，如果\t\t\t\t\t可能保持附加属性：环境、链接、xattr 等  -c                           same as --preserve=context      --sno-preserve=属性列表\t不保留指定的文件属性      --parents\t\t\t复制前在目标目录创建来源文件路径中的所有目录  -R, -r, --recursive\t\t递归复制目录及其子目录内的所有内容      --reflink[=WHEN]\t\t控制克隆/CoW 副本。请查看下面的内如。      --remove-destination\t尝试打开目标文件前先删除已存在的目的地\t\t\t\t\t文件 (相对于 --force 选项)      --sparse=WHEN\t\t控制创建稀疏文件的方式      --strip-trailing-slashes\t删除参数中所有源文件/目录末端的斜杠  -s, --symbolic-link\t\t只创建符号链接而不复制文件  -S, --suffix=后缀\t\t自行指定备份文件的后缀  -t,  --target-directory=目录\t将所有参数指定的源文件/目录                                           复制至目标目录  -T, --no-target-directory\t将目标目录视作普通文件  -u, --update                 copy only when the SOURCE file is newer                                 than the destination file or when the                                 destination file is missing  -v, --verbose                explain what is being done  -x, --one-file-system        stay on this file system  -Z, --context=CONTEXT        set security context of copy to CONTEXT      --help\t\t显示此帮助信息并退出      --version\t\t显示版本信息并退出默认情况下，源文件的稀疏性仅仅通过简单的方法判断，对应的目标文件目标文件也被为稀疏。这是因为默认情况下使用了--sparse=auto 参数。如果明确使用--sparse=always 参数则不论源文件是否包含足够长的0 序列也将目标文件创文建为稀疏件。使用--sparse=never 参数禁止创建稀疏文件。当指定了--reflink[=always] 参数时执行轻量化的复制，即只在数据块被修改的情况下才复制。如果复制失败或者同时指定了--reflink=auto，则返回标准复制模式。备份文件的后缀为&quot;~&quot;，除非以--suffix 选项或是SIMPLE_BACKUP_SUFFIX环境变量指定。版本控制的方式可通过--backup 选项或VERSION_CONTROL 环境变量来选择。以下是可用的变量值：  none, off       不进行备份(即使使用了--backup 选项)  numbered, t     备份文件加上数字进行排序  existing, nil   若有数字的备份文件已经存在则使用数字，否则使用普通方式备份  simple, never   永远使用普通方式备份有一个特别情况：如果同时指定--force 和--backup 选项，而源文件和目标文件是同一个已存在的一般文件的话，cp 会将源文件备份。\n9.rm删除 (unlink) 文件。\nrm命令可以删除一个目录中的一个或多个文件或目录，也可以将某个目录及其下属的所有文件及其子目录均删除掉。对于链接文件，只是删除整个链接文件，而原有文件保持不变。\n注意：使用rm命令要格外小心。因为一旦删除了一个文件，就无法再恢复它。所以，在删除文件之前，最好再看一下文件的内容，确定是否真要删除。rm命令可以用-i选项，这个选项在使用文件扩展名字符删除多个文件时特别有用。使用这个选项，系统会要求你逐一确定是否要删除。这时，必须输入y并按Enter键，才能删除文件。如果仅按Enter键或其他字符，文件不会被删除。\n1rm [选项]… 文件…\n12345678910111213141516171819202122232425262728删除 (unlink) 文件。  -f, --force           强制删除。忽略不存在的文件，不提示确认  -i                    在删除前需要确认  -I                    在删除超过三个文件或者递归删除前要求确认。此选项比-i 提                        示内容更少，但同样可以阻止大多数错误发生      --interactive[=WHEN]      根据指定的WHEN 进行确认提示：never，once (-I)，                                或者always (-i)。如果此参数不加WHEN 则总是提示      --one-file-system         递归删除一个层级时，跳过所有不符合命令行参                                数的文件系统上的文件      --no-preserve-roo 不特殊对待&quot;/&quot;      --preserve-root   不允许删除&quot;/&quot;(默认)  -d, --dir\t删除空目录  -r, -R, --recursive   递归删除目录及其内容  -v, --verbose         详细显示进行的步骤      --help            显示此帮助信息并退出      --version         显示版本信息并退出默认时，rm 不会删除目录。使用--recursive(-r 或-R)选项可删除每个给定的目录，以及其下所有的内容。要删除第一个字符为&quot;-&quot;的文件 (例如&quot;-foo&quot;)，请使用以下方法之一：  rm -- -foo  rm ./-foo请注意，如果使用rm 来删除文件，通常仍可以将该文件恢复原状。如果想保证该文件的内容无法还原，请考虑使用shred。\n10.touchtouch命令有两个功能：一是创建新的空文件，二是改变已有文件的时间戳属性。\ntouch命令会根据当前的系统时间更新指定文件的访问时间和修改时间。如果文件不存在，将会创建新的空文件，除非指定了”-c”或”-h”选项。\n注意：在修改文件的时间属性的时候，用户必须是文件的属主，或拥有写文件的访问权限。\n1用法：touch [选项]... 文件...\n123456789101112131415161718192021不存在的文件将会被创建为空文件，除非使用-c 或-h 选项。如果文件名为&quot;-&quot;则特殊处理，更改与标准输出相关的文件的访问时间。长选项必须使用的参数对于短选项时也是必需使用的。  -a                    只更改访问时间  -c, --no-create       不创建任何文件  -d, --date=字符串     使用指定字符串表示时间替代当前时间  -f                    (忽略)  -h, --no-dereference          会影响符号链接本身，替代符号链接所指示的目的地                                (当系统支持更改符号链接的所有者时，此选项才有用)  -m                    只更改修改时间  -r, --reference=文件  使用指定文件的时间属性替代当前时间  -t STAMP              使用[[CC]YY]MMDDhhmm[.ss] 格式的时间替代当前时间  --time=WORD           使用WORD 指定的时间：access、atime、use 都等于-a                        选项的效果，而modify、mtime 等于-m 选项的效果      --help            显示此帮助信息并退出      --version         显示版本信息并退出请注意，-d 和-t 选项可接受不同的时间/日期格式。\n11.basenamebasename - 从文件名中剥离目录和后缀\nbasename命令用于打印目录或者文件的基本名称。basename和dirname命令通常用于shell脚本中的命令替换来指定和指定的输入文件名称有所差异的输出文件名称。\n12basename 名称 [后缀]basename 选项\n1234567显示 去掉 目录成分 后的 NAME. 如果 指定了 SUFFIX, 就 同时 去掉 拖尾的SUFFIX.--help \t显示 帮助信息, 然后 结束--version\t显示 版本信息, 然后 结束\n12.dirnamedirname - 从文件名剥离非目录的后缀\ndirname命令去除文件名中的非目录部分，仅显示与目录有关的内容。dirname命令读取指定路径名保留最后一个/及其后面的字符，删除其他部分，并写结果到标准输出。如果最后一个/后无字符，dirname 命令使用倒数第二个/，并忽略其后的所有字符。dirname 和 basename通常在 shell 内部命令替换使用，以指定一个与指定输入文件名略有差异的输出文件名。\n12dirname 名字dirname 选项\n1234567打印去除了/后面部分的NAME;如果NAME没有包含/,则输出`.&#x27;(表示当前目录).--help \t显示帮助并退出--version\t输出版本信息并退出\n13.renamerename命令用字符串替换的方式批量改变文件名。\nrename 用于 对文件进行命名管理，可进行批量命名并支持正则表达式， rename命令存在两个版本用法上有所区别 一个是 C语言版本支持通配符，另一个是 Perl版本.支持正则表达式\n123456789rename [选项] 表达式 替换文件…rename [options] expression replacement file…expression 将文件名需要替换的字符串replacement 将文件名中含有的原字符替换成目标字符串file 指定要改变文件名的文件列表\n1234567891011121314151617 -v, --verbose    解释正在进行的操作 -s, --symlink    在符号链接上执行 -h, --help     显示此帮助并退出 -V, --version  输出版本信息并退出 【常用通配符说明】 ?    表示一个任意字符 *    表示一个或一串任意字符-------------------------------------------------【常用正则表达式符号说明】^    匹配输入的开始位置$    匹配输入的结尾.    匹配除换行符外的任意字符+    匹配前一个字符一次或多次 例如，&quot;zo+&quot;可以匹配&quot;zoo&quot;,但不匹配&quot;z&quot;[a-z]    表示某个范围内的字符，例如，&quot;[a-z]&quot;匹配&quot;a&quot;与&quot;z&quot;之间的任何一个小写字母字符。[^m-z]    否定的字符区间。与不在指定区间内的字符匹配。\n14.filefile - 确定文件类型\nfile命令用来识别文件类型，也可用来辨别一些文件的编码格式。它是通过查看文件的头部信息来获取文件类型，而不是像Windows通过扩展名来确定文件类型的。\n1file [ -bcnsvzL ] [ -f 命名文件 ] [ -m 幻数文件 ] file …\n123456789101112131415161718-b    不输出文件名 (简要模式).-c    检查时打印输出幻数文件的解析结果.常与 -m 一起使用，用来在安装幻数文件之前调试它.-f 命名文件    从在参数表前的 命名文件 中读出将要检查的文件名(每行一个文件).要有 命名文件 ，或者至少有一个文件名参数; 如果要检查标准输入, 使用``-作为文件参数.-m list    指定包含幻数的文件列表.可以是单个文件，也可以是 用冒号分开的多个文件.-n    每检查完一个文件就强制刷新标准输出. 仅在检查一组文件时才有效. 一般在将文件类型输出到管道时才采用此选项.-v    打印程序版本并退出.-z    试图查看压缩文件内部信息.-L    (在支持符号链接的系统上)选项显示符号链接文件的原文件, 就像 ls(1) 命令的like-named 选项.-s    通常, file 只是试图去检查在文件列表中那些 stat(2) 报告为正常文件的文件的类型.由于读特殊文件将可能导致 不可知后果，所以这样可以防止发生问题.使用 -s 选项时 file 命令也将去读文件列表中的块特殊文件和字符特殊文件. 一般用于从原始磁盘分区中获得文件系统类型，此文件为块 特殊文件. 这个选项也导致 file 命令忽略 stat(2) 报告的文件大小，因为在有些系统中原始磁盘分区的大小报告为0.\n15.cat将[文件]或标准输入组合输出到标准输出。\ncat 命令连接文件并打印到标准输出设备上，经常用来显示整个文件的内容。cat 只能查看文本内容的文件，如查看二进制文件，则屏幕会显示乱码。另外，cat 还可以用来创建文件、合并文件等。\n1用法：cat [选项] [文件]...\n12345678910111213141516171819202122将文件列表中的文件或标准输入连接到标准输出。-A, --show-all\t等价于 -vET 。-b, --number-nonblank\t给非空输出行编号。-e     等价于 -vE 。-E, --show-ends\t在每行结束显示 $ 。-n, --number\t给所有输出行编号。-s, --squeeze-blank\t将所有的连续的多个空行替换为一个空行。-t     等价于 -vT 。-T, --show-tabs\t把 TAB 字符显示为 ^I 。-u     (被忽略的选项)-v, --show-nonprinting\t除了 LFD 和 TAB 之外所有控制符用 ^ 和 M- 记方式显示。--help 显示帮助并退出。--version\t显示版本信息并退出。\t没有指定文件或指定的文件是 -，则从标准输入读取。\n16.moremore命令用于将内容较长的文本文件内容（不能在一屏显示完）进行分屏显示，并且支持在显示时定位关键字。而对于内容较少的文本文件内容则推荐使用cat命令查看\nmore命令，功能类似 cat ，cat命令是整个文件的内容从上到下显示在屏幕上。 more会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示，而且还有搜寻字串的功能 。more命令从前向后读取文件，因此在启动时就加载整个文件。\n1more [-dlfpcsu] [-num] [+/ pattern] [+ linenum] [file …]\n1234567891011-d\t显示帮助，而不是响铃-f\t统计逻辑行数而不是屏幕行数-l\t抑制换页(form feed)后的暂停-p\t不滚屏，清屏并显示文本-c\t不滚屏，显示文本并清理行尾-u\t抑制下划线-s\t将多个空行压缩为一行-NUM\t指定每屏显示的行数为 NUM+NUM\t从文件第 NUM 行开始显示+/STRING\t从匹配搜索字符串 STRING 的文件位置开始显示-V\t显示版本信息并退出\n17.lessless 与more命令类似，但可以通过翻页键查看上下页的内容\nless命令的作用与more十分相似，都可以用来浏览文字档案的内容，不同的是less命令允许用户向前或向后浏览文件，而more命令只能向下浏览。用less命令显示文件时，用PageUp键向上翻页，用PageDown键向下翻页。要退出less程序，应按Q键。\n1less [参数] [文件]\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182-b&lt;缓冲区大小&gt; 设置缓冲区的大小-e 当文件显示结束后，自动离开-f 强迫打开特殊文件，例如外围设备代号、目录和二进制文件-g 只标志最后搜索的关键词-i 忽略搜索时的大小写-m 显示类似more命令的百分比-N 显示每行的行号-o&lt;文件名&gt; 将less 输出的内容在指定文件中保存起来-Q 不使用警告音-s 显示连续空行为一行-S 行过长时间将超出部分舍弃-x&lt;数字&gt; 将“tab”键显示为规定的数字空格命令内部操作按键功能如下：b 向后翻一页d 向后翻半页h 显示帮助界面Q 退出less 命令u 向前滚动半页y 向前滚动一行空格键 滚动一页回车键 滚动一行 1) 向前搜索    / ： 使用一个模式进行搜索，并定位到下一个匹配的文本    n ： 向前查找下一个匹配的文本    N ： 向后查找前一个匹配的文本2) 向后搜索    ? ： 使用模式进行搜索，并定位到前一个匹配的文本    n ： 向后查找下一个匹配的文本    N ： 向前查找前一个匹配的文本2 全屏导航    ctrl + F ：向前移动一屏    ctrl + B ：向后移动一屏    ctrl + D ：向前移动半屏    ctrl + U ：向后移动半屏3 单行导航    j ： 向前移动一行    k ： 向后移动一行4 其它导航    G ： 移动到最后一行    g ： 移动到第一行    q / ZZ ： 退出 less 命令5 编辑文件    v ： 进入编辑模式，使用配置的编辑器编辑当前文件6 标记导航    当使用 less 查看大文件时，可以在任何一个位置作标记，可以通过命令导航到标有特定标记的文本位置。    ma ： 使用 a 标记文本的当前位置    &#x27;a ： 导航到标记 a 处7 浏览多个文件    方式一，传递多个参数给 less，就能浏览多个文件。    less file1 file2    方式二，正在浏览一个文件时，使用 :e 打开另一个文件。    less file1    :e file2         当打开多个文件时，使用如下命令在多个文件之间切换    :n - 浏览下一个文件    :p - 浏览前一个文件\n18.grepLinux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。\ngrep命令的选项用于对搜索过程的补充，而其命令的模式十分灵活，可以是变量、字符串、正则表达式。需要注意的是：一当模式中包含了空格，务必要用双引号将其引起来。\nlinux系统支持三种形式的grep命令，大儿子就是grep，标准，模仿的代表。二儿子兴趣爱好多-egrep，简称扩展grep命令，其实和grep -E等价，支持基本和扩展的正则表达式。小儿子跑的最快-fgrep，简称快速grep命令，其实和grep -F等价，不支持正则表达式，按照字符串表面意思进行匹配。\n12grep [options] PATTERN [FILE...]grep [options] [-e PATTERN | -f FILE] [FILE...]\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889匹配模式选择: -E, --extended-regexp     扩展正则表达式egrep -F, --fixed-strings       一个换行符分隔的字符串的集合fgrep -G, --basic-regexp        基本正则 -P, --perl-regexp         调用的perl正则 -e, --regexp=PATTERN      后面根正则模式，默认无 -f, --file=FILE           从文件中获得匹配模式 -i, --ignore-case         不区分大小写 -w, --word-regexp         匹配整个单词 -x, --line-regexp         匹配整行 -z, --null-data           一个 0 字节的数据行，但不是空行杂项: -s, --no-messages         不显示错误信息 -v, --invert-match        显示不匹配的行 -V, --version             显示版本号 --help                    显示帮助信息 --mmap                use memory-mapped input if possible输入控制: -m, --max-count=NUM       匹配的最大数 -b, --byte-offset         打印匹配行前面打印该行所在的块号码。 -n, --line-number         显示的加上匹配所在的行号 --line-buffered           刷新输出每一行 -H, --with-filename       当搜索多个文件时，显示匹配文件名前缀 -h, --no-filename         当搜索多个文件时，不显示匹配文件名前缀 --label=LABEL            print LABEL as filename for standard input -o, --only-matching       只显示一行中匹配PATTERN 的部分 -q, --quiet, --silent      不显示任何东西 --binary-files=TYPE   假定二进制文件的TYPE 类型；                                      TYPE 可以是`binary&#x27;, `text&#x27;, 或`without-match&#x27; -a, --text                匹配二进制的东西 -I                        不匹配二进制的东西 -d, --directories=ACTION  目录操作，读取，递归，跳过 -D, --devices=ACTION      设置对设备，FIFO,管道的操作，读取，跳过 -R, -r, --recursive       递归调用 --include=PATTERN     只查找匹配FILE_PATTERN 的文件 --exclude=PATTERN     跳过匹配FILE_PATTERN 的文件和目录 --exclude-from=FILE   跳过所有除FILE 以外的文件 -L, --files-without-match 匹配多个文件时，显示不匹配的文件名 -l, --files-with-matches  匹配多个文件时，显示匹配的文件名 -c, --count               显示匹配的行数 -Z, --null                在FILE 文件最后打印空字符文件控制: -B, --before-context=NUM  打印匹配本身以及前面的几个行由NUM控制 -A, --after-context=NUM   打印匹配本身以及随后的几个行由NUM控制 -C, --context=NUM         打印匹配本身以及随后，前面的几个行由NUM控制 -NUM                      根-C的用法一样的 --color[=WHEN], --colour[=WHEN]       使用标志高亮匹配字串；  -U, --binary               使用标志高亮匹配字串； -u, --unix-byte-offsets   当CR 字符不存在，报告字节偏移(MSDOS 模式) 规则表达式：grep的规则表达式:^  #锚定行的开始 如：&#x27;^grep&#x27;匹配所有以grep开头的行。    $  #锚定行的结束 如：&#x27;grep$&#x27;匹配所有以grep结尾的行。    .  #匹配一个非换行符的字符 如：&#x27;gr.p&#x27;匹配gr后接一个任意字符，然后是p。    *  #匹配零个或多个先前字符 如：&#x27;*grep&#x27;匹配所有一个或多个空格后紧跟grep的行。    .*   #一起用代表任意字符。   []   #匹配一个指定范围内的字符，如&#x27;[Gg]rep&#x27;匹配Grep和grep。    [^]  #匹配一个不在指定范围内的字符，如：&#x27;[^A-FH-Z]rep&#x27;匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。    \\(..\\)  #标记匹配字符，如&#x27;\\(love\\)&#x27;，love被标记为1。    \\&lt;      #锚定单词的开始，如:&#x27;\\&lt;grep&#x27;匹配包含以grep开头的单词的行。    \\&gt;      #锚定单词的结束，如&#x27;grep\\&gt;&#x27;匹配包含以grep结尾的单词的行。    x\\&#123;m\\&#125;  #重复字符x，m次，如：&#x27;0\\&#123;5\\&#125;&#x27;匹配包含5个o的行。    x\\&#123;m,\\&#125;  #重复字符x,至少m次，如：&#x27;o\\&#123;5,\\&#125;&#x27;匹配至少有5个o的行。    x\\&#123;m,n\\&#125;  #重复字符x，至少m次，不多于n次，如：&#x27;o\\&#123;5,10\\&#125;&#x27;匹配5--10个o的行。   \\w    #匹配文字和数字字符，也就是[A-Za-z0-9]，如：&#x27;G\\w*p&#x27;匹配以G后跟零个或多个文字或数字字符，然后是p。   \\W    #\\w的反置形式，匹配一个或多个非单词字符，如点号句号等。   \\b    #单词锁定符，如: &#x27;\\bgrep\\b&#x27;只匹配grep。  POSIX字符:为了在不同国家的字符编码中保持一至，POSIX(The Portable Operating System Interface)增加了特殊的字符类，如[:alnum:]是[A-Za-z0-9]的另一个写法。要把它们放到[]号内才能成为正则表达式，如[A- Za-z0-9]或[[:alnum:]]。在linux下的grep除fgrep外，都支持POSIX的字符类。[:alnum:]    #文字数字字符   [:alpha:]    #文字字符   [:digit:]    #数字字符   [:graph:]    #非空字符（非空格、控制字符）   [:lower:]    #小写字符   [:cntrl:]    #控制字符   [:print:]    #非空字符（包括空格）   [:punct:]    #标点符号   [:space:]    #所有空白字符（新行，空格，制表符）   [:upper:]    #大写字符   [:xdigit:]   #十六进制数字（0-9，a-f，A-F）  \n19.egrepegrep命令用于在文件内查找指定的字符串。egrep执行效果与grep -E相似，使用的语法及参数可参照grep指令，与grep的不同点在于解读字符串的方法。egrep是用extended regular expression语法来解读的，而grep则用basic regular expression 语法解读，extended regular expression比basic regular expression的表达更规范。\negrep支持扩展的正则表达式\n12345egrep [选项]... PATTERN [FILE]...在每个 FILE 或是标准输入中查找 PATTERN。PATTERN 是一个可扩展的正则表达式(缩写为 ERE)。例如: egrep -i &#x27;hello world&#x27; menu.h main.c\n注意: pattern如果是表达式或者超过两个单词的, 需要用引号引用. 可以是单引号也可双引号, 区别是单引号无法引用变量而双引号可以.\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162在每个 FILE 或是标准输入中查找 PATTERN。PATTERN 是一个可扩展的正则表达式(缩写为 ERE)。例如: egrep -i &#x27;hello world&#x27; menu.h main.c正则表达式选择与解释:  -e, --regexp=PATTERN      用 PATTERN 来进行匹配操作  -f, --file=FILE           从 FILE 中取得 PATTERN  -i, --ignore-case         忽略大小写  -w, --word-regexp         强制 PATTERN 仅完全匹配字词  -x, --line-regexp         强制 PATTERN 仅完全匹配一行  -z, --null-data           一个 0 字节的数据行，但不是空行杂项:  -s, --no-messages         不显示错误信息  -v, --invert-match        选中不匹配的行  -V, --version             显示版本信息并退出      --help                显示此帮助并退出      --mmap                忽略向后兼容性输入控制:  -m, --max-count=NUM       匹配的最大数  -b, --byte-offset         打印匹配行前面打印该行所在的块号码。  -n, --line-number         显示的加上匹配所在的行号  --line-buffered           刷新输出每一行  -H, --with-filename       当搜索多个文件时，显示匹配文件名前缀  -h, --no-filename         当搜索多个文件时，不显示匹配文件名前缀  -o, --only-matching       只显示一行中匹配PATTERN 的部分  -q, --quiet, --silent     不显示所有输出      --binary-files=TYPE   假定二进制文件的TYPE 类型；                            TYPE 可以是`binary&#x27;, `text&#x27;, 或`without-match&#x27;  -a, --text                等同于 --binary-files=text  -I                        等同于 --binary-files=without-match  -d, --directories=ACTION  操作目录的方式；                            ACTION 可以是`read&#x27;, `recurse&#x27;,或`skip&#x27;  -D, --devices=ACTION      操作设备、先入先出队列、套接字的方式；                            ACTION 可以是`read&#x27;或`skip&#x27;  -R, -r, --recursive       等同于 --directories=recurse      --include=FILE_PATTERN  只查找匹配FILE_PATTERN 的文件      --exclude=FILE_PATTERN  跳过匹配FILE_PATTERN 的文件和目录      --exclude-from=FILE   跳过所有除FILE 以外的文件      --exclude-dir=PATTERN  跳过所有匹配PATTERN 的目录。  -L, --files-without-match  只打印不匹配FILEs 的文件名  -l, --files-with-matches  只打印匹配FILES 的文件名  -c, --count               只打印每个FILE 中的匹配行数目  -T, --initial-tab         行首tabs 分隔（如有必要）  -Z, --null                在FILE 文件最后打印空字符文件控制:  -B, --before-context=NUM  打印以文本起始的NUM 行  -A, --after-context=NUM   打印以文本结尾的NUM 行  -C, --context=NUM         打印输出文本NUM 行  -NUM                      等同于 --context=NUM      --color[=WHEN],      --colour[=WHEN]       使用标志高亮匹配字串；                            WHEN 可以是`always&#x27;, `never&#x27;或`auto&#x27;  -U, --binary              不要清除行尾的CR 字符(MSDOS 模式)  -u, --unix-byte-offsets   当CR 字符不存在，报告字节偏移(MSDOS 模式)不带 FILE 参数，或是 FILE 为 -，将读取标准输入。如果少于两个 FILE 参数就要默认使用 -h 参数。如果选中任意一行，那退出状态为 0，否则为 1；如果有错误产生，且未指定 -q 参数，那退出状态为 2。\n20.wcwc - 输出文件中的行数、单词数、字节数\n1wc [选项列表]... [文件名列表]...\n1234567891011121314对每个文件输出行、单词、和字节统计数，如果指定了多于一个文件则还有个行数的总计。没有指定文件或指定的文件是 -，则读取标准输入。-c, --bytes, --chars\t输出字节统计数。-l, --lines\t输出换行符统计数。-L, --max-line-length\t输出最长的行的长度。-w, --words\t输出单词统计数。--help \t显示帮助并退出--version\t输出版本信息并退出\n21.sortsort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。sort命令既可以从特定的文件，也可以从stdin中获取输入。\n1用法：sort [选项]... [文件]...\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556长选项必须使用的参数对于短选项时也是必需使用的。排序选项：  -b, --ignore-leading-blanks   忽略前导的空白区域  -d, --dictionary-order        只考虑空白区域和字母字符  -f, --ignore-case             忽略字母大小写  -g, --general-numeric-sort    按照常规数值排序  -i, --ignore-nonprinting      只排序可打印字符  -M, --month-sort              比较 (未知) &lt; &quot;一月&quot; &lt; ... &lt; &quot;十二月&quot;                                在LC_ALL=C 时为(unknown) &lt; `JAN&#x27; &lt; ... &lt; `DEC&#x27;  -h, --human-numeric-sort    使用易读性数字(例如： 2K 1G)  -n, --numeric-sort            根据字符串数值比较  -R, --random-sort             根据随机hash 排序      --random-source=文件      从指定文件中获得随机字节  -r, --reverse                 逆序输出排序结果      --sort=WORD               按照WORD 指定的格式排序：                                        一般数字-g，高可读性-h，月份-M，数字-n，                                        随机-R，版本-V  -V, --version-sort            在文本内进行自然版本排序其他选项：      --batch-size=NMERGE       一次最多合并NMERGE 个输入；如果输入更多                                        则使用临时文件  -c, --check, --check=diagnose-first   检查输入是否已排序，若已有序则不进行操作  -C, --check=quiet, --check=silent     类似-c，但不报告第一个无序行      --compress-program=程序   使用指定程序压缩临时文件；使用该程序                                        的-d 参数解压缩文件      --debug                   为用于排序的行添加注释，并将有可能有问题的                                        用法输出到标准错误输出      --files0-from=文件        从指定文件读取以NUL 终止的名称，如果该文件被                                        指定为&quot;-&quot;则从标准输入读文件名  -k, --key=位置1[,位置2]       在位置1 开始一个key，在位置2 终止(默认为行尾)                                参看POS 语法。  -m, --merge                   合并已排序的文件，不再进行排序  -o, --output=文件             将结果写入到文件而非标准输出  -s, --stable                  禁用last-resort 比较以稳定比较算法  -S, --buffer-size=大小        指定主内存缓存大小  -t, --field-separator=分隔符  使用指定的分隔符代替非空格到空格的转换  -T, --temporary-directory=目录        使用指定目录而非$TMPDIR 或/tmp 作为                                        临时目录，可用多个选项指定多个目录      --parallel=N              将同时运行的排序数改变为N  -u, --unique          配合-c，严格校验排序；不配合-c，则只输出一次排序结果  -z, --zero-terminated 以0 字节而非新行作为行尾标志      --help            显示此帮助信息并退出      --version         显示版本信息并退出POS 是F[.C][OPTS]，F 代表域编号，C 是域中字母的位置，F 和C 均从1开始计数如果没有有效的-t 或-b 选项存在，则从前导空格后开始计数字符。OPTS 是一个或多个由单个字母表示的顺序选项，以此覆盖此key 的全局顺序设置。如果没有指定key 则将其整个行。指定的大小可以使用以下单位之一：内存使用率% 1%，b 1、K 1024 (默认)，M、G、T、P、E、Z、Y 等依此类推。如果不指定文件，或者文件为&quot;-&quot;，则从标准输入读取数据。\n22.uniquniq - 删除排序文件中的重复行\nuniq命令用于报告或忽略文件中的重复行，一般与sort命令结合使用。\nuniq命令全称是“unique”，中文释义是“独特的，唯一的”。该命令的作用是用来去除文本文件中连续的重复行，中间不能夹杂其他文本行。去除了重复的，保留的都是唯一的，也就是独特的，唯一的了。\n我们应当注意的是，它和sort的区别，sort只要有重复行，它就去除，而uniq重复行必须要连续，也可以用它忽略文件中的重复行。\n1用法：uniq [选项]... [文件]\n123456789101112131415161718192021222324从输入文件或者标准输入中筛选相邻的匹配行并写入到输出文件或标准输出。不附加任何选项时匹配行将在首次出现处被合并。长选项必须使用的参数对于短选项时也是必需使用的。  -c, --count           在每行前加上表示相应行目出现次数的前缀编号  -d, --repeated        只输出重复的行  -D, --all-repeated[=delimit-method    显示所有重复的行                        delimit-method=&#123;none(default),prepend,separate&#125;                        以空行为界限  -f, --skip-fields=N   比较时跳过前N 列  -i, --ignore-case     在比较的时候不区分大小写  -s, --skip-chars=N    比较时跳过前N 个字符  -u, --unique          只显示唯一的行  -z, --zero-terminated 使用&#x27;\\0&#x27;作为行结束符，而不是新换行  -w, --check-chars=N   对每行第N 个字符以后的内容不作对照      --help            显示此帮助信息并退出      --version         显示版本信息并退出若域中为先空字符(通常包括空格以及制表符)，然后非空字符，域中字符前的空字符将被跳过。提示：uniq 不会检查重复的行，除非它们是相邻的行。如果您想先对输入排序，使用没有uniq 的&quot;sort -u&quot;。同时，比较服从&quot;LC_COLLATE&quot; 变量所指定的规则。\n23.whichwhich 命令用于查找并显示给定命令的绝对路径，环境变量 PATH 中保存了查找命令时需要遍历的目录。which 指令会在环境变量 $PATH 设置的目录里查找符合条件的文件。也就是说，使用 which 命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。\n1which [选项] 执行文件名 […]\n1234567891011121314151617181920查找环境变量中的文件-a   查找全部内容，而非第一个文件-n   &lt;文件名长度&gt; 　指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 -p   &lt;文件名长度&gt; 　与-n参数相同，但此处的&lt;文件名长度&gt;包括了文件的路径。 -w 　指定输出时栏位的宽度。 -V 　显示版本信息。--version, -[vV]\t显示版本信息并退出--help\t显示帮助信息并退出--skip-dot\t跳过 PATH 中以点开头的目录--skip-tilde\t跳过 PATH 中以波形符号开头的目录--show-dot\t不要在输出中将点扩展到当前目录--show-tilde\t为 HOME 目录（非根目录）输出波形--tty-only\t如果不在 tty 上，停止右边的处理选项--all, -a\t打印 PATH 中的所有匹配项，而不仅仅是第一个--read-alias, -i\t从 stdin 中读取别名列表--skip-alias\t忽略选项 --read-alias；不读 stdin--read-functions\t从 stdin 读取 shell 函数--skip-functions\t忽略选项 --read-functions；不读 stdin\n24.lnln软链接也称为符号链接，类似于 windows 里的快捷方式，有自己的数据块，主要存放 了链接其他文件的路径。 1）基本语法\n ln -s [原文件或目录] [软链接名] （功能描述：给原文件创建一个软链接，注意要使用绝对路径） \n2）经验技巧\n删除软链接： rm -rf 软链接名，而不是 rm -rf 软链接名/ \n如果使用 rm -rf 软链接名/ 删除，会把软链接对应的真实目录下内容删掉 \n查询：通过 ll 就可以查看，列表属性第 1 位是 l，尾部会有位置指向。\n硬链接（类似复制文件）\n1）基本语法\nln [原文件或目录] [硬链接名]\n如果文件被删除，则软链接文件失去指向，变为不可用如果文件被删除，由于硬链接文件直接指向内容，因此不受影响\n 输出重定向和&gt;&gt;追加\">25.> 输出重定向和&gt;&gt;追加可将本应显示在终端上的内容保存到指定文件中。\n如：ls &gt; test.txt ( test.txt 如果不存在，则创建，存在则覆盖其内容 )\n1）ls -l &gt; 文件 （功能描述：列表的内容写入文件 a.txt 中（覆盖写））\n2）ls -al &gt;&gt; 文件 （功能描述：列表的内容追加到文件 aa.txt 的末尾） \n3）cat 文件 1 &gt; 文件 2 （功能描述：将文件 1 的内容覆盖到文件 2） \n4）echo “内容” &gt;&gt; 文件\n注意： &gt;输出重定向会覆盖原来的内容，&gt;&gt;输出重定向则会追加到文件的尾部。\n26.|管道：一个命令的输出可以通过管道做为另一个命令的输入。\n“ | ”的左右分为两端，从左端写入到右端。\n123456789101112python@ubuntu:/bin$ ll -h |more   总用量 13Mdrwxr-xr-x  2 root root  4.0K 8月   4  2016 ./drwxr-xr-x 26 root root  4.0K 7月  30  2016 ../-rwxr-xr-x  1 root root 1014K 6月  24  2016 bash*-rwxr-xr-x  1 root root   31K 5月  20  2015 bunzip2*-rwxr-xr-x  1 root root  1.9M 8月  19  2015 busybox*-rwxr-xr-x  1 root root   31K 5月  20  2015 bzcat*lrwxrwxrwx  1 root root     6 5月  16  2016 bzcmp -&gt; bzdiff*-rwxr-xr-x  1 root root  2.1K 5月  20  2015 bzdiff*lrwxrwxrwx  1 root root     6 5月  16  2016 bzegrep -&gt; bzgrep*--更多--\n\n十、 文件压缩解压命令1.tartar命令：用来压缩和解压文件。tar本身不具有压缩功能。他是调用压缩功能实现的\ntar命令可以为linux的文件和目录创建档案。利用tar，可以为某一特定文件创建档案（备份文件），也可以在档案中改变文件，或者向档案中加入新的文件。tar最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案。利用tar命令，可以把一大堆的文件和目录全部打包成一个文件，这对于备份文件或将几个文件组合成为一个文件以便于网络传输是非常有用的。\n首先要弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。\n为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（bzip2和gzip命令）。\n1用法：tar 选项... 参数...\n\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203主操作模式: -A, --catenate, --concatenate   追加 tar 文件至归档 -c, --create               创建一个新归档 -d, --diff, --compare      找出归档和文件系统的差异 --delete               从归档(非磁带！)中删除 -r, --append               追加文件至归档结尾 -t, --list                 列出归档内容 --test-label           测试归档卷标并退出 -u, --update               仅追加比归档中副本更新的文件 -x, --extract, --get       从归档中解出文件 操作修饰符: --check-device         当创建增量归档时检查设备号(默认) -g, --listed-incremental=文件处理新式的 GNU 格式的增量备份 -G, --incremental          处理老式的 GNU 格式的增量备份 --ignore-failed-read当遇上不可读文件时不要以非零值退出 -n, --seek                 归档可检索 --no-check-device      当创建增量归档时不要检查设备号 --occurrence[=NUMBER]  仅处理归档中每个文件的第 NUMBER个事件；仅当与以下子命令 --delete, --diff, --extract 或是 --list中的一个联合使用时，此选项才有效。而且不管文件列表是以命令行形式给出或是通过 -T 选项指定的；NUMBER 值默认为 1 --sparse-version=MAJOR[.MINOR]设置所用的离散格式版本(隐含--sparse) -S, --sparse               高效处理离散文件 重写控制: -k, --keep-old-files       解压时不要替换存在的文件 --keep-newer-files不要替换比归档中副本更新的已存在的文件 --no-overwrite-dir     保留已存在目录的元数据 --overwrite            解压时重写存在的文件 --overwrite-dir解压时重写已存在目录的元数据(默认) --recursive-unlink     解压目录之前先清除目录层次 --remove-files         在添加文件至归档后删除它们 -U, --unlink-first         在解压要重写的文件之前先删除它们 -W, --verify               在写入以后尝试校验归档 选择输出流: --ignore-command-error 忽略子进程的退出代码 --no-ignore-command-error将子进程的非零退出代码认为发生错误 -O, --to-stdout            解压文件至标准输出 --to-command=COMMAND将解压的文件通过管道传送至另一个程序 操作文件属性: --atime-preserve[=METHOD]在输出的文件上保留访问时间，要么通过在读取(默认 METHOD=‘replace’)后还原时间，要不就不要在第一次(METHOD=‘system’)设置时间 --delay-directory-restore 直到解压结束才设置修改时间和所解目录的权限 --group=名称         强制将 NAME作为所添加的文件的组所有者 --mode=CHANGES         强制将所添加的文件(符号)更改为权限CHANGES --mtime=DATE-OR-FILE   从 DATE-OR-FILE 中为添加的文件设置mtime -m, --touch                不要解压文件的修改时间 --no-delay-directory-restore取消 --delay-directory-restore 选项的效果 --no-same-owner        将文件解压为您所有 --no-same-permissions从归档中解压权限时使用用户的掩码位(默认为普通用户服务) --numeric-owner        总是以数字代表用户/组的名称 --owner=名称         强制将 NAME作为所添加的文件的所有者 -p, --preserve-permissions, --same-permissions解压文件权限信息(默认只为超级用户服务) --preserve             与 -p 和 -s 一样 --same-owner           尝试解压时保持所有者关系一致 -s, --preserve-order, --same-order为解压至匹配归档排序名称 设备选择和切换: -f, --file=ARCHIVE         使用归档文件或 ARCHIVE 设备 --force-local即使归档文件存在副本还是把它认为是本地归档 -F, --info-script=名称, --new-volume-script=名称在每卷磁带最后运行脚本(隐含 -M) -L, --tape-length=NUMBER   写入 NUMBER × 1024 字节后更换磁带 -M, --multi-volume         创建/列出/解压多卷归档文件 --rmt-command=COMMAND  使用指定的 rmt COMMAND 代替 rmt --rsh-command=COMMAND  使用远程 COMMAND 代替 rsh --volno-file=文件    使用/更新 FILE 中的卷数 设备分块: -b, --blocking-factor=BLOCKS   每个记录 BLOCKS x 512 字节 -B, --read-full-records    读取时重新分块(只对 4.2BSD 管道有效) -i, --ignore-zeros         忽略归档中的零字节块(即文件结尾) --record-size=NUMBER   每个记录的字节数 NUMBER，乘以 512 选择归档格式: -H, --format=FORMAT        创建指定格式的归档 FORMAT 是以下格式中的一种: gnu                      GNU tar 1.13.x 格式 oldgnu                   GNU 格式 as per tar &lt;= 1.12 pax                      POSIX 1003.1-2001 (pax) 格式 posix                    等同于 pax ustar                    POSIX 1003.1-1988 (ustar) 格式 v7                       old V7 tar 格式 --old-archive, --portability等同于 --format=v7 --pax-option=关键字[[:]=值][,关键字[[:]=值]]...控制 pax 关键字 --posix                等同于 --format=posix -V, --label=TEXT           创建带有卷名 TEXT的归档；在列出/解压时，使用 TEXT作为卷名的模式串 压缩选项: -a, --auto-compress        使用归档后缀来决定压缩程序 -I, --use-compress-program=PROG通过 PROG 过滤(必须是能接受 -d选项的程序) -j, --bzip2                通过 bzip2 过滤归档 --lzma                 通过 lzma 过滤归档 --no-auto-compress     do not use archive suffix to determine thecompression program -z, --gzip, --gunzip, --ungzip   通过 gzip 过滤归档 -Z, --compress, --uncompress   通过 compress 过滤归档 -J, --xz                   filter the archive through xz --lzop                 通过 lzop 过滤归档 本地文件选择: --add-file=文件      添加指定的 FILE 至归档(如果名字以 -开始会很有用的) --backup[=CONTROL]     在删除前备份，选择 CONTROL 版本 -C, --directory=DIR        改变至目录 DIR --exclude=PATTERN      排除以 PATTERN 指定的文件 --exclude-caches       除标识文件本身外，排除包含CACHEDIR.TAG 的目录中的内容 --exclude-caches-all   排除包含 CACHEDIR.TAG 的目录 --exclude-caches-under 排除包含 CACHEDIR.TAG的目录中所有内容 --exclude-tag=文件   除 FILE 自身外，排除包含 FILE的目录中的内容 --exclude-tag-all=文件   排除包含 FILE 的目录 --exclude-tag-under=文件   排除包含 FILE的目录中的所有内容 --exclude-vcs          排除版本控制系统目录 -h, --dereference跟踪符号链接；将它们所指向的文件归档并输出 --hard-dereference 跟踪硬链接；将它们所指向的文件归档并输出 -K, --starting-file=MEMBER-NAME从归档中的 MEMBER-NAME 成员处开始 --newer-mtime=DATE     当只有数据改变时比较数据和时间 --no-null              禁用上一次的效果 --null 选项 --no-recursion         避免目录中的自动降级 --no-unquote           不以 -T 读取的文件名作为引用结束 --null                 -T 读取以空终止的名字，-C 禁用 -N, --newer=DATE-OR-FILE, --after-date=DATE-OR-FILE只保存比 DATE-OR-FILE 更新的文件 --one-file-system      创建归档时保存在本地文件系统中 -P, --absolute-names       不要从文件名中清除引导符‘/’ --recursion            目录递归(默认) --suffix=STRING        在删除前备份，除非被环境变量SIMPLE_BACKUP_SUFFIX覆盖，否则覆盖常用后缀(‘’) -T, --files-from=文件    从 FILE中获取文件名来解压或创建文件 --unquote              以 -T读取的文件名作为引用结束(默认) -X, --exclude-from=文件  排除 FILE 中列出的模式串 文件名变换: --strip-components=NUMBER   解压时从文件名中清除 NUMBER个引导部分 --transform=EXPRESSION, --xform=EXPRESSION使用 sed 代替 EXPRESSION 来进行文件名变换 文件名匹配选项(同时影响排除和包括模式串): --anchored             模式串匹配文件名头部 --ignore-case          忽略大小写 --no-anchored          模式串匹配任意‘/’后字符(默认对 exclusion 有效) --no-ignore-case       匹配大小写(默认) --no-wildcards         逐字匹配字符串 --no-wildcards-match-slash   通配符不匹配‘/’ --wildcards            使用通配符(默认对 exclusion ) --wildcards-match-slash通配符匹配‘/’(默认对排除操作有效) 提示性输出: --checkpoint[=NUMBER]  每隔 NUMBER个记录显示进度信息(默认为 10 个) --checkpoint-action=ACTION   在每个检查点上执行 ACTION --index-file=文件    将详细输出发送至 FILE -l, --check-links只要不是所有链接都被输出就打印信息 --no-quote-chars=STRING   禁用来自 STRING 的字符引用 --quote-chars=STRING   来自 STRING 的额外的引用字符 --quoting-style=STYLE  设置名称引用风格；有效的 STYLE值请参阅以下说明 -R, --block-number         每个信息都显示归档内的块数 --show-defaults        显示 tar 默认选项 --show-omitted-dir 列表或解压时，列出每个不匹配查找标准的目录 --show-transformed-names, --show-stored-names显示变换后的文件名或归档名 --totals[=SIGNAL]      处理归档后打印出总字节数；当此SIGNAL 被触发时带参数 -打印总字节数；允许的信号为: SIGHUP，SIGQUIT，SIGINT，SIGUSR1 和 SIGUSR2；同时也接受不带 SIG 前缀的信号名称 --utc                  以 UTC 格式打印文件修改信息 -v, --verbose              详细地列出处理的文件 -w, --interactive, --confirmation每次操作都要求确认 兼容性选项: -o                         创建归档时，相当于 --old-archive；展开归档时，相当于 --no-same-owner 其它选项: -?, --help                 显示此帮助列表 --restrict             禁用某些潜在的有危险的选项 --usage                显示简短的用法说明 --version              打印程序版本长选项和相应短选项具有相同的强制参数或可选参数。除非以 --suffix 或 SIMPLE_BACKUP_SUFFIX设置备份后缀，否则备份后缀就是“~”。可以用 --backup 或 VERSION_CONTROL 设置版本控制，可能的值为： none, off       从不做备份 t, numbered     进行编号备份 nil, existing如果编号备份存在则进行编号备份，否则进行简单备份 never, simple   总是使用简单备份\n2.gzipgzip命令用来压缩文件。gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多处“.gz”扩展名。\ngzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。gzip不仅可以用来压缩大的、较少使用的文件以节省磁盘空间，还可以和tar命令一起构成Linux操作系统中比较流行的压缩文件格式。据统计，gzip命令对文本文件有60%～70%的压缩率。减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。\n1gzip [ -acdfhlLnNrtvV19 ] [-S 后缀] [ 文件名 ...  ]\n1234567891011121314151617-a或——ascii：使用ASCII文字模式；-d或--decompress或----uncompress：解开压缩文件；-f或——force：强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接；-h或——help：在线帮助；-l或——list：列出压缩文件的相关信息；-L或——license：显示版本与版权信息；-n或--no-name：压缩文件时，不保存原来的文件名称及时间戳记；-N或——name：压缩文件时，保存原来的文件名称及时间戳记；-q或——quiet：不显示警告信息；-r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串；-t或——test：测试压缩文件是否正确无误；-v或——verbose：显示指令执行过程；-V或——version：显示版本信息；-&lt;压缩效率&gt;：压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高；--best：此参数的效果和指定“-9”参数相同；--fast：此参数的效果和指定“-1”参数相同。\n3.gunzipgunzip命令用来解压缩文件。gunzip是个使用广泛的解压缩程序，它用于解开被gunzip压缩过的文件，这些压缩文件预设最后的扩展名为.gz。事实上gunzip就是gzip的硬连接，因此不论是压缩或解压缩，都可通过gzip指令单独完成。\n1gunzip [ -acfhlLnNrtvV ] [-S 后缀] [ 文件名 ...  ]\n1234567891011121314-a或——ascii：使用ASCII文字模式；-c或--stdout或--to-stdout：把解压后的文件输出到标准输出设备；-f或-force：强行解开压缩文件，不理会文件名称或硬连接是否存在以及该文件是否为符号连接；-h或——help：在线帮助；-l或——list：列出压缩文件的相关信息；-L或——license：显示版本与版权信息；-n或--no-name：解压缩时，若压缩文件内含有原来的文件名称及时间戳记，则将其忽略不予处理；-N或——name：解压缩时，若压缩文件内含有原来的文件名称及时间戳记，则将其回存到解开的文件上；-q或——quiet：不显示警告信息；-r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串；-t或——test：测试压缩文件是否正确无误；-v或——verbose：显示指令执行过程；-V或——version：显示版本信息；\n4.bzip2Linux系统中bzip2命令的英文是“bunzip2”，即.bz2文件格式的压缩程序； bzip2命令系统默认是没有安装的，需要安装bzip2库才可以使用此命令。\nbzip2命令采用新的压缩演算法，压缩效果比传统的LZ77/LZ78压缩演算法来得好。若没有加上任何参数，bzip2压缩完文件后会产生.bz2的压缩文件，并删除原始的文件。\n1bzip2 [ -cdfkqstvzVL123456789 ] [ filenames ...  ]\n123456789101112-c或——stdout：将压缩与解压缩的结果送到标准输出；-d或——decompress：执行解压缩；-f或-force：bzip2在压缩或解压缩时，若输出文件与现有文件同名，预设不会覆盖现有文件。若要覆盖。请使用此参数；-h或——help：在线帮助；-k或——keep：bzip2在压缩或解压缩后，会删除原始文件。若要保留原始文件，请使用此参数；-s或——small：降低程序执行时内存的使用量；-t或——test：测试.bz2压缩文件的完整性；-v或——verbose：压缩或解压缩文件时，显示详细的信息；-z或——compress：强制执行压缩；-V或——version：显示版本信息；--repetitive-best：若文件中有重复出现的资料时，可利用此参数提高压缩效果；--repetitive-fast：若文件中有重复出现的资料时，可利用此参数加快执行效果。\n5.bunzip2bunzip2命令解压缩由bzip2指令创建的”.bz2”压缩包。对文件进行压缩与解压缩。此命令类似于gzip/gunzip)”命令，只能对文件进行压缩。对于目录只能压缩目录下的所有文件，压缩完成后，在目录下生成以“.bz2”为后缀的压缩包。bunzip2其实是bzip2的符号链接，即软链接，因此压缩解压都可以通过bzip2实现。\n1bunzip2 [ -fkvsVL ] [ filenames ...  ]\n12345-f或--force：解压缩时，若输出的文件与现有文件同名时，预设不会覆盖现有的文件；-k或——keep：在解压缩后，预设会删除原来的压缩文件。若要保留压缩文件，请使用此参数；-s或——small：降低程序执行时，内存的使用量；-v或——verbose：解压缩文件时，显示详细的信息；-l，--license，-V或——version：显示版本信息。\n6.bzip2recoverbzip2recover命令用来修复损坏的.bz2文件 ， bzip2是以区块的方式来压缩文件，每个区块视为独立的单位。因此，当某一区块损坏时，便可利用bzip2recover，试着将文件中的区块隔开来，以便解压缩正常的区块。通常只适用在压缩文件很大的情况。\n1bzip2recover filename\n\n7.zipzip 命令是一个应用广泛的跨平台的压缩工具，压缩文件的后缀为 .zip文件\nzip程序将一个或多个压缩文件与有关文件的信息(名称、路径、日期、上次修改的时间、保护和检查信息以验证文件完整性)一起放入一个压缩存档中。可以使用一个命令将整个目录结构打包到zip存档中。\n对于文本文件来说，压缩比为2：1和3：1是常见的。zip只有一种压缩方法(通缩)，并且可以在不压缩的情况下存储文件。(如果添加了bzip 2支持，zip也可以使用bzip 2压缩，但这些条目需要一个合理的现代解压缩来解压缩。当选择bzip 2压缩时，它将通货紧缩替换为默认方法。)zip会自动为每个要压缩的文件选择更好的两个文件(通缩或存储，如果选择bzip2，则选择bzip2或Store)。\n\n1zip [参数] [文件]\n12345678910111213141516171819202122232425262728293031323334-A：调整可执行的自动解压缩文件；-b&lt;工作目录&gt;：指定暂时存放文件的目录；-c：替每个被压缩的文件加上注释；-d：从压缩文件内删除指定的文件；-D：压缩文件内不建立目录名称；-f：此参数的效果和指定“-u”参数类似，但不仅更新既有文件，如果某些文件原本不存在于压缩文件内，使用本参数会一并将其加入压缩文件中；-F：尝试修复已损坏的压缩文件；-g：将文件压缩后附加在已有的压缩文件之后，而非另行建立新的压缩文件；-h：在线帮助；-i&lt;范本样式&gt;：只压缩符合条件的文件；-j：只保存文件名称及其内容，而不存放任何目录名称；-J：删除压缩文件前面不必要的数据；-k：使用MS-DOS兼容格式的文件名称；-l：压缩文件时，把LF字符置换成LF+CR字符；-ll：压缩文件时，把LF+cp字符置换成LF字符；-L：显示版权信息；-m：将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中；-n&lt;字尾字符串&gt;：不压缩具有特定字尾字符串的文件；-o：以压缩文件内拥有最新更改时间的文件为准，将压缩文件的更改时间设成和该文件相同；-q：不显示指令执行过程；-r：递归处理，将指定目录下的所有文件和子目录一并处理；-S：包含系统和隐藏文件；-t&lt;日期时间&gt;：把压缩文件的日期设成指定的日期；-T：检查备份文件内的每个文件是否正确无误；-u：更换较新的文件到压缩文件内；-v：显示指令执行过程或显示版本信息；-V：保存VMS操作系统的文件属性；-w：在文件名称里假如版本编号，本参数仅在VMS操作系统下有效；-x&lt;范本样式&gt;：压缩时排除符合条件的文件；-X：不保存额外的文件属性；-y：直接保存符号连接，而非该链接所指向的文件，本参数仅在UNIX之类的系统下有效；-z：替压缩文件加上注释；-$：保存第一个被压缩文件所在磁盘的卷册名称；-&lt;压缩效率&gt;：压缩效率是一个介于1~9的数值。\n8.unzipunzip命令用于解压缩由zip命令压缩的“.zip”压缩包。\n默认行为（就是没有选项）是从指定的ZIP存档中提取所有的文件到当前目录（及其下面的子目录）。一个配套程序zip（1L）创建ZIP存档；这两个程序都与PKWARE的PKZIP和PKUNZIP为MS-DOS创建的存档文件兼容，但许多情况下，程序选项或默认行为是不同的。\n1unzip [-Z] [-cflptTuvz[abjnoqsCDKLMUVWX$/:^]] file[.zip] [file(s) ...] [-x xfile(s) ...] [-d exdir]\n123456789101112131415161718192021222324-c：将解压缩的结果显示到屏幕上，并对字符做适当的转换；-f：更新现有的文件；-l：显示压缩文件内所包含的文件；-p：与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换；-t：检查压缩文件是否正确；-u：与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中；-v：执行时显示详细的信息；-z：仅显示压缩文件的备注文字；-a：对文本文件进行必要的字符转换；-b：不要对文本文件进行字符转换；-C：压缩文件中的文件名称区分大小写；-j：不处理压缩文件中原有的目录路径；-L：将压缩文件中的全部文件名改为小写；-M：将输出结果送到more程序处理；-n：解压缩时不要覆盖原有的文件；-o：不必先询问用户，unzip执行后覆盖原有的文件；-P&lt;密码&gt;：使用zip的密码选项；-q：执行时不显示任何信息；-s：将文件名中的空白字符转换为底线字符；-V：保留VMS的文件版本信息；-X：解压缩时同时回存文件原来的UID/GID；-d&lt;目录&gt;：指定文件解压缩后所要存储的目录；-x&lt;文件&gt;：指定不要处理.zip压缩文件中的哪些文件；-Z：unzip-Z等于执行zipinfo指令。\n9.zipinfozipinfo命令的全称为“zip information”，该命令用于列出压缩文件信息。执行zipinfo指令可得知zip压缩文件的详细信息。\n1zipinfo [-12hlmMstTvz][压缩文件][文件...][-x &lt;范本样式&gt;]\n123456789101112-1：只列出文件名称；-2：此参数的效果和指定“-1”参数类似，但可搭配“-h”，“-t”和“-z”参数使用；-h：只列出压缩文件的文件名称；-l：此参数的效果和指定“-m”参数类似，但会列出原始文件的大小而非每个文件的压缩率；-m：此参数的效果和指定“-s”参数类似，但多会列出每个文件的压缩率；-M：若信息内容超过一个画面，则采用类似more指令的方式列出信息；-s：用类似执行“ls-l”指令的效果列出压缩文件内容；-t：只列出压缩文件内所包含的文件数目，压缩前后的文件大小及压缩率；-T：将压缩文件内每个文件的日期时间用年，月，日，时，分，秒的顺序列出；-v：详细显示压缩文件内每一个文件的信息；-x&lt;范本样式&gt;：不列出符合条件的文件的信息；-z：如果压缩文件内含有注释，就将注释显示出来。\n\n十一、信息显示命令1.unameuname命令的英文全称即“Unix name”。\n用于显示系统相关信息，比如主机名、内核版本号、硬件架构、操作系统类型等。\n如果未指定任何选项，其效果相当于执行uname -s命令，即显示系统内核的名字。\n1用法：uname [选项]...\n1234567891011121314输出一组系统信息。如果不跟随选项，则视为只附加-s 选项。  -a, --all                     以如下次序输出所有信息。其中若-p 和                                -i 的探测结果不可知则被省略：  -s, --kernel-name             输出内核名称  -n, --nodename                输出网络节点上的主机名  -r, --kernel-release          输出内核发行号  -v, --kernel-version          输出内核版本  -m, --machine         输出主机的硬件架构名称  -p, --processor               输出处理器类型或&quot;unknown&quot;  -i, --hardware-platform       输出硬件平台或&quot;unknown&quot;  -o, --operating-system        输出操作系统名称      --help            显示此帮助信息并退出      --version         显示版本信息并退出\n2.hostnamehostname命令用于显示和设置系统的主机名称。环境变量HOSTNAME也保存了当前的主机名。在使用hostname命令设置主机名后，系统并不会永久保存新的主机名，重新启动机器之后还是原来的主机名。如果需要永久修改主机名，需要同时修改/etc/hosts和/etc/sysconfig/network的相关内容。\n1hostname [选项] [参数]\n1234567891011121314151617181920212223242526hostname - 用来显示或者设置当前系统的主机名，主机名被许多网络程序使用，来标识主机。-a,--alias    显示主机的别名(如果使用了的话).-d,--domain    显示DNS域名.不要使用命令 domainname 来获得DNS域名,因为这会显示NIS域名而非DNS域名.可使用 dnsdomainname 替换之.-F,--file filename    从指定文件中读取主机名.注释(以一个`#&#x27;开头的行)可忽略.-f,--fqdn,--long    显示FQDN(完全资格域名).一个FQDN包括一个短格式主机名和DNS域名.除非你正在使用bind或 者NIS来作主机查询,否则你可以在/etc/hosts文件中修改FQDN和DNS域名(这是FQDN的一 部分).-h,--help    打印用法信息并退出.-I, --all-ip-addresses all addresses for the host     显示主机的所有地址-i,--ip-address    显示主机的IP地址(组).-n,--node    显示DECnet节点名.如果指定了参数(或者指定了 --file name ),那么root也可以设置一个新的节点名.-s,--short    显示短格式主机名.这是一个去掉第一个圆点后面部分的主机名.-V,--version    在标准输出上打印版本信息并以成功的状态退出.-v,--verbose    详尽说明并告知所正在执行的.-y,--yp,--nis    显示NIS域名.如果指定了参数(或者指定了 --file name ),那么root也可以设置一个新的NIS域.\n3.uptimeuptime命令能够打印系统总共运行了多长时间和系统的平均负载。uptime命令可以显示的信息显示依次为：现在时间、系统已经运行了多长时间、目前有多少登陆用户、系统在过去的1分钟、5分钟和15分钟内的平均负载。\n1uptime [参数]\n\n\n\n\n-p\n以漂亮的格式显示机器正常运行的时间\n\n\n\n\n-s\n系统自开始运行时间，格式为yyyy-mm-dd hh:mm:ss\n\n\n-h\n显示帮助信息\n\n\n\n\n4.statstat命令用来显示文件或文件系统的详细信息\n1stat [选项]... 文件...\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162显示文件或文件系统的状态。  -L, --dereference     跟随链接  -f, --file-system     显示文件系统状态而非文件状态  -c --format=格式      使用指定输出格式代替默认值，每用一次指定格式换一新行      --printf=格式     类似 --format，但是会解释反斜杠转义符，不使用换行作                                输出结尾。如果您仍希望使用换行，可以在格式中                                加入&quot;\\n&quot;  -t, --terse           使用简洁格式输出      --help            显示此帮助信息并退出      --version         显示版本信息并退出有效的文件格式序列(不使用 --file-system)：  %a    八进制权限  %A   用可读性较好的方式输出权限  %b   计算已分配块数(参见%B)  %B   以字节为单位输出%b 所报告的每个块的大小  %C   SELinux 安全环境字符串  %d    十进制设备编号  %D    十六进制设备编号  %f    十六进制原始模式  %F    文件类型  %g    文件的属组ID  %G    文件的属组组名  %h    硬链接数量  %i    Inode 编号  %m    挂载点  %n    文件名  %N    如果对象是一个符号链接，显示引用到的其它文件名  %o    I/O 块大小  %s    总计大小，以字节为单位  %t    十六进制主设备类型  %T    十六进制子设备类型  %u    文件的属主ID  %U    文件的属主用户名  %w    文件创建时间，若未知则显示&quot;-&quot;  %W    从UNIX 元年起以秒计的文件创建时间，若未知则显示&quot;-&quot;  %x    上次访问时间  %X    从UNIX 元年起以秒计的上次访问时间  %y    上次修改时间  %Y    从UNIX 元年起以秒计的上次修改时间  %z    上次更改时间  %Z    从UNIX 元年起以秒计的上次更改时间有效的文件系统格式序列：  %a    非超级用户可用的剩余块数  %b    文件系统的总数据块数  %c    文件系统中文件节点总数  %d    文件系统中空闲文件节点数  %f    文件系统中空闲块数  %i    十六进制文件系统ID  %I    允许的文件名最大长度  %n    文件名  %s    块大小(用于快速传输)  %S    基本块大小(用于块计数)  %t    十六进制类型描述  %T    可读性较好的类型描述注意：您的shell 内含自己的stat 程序版本，它会覆盖这里所提及的相应版本。请查阅您的shell 文档获知它所支持的选项。\n\n十二、用户和组管理命令1.susu命令用于切换当前用户身份到其他用户身份，变更时须输入所要变更的用户帐号与密码。\n普通用户切换到root用户，可以使用su – 或su root,但是必须输入root密码才能完成切换。root用户切换到普通用户，可以使用su username,不需要输入任何密码即可完成切换。\n1su [选项] [-] [USER [参数]...]\n12345678910111213141516171819202122232425修改有效用户标识和组标识为USER的.-, -l, --login       使得shell为可登录的shell-c, --commmand=COMMAND       传递单个COMMAND给-c的shell.-f, --fast       传递-f给shell(针对csh或tcsh)-m, --preserve-environment       不重置环境变量-p     与-m同-s, --shell=SHELL       如果/etc/shells允许,运行SHELL.--help 显示帮助并退出--version       输出版本信息并退出单一的-意味着-l.如果没有给定USER,则假定为root.\n2.sudosudo - 以其他用户身份执行一条命令\nsudo命令用来以其他身份来执行命令，预设的身份为root。在/etc/sudoers中设置了可执行sudo指令的用户。若其未经授权的用户企图使用sudo，则会发出警告的邮件给管理员。用户使用sudo时，必须先输入密码，之后有5分钟的有效期限，超过期限则必须重新输入密码。\nsudo 是一种权限管理机制，管理员可以给一些普通用户授权去执行一些 root 执行的操作，而不需要知道 root 的密码。\nsudo 允许一个已授权用户以超级用户或者其它用户的角色运行一个命令。当然，能做什么不能做什么都是通过安全策略来指定的。sudo 支持插件架构的安全策略，并能把输入输出写入日志。第三方可以开发并发布自己的安全策略和输入输出日志插件，并让它们无缝的和 sudo 一起工作。默认的安全策略记录在 /etc/sudoers 文件中。而安全策略可能需要用户通过密码来验证他们自己。也就是在用户执行 sudo 命令时要求用户输入自己账号的密码。如果验证失败，sudo 命令将会退出。\n12345678usage: sudo -h | -K | -k | -Vusage: sudo -v [-AknS] [-g group] [-h host] [-p prompt] [-u user]usage: sudo -l [-AknS] [-g group] [-h host] [-p prompt] [-U user] [-u user]            [command]usage: sudo [-AbEHknPS] [-r role] [-t type] [-C num] [-g group] [-h host] [-p            prompt] [-u user] [VAR=value] [-i|-s] [&lt;command&gt;]usage: sudo -e [-AknS] [-r role] [-t type] [-C num] [-g group] [-h host] [-p            prompt] [-u user] file ...\n12345678910111213141516171819202122232425262728293031选项：  -A, --askpass               使用助手程序进行密码提示  -b, --background            在后台运行命令  -C, --close-from=num        关闭所有 &gt;= num 的文件描述符  -E, --preserve-env          在执行命令时保留用户环境  -e, --edit                  编辑文件而非执行命令  -g, --group=group           以指定的用户组或 ID 执行命令  -H, --set-home              将 HOME 变量设为目标用户的主目录。  -h, --help                  显示帮助消息并退出  -h, --host=host             在主机上运行命令(如果插件支持)  -i, --login                 以目标用户身份运行一个登录                              shell；可同时指定一条命令  -K, --remove-timestamp      完全移除时间戳文件  -k, --reset-timestamp       无效的时间戳文件  -l, --list                                               列出用户权限或检查某个特定命令；对于长格式，使用两次  -n, --non-interactive       非交互模式，不提示  -P, --preserve-groups                                    保留组向量，而非设置为目标的组向量  -p, --prompt=prompt         使用指定的密码提示  -r, --role=role             以指定的角色创建 SELinux 安全环境  -S, --stdin                 从标准输入读取密码  -s, --shell                 以目标用户运行                              shell；可同时指定一条命令  -t, --type=type             以指定的类型创建 SELinux 安全环境  -U, --other-user=user       在列表模式中显示用户的权限  -u, --user=user             以指定用户或 ID                              运行命令(或编辑文件)  -V, --version               显示版本信息并退出  -v, --validate              更新用户的时间戳而不执行命令  --                          停止处理命令行参数\nsudo配置文件sudo默认配置文件是/etc/sudoers ，一般使用Linux指定编辑工具visudo ，此工具的好处是可以进行错误检查。在添加规则不符合语法规则时，保存退出时会提示给我们错误信息；配置好后，可以用切换到您授权的普通用户下，通过sudo -l来查看哪些命令是可以执行的或禁止的；\n/etc/sudoers 文件中每行是一个规则，前面带有#号可以当作是注释的内容，并不执行；如果规则很长，可以写在多列上，可以用\\号来续行。\n/etc/sudoers 的规则可分为两类；一类是授权规则，另一类是别名定义；别名定义并不是必须的，但授权规则是必须的；\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125[root@itcast ~]# cat /etc/sudoers## Sudoers allows particular users to run various commands as## the root user, without needing the root password.##该文件允许特定用户像root用户一样使用各种各样的命令，而不需要root用户的密码 #### Examples are provided at the bottom of the file for collections## of related commands, which can then be delegated out to particular## users or groups.## 在文件的底部提供了很多相关命令的示例以供选择，这些示例都可以被特定用户或  ## ## 用户组所使用  ## This file must be edited with the &#x27;visudo&#x27; command.## 该文件必须使用&quot;visudo&quot;命令编辑## Host Aliases#主机别名## Groups of machines. You may prefer to use hostnames (perhap using ## wildcards for entire domains) or IP addresses instead.## 对于一组服务器，你可能会更喜欢使用主机名（可能是全域名的通配符）## 或IP地址代替，这时可以配置主机别名  # Host_Alias     FILESERVERS = fs1, fs2# Host_Alias     MAILSERVERS = smtp, smtp2## User Aliases#用户别名## These aren&#x27;t often necessary, as you can use regular groups## (ie, from files, LDAP, NIS, etc) in this file - just use %groupname ## rather than USERALIAS## 这并不很常用，因为你可以通过使用组来代替一组用户的别名  # User_Alias ADMINS = jsmith, mikem## Command Aliases## These are groups of related commands...## 指定一系列相互关联的命令（当然可以是一个）的别名，通过赋予该别名sudo权限，  ## 可以通过sudo调用所有别名包含的命令，下面是一些示例## Networking#网络操作相关命令别名  Cmnd_Alias NETWORKING = /sbin/route, /sbin/ifconfig, /bin/ping, /sbin/dhclient, /usr/bin/net, /sbin/iptables, /usr/bin/rfcomm, /usr/bin/wvdial, /sbin/iwconfig,  /sbin/mii-tool## Installation and management of software#软件安装管理相关命令别名  Cmnd_Alias SOFTWARE = /bin/rpm, /usr/bin/up2date, /usr/bin/yum## Services#服务相关命令别名 Cmnd_Alias SERVICES = /sbin/service, /sbin/chkconfig## Updating the locate database#本地数据库升级命令别名  Cmnd_Alias LOCATE = /usr/sbin/updatedb## Storage#磁盘操作相关命令别名Cmnd_Alias STORAGE = /sbin/fdisk, /sbin/sfdisk, /sbin/parted, /sbin/partprobe, /bin/mount, /bin/umount## Delegating permissions#代理权限相关命令别名 Cmnd_Alias DELEGATING = /usr/sbin/visudo, /bin/chown, /bin/chmod, /bin/chgrp## Processes#进程相关命令别名Cmnd_Alias PROCESSES = /bin/nice, /bin/kill, /usr/bin/kill, /usr/bin/killall## Drivers#驱动命令别名Cmnd_Alias DRIVERS = /sbin/modprobe#环境变量的相关配置# Defaults specification## Disable &quot;ssh hostname sudo &lt;cmd&gt;&quot;, because it will show the password in clear. #         You have to run &quot;ssh -t hostname sudo &lt;cmd&gt;&quot;.#Defaults    requirettyDefaults    env_resetDefaults    env_keep = &quot;COLORS DISPLAY HOSTNAME HISTSIZE INPUTRC KDEDIR \\                        LS_COLORS MAIL PS1 PS2 QTDIR USERNAME \\                        LANG LC_ADDRESS LC_CTYPE LC_COLLATE LC_IDENTIFICATION \\                        LC_MEASUREMENT LC_MESSAGES LC_MONETARY LC_NAME LC_NUMERIC \\                        LC_PAPER LC_TELEPHONE LC_TIME LC_ALL LANGUAGE LINGUAS \\                        _XKB_CHARSET XAUTHORITY&quot;## Next comes the main part: which users can run what software on## which machines (the sudoers file can be shared between multiple## systems).## 下面是规则配置：什么用户在哪台服务器上可以执行哪些命令（sudoers文件可以在多个系统上共享）## Syntax:##语法##      user    MACHINE=COMMANDS##  用户 登录的主机=（可以变换的身份） 可以执行的命令  #### The COMMANDS section may have other options added to it.## 命令部分可以附带一些其它的选项  #### Allow root to run any commands anywhere ## 允许root用户执行任意路径下的任意命令 root    ALL=(ALL)       ALL## Allows members of the &#x27;sys&#x27; group to run networking, software,## service management apps and more.# %sys ALL = NETWORKING, SOFTWARE, SERVICES, STORAGE, DELEGATING, PROCESSES, LOCATE, DRIVERS## 允许sys中户组中的用户使用NETWORKING等所有别名中配置的命令  ## Allows people in group wheel to run all commands# %wheel        ALL=(ALL)       ALL## 允许wheel用户组中的用户执行所有命令  ## Same thing without a password## 允许wheel用户组中的用户在不输入该用户的密码的情况下使用所有命令# %wheel        ALL=(ALL)       NOPASSWD: ALL## Allows members of the users group to mount and unmount the## cdrom as root## 允许users用户组中的用户像root用户一样使用mount、unmount、chrom命令 # %users  ALL=/sbin/mount /mnt/cdrom, /sbin/umount /mnt/cdrom## Allows members of the users group to shutdown this system# %users  localhost=/sbin/shutdown -h now## 允许users用户组中的用户像root用户一样使用shutdown命令\nsudo命令别名规则别名规则定义格式如下\n123Alias_Type NAME = item1, item2, ...或者Alias_Type NAME = item1, item2, item3 : NAME = item4, item5\n别名类型（Alias_Type）：别名类型包括如下\n\nHost_Alias 定义主机别名；\nUser_Alias 用户别名，别名成员可以是用户，用户组（前面要加%号）\nRunas_Alias 用来定义runas别名，这个别名指定的是“目的用户”，即sudo 允许切换至的用户；\nCommand_Alias 定义命令别名；\n\n别名规则格式解析\nNAME 就是别名了，NMAE的命名是包含大写字母、下划线以及数字，但必须以一个大写字母开头，比如SYNADM、SYN_ADM或SYNAD0是合法的，sYNAMDA或1SYNAD是不合法的；\nitem 按中文翻译是项目，在这里我们可以译成成员，如果一个别名下有多个成员，成员与成员之间，通过半角逗号分隔；成员在必须是有效并事实存在的。什么是有效的呢？比如主机名，可以通过w查看用户的主机名（或ip地址），如果您只是本地机操作，只通过hostname 命令就能查看；用户名当然是在系统中存在 的，在/etc/paswd中必须存在；对于定义命令别名，成员也必须在系统中事实存在的文件名（需要绝对路径）；\nitem成员受别名类型 Host_Alias、User_Alias、Runas_Alias、Command_Alias 制约，定义什么类型的别名，就要有什么类型的成员相配。我们用Host_Alias定义主机别名时，成员必须是与主机相关相关联，比如是主机名（包括远程登录的主机名）、ip地址（单个或整段）、掩码等； 当用户登录时，可以通过w命令来查看登录用户主机信息；用User_Alias和 Runas_Alias定义时，必须要用系统用户做为成员；用 Cmnd_Alias 定义执行命令的别名时，必须是系统存在的文件，文件名可以用通配符表示，配置Cmnd_Alias时命令需要绝对路径；\n其中 Runas_Alias 和User_Alias 有点相似，但与User_Alias 绝对不是同一个概念，Runas_Alias 定义的是某个系统用户可以sudo 切换身份到Runas_Alias 下的成员；我们在授权规则中以实例进行解说；\n别名规则是每行算一个规则，如果一个别名规则一行容不下时，可以通过\\来续行；同一类型别名的定义，一次也可以定义几个别名，他们中间用:号分隔，\nsudo 设置普通用户具有 root 权限1）添加 用户，并对其设置密码。\n [root@ ~]#useradd xxx\n [root@ ~]#passwd xxx\n2）修改配置文件 \n[root@ ~]#vim /etc/sudoers       修改 /etc/sudoers 文件，找到下面一行，在 root 下面添加一行，如下所示： \n1234## Allow root to run any commands anywhere root ALL=(ALL) ALL root xxx ALL=(ALL) ALL xxx\n或者配置成采用 sudo 命令时，不需要输入密码 \n123## Allow root to run any commands anywhere root ALL=(ALL) ALL root xxx  ALL=(ALL) NOPASSWD:ALL \n修改完毕，现在可以用 帐号登录，然后用命令 sudo ，即可获得 root 权限进行操作\n3.useradduseradd - 创建一个新用户或更新默认新用户信息\nuseradd命令用于Linux中创建的新的系统用户。useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。\ncat /etc/passwd 查看创建了哪些用户\n123useradd [选项] 用户名useradd -Duseradd -D [选项]\n123456789101112131415161718192021222324选项：  -b, --base-dir BASE_DIR       新账户的主目录的基目录  -c, --comment COMMENT         新账户的 GECOS 字段  -d, --home-dir HOME_DIR       新账户的主目录  -D, --defaults                显示或更改默认的 useradd 配置 -e, --expiredate EXPIRE_DATE  新账户的过期日期  -f, --inactive INACTIVE       新账户的密码不活动期  -g, --gid GROUP               新账户主组的名称或 ID  -G, --groups GROUPS   新账户的附加组列表  -h, --help                    显示此帮助信息并推出  -k, --skel SKEL_DIR   使用此目录作为骨架目录  -K, --key KEY=VALUE           不使用 /etc/login.defs 中的默认值  -l, --no-log-init     不要将此用户添加到最近登录和登录失败数据库  -m, --create-home     创建用户的主目录  -M, --no-create-home          不创建用户的主目录  -N, --no-user-group   不创建同名的组  -o, --non-unique              允许使用重复的 UID 创建用户  -p, --password PASSWORD               加密后的新账户密码  -r, --system                  创建一个系统账户  -R, --root CHROOT_DIR         chroot 到的目录  -s, --shell SHELL             新账户的登录 shell  -u, --uid UID                 新账户的用户 ID  -U, --user-group              创建与用户同名的组  -Z, --selinux-user SEUSER             为 SELinux 用户映射使用指定 SEUSER\n4.userdeluserdel - 删除用户账户和相关文件\nuserdel命令用于删除指定的用户及与该用户相关的文件，英文全称即“user delete”。其实userdel命令实际上是修改了系统的用户账号文件 /etc/passwd、/etc/shadow以及/etc/group文件。这与Linux系统”一切操作皆文件”的思想正好吻合。\n值得注意的是，但是如果有该要删除用户相关的进程正在运行，userdel命令通常不会删除一个用户账号。如果确实必须要删除，可以先终止用户进程，然后再执行userdel命令进行删除。但是userdel命令也提供了一个面对该种情况的参数，即”-f”选项。\n1userdel [选项] 用户名\n12345678选项：  -f, --force                   force some actions that would fail otherwise                                e.g. removal of user still logged in                                or files, even if not owned by the user  -h, --help                    显示此帮助信息并推出  -r, --remove                  删除主目录和邮件池  -R, --root CHROOT_DIR         chroot 到的目录  -Z, --selinux-user            为用户删除所有的 SELinux 用户映射\n5.passwdpasswd命令用于设置用户的认证信息，包括用户密码、账户锁定、密码失效等。直接运行passwd命令修改当前的用户密码，对其他用户的密码操作需要管理员权限。\n1用法: passwd [选项...] &lt;帐号名称&gt;\n12345678910111213141516  -k, --keep-tokens       保持身份验证令牌不过期  -d, --delete            删除已命名帐号的密码(只有根用户才能进行此操作)  -l, --lock              锁定指名帐户的密码(仅限 root 用户)  -u, --unlock            解锁指名账户的密码(仅限 root 用户)  -e, --expire            终止指名帐户的密码(仅限 root 用户)  -f, --force             强制执行操作  -x, --maximum=DAYS      密码的最长有效时限(只有根用户才能进行此操作)  -n, --minimum=DAYS      密码的最短有效时限(只有根用户才能进行此操作)  -w, --warning=DAYS      在密码过期前多少天开始提醒用户(只有根用户才能进行此操作)  -i, --inactive=DAYS     当密码过期后经过多少天该帐号会被禁用(只有根用户才能进行此操作)  -S, --status            报告已命名帐号的密码状态(只有根用户才能进行此操作)  --stdin                 从标准输入读取令牌(只有根用户才能进行此操作)Help options:  -?, --help              Show this help message  --usage                 Display brief usage message\n6.usermodusermod命令用于修改用户账号 。usermod可用来修改用户账号的各项设定，修改系统账号文件来反映通过命令行指定的变化。\n1用法：usermod [选项] 用户名\n1234567891011121314151617181920选项：  -c, --comment 注释            GECOS 字段的新值  -d, --home HOME_DIR           用户的新主目录  -e, --expiredate EXPIRE_DATE  设定帐户过期的日期为 EXPIRE_DATE  -f, --inactive INACTIVE       过期 INACTIVE 天数后，设定密码为失效状态  -g, --gid GROUP               强制使用 GROUP 为新主组  -G, --groups GROUPS           新的附加组列表 GROUPS  -a, --append GROUP            将用户追加至上边 -G 中提到的附加组中，                                并不从其它组中删除此用户  -h, --help                    显示此帮助信息并推出  -l, --login LOGIN             新的登录名称  -L, --lock                    锁定用户帐号  -m, --move-home               将家目录内容移至新位置 (仅于 -d 一起使用)  -o, --non-unique              允许使用重复的(非唯一的) UID  -p, --password PASSWORD       将加密过的密码 (PASSWORD) 设为新密码  -R, --root CHROOT_DIR         chroot 到的目录  -s, --shell SHELL             该用户帐号的新登录 shell  -u, --uid UID                 用户帐号的新 UID  -U, --unlock                  解锁用户帐号  -Z, --selinux-user  SEUSER       用户账户的新 SELinux 用户映射\n7.groupaddgroupadd - 创建一个新组\ngroupadd命令用于创建一个新的工作组，新工作组的信息将被添加到系统文件中。\n1groupadd [选项] group\n12345678910选项:  -f, --force           如果组已经存在则成功退出                        并且如果 GID 已经存在则取消 -g  -g, --gid GID                 为新组使用 GID  -h, --help                    显示此帮助信息并推出  -K, --key KEY=VALUE           不使用 /etc/login.defs 中的默认值  -o, --non-unique              允许创建有重复 GID 的组  -p, --password PASSWORD       为新组使用此加密过的密码  -r, --system                  创建一个系统账户  -R, --root CHROOT_DIR         chroot 到的目录\n8.groupdelgroupdel - 删除一个组\ngroupdel命令用于删除指定的工作组，本命令要修改的系统文件包括/ect/group和/ect/gshadow。\nuserdel修改系统账户文件，删除与 GROUP 相关的所有项目。给出的组名必须存在。若该群组中仍包括某些用户，则必须先删除这些用户后，方能删除群组。\n1用法：groupdel [选项] 组\n123选项:  -h, --help                    显示此帮助信息并推出  -R, --root CHROOT_DIR         chroot 到的目录\n9.groupmodgroupmod命令更改群组识别码或名称。需要更改群组的识别码或名称时，可用groupmod指令来完成这项工作。\n不过大家还是要注意，用户名不要随意修改，组名和 GID 也不要随意修改，因为非常容易导致管理员逻辑混乱。如果非要修改用户名或组名，则建议大家先删除旧的，再建立新的。\n1用法：groupmod [选项] 组\n1234567选项:  -g, --gid GID                 将组 ID 改为 GID  -h, --help                    显示此帮助信息并推出  -n, --new-name NEW_GROUP      改名为 NEW_GROUP  -o, --non-unique              允许使用重复的 GID  -p, --password PASSWORD       将密码更改为(加密过的) PASSWORD  -R, --root CHROOT_DIR         chroot 到的目录\n10.gpasswdgpasswd命令是Linux下工作组文件/etc/group和/etc/gshadow管理工具。\ngpasswd命令是Linux下工作组文件/etc/group和/etc/gshadow的管理工具 ，系统管理员可以使用-a选项定义组管理员，使用-m选项定义成员，由组管理员用组名调用的gpasswd只提示输入组的新密码。\n1用法：gpasswd [选项] 组\n12345678910选项：  -a, --add USER                向组 GROUP 中添加用户 USER  -d, --delete USER             从组 GROUP 中添加或删除用户  -h, --help                    显示此帮助信息并推出  -Q, --root CHROOT_DIR         要 chroot 进的目录  -r, --delete-password         remove the GROUP&#x27;s password  -R, --restrict                向其成员限制访问组 GROUP  -M, --members USER,...        设置组 GROUP 的成员列表  -A, --administrators ADMIN,...        设置组的管理员列表除非使用 -A 或 -M 选项，不能结合使用这些选项。\n11.newgrpnewgrp - 登录到一个新组\nnewgrp命令类的英文全称为“new group”,该命令类似login指令，当它是以相同的帐号，另一个群组名称，再次登入系统。欲使用newgrp指令切换群组，您必须是该群组的用户，否则将无法登入指定的群组。\n单一用户要同时隶属多个群组，需利用交替用户的设置。若不指定群组名称，则newgrp指令会登入该用户名称的预设群组。\n1用法：newgrp [-] [用户组名]\n\n\n\n\n–help\n在线帮助\n\n\n\n\n–vesion\n显示版本信息\n\n\n\n\n12.chagechage - 更改用户密码过期信息\nchage命令是用来修改帐号和密码的有效期限；这个信息由系统用于确定用户何时必须更改其密码。\n1用法：chage [选项] 登录名\n12345678910选项：  -d, --lastday 最近日期        将最近一次密码设置时间设为“最近日期”  -E, --expiredate 过期日期     将帐户过期时间设为“过期日期”  -h, --help                    显示此帮助信息并推出  -I, --inactive INACITVE       过期 INACTIVE 天数后，设定密码为失效状态  -l, --list                    显示帐户年龄信息  -m, --mindays 最小天数        将两次改变密码之间相距的最小天数设为“最小天数”  -M, --maxdays 最大天数        将两次改变密码之间相距的最大天数设为“最大天数”  -R, --root CHROOT_DIR         chroot 到的目录  -W, --warndays 警告天数       将过期警告天数设为“警告天数”\n13.groupsgroups - 显示用户所在的组\ngroups命令在标准输入输出上输出指定用户所在组的组成员，每个用户属于/etc/passwd中指定的一个组和在/etc/group中指定的其他组。\n1用法：groups [选项]... [用户名]...\n1234显示每个输入的用户名所在的全部组，如果没有指定用户名则默认为当前进程用户(当用户组数据库发生变更时可能导致差异)。      --help            显示此帮助信息并退出      --version         显示版本信息并退出\n14.whowho命令用来打印当前登录用户信息，包含了系统的启动时间 、 活动进程 、 使用者 ID、使用终端等信息，是系统管理员了解系统运行状态的常用命令。\n1用法：who [选项]... [ 文件 | 参数1 参数2 ]\n12345678910111213141516171819显示当前已登录的用户信息。  -a, --all             等于-b -d --login -p -r -t -T -u 选项的组合  -b, --boot            上次系统启动时间  -d, --dead            显示已死的进程  -H, --heading 输出头部的标题列  -l，--login           显示系统登录进程      --lookup          尝试通过 DNS 查验主机名  -m                    只面对和标准输入有直接交互的主机和用户  -p, --process 显示由 init 进程衍生的活动进程  -q, --count           列出所有已登录用户的登录名与用户数量  -r, --runlevel        显示当前的运行级别  -s, --short           只显示名称、线路和时间(默认)  -T, -w, --mesg        用+，- 或 ? 标注用户消息状态  -u, --users           列出已登录的用户      --message 等于-T      --writable        等于-T      --help            显示此帮助信息并退出      --version         显示版本信息并退出\n15.ww - 显示已经登录的用户以及他们在做什么\nw命令用于显示已经登陆系统的用户列表，并显示用户正在执行的指令。执行这个命令可得知目前登入系统的用户有那些人，以及他们正在执行的程序。单独执行w命令会显示所有的用户，您也可指定用户名称，仅显示某位用户的相关信息。\n1w - [husfV] [user]\n12345-h：不打印头信息；-u：当显示当前进程和cpu时间时忽略用户名；-s：使用短输出格式；-f：显示用户从哪登录；-V：显示版本信息。\n16.lastlast命令用于显示用户最近登录信息。单独执行last命令，它会读取/var/log/wtmp的文件，并把该给文件的内容记录的登入系统的用户名单全部显示出来。\nlast命令的作用是显示近期用户或终端的登录情况，通过查看系统记录的日志文件内容，进而使管理员可以获知谁曾经或者企图连接系统。\n执行last命令时，它会读取/var/log目录下名称为wtmp的文件，并把该文件记录的登录系统或终端的用户名单全部显示出来。默认显示wtmp的记录，btmp能显示的更详细，可以显示远程登录，例如ssh登录。\n1last [-R] [-num] [ -n num ] [-adiox] [ -f file ] [name...]  [tty...]\n123456-a：把从何处登入系统的主机名称或ip地址，显示在最后一行；-d：将IP地址转换成主机名称；-f &lt;记录文件&gt;：指定记录文件。-n &lt;显示列数&gt;或-&lt;显示列数&gt;：设置列出名单的显示列数；-R：不显示登入系统的主机名称或IP地址；-x：显示系统关机，重新开机，以及执行等级的改变等信息。\n17.lastloglastlog命令用于显示系统中所有用户最近一次登录信息。\nlastlog文件在每次有用户登录时被查询。可以使用lastlog命令检查某特定用户上次登录的时间，并格式化输出上次登录日志/var/log/lastlog的内容。它根据UID排序显示登录名、端口号（tty）和上次登录时间。如果一个用户从未登录过，lastlog显示Never logged。注意需要以root身份运行该命令。\n1用法：lastlog [选项]\n12345678选项：  -b, --before DAYS             仅打印早于 DAYS 的最近登录记录  -C, --clear                   clear lastlog record of an user (usable only with -u)  -h, --help                    显示此帮助信息并推出  -R, --root CHROOT_DIR         chroot 到的目录  -S, --set                     set lastlog record to current time (usable only with -u)  -t, --time DAYS               仅打印晚于 DAYS 的最近登录记录  -u, --user LOGIN              打印 LOGIN 用户的最近登录记录\n18.usersusers命令用于显示当前登录系统的所有用户的用户列表。每个显示的用户名对应一个登录会话。如果一个用户有不止一个登录会话，那他的用户名将显示相同的次数。\n1用法：users [选项]... [文件]\n12345根据文件判断输出当前有谁正登录在系统上。如果文件未予指定，则使用/var/run/utmp，/var/log/wtmp 是通用的相关文件。      --help            显示此帮助信息并退出      --version         显示版本信息并退出\n\n十三、权限命令1.chmod**chmod - 改变文件的访问权限\nchmod命令的英文原意是“change the permissions mode of a file”，我们简称为“change mode”，意为用来改变文件或目录权限的命令，但是只有文件的属主和超级用户root才能执行这个命令。有两种模式，一种是采用权限字母和操作符表达式；另一种是采用数字。\n文件属性 \nLinux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。 为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做 了不同的规定。在Linux中我们可以使用ll或者ls -l命令来显示一个文件的属性以及文件所属 的用户和组。\n\n\n\n\n如果没有权限，就会出现减号[ - ]而已。从左至右用0-9这些数字来表示:\n （1）0 首位表示类型 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等 - 代表文件 d 代表目录 l 链接文档(link file)； \n\n“d” 代表文件夹\n“-” 代表普通文件\n“c” 代表硬件字符设备\n“b” 代表硬件块设备\n“s”表示管道文件\n“l” 代表软链接文件。\n\n后9个字母分别代表三组权限：文件所有者、用户组、其他用户拥有的权限。\n（2）第1-3位确定属主（该文件的所有者）拥有该文件的权限。—-User \n（3）第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—-Group \n（4）第7-9位确定其他用户拥有该文件的权限 —-Other\nrwx 作用文件和目录的不同解释 \n（1）作用到文件：\n[ r ]代表可读(read): 可以读取，查看 \n[ w ]代表可写(write): 可以修改，但是不代表可以删除该文件，删除一个文件的前 提条件是对该文件所在的目录有写权限，才能删除该文件\n[ x ]代表可执行(execute):可以被系统执行 \n（2）作用到目录： \n[ r ]代表可读(read): 可以读取，ls查看目录内容 \n[ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录 \n[ x ]代表可执行(execute):可以进入该目录\nchmod 改变权限\n 1）基本语法\n\n权限范围的表示法如下：\nu   User，即文件或目录的拥有者；g   Group，即文件或目录的所属群组；o   Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围；a   All，即全部的用户，包含拥有者，所属群组以及其他用户；r   读取权限，数字代号为“4”;w   写入权限，数字代号为“2”；x   执行或切换权限，数字代号为“1”；\n“-“ 不具任何权限，数字代号为“0”；s   特殊功能说明：变更文件或目录的权限。\n第一种方式变更权限 \nchmod [{ugoa}{+-=}{rwx}] 文件或目录 \n第二种方式变更权限 \nchmod [mode=421 ] [文件或目录] \n权限进制表示\n\n2）经验技巧 u:所有者 g:所有组 o:其他人 a:所有人(u、g、o 的总和)\nr=4 w=2 x=1    rwx=4+2+1=7\n-rw------- (600)      只有拥有者有读写权限。\n-rw-r--r-- (644)      只有拥有者有读写权限；而属组用户和其他用户只有读权限。\n-rwx------ (700)     只有拥有者有读、写、执行权限。\n-rwxr-xr-x (755)    拥有者有读、写、执行权限；而属组用户和其他用户只有读、执行权限。\n-rwx--x--x (711)    拥有者有读、写、执行权限；而属组用户和其他用户只有执行权限。\n-rw-rw-rw- (666)   所有用户都有文件读、写权限。\n-rwxrwxrwx (777)  所有用户都有读、写、执行权限。\n权限设定\n增加权限取消权限 =唯一设定权限\n123用法：chmod [选项]... 模式[,模式]... 文件...　或：chmod [选项]... 八进制模式 文件...　或：chmod [选项]... --reference=参考文件 文件...\n12345678910111213将每个文件的模式更改为指定值。  -c, --changes         类似 --verbose，但只在有更改时才显示结果      --no-preserve-root        不特殊对待根目录(默认)      --preserve-root           禁止对根目录进行递归操作  -f, --silent, --quiet 去除大部份的错误信息  -v, --verbose         为处理的所有文件显示诊断信息      --reference=参考文件      使用指定参考文件的模式，而非自行指定权限模式  -R, --recursive               以递归方式更改所有的文件及子目录      --help            显示此帮助信息并退出      --version         显示版本信息并退出每种 MODE 都应属于这类形式&quot;[ugoa]*([-+=]([rwxXst]*|[ugo]))+&quot;。\n3.chownchown - 修改文件所有者和所属组\nLinux/Unix 属于多用户多任务操作系统，所有的文件皆有拥有者。利用 chown 命令可以将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户ID，组可以是组名或者组ID，文件是以空格分开的要改变权限的文件列表，支持通配符。 一般来说，这个指令仅限系统管理者(root)所使用，普通用户没有权限改变文件所属者及所属组。\n12用法：chown [选项]... [所有者][:[组]] 文件...　或：chown [选项]... --reference=参考文件 文件...\n12345678910111213141516171819当使用 --referebce 参数时，将文件的所有者和所属组更改为与指定参考文件相同。  -c, --changes                 类似 verbose，但只在有更改时才显示结果      --dereference             受影响的是符号链接所指示的对象，而非符号链接本身  -h, --no-dereference          会影响符号链接本身，而非符号链接所指示的目的地                                (当系统支持更改符号链接的所有者时，此选项才有用)      --from=当前所有者:当前所属组                                只当每个文件的所有者和组符合选项所指定时才更改所                                有者和组。其中一个可以省略，这时已省略的属性就不                                需要符合原有的属性。      --no-preserve-root        不特殊对待&quot;/&quot;(默认值)      --preserve-root           不允许在&quot;/&quot;上递归操作  -f, --silent, --quiet 去除大部份的错误信息      --reference=参考文件      使用参考文件的所属组，而非指定值  -R, --recursive               递归处理所有的文件及子目录  -v, --verbose                 为处理的所有文件显示诊断信息      --help            显示此帮助信息并退出      --version         显示版本信息并退出\n4.chgrpchgrp是英语单词“change group”的缩写，命令的作用和其中文释义一样，为用于变更文件或目录的所属群组。\nchgrp命令用来改变文件或目录所属的用户组。该命令用来改变指定文件所属的用户组。其中，组名可以是用户组的id，也可以是用户组的组名。文件名可以 是由空格分开的要改变属组的文件列表，也可以是由通配符描述的文件集合。如果用户不是该文件的文件主或超级用户(root)，则不能改变该文件的组。\n在UNIX系统家族里，文件或目录权限的掌控以拥有者及所属群组来管理。您可以使用chgrp指令去变更文件与目录的所属群组，设置方式采用群组名称或群组识别码皆可。\n12用法：chgrp [选项]... 用户组 文件...　或：chgrp [选项]... --reference=参考文件 文件...\n123456-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quiet或——silent：不显示错误信息；-h或--no-dereference：只对符号连接的文件作修改，而不是该其他任何相关文件；-R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理；-v或——verbose：显示指令执行过程；--reference=&lt;参考文件或目录&gt;：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同；\n5.chattrchattr命令用来改变文件属性。这项指令可改变存放在ext2文件系统上的文件或目录属性，这些属性共有以下几种模式：\n\n\n\n\n参数\n作用\n\n\n\n\ni\n无法对文件进行修改；若对目录设置了该参数，则仅能修改其中的子文件内容而不能新建或删除文件\n\n\na\n仅允许补充（追加）内容，无法覆盖/删除内容（Append Only）\n\n\nS\n文件内容在变更后立即同步到硬盘（sync）\n\n\ns\n彻底从硬盘中删除，不可恢复（用0填充原文件所在硬盘区域）\n\n\nA\n不再修改这个文件或目录的最后访问时间（atime）\n\n\nb\n不再修改文件或目录的存取时间\n\n\nD\n检查压缩文件中的错误\n\n\nd\n使用dump命令备份时忽略本文件/目录\n\n\nc\n默认将文件或目录进行压缩\n\n\nu\n当删除该文件后依然保留其在硬盘中的数据，方便日后恢复\n\n\nt\n让文件系统支持尾部合并（tail-merging）\n\n\nx\n可以直接访问压缩文件中的内容\n\n\n\n\n如果想要把某个隐藏功能添加到文件上，则需要在命令后面追加“+参数”，如果想要把某个隐藏功能移出文件，则需要追加“-参数”。\n1用法：chattr [参数] 文件\n123456-R：递归处理，将指令目录下的所有文件及子目录一并处理；-v&lt;版本编号&gt;：设置文件或目录版本；-V：显示指令执行过程；+&lt;属性&gt;：开启文件或目录的该项属性；-&lt;属性&gt;：关闭文件或目录的该项属性；=&lt;属性&gt;：指定文件或目录的该项属性。\n6.lsattrlsattr命令的英文全称即“list attribute”，用于查看特定设备或特定文件在Linux第二扩展文件系统上的特有属性信息 。该命令常与chattr一起使用，chattr命令用于改变文件或目录的隐藏属性，而lsattr命令则用于查看其属性 。\n1格式：lsattr [参数] [文件]\n12345-a \t列出目录中的所有文件，包括隐藏文件-d \t只显示目录名称-R\t递归地处理指定目录下的所有文件及子目录-v \t显示文件或目录版本-V\t显示版本信息\n7.setfaclsetfacl的英文全称是“ set file access control list ”,即“设置文件访问控制列表”。改命令可以更精确的控制权限的分配，比如让某一个用户对某一个文件具有某种权限。\nACL指文件的所有者、所属组、其他人的读/写/执行之外的特殊的权限， 对于需要特殊权限的使用状况有一定帮助。 如某一个文件，不让单一的某个用户访问。\n1格式：setfacl [参数] [文件]\n123456789101112131415161718-m, --modify=acl 更改文件的访问控制列表-M, --modify-file=file 从文件读取访问控制列表条目更改-x, --remove=acl 根据文件中访问控制列表移除条目-X, --remove-file=file 从文件读取访问控制列表条目并删除-b, --remove-all 删除所有扩展访问控制列表条目-k, --remove-default 移除默认访问控制列表    --set=acl 设定替换当前的文件访问控制列表    --set-file=file 从文件中读取访问控制列表条目设定    --mask 重新计算有效权限掩码-n, --no-mask 不重新计算有效权限掩码-d, --default 应用到默认访问控制列表的操作-R, --recursive 递归操作子目录-L, --logical 依照系统逻辑，跟随符号链接-P, --physical 依照自然逻辑，不跟随符号链接    --restore=file 恢复访问控制列表，和“getfacl -R”作用相反    --test 测试模式，并不真正修改访问控制列表属性-v, --version           显示版本并退出-h, --help              显示本帮助信息\n选项-m和-x后边跟以acl规则。多条acl规则以逗号(,)隔开。选项-M和-X用来从文件或标准输入读取acl规则。选项—set和—set-file用来设置文件或目录的acl规则，先前的设定将被覆盖。选项-m(—modify)和-M(—modify-file)选项修改文件或目录的acl规则。选项-x(—remove)和-X(—remove-file)选项删除acl规则。当使用-M，-X选项从文件中读取规则时，setfacl接受getfacl命令输出的格式。每行至少一条规则，以#开始的行将被视为注释。\n当在不支持ACLs的文件系统上使用setfacl命令时，setfacl将修改文件权限位。如果acl规则并不完全匹配文件权限位，setfacl将会修改文件权限位使其尽可能的反应acl规则，并会向standard error发送错误消息，以大于0的状态返回。\n8.getfacl获取文件访问控制列表\n1getfacl  [选项]  文件 ...\n1234567891011121314-a,  --access           仅显示文件访问控制列表-d, --default           仅显示默认的访问控制列表-c, --omit-header       不显示注释表头-e, --all-effective     显示所有的有效权限-E, --no-effective      显示无效权限-s, --skip-base         跳过只有基条目(base entries)的文件-R, --recursive         递归显示子目录-L, --logical           逻辑遍历(跟随符号链接)-P, --physical          物理遍历(不跟随符号链接)-t, --tabular           使用制表符分隔的输出格式-n, --numeric           显示数字的用户/组标识-p, --absolute-names    不去除路径前的 &#x27;/&#x27; 符号-v, --version           显示版本并退出-h, --help              显示本帮助信息\n9.umaskumask命令用来设置限制新建文件权限的掩码。当新文件被创建时，其最初的权限由文件创建掩码决定。用户每次注册进入系统时，umask命令都被执行， 并自动设置掩码mode来限制新文件的权限。用户可以通过再次执行umask命令来改变默认值，新的权限将会把旧的覆盖掉。\n1语法格式：umask [参数] [权限掩码]\n12-S:以字符的形式显示当前创建文件的默认权限。-p：带umask开头以数字的形势显示当前掩码\n\n十四、进程管理和服务管理命令1.systemctlsystemctl命令是系统服务管理器指令，它实际上将 service和chkconfig这两个命令组合到一起。\nCentos7之后从init完全换成了systemd的启动方式，systemd 启动服务的机制主要是通过 systemctl 的这个系统服务管理指令来处理。systemctl在用法上也囊括 service / chkconfig / setup / init 的大部分功能。\n\n\n\n\n任务\n旧指令\n新指令\n\n\n\n\n使某服务自动启动\nchkconfig —level 3 httpd on\nsystemctl enable httpd.service\n\n\n使某服务不自动启动\nchkconfig —level 3 httpd off\nsystemctl disable httpd.service\n\n\n检查服务状态\nservice httpd status\nsystemctl status httpd.service （服务详细信息） systemctl is-enabled httpd.service （仅显示是否 Active)\n\n\n显示所有已启动的服务\nchkconfig —list\nsystemctl list-units —type=service\n\n\n启动某服务\nservice httpd start\nsystemctl start httpd.service\n\n\n停止某服务\nservice httpd stop\nsystemctl stop httpd.service\n\n\n重启某服务\nservice httpd restart\nsystemctl restart httpd.service\n\n\n某服务重新加载配置文件\nservice httpd reload\nsystemctl reload httpd.service\n\n\n\n\n1格式:systemctl [选项...] &#123;命令&#125; ...\n1234567-start\t启动服务-stop\t停止服务-restart\t重启服务-enable\t使某服务开机自启-disable\t关闭某服务开机自启-status\t查看服务状态-list-units –type=service\t列举所有已启动服务\n2.initinit命令是Linux下的进程初始化工具，init进程是所有Linux进程的父进程，它的进程号为1。init命令是Linux操作系统中不可缺少的程序之一，init进程是Linux内核引导运行的，是系统中的第一个进程。\n注意：Centos7.5 中第一个进程是systemd进程\n运行级别\n到底什么是运行级呢？简单的说，运行级就是操作系统当前正在运行的功能级别。这个级别从0到6 ，具有不同的功能。你也可以在/etc/inittab中查看它的英文介绍。\n1234567#0  停机（千万不能把initdefault 设置为0）#1  单用户模式#2  多用户，没有 NFS(和级别3相似，会停止部分服务)#3  完全多用户模式#4  没有用到#5  x11(Xwindow)#6  重新启动（千万不要把initdefault 设置为6）\n1格式：init [选项] [参数]\n123456789101112Send control commands to the init daemon.     --help      Show this help     --no-wall   Don&#x27;t send wall message before halt/power-off/rebootCommands:  0              Power-off the machine  6              Reboot the machine  2, 3, 4, 5     Start runlevelX.target unit  1, s, S        Enter rescue mode  q, Q           Reload init daemon configuration  u, U           Reexecute init daemon\n3.telinit设置当前系统的运行等级\n说明:\n\nThis is a legacy command available for compatibility only. It should not be used anymore, as the concept of runlevels is obsolete.\n\n1用法：telinit [选项] [参数]\n123456789101112Send control commands to the init daemon.     --help      Show this help     --no-wall   Don&#x27;t send wall message before halt/power-off/rebootCommands:  0              Power-off the machine  6              Reboot the machine  2, 3, 4, 5     Start runlevelX.target unit  1, s, S        Enter rescue mode  q, Q           Reload init daemon configuration  u, U           Reexecute init daemon\n4.psps命令用于报告当前系统的进程状态。ps命令是“process status”的缩写。可以搭配kill指令随时中断、删除不必要的程序。ps命令是最基本同时也是非常强大的进程查看命令，使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等，总之大部分信息都是可以通过执行该命令得到的。\n\n（1）ps aux \nUSER：该进程是由哪个用户产生的 \nPID：进程的ID号 \n%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源； \n%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源； \nVSZ：该进程占用虚拟内存的大小，单位KB； \nRSS：该进程占用实际物理内存的大小，单位KB； \nTTY：该进程是在哪个终端中运行的。对于CentOS来说，tty1是图形化终端， tty2-tty6 是本地的字符界面终端。pts/0-255代表虚拟终端。 \nSTAT：进程状态。常见的状态有：R：运行状态、S：睡眠状态、T：暂停状态、 Z：僵尸状态、s：包含子进程、l：多线程、+：前台显示 \n123456789101112131415R 运行    Runnable (on run queue) 正在运行或在运行队列中等待。S 睡眠    Sleeping                休眠中, 受阻, 在等待某个条件的形成或接受到信号。I 空闲    IdleZ 僵死    Zombie（a defunct process) 进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放。D 不可中断    Uninterruptible sleep (ususally IO)    收到信号不唤醒和不可运行, 进程必须等待直到有中断发生。T 终止    Terminate 进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行。P 等待交换页W 无驻留页    has no resident pages 没有足够的记忆体分页可分配。X 死掉的进程&lt; 高优先级进程 高优先序的进程N 低优先    级进程 低优先序的进程L 内存锁页    Lock 有记忆体分页分配并缩在记忆体内s 进程的领导者（在它之下有子进程）；l 多进程的（使用 CLONE_THREAD, 类似 NPTL pthreads）+ 位于后台的进程组 \nSTART：该进程的启动时间 \nTIME：该进程占用CPU的运算时间，注意不是系统时间 \nCOMMAND：产生此进程的命令名 \n（2）ps-ef\nUID：用户ID \nPID：进程ID \nPPID：父进程ID \nC：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算， 执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高 \nSTIME：进程启动的时间 \nTTY：完整的终端名称 \nTIME：CPU时间 \nCMD：启动进程所用的命令和参数\n1格式：ps [参数]\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162-a  显示所有终端机下执行的进程，除了阶段作业领导者之外。　　 a  显示现行终端机下的所有进程，包括其他用户的进程。　　-A  显示所有进程。　　-c  显示CLS和PRI栏位。　　 c  列出进程时，显示每个进程真正的指令名称，而不包含路径，参数或常驻服务的标示。　　-C&lt;指令名称&gt; 　指定执行指令的名称，并列出该指令的进程的状况。　　-d 　显示所有进程，但不包括阶段作业领导者的进程。　　-e 　此参数的效果和指定&quot;A&quot;参数相同。　　 e 　列出进程时，显示每个进程所使用的环境变量。　　-f 　显示UID,PPIP,C与STIME栏位。　　 f 　用ASCII字符显示树状结构，表达进程间的相互关系。　　-g&lt;群组名称&gt; 　此参数的效果和指定&quot;-G&quot;参数相同，当亦能使用阶段作业领导者的名称来指定。　　 g 　显示现行终端机下的所有进程，包括群组领导者的进程。　　-G&lt;群组识别码&gt; 　列出属于该群组的进程的状况，也可使用群组名称来指定。　　 h 　不显示标题列。　　-H 　显示树状结构，表示进程间的相互关系。　　-j或j 　采用工作控制的格式显示进程状况。　　-l或l 　采用详细的格式来显示进程状况。　　 L 　列出栏位的相关信息。　　-m或m 　显示所有的执行绪。　　 n 　以数字来表示USER和WCHAN栏位。　　-N 　显示所有的进程，除了执行ps指令终端机下的进程之外。　　-p&lt;进程识别码&gt; 　指定进程识别码，并列出该进程的状况。　 　p&lt;进程识别码&gt; 　此参数的效果和指定&quot;-p&quot;参数相同，只在列表格式方面稍有差异。　　 r 　只列出现行终端机正在执行中的进程。　　-s&lt;阶段作业&gt; 　指定阶段作业的进程识别码，并列出隶属该阶段作业的进程的状况。　 　s 　采用进程信号的格式显示进程状况。　　 S 　列出进程时，包括已中断的子进程资料。　　-t&lt;终端机编号&gt; 　指定终端机编号，并列出属于该终端机的进程的状况。　　 t&lt;终端机编号&gt; 　此参数的效果和指定&quot;-t&quot;参数相同，只在列表格式方面稍有差异。　　-T 　显示现行终端机下的所有进程。　　-u&lt;用户识别码&gt; 　此参数的效果和指定&quot;-U&quot;参数相同。　　 u 　以用户为主的格式来显示进程状况。　　-U&lt;用户识别码&gt; 　列出属于该用户的进程的状况，也可使用用户名称来指定。　　 U&lt;用户名称&gt; 　列出属于该用户的进程的状况。　　 v 　采用虚拟内存的格式显示进程状况。　　-V或V 　显示版本信息。　　-w或w 　采用宽阔的格式来显示进程状况。　　 　x 　显示所有进程，不以终端机来区分。　　 X 　采用旧式的Linux i386登陆格式显示进程状况。　　 -y 配合参数&quot;-l&quot;使用时，不显示F(flag)栏位，并以RSS栏位取代ADDR栏位　　-&lt;进程识别码&gt; 　此参数的效果和指定&quot;p&quot;参数相同。　　--cols&lt;每列字符数&gt; 　设置每列的最大字符数。　　--columns&lt;每列字符数&gt; 　此参数的效果和指定&quot;--cols&quot;参数相同。　　--cumulative 　此参数的效果和指定&quot;S&quot;参数相同。　　--deselect 　此参数的效果和指定&quot;-N&quot;参数相同。　　--forest 　此参数的效果和指定&quot;f&quot;参数相同。　　--headers 　重复显示标题列。　　--help 　在线帮助。　　--info 　显示排错信息。　　--lines&lt;显示列数&gt; 设置显示画面的列数。　　--no-headers  此参数的效果和指定&quot;h&quot;参数相同，只在列表格式方面稍有差异。　　--group&lt;群组名称&gt; 　此参数的效果和指定&quot;-G&quot;参数相同。　　--Group&lt;群组识别码&gt; 　此参数的效果和指定&quot;-G&quot;参数相同。　　--pid&lt;进程识别码&gt; 　此参数的效果和指定&quot;-p&quot;参数相同。　　--rows&lt;显示列数&gt; 　此参数的效果和指定&quot;--lines&quot;参数相同。　　--sid&lt;阶段作业&gt; 　此参数的效果和指定&quot;-s&quot;参数相同。　　--tty&lt;终端机编号&gt; 　此参数的效果和指定&quot;-t&quot;参数相同。　　--user&lt;用户名称&gt; 　此参数的效果和指定&quot;-U&quot;参数相同。　　--User&lt;用户识别码&gt; 　此参数的效果和指定&quot;-U&quot;参数相同。　　--version 　此参数的效果和指定&quot;-V&quot;参数相同。　　--widty&lt;每列字符数&gt; 　此参数的效果和指定&quot;-cols&quot;参数相同。 \n5.pstreelinux系统中pstree命令的英文全称是“process tree”，即将所有行程以树状图显示，树状图将会以 pid (如果有指定) 或是以 init 这个基本行程为根 (root)，如果有指定使用者 id，则树状图会只显示该使用者所拥有的行程。\n1格式： pstree [参数]\n1234567891011-a 　显示每个程序的完整指令，包含路径，参数或是常驻服务的标示。-c 　不使用精简标示法。-G 　使用VT100终端机的列绘图字符。-h 　列出树状图时，特别标明执行的程序。-H&lt;程序识别码&gt; 　此参数的效果和指定&quot;-h&quot;参数类似，但特别标明指定的程序。-l 　采用长列格式显示树状图。-n 　用程序识别码排序。预设是以程序名称来排序。-p 　显示程序识别码。-u 　显示用户名称。-U 　使用UTF-8列绘图字符。-V 　显示版本信息。\n6.toptop命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，常用于服务端性能分析。\n在top命令中按f按可以查看显示的列信息，按对应字母来开启/关闭列，大写字母表示开启，小写字母表示关闭。带*号的是默认列。\n1格式：top [参数]\n12345678910-b：以批处理模式操作；-c：显示完整的进程信息；-d：屏幕刷新间隔时间；-I：忽略失效过程；-s：保密模式；-S：累积模式；-i&lt;时间&gt;：设置间隔时间；-u&lt;用户名&gt;：指定用户名；-p&lt;进程号&gt;：指定进程；-n&lt;次数&gt;：循环显示的次数。\n7.killkill命令用来删除执行中的程序或工作。kill可将指定的信息送至程序。预设的信息为SIGTERM(15),可将指定程序终止。若仍无法终止该程序，可使用SIGKILL(9)信息尝试强制删除程序。程序或工作的编号可利用ps指令或job指令查看。\n1语法格式：kill [参数] [进程号]\n1234-9  强制停止-l  &lt;信号变化哦啊&gt;，若不加信号的编号参数，则使用“-l”参数会列出全部的信号名称-p  指定kill 命令只打印相关进程的进程号，而不发送任何信号-s  指定发送信号\n8.killallkillall命令使用进程的名称来杀死进程，使用此指令可以杀死一组同名进程。我们可以使用kill命令杀死指定进程PID的进程，如果要找到我们需要杀死的进程，我们还需要在之前使用ps等命令再配合grep来查找进程，而killall把这两个过程合二为一，是一个很好用的命令。\n123killall [-egiqvw] [-signal] name ...killall -lkillall -V\n123456789-e：对长名称进行精确匹配；-l：忽略大小写的不同；-p：杀死进程所属的进程组；-i：交互式杀死进程，杀死进程前需要进行确认；-l：打印所有已知信号列表；-q：如果没有进程被杀死。则不输出任何信息；-r：使用正规表达式匹配要杀死的进程名称；-s：用指定的进程号代替默认信号“SIGTERM”；-u：杀死指定用户的进程。\n9.nicenice命令用于以指定的进程调度优先级启动其他的程序。\n系统的后台工作中，某些比较不重要的进程在运行，例如备份，由于备份工作相当耗系统资源，这个时候就可以调大备份命令的nice值，可以使系统资源更合理使用。\n1234用法： nice [-n] &lt;优先级&gt; [-p|--pid] &lt;pid&gt;... nice [-n] &lt;优先级&gt;  -g|--pgrp &lt;pgid&gt;... nice [-n] &lt;优先级&gt;  -u|--user &lt;用户&gt;...\n1234567选项： -g, --pgrp &lt;id&gt;        将参数解释为进程组 ID -n, --priority &lt;数字&gt;  指定 nice 增加值 范围在-20~19 -p, --pid &lt;id&gt;         将参数解释为进程 ID (默认) -u, --user &lt;name|id&gt;   将参数解释为用户名或用户 ID -h, --help             显示帮助文本并退出 -V, --version          显示版本信息并退出\n10.renicerenice命令可以修改正在运行的进程的调度优先级。\n该命令预设是以程序识别码指定程序调整其优先权，您亦可以指定程序群组或用户名称调整优先权等级，并修改所有隶属于该程序群组或用户的程序的优先权。只有系统管理者可以改变其他用户程序的优先权，也仅有系统管理者可以设置负数等级。\n1234用法： renice [-n] &lt;优先级&gt; [-p|--pid] &lt;pid&gt;... renice [-n] &lt;优先级&gt;  -g|--pgrp &lt;pgid&gt;... renice [-n] &lt;优先级&gt;  -u|--user &lt;用户&gt;...\n1234567选项： -g, --pgrp &lt;id&gt;        将参数解释为进程组 ID -n, --priority &lt;数字&gt;  指定 nice 增加值 -p, --pid &lt;id&gt;         将参数解释为进程 ID (默认) -u, --user &lt;name|id&gt;   将参数解释为用户名或用户 ID -h, --help             显示帮助文本并退出 -V, --version          显示版本信息并退出\n11.jobsjobs命令主要用于显示系统中的任务列表及其运行状态。\n该命令可以显示任务号及其对应的进程号，其中，任务号是以普通用户的角度进行的，而进程号则是从系统管理员的角度来看的。一个任务可以对应一个或者多个进程号。\n12jobs [-lnprs] [任务声明 ...]jobs -x 命令 [参数]\n12345678910111213141516列出活动的任务。JPBSPEC 限制仅输出指定的任务。不带选项时，所有活动任务的状态都会显示。选项：  -l        在正常信息基础上列出进程号  -n        列出上次通告之后改变了状态的进程  -p        仅列出进程号  -r        限制仅输出运行中的任务  -s        限制仅输出停止的任务如果使用了 -x 选项，ARG 参数中的所有任务声明会被替换为该任务的进程组头领的进程号，然后执行 COMMAND 命令。退出状态：返回成功，除非使用了无效的选项或者有错误发生。如果使用 -x选项，则返回 COMMAND 命令的退出状态。\n12.fgfg命令用于将后台作业（在后台运行的或者在后台挂起的作业）放到前台终端运行。与bg命令一样，若后台任务中只有一个，则使用该命令时，可以省略任务号。\n1语法格式：fg [参数]\n123456789fg: fg [任务声明]    将任务移至前台。        将以 JOB_SPEC 标识的任务放至前台，使其成为    当前任务。如果 JOB_SPEC 不存在，shell 观念中的当前任务     将被使用。        退出状态：    放至前台的命令状态，或者当错误发生时为失败。\n13.bgbg命令用于将作业放到后台运行，使前台可以执行其他任务。该命令的运行效果与在指令后面添加符号&amp;的效果是相同的，都是将其放到系统后台执行。\n1语法格式：bg [参数]\n123456789bg: bg [任务声明 ...]    移动任务至后台。        将 JOB_SPEC 标识的任务放至后台，就像它们    是带 `&amp;&#x27; 启动的一样。如果 JOB_SPEC 不存在，shell 观念中的    当前任务将会被使用。        退出状态：    返回成功除非任务管理没有启用或者错误发生。\n14.nohupnohup命令可以将程序以忽略挂起信号的方式运行起来，被运行的程序的输出信息将不会显示到终端。\n无论是否将 nohup 命令的输出重定向到终端，输出都将附加到当前目录的 nohup.out 文件中。如果当前目录的 nohup.out 文件不可写，输出重定向到$HOME/nohup.out文件中。如果没有文件能创建或打开以用于追加，那么 command参数指定的命令不可调用。如果标准错误是一个终端，那么把指定的命令写给标准错误的所有输出作为标准输出重定向到相同的文件描述符。\n12用法：nohup 命令 [参数]...　或：nohup 选项\n123456执行 COMMAND 命令, 忽略 hangup (挂起) 信号.--help 显示此帮助, 然后退出--version       显示版本信息, 然后退出\n15.crontab1crontab [选项]\n\n\n\n\n\n十五、网络管理1.pingping命令用来测试主机之间网络的连通性。执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。\nping, ping6 - 向网络主机发送ICMP回显请求(ECHO_REQUEST)分组。\n注意：Linux系统下的ping命令与Windows系统下的ping命令稍有不同。Windows下运行ping命令一般会发出4个请求就结束运行该命令；而Linux下不会自动终止，此时需要我们按CTR+C终止或者使用-c参数为ping命令指定发送的请求数目。\n1格式：ping [选项] [参数]\n123456789101112-d\t使用Socket的SO_DEBUG功能-c\t指定发送报文的次数-i\t指定收发信息的间隔时间-I\t使用指定的网络接口送出数据包-l\t设置在送出要求信息之前，先行发出的数据包-n\t只输出数值-p\t设置填满数据包的范本样式-q\t不显示指令执行过程-R\t记录路由过程-s\t设置数据包的大小-t\t设置存活数值TTL的大小-v\t详细显示指令的执行过程\n2.ifconfigifconfig命令的英文全称是“network interfaces configuring”，即用于配置和显示Linux内核中网络接口的网络参数。用ifconfig命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在。要想将上述的配置信息永远的存的电脑里，那就要修改网卡的配置文件了。\n12ifconfig [接口]ifconfig 接口 [aftype] options | address ...\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768interface接口的名称。这通常是一个驱动程序名，后面跟着一个单元号，例如用于第一个以太网接口的eth0。up此标志将导致激活接口。如果将地址分配给接口，则会隐式指定该地址。down此标志导致关闭此接口的驱动程序。[-]arp启用或禁用在此接口上使用ARP协议。[-]promisc启用或禁用接口的混杂模式。如果选中，网络上的所有数据包都将由接口接收。[-]allmulti启用或禁用所有多播模式。如果选中，则接口将接收网络上的所有多播数据包。metric N此参数设置接口度量。它在GNU/Linux下不可用mtu N此参数设置接口的最大传输单元(MTU)。dstaddr addr为点对点链路(如PPP)设置远程IP地址.这个关键字现在已经过时了；使用pointopoint关键字代替。netmask addr设置此接口的IP网络掩码。此值默认为通常的A、B或C类网络掩码(从接口IP地址派生)，但可以设置为任何值。add addr/prefixlen向接口添加IPv 6地址del addr/prefixlen从接口中删除IPv 6地址tunnel ::aa.bb.cc.dd创建一个新的SIT(IPv6-in-IPv4)设备，通过隧道到达给定的目的地。irq addr设置此设备使用的中断行。并非所有设备都可以动态更改其IRQ设置。io_addr addr为该设备设置I/O空间中的起始地址mem_start addr设置此设备使用的共享内存的起始地址。只有少数几个设备需要这个media type设置设备要使用的物理端口或介质类型。并非所有设备都可以更改此设置，以及那些可以更改其支持的值的设备。典型的类型值是10 base 2(细以太网)、10 base T(双绞线10 Mbps以太网)、AUI(外收发信机)等。驱动的特殊介质类型可以用来告诉驱动对媒体进行自动感知。同样，并不是所有的驱动都能做到这一点。[-]broadcast [addr]如果地址参数给定，则为该接口设置协议广播地址。否则，设置(或清除)接口的IFF_BROADCAST标志。[-]pointopoint [addr]这个关键字启用了接口的点对点模式，这意味着它是两台机器之间的直接链接，没有其他人监听它。如果地址参数也给出了，就像过时的dstaddr关键字一样，设置链接另一端的协议地址。否则，设置或清除接口的IFF_POINTOPOINT标志。hw class address如果设备驱动程序支持此操作，则设置此接口的硬件地址。关键字后面必须跟着硬件类的名称和相当于硬件地址的可打印的ASCII。目前支持的硬件类包括ether (以太网)、ax25(AMPRAX.25)、ARCnet和netrom(AMPR NET/ROM)。multicast在接口上设置多播标志。这通常不应该需要，因为驱动程序本身设置正确的标志。address要分配给此接口的IP地址。txqueuelen length设置设备的传输队列的长度。对于具有高延迟(调制解调器链路，ISDN)的较慢设备，将其设置为小值是有用的，以防止快速批量传输过多地干扰诸如telnet之类的交互通信。\n3.ifupifup命令用于激活指定的网络接口。ifup命令会去读取/etc/sysconfig/network-scripts/目录下的相关网络接口的配置文件，并根据配置文件的内容来激活该网络接口。\n注意：网络接口名称必须是/etc/sysconfig/network-scripts/目录配置文件中设置的才可以。如果使用ifconfig命令改变了网络接口后，ifup命令就不会识别了。因为ifup命令会对比当前网络的参数与/etc/sysconfig/network-scripts/中配置文件的内容是否相符。\n1用法：ifup &lt;设备名&gt;\n4.ifdownifdown命令用于禁用指定的网络接口。该命令会去读取/etc/sysconfig/network-scripts/目录下的相关网络接口的配置文件，并根据配置文件的内容来关闭该网络接口。\n注意：网络接口名称必须是/etc/sysconfig/network-scripts/目录配置文件中设置的才可以。如果使用ifconfig命令改变了网络接口后，ifdown命令就不会识别了。因为ifdown命令会对比当前网络的参数与/etc/sysconfig/network-scripts/中配置文件的内容是否相符。\n1格式：ifdown [网络接口]\n5.ifcfgifcfg命令是一个Bash脚本程序，用来设置Linux中的网络接口参数。\n1语法格式： ifcfg [参数]\n1234网络接口：指定要操作的网络接口add/del：添加或删除网络接口上的地址ip地址：指定IP地址和子网掩码stop：停用指定的网络接口的IP地址\n6.dhclientdhclient命令使用动态主机配置协议动态的配置网络接口的网络参数，也支持BOOTP协议。\n1dhclient [参数] [网络接口]\n12345678910-4 : 使用DHCPv4-6 : 使用DHCPv6-p\t指定dhcp客户端监听的端口号（默认端口号86）-d\t总是以前台方式运行程序-q\t安静模式，不打印任何错误的提示信息-r\t释放ip地址-n\t不配置任何接口-x\t停止正在运行的DHCP客户端，而不释放当前租约，杀死现有的dhclient-s\t在获取ip地址之前指定DHCP服务器-w\t即使没有找到广播接口，也继续运行\n7.ncnc是netcat的简写\nnc的作用\n（1）实现任意TCP/UDP端口的侦听，nc可以作为server以TCP或UDP方式侦听指定端口\n（2）端口的扫描，nc可以作为client发起TCP或UDP连接\n（3）机器之间传输文件\n（4）机器之间网络测速\n1格式：nc [选项] [参数]\n1234567891011121314-g&lt;网关&gt;：设置路由器跃程通信网关，最多设置8个；-G&lt;指向器数目&gt;：设置来源路由指向器，其数值为4的倍数；-h：在线帮助；-i&lt;延迟秒数&gt;：设置时间间隔，以便传送信息及扫描通信端口；-l：使用监听模式，监控传入的资料；-n：直接使用ip地址，而不通过域名服务器；-o&lt;输出文件&gt;：指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存；-p&lt;通信端口&gt;：设置本地主机使用的通信端口；-r：指定源端口和目的端口都进行随机的选择；-s&lt;来源位址&gt;：设置本地主机送出数据包的IP地址；-u：使用UDP传输协议；-v：显示指令执行过程；-w&lt;超时秒数&gt;：设置等待连线的时间；-z：使用0输入/输出模式，只在扫描通信端口时使用。\n8.nslookupnslookup命令是常用域名查询工具，就是查DNS信息用的命令。\nnslookup命令的英文全称为 “query Internet name server interactively ”。nslookup有两种工作模式，即“交互模式”和“非交互模式”。在“交互模式”下，用户可以向域名服务器查询各类主机、域名的信息，或者输出域名中的主机列表。而在“非交互模式”下，用户可以针对一个主机或域名仅仅获取特定的名称或所需信息。\n进入交互模式，直接输入nslookup命令，不加任何参数，则直接进入交互模式，此时nslookup会连接到默认的域名服务器（即/etc/resolv.conf的第一个dns地址）。或者输入nslookup -nameserver/ip。进入非交互模式，就直接输入nslookup 域名就可以了。\n1格式：nslookup [参数] [域名]\n123456-sil\t不显示任何警告信息exit\t退出命令server\t指定解析域名的服务器地址set type=soa\t设置查询域名授权起始信息set type=a\t设置查询域名A记录set type=mx\t设置查询域名邮件交换记录\n9.hosthost命令是常用的分析域名查询工具，可以用来测试域名系统工作是否正常。\nhost命令是一个用于执行DNS查找的简单实用程序。它通常用于将名称转换为IP地址，反之亦然。 如果没有给出参数或选项，host将打印其命令行参数和选项的简短摘要。\n1格式：host [选项] [参数]\n12345678910-a\t显示详细的DNS信息-c\t指定查询类型，默认值为“IN”-C\t查询指定主机的完整的SOA记录-r\t不使用递归的查询方式查询域名-t\t指定查询的域名信息类型-v\t显示指令执行的详细信息-w\t如果域名服务器没有给出应答信息，则总是等待，直到域名服务器给出应答-W\t指定域名查询的最长时间，如果在指定时间内域名服务器没有给出应答信息则退出-4\t使用IPv4查询传输 （默认）-6\t使用IPv6查询传输\n10.arparp命令用于操作主机的arp缓冲区，它可以显示arp缓冲区中的所有条目、删除指定的条目或者添加静态的ip地址与MAC地址对应关系。\narp命令的英文全拼“Address Resolution Protocol” 。\n1格式：arp [选项] [参数]\n123456789-a\t显示arp缓存的所有条目，主机位可选参数-H\t指定arp指令使用的地址类型-d\t从arp缓存中删除指定主机的arp条目-D\t使用指定接口的硬件地址-e\t以linux的显示风格显示arp缓存中的条目-i\t指定要操作arp缓存的网络接口-n\t以数字方式显示arp缓存中的条目-v\t显示详细的arp缓存条目，包括缓存条目的统计信息-f\t设置主机的IP地址与MAC地址的静态映射\n11.netstatnetstat命令用于显示各种网络相关信息，如网络连接，路由表，接口状态 (Interface Statistics)，masquerade 连接，多播成员 (Multicast Memberships) 等等。\n从整体上看，netstat的输出结果可以分为两个部分：一个是Active Internet connections，称为有源TCP连接，其中”Recv-Q”和”Send-Q”指%0A的是接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积。这种情况只能在非常少的情况见到；另一个是Active UNIX domain sockets，称为有源Unix域套接口(和网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。\n1格式：netstat [选项] [参数]\n\n123456789101112131415161718192021222324-a或--all：显示所有连线中的Socket；-A&lt;网络类型&gt;或--&lt;网络类型&gt;：列出该网络类型连线中的相关地址；-c或--continuous：持续列出网络状态；-C或--cache：显示路由器配置的快取信息；-e或--extend：显示网络其他相关信息；-F或--fib：显示FIB；-g或--groups：显示多重广播功能群组组员名单；-h或--help：在线帮助；-i或--interfaces：显示网络界面信息表单；-l或--listening：显示监控中的服务器的Socket；-M或--masquerade：显示伪装的网络连线；-n或--numeric：直接使用ip地址，而不通过域名服务器；-N或--netlink或--symbolic：显示网络硬件外围设备的符号连接名称；-o或--timers：显示计时器；-p或--programs：显示正在使用Socket的程序识别码和程序名称；-r或--route：显示Routing Table；-s或--statistice：显示网络工作信息统计表；-t或--tcp：显示TCP传输协议的连线状况；-u或--udp：显示UDP传输协议的连线状况；-v或--verbose：显示指令执行过程；-V或--version：显示版本信息；-w或--raw：显示RAW传输协议的连线状况；-x或--unix：此参数的效果和指定&quot;-A unix&quot;参数相同；--ip或--inet：此参数的效果和指定&quot;-A inet&quot;参数相同。\n12.traceroutetraceroute命令用于追踪数据包在网络上的传输时的全部路径，它默认发送的数据包大小是40字节。通过traceroute我们可以知道信息从你的计算机到互联网另一端的主机是走的什么路径。当然每次数据包由某一同样的出发点（source）到达某一同样的目的地(destination)走的路径可能会不一样，但基本上来说大部分时候所走的路由是相同的。\ntraceroute通过发送小的数据包到目的设备直到其返回，来测量其需要多长时间。一条路径上的每个设备traceroute要测3次。输出结果中包括每次测试的时间(ms)和设备的名称（如有的话）及其ip地址。\n1格式：traceroute [选项] [参数]\n123456789101112131415-d\t使用Socket层级的排错功能-f&lt;存活数值&gt;\t设置第一个检测数据包的存活数值TTL的大小-F\t设置勿离断位-g&lt;网关&gt;\t设置来源路由网关，最多可设置8个-i&lt;网络界面&gt;\t使用指定的网络界面送出数据包-I\t使用ICMP回应取代UDP资料信息-m&lt;存活数值&gt;\t设置检测数据包的最大存活数值TTL的大小-n\t直接使用IP地址而非主机名称-p&lt;通信端口&gt;\t设置UDP传输协议的通信端口-r\t忽略普通的Routing Table，直接将数据包送到远端主机上-s&lt;来源地址&gt;\t设置本地主机送出数据包的IP地址-t&lt;服务类型&gt;\t设置检测数据包的TOS数值-v\t详细显示指令的执行过程-w\t设置等待远端主机回报的时间-x\t开启或关闭数据包的正确性检验\n13.tracepathtracepath命令用来追踪并显示报文到达目的主机所经过的路由信息，能够发现路由中的MTU值。tracepath使用套接字API来实现其所有功能，不需要root权限。\n1用法：tracepath [-n] [-b] [-l &lt;len&gt;] [-p port] &lt;destination&gt;\n12345-n\t只显示ip地址-b\t同时显示ip地址和主机名-l\t设置初始化的数据包长度，默认为65535-m\t设置最大TTL值，默认为30-p\t设置要使用的初始目标端口\n14.routeroute命令用来显示并设置linux内核中的网络路由表，route命令设置的路由主要是静态路由。要实现两个不同的子网之间的通信，需要一台连接两个网络的路由器，或者同时位于两个网络的网关来实现。\n在linux系统中设置路由通常是为了解决以下问题：该linux系统在一个局域网中，局域网中有一个网关，能够让主机访问Internet，那么就需要将这台机器的ip地址设置为linux机器的默认路由。\n要注意的是：直接在命令行下执行route命令来添加路由，不会永久保存。当网卡重启或者机器重启之后，该路由就失效了。可以在/etc/rc.local中添加route命令来保证该路由设置永久有效。\n1234route [-nNvee] [-FC] [&lt;AF&gt;]           List kernel routing tablesroute [-v] [-FC] &#123;add|del|flush&#125; ...  Modify routing table for AF.route &#123;-h|--help&#125; [&lt;AF&gt;]              Detailed usage syntax for specified AF.route &#123;-V|--version&#125;                  Display version/author and exit.\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748-v    选用细节操作模式-A family    用指定的地址族(如`inet&#x27;，`inet6&#x27;)。-n    以数字形式代替解释主机名形式来显示地址。此项对试图检测对域名服务器进行路由发生故障的原因非常有用。-e    用netstat(8)的格式来显示选路表。-ee将产生包括选路表所有参数在内的大量信息。-net    路由目标为网络。-host    路由目标为主机。-F    显示内核的FIB选路表。其格式可以用-e 和 -ee选项改变。-C    显示内核的路由缓存。del    删除一条路由。add    添加一条路由。target    指定目标网络或主机。可以用点分十进制形式的IP地址或主机/网络名。netmask Nm    为添加的路由指定网络掩码。gw Gw    为发往目标网络/主机的任何分组指定网关。注意：指定的网关首先必须是可达的。也就是说必须为该网关预先    指定一条静态路由。如果你为本地接口之一指定这个网关地址的话，那么此网关地址将用于决定此接口上的分组    将如何进行路由。这是BSD风格所兼容的。metric M    把选路表中的路由值字段(由选路进程使用)设为M。mss M    把基于此路由之上的连接的TCP最大报文段长度设为M字节。这通常只用于优化选路设置。默认值为536。window W    把基于此路由之上的连接的TCP窗口长度设为W字节。这通常只用于AX.25网络和不能处理背对背形式的帧的设    备。irtt I    把基于此路由之上的TCP连接的初始往返时间设为I毫秒(1-12000)。这通常也只用于AX.25网络。如果省略此    选项，则使用RFC1122的缺省值300ms。reject    设置一条阻塞路由以使一条路由查找失败。这用于在使用缺省路由前先屏蔽掉一些网络。但这并不起到防火墙的    作用。mod, dyn, reinstate    设置一条动态的或更改过的路由。这些标志通常只由选路进程来设置。这只用于诊断目的，dev If    强制使路由与指定的设备关联，因为否则内核会自己来试图检测相应的设备(通常检查已存在的路由和加入路由    的设备的规格)。在多数正常的网络上无需使用。    如果dev If是命令行上最后一个指定的选项，那么可以省略关键字dev，因为它是缺省值。否则路由修改对象    (metric - netmask- gw - dev)无关紧要。\n15.ssss是Socket Statistics的缩写。\nss命令用来显示处于活动状态的套接字信息。ss命令可以用来获取socket统计信息，它可以显示和netstat类似的内容。但ss的优势在于它能够显示更多更详细的有关TCP和连接状态的信息，而且比netstat更快速更高效。\n当服务器的socket连接数量变得非常大时，无论是使用netstat命令还是直接cat /proc/net/tcp，执行速度都会很慢。可能你不会有切身的感受，但请相信我，当服务器维持的连接达到上万个的时候，使用netstat等于浪费 生命，而用ss才是节省时间。\n天下武功唯快不破。ss快的秘诀在于，它利用到了TCP协议栈中tcp_diag。tcp_diag是一个用于分析统计的模块，可以获得Linux 内核中第一手的信息，这就确保了ss的快捷高效。当然，如果你的系统中没有tcp_diag，ss也可以正常运行，只是效率会变得稍慢。\n12ss [选项]ss [选项] [过滤]\n12345678910111213141516171819202122232425262728-h, --help\t帮助信息-V, --version\t程序版本信息-n, --numeric\t不解析服务名称-r, --resolve        解析主机名-a, --all\t显示所有套接字（sockets）-l, --listening\t显示监听状态的套接字（sockets）-o, --options        显示计时器信息-e, --extended       显示详细的套接字（sockets）信息-m, --memory         显示套接字（socket）的内存使用情况-p, --processes\t显示使用套接字（socket）的进程-i, --info\t显示 TCP内部信息-s, --summary\t显示套接字（socket）使用概况-4, --ipv4           仅显示IPv4的套接字（sockets）-6, --ipv6           仅显示IPv6的套接字（sockets）-0, --packet\t        显示 PACKET 套接字（socket）-t, --tcp\t仅显示 TCP套接字（sockets）-u, --udp\t仅显示 UCP套接字（sockets）-d, --dccp\t仅显示 DCCP套接字（sockets）-w, --raw\t仅显示 RAW套接字（sockets）-x, --unix\t仅显示 Unix套接字（sockets）-f, --family=FAMILY  显示 FAMILY类型的套接字（sockets），FAMILY可选，支持  unix, inet,   \tinet6, link, netlink-A, --query=QUERY, --socket=QUERY      QUERY := &#123;all|inet|tcp|udp|raw|unix|packet|netlink&#125;[,QUERY]-D, --diag=FILE     将原始TCP套接字（sockets）信息转储到文件-F, --filter=FILE   从文件中都去过滤器信息       FILTER := [ state TCP-STATE ] [ EXPRESSION ]\n16.sshssh命令是openssh套件中的客户端连接工具，可以给予ssh加密协议实现安全的远程登录服务器，实现对服务器的远程管理。\n1格式：ssh [选项] [参数]\n1234567891011121314151617181920-1\t强制使用ssh协议版本1-2\t强制使用ssh协议版本2-4\t强制使用IPv4地址-6\t强制使用IPv6地址-A\t开启认证代理连接转发功能-a\t关闭认证代理连接转发功能-b&lt;IP地址&gt;\t使用本机指定的地址作为对位连接的源IP地址-C\t请求压缩所有数据-F&lt;配置文件&gt;\t指定ssh指令的配置文件，默认的配置文件为“/etc/ssh/ssh_config”-f\t后台执行ssh指令-g\t允许远程主机连接本机的转发端口-i&lt;身份文件&gt;\t指定身份文件（即私钥文件）-l&lt;登录名&gt;\t指定连接远程服务器的登录用户名-N\t不执行远程指令-o&lt;选项&gt;\t指定配置选项-p&lt;端口&gt;\t指定远程服务器上的端口-q\t静默模式，所有的警告和诊断信息被禁止输出-X\t开启X11转发功能-x\t关闭X11转发功能-y\t开启信任X11转发功能\n17.sshdsshd命令是opensshd软件套件中的服务器守护进程。\nopenssh套件在不安全的网络中为两台为信任的主机之间建立加密的数据通信，是rlogin、rsh等明文传输数据的通信工具的替代品。sshd指令是openssh套件中的核心程序，其他的指令（如，sftp-server、slogin、scp）等都是基于sshd命令的。\n1格式: sshd [参数]\n12345678910111213-4\t强制使用IPv4地址-6\t强制使用IPv6地址-D\t以非后台守护进程的方式运行服务器-d\t调试模式-e\t将错误发送到标准错误设备，而不是将其发送到系统日志-f&lt;配置文件&gt;\t指定服务器的配置文件-g&lt;登录过期时间&gt;\t指定客户端登录的过期时间（默认时间为120秒），如果在此期限内，用户没有正确认证，则服务器断开此客户端的连接-h&lt;主机key文件&gt;\t指定读取主机key文件-i\tsshd以inetd方式运行-o&lt;选项&gt;\t指定sshd的配置选项-p&lt;端口&gt;\t指定使用的端口号-q\t静默模式，没有任何信息写入系统日志-t\t测试模式\n18.ssh-keygenssh-keygen命令用于为“ssh”生成、管理和转换认证密钥，它支持RSA和DSA两种认证密钥。\n1234567891011121314151617语法     ssh-keygen [-q] [-b bits] -t type [-N new_passphrase] [-C comment] [-f output_keyfile]     ssh-keygen -p [-P old_passphrase] [-N new_passphrase] [-f keyfile]     ssh-keygen -i [-f input_keyfile]     ssh-keygen -e [-f input_keyfile]     ssh-keygen -y [-f input_keyfile]     ssh-keygen -c [-P passphrase] [-C comment] [-f keyfile]     ssh-keygen -l [-f input_keyfile]     ssh-keygen -B [-f input_keyfile]     ssh-keygen -D reader     ssh-keygen -F hostname [-f known_hosts_file]     ssh-keygen -H [-f known_hosts_file]     ssh-keygen -R hostname [-f known_hosts_file]     ssh-keygen -U reader [-f input_keyfile]     ssh-keygen -r hostname [-f input_keyfile] [-g]     ssh-keygen -G output_file [-v] [-b bits] [-M memory] [-S start_point]     ssh-keygen -T output_file -f input_file [-v] [-a num_trials] [-W generator]\n12345678910-b：指定密钥长度；-e：读取openssh的私钥或者公钥文件；-C：添加注释；-f：指定用来保存密钥的文件名；-i：读取未加密的ssh-v2兼容的私钥/公钥文件，然后在标准输出设备上显示openssh兼容的私钥/公钥；-l：显示公钥文件的指纹数据；-N：提供一个新密语；-P：提供（旧）密语；-q：静默模式；-t：指定要创建的密钥类型。\n19.ipip命令用来显示或操纵linux主机的路由、网络设备、策略路由和隧道，是Linux下较新的功能强大的网络配置工具。\n1格式：ip [参数] [选项]\n1234567-V：显示指令版本信息-s：输出更详细的信息-f：强制使用指定的协议族-4：指定使用的网络层协议是IPv4协议-6：指定使用的网络层协议是IPv6协议-0：输出信息每条记录输出一行，即使内容较多也不换行显示-r：显示主机时，不使用IP地址，而使用主机的域名\n20.tcpdumptcpdump命令是一款sniffer工具，它可以打印所有经过网络接口的数据包的头信息，也可以使用-w选项将数据包保存到文件中，方便以后分析。\n1格式：tcpdump [选项] [参数]\n12345678910111213141516171819202122232425-a：尝试将网络和广播地址转换成名称-c&lt;数据包数目&gt;：收到指定的数据包数目后，就停止进行倾倒操作-d：把编译过的数据包编码转换成可阅读的格式，并倾倒到标准输出-dd：把编译过的数据包编码转换成C语言的格式，并倾倒到标准输出-ddd：把编译过的数据包编码转换成十进制数字的格式，并倾倒到标准输出-e：在每列倾倒资料上显示连接层级的文件头-f：用数字显示网际网络地址-F&lt;表达文件&gt;：指定内含表达方式的文件-i&lt;网络界面&gt;：使用指定的网络截面送出数据包-l：使用标准输出列的缓冲区-n：不把主机的网络地址转换成名字-N：不列出域名-O：不将数据包编码最佳化-p：不让网络界面进入混杂模式-q ：快速输出，仅列出少数的传输协议信息-r&lt;数据包文件&gt;：从指定的文件读取数据包数据-s&lt;数据包大小&gt;：设置每个数据包的大小-S：用绝对而非相对数值列出TCP关联数-t：在每列倾倒资料上不显示时间戳记-tt： 在每列倾倒资料上显示未经格式化的时间戳记-T&lt;数据包类型&gt;：强制将表达方式所指定的数据包转译成设置的数据包类型-v：详细显示指令执行过程-vv：更详细显示指令执行过程-x：用十六进制字码列出数据包资料-w&lt;数据包文件&gt;：把数据包数据写入指定的文件\n21.scpscp 就是 secure copy ，是一个在 Linux 下用来进行 远程拷贝文件 的命令它的地址格式与 ssh 基本相同，需要注意的是，在指定端口时用的是大写的 -P 而不是小写的\n12345678910# 把本地当前目录下的 01.py 文件 复制到 远程 家目录下的 Desktop/01.py# 注意：`:` 后面的路径如果不是绝对路径，则以用户的家目录作为参照路径scp -P port 01.py user@remote:Desktop/01.py# 把远程 家目录下的 Desktop/01.py 文件 复制到 本地当前目录下的 01.pyscp -P port user@remote:Desktop/01.py 01.py# 加上 -r 选项可以传送文件夹# 把当前目录下的 demo 文件夹 复制到 远程 家目录下的 Desktopscp -r demo user@remote:Desktop# 把远程 家目录下的 Desktop 复制到 当前目录下的 demo 文件夹scp -r user@remote:Desktop demo\n22.wgetwget命令用来从指定的URL下载文件。wget非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性，如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。\nwget支持HTTP，HTTPS和FTP协议，可以使用HTTP代理。所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。\n1用法： wget [选项]... [URL]...\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141长选项所必须的参数在使用短选项时也是必须的。启动：  -V,  --version           显示 Wget 的版本信息并退出。  -h,  --help              打印此帮助。  -b,  --background        启动后转入后台。  -e,  --execute=COMMAND   运行一个“.wgetrc”风格的命令。日志和输入文件：  -o,  --output-file=FILE    将日志信息写入 FILE。  -a,  --append-output=FILE  将信息添加至 FILE。  -d,  --debug               打印大量调试信息。  -q,  --quiet               安静模式 (无信息输出)。  -v,  --verbose             详尽的输出 (此为默认值)。  -nv, --no-verbose          关闭详尽输出，但不进入安静模式。  -i,  --input-file=FILE     下载本地或外部 FILE 中的 URLs。  -F,  --force-html          把输入文件当成 HTML 文件。  -B,  --base=URL            解析与 URL 相关的                             HTML 输入文件 (由 -i -F 选项指定)。       --config=FILE         Specify config file to use.下载：  -t,  --tries=NUMBER            设置重试次数为 NUMBER (0 代表无限制)。       --retry-connrefused       即使拒绝连接也是重试。  -O,  --output-document=FILE    将文档写入 FILE。  -nc, --no-clobber              skip downloads that would download to                                 existing files (overwriting them).  -c,  --continue                断点续传下载文件。       --progress=TYPE           选择进度条类型。  -N,  --timestamping            只获取比本地文件新的文件。  --no-use-server-timestamps     不用服务器上的时间戳来设置本地文件。  -S,  --server-response         打印服务器响应。       --spider                  不下载任何文件。  -T,  --timeout=SECONDS         将所有超时设为 SECONDS 秒。       --dns-timeout=SECS        设置 DNS 查寻超时为 SECS 秒。       --connect-timeout=SECS    设置连接超时为 SECS 秒。       --read-timeout=SECS       设置读取超时为 SECS 秒。  -w,  --wait=SECONDS            等待间隔为 SECONDS 秒。       --waitretry=SECONDS       在获取文件的重试期间等待 1..SECONDS 秒。       --random-wait             获取多个文件时，每次随机等待间隔                                 0.5*WAIT...1.5*WAIT 秒。       --no-proxy                禁止使用代理。  -Q,  --quota=NUMBER            设置获取配额为 NUMBER 字节。       --bind-address=ADDRESS    绑定至本地主机上的 ADDRESS (主机名或是 IP)。       --limit-rate=RATE         限制下载速率为 RATE。       --no-dns-cache            关闭 DNS 查寻缓存。       --restrict-file-names=OS  限定文件名中的字符为 OS 允许的字符。       --ignore-case             匹配文件/目录时忽略大小写。  -4,  --inet4-only              仅连接至 IPv4 地址。  -6,  --inet6-only              仅连接至 IPv6 地址。       --prefer-family=FAMILY    首先连接至指定协议的地址                                 FAMILY 为 IPv6，IPv4 或是 none。       --user=USER               将 ftp 和 http 的用户名均设置为 USER。       --password=PASS           将 ftp 和 http 的密码均设置为 PASS。       --ask-password            提示输入密码。       --no-iri                  关闭 IRI 支持。       --local-encoding=ENC      IRI (国际化资源标识符) 使用 ENC 作为本地编码。       --remote-encoding=ENC     使用 ENC 作为默认远程编码。       --unlink                  remove file before clobber.目录：  -nd, --no-directories           不创建目录。  -x,  --force-directories        强制创建目录。  -nH, --no-host-directories      不要创建主目录。       --protocol-directories     在目录中使用协议名称。  -P,  --directory-prefix=PREFIX  以 PREFIX/... 保存文件       --cut-dirs=NUMBER          忽略远程目录中 NUMBER 个目录层。HTTP 选项：       --http-user=USER        设置 http 用户名为 USER。       --http-password=PASS    设置 http 密码为 PASS。       --no-cache              不在服务器上缓存数据。       --default-page=NAME     改变默认页                               (默认页通常是“index.html”)。  -E,  --adjust-extension      以合适的扩展名保存 HTML/CSS 文档。       --ignore-length         忽略头部的‘Content-Length’区域。       --header=STRING         在头部插入 STRING。       --max-redirect          每页所允许的最大重定向。       --proxy-user=USER       使用 USER 作为代理用户名。       --proxy-password=PASS   使用 PASS 作为代理密码。       --referer=URL           在 HTTP 请求头包含‘Referer: URL’。       --save-headers          将 HTTP 头保存至文件。  -U,  --user-agent=AGENT      标识为 AGENT 而不是 Wget/VERSION。       --no-http-keep-alive    禁用 HTTP keep-alive (永久连接)。       --no-cookies            不使用 cookies。       --load-cookies=FILE     会话开始前从 FILE 中载入 cookies。       --save-cookies=FILE     会话结束后保存 cookies 至 FILE。       --keep-session-cookies  载入并保存会话 (非永久) cookies。       --post-data=STRING      使用 POST 方式；把 STRING 作为数据发送。       --post-file=FILE        使用 POST 方式；发送 FILE 内容。       --content-disposition   当选中本地文件名时                               允许 Content-Disposition 头部 (尚在实验)。       --auth-no-challenge     发送不含服务器询问的首次等待                               的基本 HTTP 验证信息。HTTPS (SSL/TLS) 选项：       --secure-protocol=PR     选择安全协议，可以是 auto、SSLv2、                                SSLv3 或是 TLSv1 中的一个。       --no-check-certificate   不要验证服务器的证书。       --certificate=FILE       客户端证书文件。       --certificate-type=TYPE  客户端证书类型，PEM 或 DER。       --private-key=FILE       私钥文件。       --private-key-type=TYPE  私钥文件类型，PEM 或 DER。       --ca-certificate=FILE    带有一组 CA 认证的文件。       --ca-directory=DIR       保存 CA 认证的哈希列表的目录。       --random-file=FILE       带有生成 SSL PRNG 的随机数据的文件。       --egd-file=FILE          用于命名带有随机数据的 EGD 套接字的文件。FTP 选项：       --ftp-user=USER         设置 ftp 用户名为 USER。       --ftp-password=PASS     设置 ftp 密码为 PASS。       --no-remove-listing     不要删除‘.listing’文件。       --no-glob               不在 FTP 文件名中使用通配符展开。       --no-passive-ftp        禁用“passive”传输模式。       --retr-symlinks         递归目录时，获取链接的文件 (而非目录)。递归下载：  -r,  --recursive          指定递归下载。  -l,  --level=NUMBER       最大递归深度 (inf 或 0 代表无限制，即全部下载)。       --delete-after       下载完成后删除本地文件。  -k,  --convert-links      让下载得到的 HTML 或 CSS 中的链接指向本地文件。  -K,  --backup-converted   在转换文件 X 前先将它备份为 X.orig。  -m,  --mirror             -N -r -l inf --no-remove-listing 的缩写形式。  -p,  --page-requisites    下载所有用于显示 HTML 页面的图片之类的元素。       --strict-comments    用严格方式 (SGML) 处理 HTML 注释。递归接受/拒绝：  -A,  --accept=LIST               逗号分隔的可接受的扩展名列表。  -R,  --reject=LIST               逗号分隔的要拒绝的扩展名列表。  -D,  --domains=LIST              逗号分隔的可接受的域列表。       --exclude-domains=LIST      逗号分隔的要拒绝的域列表。       --follow-ftp                跟踪 HTML 文档中的 FTP 链接。       --follow-tags=LIST          逗号分隔的跟踪的 HTML 标识列表。       --ignore-tags=LIST          逗号分隔的忽略的 HTML 标识列表。  -H,  --span-hosts                递归时转向外部主机。  -L,  --relative                  只跟踪有关系的链接。  -I,  --include-directories=LIST  允许目录的列表。  --trust-server-names             use the name specified by the redirection                                   url last component.  -X,  --exclude-directories=LIST  排除目录的列表。  -np, --no-parent                 不追溯至父目录。\n\n十六、内建命令1.typetype命令用来显示指定命令的类型，判断给出的命令是内建命令还是外部命令。\n1格式：type [选项] [参数]\n1234-a 显示一个名字的所有可能-t 判断一个名字当前是否是alias、keyword、function、builtin、file-p 查看一个外部命令的执行路径-P 查看内部命令路径\n命令类型：\n\nalias：别名。\nkeyword：关键字，Shell保留字。\nfunction：函数，Shell函数。\nbuiltin：内建命令，Shell内建命令。\nfile：文件，磁盘文件，外部命令。\nunfound：没有找到。\n\n2.envenv命令用于显示系统中已存在的环境变量，以及在定义的环境中执行指令。该命令只使用”-“作为参数选项时，隐藏了选项”-i”的功能。若没有设置任何选项和参数时，则直接显示当前的环境变量。\n如果使用env命令在新环境中执行指令时，会因为没有定义环境变量”PATH”而提示错误信息”such file or directory”。此时，用户可以重新定义一个新的”PATH”或者使用绝对路径。\n1用法：env [OPTION]... [-] [NAME=VALUE]... [COMMAND [ARG]...]\n12345678910-i, --ignore-environment    不带环境变量启动 -u, --unset=NAME    从环境变量中删除一个变量 --help    显示帮助并退出 --version    输出版本信息并退出 单独的-隐含-i.如果没有COMMAND,那么打印结果环境变量. \n3.setset命令作用主要是显示系统中已经存在的shell变量，以及设置shell变量的新变量值。使用set更改shell特性时，符号”+“和”-“的作用分别是打开和关闭指定的模式。set命令不能够定义新的shell变量。如果要定义新的变量，可以使用declare命令以变量名=值的格式进行定义即可。\n1用法：set [选项] [参数]\n123456789101112131415161718-a\t标示已修改的变量，以供输出至环境变量-b\t使被中止的后台程序立刻回报执行状态-C\t转向所产生的文件无法覆盖已存在的文件-d\tShell预设会用杂凑表记忆使用过的指令，以加速指令的执行。使用-d参数可取消-e\t若指令传回值不等于0，则立即退出shell-f\t取消使用通配符-h\t自动记录函数的所在位置-H \tShell可利用”!”加&lt;指令编号&gt;的方式来执行history中记录的指令-k\t指令所给的参数都会被视为此指令的环境变量-l\t记录for循环的变量名称-m\t使用监视模式-n\t只读取指令，而不实际执行-p\t启动优先顺序模式-P\t启动-P参数后，执行指令时，会以实际的文件或目录来取代符号连接-t\t执行完随后的指令，即退出shell-u\t当执行时使用到未定义过的变量，则显示错误信息-v\t显示shell所读取的输入值-x\t执行指令后，会先显示该指令及所下的参数\n4.unsetunset命令用于删除已定义的shell变量（包括环境变量）和shell函数。unset命令不能够删除具有只读属性的shell变量和只读属性的环境变量。\n1用法：unset [-f] [-v] [名称 ...]\n12-f\t将每个 NAME 名称当作函数对待-v\t将每个 NAME 名称当作变量对待\n5.echoecho命令用于在shell中打印shell变量的值，或者直接输出指定的字符串。linux的echo命令，在shell编程中极为常用, 在终端下打印变量value的时候也是常常用到的，因此有必要了解下echo的用法echo命令的功能是在显示器上显示一段文字，一般起到一个提示的作用。\n1格式：echo[OPTION]... [STRING]...\n12345678910111213141516-n 不输出行尾的换行符.-e 允许对下面列出的加反斜线转义的字符进行解释.-E 禁止对在STRINGs中的那些序列进行解释.--help 显示帮助并退出(须单独运行)--version 输出版本信息并退出(须单独运行)\\NNN  字符的ASCII代码为NNN(八进制)\\\\    反斜线\\a    报警符(BEL)\\b    退格符\\c    禁止尾随的换行符\\f    换页符\\n    换行符\\r    回车符\\t    水平制表符\\v    纵向制表符\n6.aliasalias命令用来设置指令的别名。我们可以使用该命令可以将一些较长的命令进行简化。使用alias时，用户必须使用单引号 ‘ ‘ 将原来的命令引起来，防止特殊字符导致错误。\nalias命令的作用只局限于该次登入的操作。若要每次登入都能够使用这些命令别名，则可将相应的alias命令存放到bash的初始化文件 /etc/bashrc中。\n1用法:alias [-p] [名称[=值] ... ]\n1-p\t以可重用的格式打印所有的已定义的别名\n7.unaliasunalias命令用来取消命令别名，是为shell内建命令。如果需要取消任意一个命令别名，则使用该命令别名作为指令的参数选项即可。如果使用-a选项，则表示取消所有已经存在的命令别名。\n1用法:unalias [-a] 名称 [名称 ...]\n1-a\t删除所有的别名定义.\n8.enableenable命令可以用于启动或关闭 shell 的内建指令。 如要执行的文件名称与shell内建指令相同，可用enable -n来关闭shell内建指令。若不加-n参数，enable可重新启动关闭的指令。\n注意：\nlinux shell命令执行时，shell总是先在自己的shell builtin中查找该命令，如果找到则执行该命令；如果找不到该命令，则会从环境变量$PATH指定的路径中依次去查找待执行的命令。因为了解了这一点，所以看起来好像没有办法编写用户自己的命令来替代shell builtin命令。有了enable命令我们就能做到了。\n12用法：\tenable [-a] [-dnps] [-f 文件名] [名称 ...]\n123456789101112131415选项：      -a        打印一个内嵌的列表，并显示其中每一个是否启用      -n        禁用每一个 NAME 内嵌或者显示一个被禁用的内嵌的列表      -p        以可重用的格式打印一个内嵌的列表      -s        仅打印Posix `special&#x27;  内嵌的名称        控制动态加载的选项：      -f        从共享对象 FILENAME 文件中加载 NAME 内嵌      -d        删除以 -f 选项加载的内嵌        不带选项时，每一个 NAME 内嵌都被启用。        如果要使用 $PATH 中找到的 `test&#x27; 而不是 shell 内嵌的版本，    输入 `enable -n test&#x27;。   \n9.readread命令用于从标准输入读取数值。read 内部命令被用来从标准输入读取单行数据。这个命令可以用来读取键盘输入，当使用重定向的时候，可以读取文件中的一行数据。\nread命令一般用在shell脚本中。\n1用法：read [选项] [参数]\n123456789-a\t后跟一个变量，该变量会被认为是个数组，然后给其赋值，默认是以空格为分割符– d\t后面跟一个标志符，其实只有其后的第一个字符有用，作为结束的标志– p\t后面跟提示信息，即在输入前打印提示信息– e\t在输入的时候可以使用命令补全功能– n\t后跟一个数字，定义输入文本的长度– r\t屏蔽\\，如果没有该选项，则\\作为一个转义字符，有的话 \\就是个正常的字符了– s\t安静模式，在输入字符时不再屏幕上显示– t\t后面跟秒数，定义输入字符的等待时间– u\t后面跟fd，从文件描述符中读入，该文件描述符可以是exec新开启的\n10.letlet命令是bash中用于计算的工具，用于执行一个或多个表达式，变量计算中不需要加上 $ 来表示变量。如果表达式中包含了空格或其他特殊字符，则必须引起来。\n1用法：let 参数 [参数 ...]\n1234567891011121314151617181920id++, id--      variable post-increment, post-decrement++id, --id      variable pre-increment, pre-decrement-, +            unary minus, plus!, ~            logical and bitwise negation**              exponentiation*, /, %         multiplication, division, remainder+, -            addition, subtraction&lt;&lt;, &gt;&gt;          left and right bitwise shifts&lt;=, &gt;=, &lt;, &gt;    comparison==, !=          equality, inequality&amp;               bitwise AND^               bitwise XOR|               bitwise OR&amp;&amp;              logical AND||              logical ORexpr ? expr : expr                conditional operator=, *=, /=, %=,+=, -=, &lt;&lt;=, &gt;&gt;=,&amp;=, ^=, |=      assignment\n11.fcfc命令自动调用vi编辑器修改已有历史命令，当保存时立即执行修改后的命令，也可以用来显示历史命令。fc命令编辑历史命令时，会自动调用vi编辑器。fc保存文件后，会自动执行所编辑过的命令。\n1用法：fc [选项] [参数]\n12345-e&lt;文本编辑器&gt;\t指定用来编辑命令的文本编辑器，默认是vi-l\t列出第一条和最后一天命令范围内的历史命令，如果不跟命令范围则默认显示最近使用过的16条历史命令-n\t显示历史命令时不显示命令序号-r\t反序显示所有历史命令-s&lt;命令名&gt;\t从历史命令中当前位置往前找到指定命令，并执行\n12.commandcommand命令调用指定的指令并执行，命令执行时不查询shell函数。command命令只能够执行shell内部的命令。\n1用法： command [-pVv] 命令 [参数 ...]\n123-p\t使用 PATH 变量的一个默认值以确保所有的标准工具都能被找到。-v\t打印 COMMAND 命令的描述，和 `type&#x27; 内嵌相似-V\t打印每个 COMMAND 命令的详细描述\n13.exitexit命令用来退出当前的shell或退出终端 ，并返回给定值。\n执行exit可使shell以指定的状态值退出。若不设置状态值参数，则shell以预设值退出。状态值0代表执行成功，其他值代表执行失败。状态值参数多用于脚本中，在终端状态下，直接输入“exit” 退出终端 。\n1用法：exit [状态值]\n14.execexec命令用于调用并执行指令的命令。exec命令通常用在shell脚本程序中，可以调用其他的命令。如果在当前终端中使用命令，则当指定的命令执行完毕后会立即退出终端。\n1用法:exec [-cl] [-a 名称] [命令 [参数 ...]] [重定向 ...]\n1234选项：-a 名称   作为第0个参数传递给 COMMAND 命令-c\t在一个空环境中执行 COMMAND 命令-l\t在COMMAND 命令的第0个参数中加一个短线\n15.logoutlogout指令退出一个登录 shell，其功能和login指令相互对应。\n123用法：logout [n]--help\t在线帮助--vesion\t显示版本信息\n16.loginlogin指令让用户登入系统，您亦可通过它的功能随时更换登入身份。在Slackware发行版中 ，您可在指令后面附加欲登入的用户名称，它会直接询问密码，等待用户输入。当/etc目录里含名称为nologin的文件时，系统只root帐号登入系统，其他用户一律不准登入。\n1用法：login [ -p ] [ -h host ] [ -H ] [ -f username | username ]\n12-p：告诉login指令不销毁环境变量-h：指定远程服务器的主机名\n\n十七、Shell编程 Shell 指 “提供给使用者使用界面” 的软件，即 Command Interpreter - 命令解析器。Shell 接收用户或者其他应用程序的命令，然后将这些命令转化成内核能够理解的语言并传递给内核，内核执行命令完成后，再将执行结果返回给用户或者应用程序。\n Shell 是包裹在操作系统外层的一道程序，负责外界与 Linux “内核” 的交互，但它隐藏了操作系统底层的具体细节，就像是 Linux 内核的一个 “外壳”，所以 Shell（壳）的名称也由此而来。\n图形化界面也是一种广义的 Shell，因为图形界面操作的本质也是 —— 将用户的命令传递给内核执行\n终端 只是人机交互的一个接口，提供输入输出命令的交互界面。终端的主要任务是接收用户输入的命令，并提交给 Shell。\nShell 是命令解析器，主要任务是翻译命令。Shell 将终端输入的命令转化成内核能够理解的语言并传递给内核，由内核执行命令，并将执行结果返回给终端。\n当我们打开终端时，Shell 也会自动启动，操作系统会将终端和 Shell 关联起来。接着我们在终端输入命令，Shell 就负责解释命令。\n17.1shell脚本的执行123[root@localhost ~]$ vim test.sh#!/bin/bashecho &quot;hello world&quot;\n两种方式执行shell脚本\n第一种：给文件增加执行权限\n12[root@localhost ~]$ chmod u+x test.sh[root@localhost ~]$ ./test.sh  #绝对路径或相对路径执行\n第二种：通过Bash调用执行脚本\n1[root@localhost ~]$ bash test.sh\n第三种：.\n1[root@localhost ~]$ . test.sh\n17.2shell变量在一个脚本周期内,其值可以发生改变的量就是变量。\n\n局部变量：shell也有自定义函数，函数里面的变量为局部变量，但是它也是相当于全局变量，函数中的变量，在函数外调用也是可以的，如果要仅限函数使用，需要在函数变量前加个关键字：local\n全局变量：每打开一个终端就是一个shell会话，在这个shell会话（终端）定义的变量就是全局变量，它在这个shell会话有效，当你打开另一个终端就是另一个shell会话，这个变量在另一个终端就失效了。\n环境变量：在全局变量前加 export ，如：export a=1  那么这个变量就是环境变量了。创建这个变量的shell成为父shell，这个shell中，在创建一个shell叫做子shell，环境变量可以由父shell往下一级一级传，而不能逆转往上传递。当shell会话销毁时，这个环境变量也会随之销毁。想要永久保存就得环境变量写到启动文件中去。\n\n变量的命名规则\n命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。\n等号左右两侧不能有空格，可以使用下划线“_”，变量的值如果有空格，需要使用单引号或双引号包括。如:“test=“hello world!””。其中双引号括起来的内容“$”，“(”和反引号都拥有特殊含义，而单引号括起来的内容都是普通字符。\n不能使用标点符号，不能使用bash里的关键字（可用help命令查看保留关键字）。\n环境变量建议大写，便于区分\n如果需要增加变量的值，那么可以进行变量值的叠加。不过变量需要用双引号包含”$变量名”或用${变量名}包含变量名。\n\n123456789[root@localhost ~]$ test=123[root@localhost ~]$ test=&quot;$test&quot;456[root@localhost ~]$ echo $test123456#叠加变量test，变量值变成了123456[root@localhost ~]$ test=$&#123;test&#125;789[root@localhost ~]$ echo $test123456789#再叠加变量test，变量值编程了123456789\nShell特殊符号\n\n用户自定义变量变量定义\n123456789[root@localhost ~]$ 2name=&quot;shen chao&quot;-bash: 2name=shen chao: command not found#变量名不能用数字开头[root@localhost ~]$ name = &quot;shenchao&quot;-bash: name: command not found#等号左右两侧不能有空格[root@localhost ~]$ name=shen chao-bash: chao: command not found#变量的值如果有空格，必须用引号包含\n 变量调用\n12345[root@localhost ~]$ name=&quot;shen chao&quot;#定义变量name[root@localhost ~]$ echo $name #调用变量使用  $变量名shen chao#输出变量name的值\n变量查看\n1[root@localhost ~]$ set [选项]\n选项:-u:如果设定此选项，调用未声明变量时会报错（默认无任何提示）-x:如果设定此选项，在命令执行之前，会把命令先输出一次+&lt;参数&gt; :取消某个set曾启动的参数。\n1234567891011121314151617[root@localhost ~]$ setBASH=/bin/bash…省略部分输出…name=&#x27;shen chao&#x27;#直接使用set 命令，会查询系统中所有的变量，包含用户自定义变量和环境变量[root@localhost ~]$ set -u[root@localhost ~]$ echo $file-bash: file: unbound variable#当设置了-u选项后，如果调用没有设定的变量会有报错。默认是没有任何输出的。[root@localhost ~]$ set -x[root@localhost ~]$ ls+ls --color=autoanaconda-ks.cfginstall.loginstall.log.syslog sh tdir testtestfile#如果设定了-x选项，会在每个命令执行之前，先把命令输出一次[root@localhost ~]$ set +x#取消启动的x参数\n变量删除\n1[root@localhost ~]$ unset 变量名\n环境变量环境变量设置\n12[root@localhost ~]$  export age=&quot;18&quot;#使用export声明的变量即是环境变量\n环境变量查询和删除env命令和set命令的区别：set命令可以查看所有变量，而env命令只能查看环境变量。\n12[root@localhost ~]$ unset gender   #删除环境变量gender[root@localhost ~]$ env | grep gender\n系统默认环境变量\n12345678910[root@localhost ~]$ envHOSTNAME=localhost.localdomain      #主机名SHELL=/bin/bash                     #当前的shellTERM=linux                          #终端环境HISTSIZE=1000                       #历史命令条数SSH_CLIENT=192.168.4.1594824 22     #当前操作环境是用ssh连接的，这里记录客户端ipSSH_TTY=/dev/pts/1                  #ssh连接的终端时pts/1USER=root                           #当前登录的用户..........更多参数可以使用set和env命令查看.............\n位置参数变量\n$1 是你给你写的shell脚本传的第一个参数，$2 是你给你写的shell脚本传的第二个参数…\n12345[root@localhost sh]$ vim test.sh#!/bin/shecho &quot;shell脚本本身的名字: $0&quot;echo &quot;传给shell的第一个参数: $1&quot;echo &quot;传给shell的第二个参数: $2&quot;\n保存退出后，你在Test.sh所在的目录下输入 bash Test.sh 1 2\n结果输出：\nshell脚本本身的名字: Test.sh传给shell的第一个参数: 1传给shell的第二个参数: 2\n$*会把接收的所有参数当成一个整体对待，而$@则会区分对待接收到的所有参数:\n123456789101112131415161718192021[root@localhost sh]$ vi parameter2.sh#!/bin/bashfor i in&quot;$*&quot;#定义for循环，in后面有几个值，for会循环多少次，注意“$*”要用双引号括起来#每次循环会把in后面的值赋予变量i#Shell把$*中的所有参数看成是一个整体，所以这个for循环只会循环一次\tdo\t\techo &quot;The parameters is: $i&quot;\t\t#打印变量$i的值\tdonex=1#定义变量x的值为1,用于跟踪参数的索引for y in&quot;$@&quot;#同样in后面的有几个值，for循环几次，每次都把值赋予变量y#可是Shel1中把“$@”中的每个参数都看成是独立的，所以“$@”中有几个参数，就会循环几次\tdo\t\techo &quot;The parameter$x is: $y&quot;\t\t#输出变量y的值\t\tx=$((x +1))\t\t#然变量x每次循环都加1，为了输出时看的更清楚\tdone\n预定义变量\n”$?”变量例子说明\n123456789101112[root@localhost sh]$ lscount.sh hello.sh parameter2.sh parameter.sh#ls命令正确执行[root@localhost sh]$ echo $?#预定义变量“$?”的值是0，证明上一个命令执行正确[root@localhost sh]$ ls install.logls:无法访问install.log:没有那个文件或目录#当前目录中没有install.log文件，所以ls命令报错了[root@localhost sh]$ echo $?2#变量“$?”返回一个非О的值，证明上一个命令没有正确执行#至于错误的返回值到底是多少，是在编写ls命令时定义好的，如果碰到文件不存在就返回数值2\n说明下”$$”和”$!”这两个预定义变量\n12345678910[root@localhost sh]$ vi variable.sh#!/bin/bashecho &quot;The current process is $$&quot;#输出当前进程的PID.#这个PID就是variable.sh这个脚本执行时，生成的进程的PIDfind /root -name hello.sh &amp;#使用find命令在root目录下查找hello.sh文件#符号&amp;的意思是把命令放入后台执行，工作管理我们在系统管理章节会详细介绍echo &quot;The last one Daemon process is $!&quot;#输出这个后台执行命令的进程的PID，也就是输出find命令的PID号\n只读变量1234567[root@localhost sh]$ vi readonly.sh#!/bin/basha=10#语法：readonly 变量名readonly aa=20   #会报错readonly variableecho $a\n接受键盘输入12345678910111213[root@localhost ~]$ read [选项][变量名]选项:\t-a 后跟一个变量，该变量会被认为是个数组，然后给其赋值，默认是以空格为分割符。\t-p： “提示信息”：在等待read输入时，输出提示信息\t-t： 秒数：read命令会一直等待用户输入，使用此选项可以指定等待时间\t-n： 数字：read命令只接受指定的字符数，就会执行\t-s： 隐藏输入的数据，适用于机密信息的输入    -d： 后面跟一个标志符，其实只有其后的第一个字符有用，作为结束的标志。    -e： 在输入的时候可以使用命令补全功能。变量名:变量名可以自定义，如果不指定变量名，会把输入保存入默认变量REPLY.如果只提供了一个变量名，则整个输入行赋予该变量.如果提供了一个以上的变量名，则输入行分为若干字，一个接一个地赋予各个变量，而命令行上的最后一个变量取得剩余的所有字\n123456789101112131415161718192021[root@localhost sh]$ vi read.sh#!/bin/bashread -t 30 -p &quot;Please input your name: &quot; name#提示“请输入姓名”并等待30 秒，把用户的输入保存入变量name 中echo &quot;Name is $name&quot;#看看变量“$name”中是否保存了你的输入read -s -t 30 -p &quot;Please enter your age: &quot; age#提示“请输入年龄”并等待30秒，把用户的输入保存入变量age中#年龄是隐私，所以我们用“-s”选项隐藏输入echo -e &quot;\\n&quot;#调整输出格式，如果不输出换行，一会的年龄输出不会换行echo &quot;Age is $age&quot;read -n 1 -t 30 -p &quot;Please select your gender[M/F]:&quot; gender#提示“请选择性别”并等待30秒，把用户的输入保存入变量gender#使用“-n1”选项只接收一个输入字符就会执行（都不用输入回车）echo -e &quot;\\n&quot;echo &quot;Sex is $gender&quot;\n17.3shell运算符算数运算符原生bash不支持简单的数学运算，但是可以通过其他命令来实现，例如 awk 和 expr，expr 最常用。expr 是一款表达式计算工具，使用它能完成表达式的求值操作。例如，两个数相加(注意使用的是反引号 ` 而不是单引号 ‘)：\n1234567[root@localhost ~]$ vi computer.sh#!/bin/bashval=`expr 2 + 2`echo &quot;两数之和为 : $val&quot;#注意#表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。#完整的表达式要被 ` ` 包含，注意这个字符不是常用的单引号，在 Esc 键下边。\n\n加法 |expr $a + $b 结果为 30。\n\n减法 |expr $a - $b 结果为 -10。\n\n乘法 |expr $a * $b 结果为 200。\n除法 |expr $b / $a 结果为 2。\n取余 | expr $b % $a 结果为 0。\n赋值 | a=$b 将把变量 b 的值赋给 a。\n相等。用于比较两个数字，相同则返回 true（真）。| [ $a == $b ] 返回 false（假）。\n不相等。用于比较两个数字，不相同则返回 true。    |[ $a != $b ] 返回 true。\n\n注意：条件表达式要放在方括号之间，并且要有空格，必须写成 [ $a == $b ]。\n123456789101112131415161718[root@localhost ~]$ vi computers.sh#!/bin/basha=10b=20echo &#x27; &#x27;echo &#x27;a+b= &#x27; `expr $a + $b`echo &#x27;a-b= &#x27; `expr $a - $b`echo &#x27;a*b= &#x27; `expr $a \\* $b`echo &#x27;a/b= &#x27; `expr $a / $b`echo &#x27;a%b= &#x27; `expr $a % $b`#判断是否相等if [ $a == $b ]then\techo &#x27;a等于b&#x27;else\techo &#x27;a不等于b&#x27;fi\n关系运算符关系运算符只支持数字，不支持字符串，除非字符串的值是数字。\n常用的关系运算符，假定变量 a 为 10，变量 b 为 20：\n\n注意括号前后空格\n123456[root@localhost ~]$ [ 10 -gt 10 ] [root@localhost ~]$ echo $?  1[root@localhost ~]$ [ 10 -eq 10 ] [root@localhost ~]$ echo $?  0\n如果要在shell脚本使用linux命令，可以使用$()包裹命令\n12345678910111213141516[root@localhost ~]$ vim demo.sh #!/bin/bash#接受用户的输入read -p &#x27;请输入需要查询的用户名:&#x27; username#获取指定用户名在passwd文件中出现的次数count=$(cat /etc/passwd | grep $username | wc -l)#count=`cat /etc/passwd | grep $username | wc -l`#判断出现的次数，如果次数=0则用户不存在，反之存在if [  $count == 0 ]then \t\techo &#x27;用户不存在&#x27;\telse \t\techo &#x27;用户存在&#x27;fi\n逻辑运算符常用的布尔运算符，假定变量 a 为 10，变量 b 为 20：\n\n字符串运算符常用的字符串运算符，假定变量 a 为 “abc”，变量 b 为 “efg”：\n\n文件测试运算符（重点）文件测试运算符用于检测 Unix/Linux 文件的各种属性。\n\n17.4流程控制if条件判断单分支if条件语法：\n1234if [ 条件判断式 ]\tthen\t\t程序fi\n案例：统计根分区使用率\n123456789101112[root@localhost ~]$ vi sh/if1.sh#!/bin/bash#统计根分区使用率rate=$(df -h | grep &quot;/dev/sda2&quot; | awk &#x27;&#123;print $5&#125;’| cut -d &quot;%&quot;-f1)#把根分区使用率作为变量值赋予变量rateif [ $rate -ge 80 ]#判断rate的值如果大于等于80，则执行then程序\tthen\t\techo &quot;Warning!/dev/sda3 is fu11!!&quot;\t#打印警告信息。在实际工作中，也可以向管理员发送邮件。fi\n案例：创建目录\n12345678910[root@localhost ~]$ vi sh/add_dir.sh#!/bin/bash#创建目录，判断是否存在，存在就结束，反之创建echo &quot;当前脚本名称为$0&quot;DIR=&quot;/media/cdrom&quot;if [ ! -e $DIR ]then\tmkdir -p $DIRfiecho &quot;$DIR 创建成功&quot;\n双分支if条件语句语法：\n123456if [ 条件判断式 ]\tthen\t\t条件成立时，执行的程序\telse\t\t条件不成立时，执行的另一个程序fi\n案例1：备份mysql数据库\n12345678910111213141516171819202122232425262728293031323334353637383940[root@localhost ~]$ vi sh/bakmysql.sh#!/bin/bash#备份mysql数据库。ntpdate asia.pool.ntp.org &amp;&gt;/dev/null#同步系统时间date=$(date +%y%m%d)#把当前系统时间按照“年月日”格式赋子变量datesize=$(du -sh/var/lib/mysql)#统计mysql数据库的大小，并把大小赋予size变量if [ -d /tmp/dbbak ]#判断备份目录是否存在，是否为目录\tthen\t#如果判断为真，执行以下脚本\techo &quot;Date : $date!&quot; &gt; /tmp/dbbak/dbinfo.txt\t#把当前日期写入临时文件\techo &quot;Data size : $size&quot; &gt;&gt; /tmp/dbbak/dbinfo.txt\t#把数据库大小写入临时文件\tcd/tmp/dbbak\t\t#进入备份目录\ttar -zcf mysql-lib-$date.tar.gz /var/lib/mysql dbinfo.txt &amp;&gt; /dev/null\t#打包压缩数据库与临时文件，把所有输出丢入垃圾箱（不想看到任何输出）\trm -rf /tmp/dbbak/dbinfo.txt\t#删除临时文件else\tmkdir /tmp/dbbak\t#如果判断为假，则建立备份目录\techo &quot;Date : $date!&quot; &gt; /tmp/dbbak/dbinfo.txt\techo &quot;Data size : $size&quot; &gt;&gt; /tmp/dbbak/dbinfo.txt\t#把日期和数据库大小保存如临时文件\tcd /tmp/dbbak\ttar -zcf mysql-lib-$date.tar. gz dbinfo.txt /var/lib/mysql &amp;&gt; /dev/null\t#压缩备份数据库与临时文件\trm -rf/tmp/dbbak/dbinfo.txt\t#删除临时文件fi\n案例2：判断apache是否启动，如果没有启动则自动启动\n123456789101112131415161718[root@localhost ~]$ vi sh/autostart.sh#!/bin/bash#判断apache是否启动，如果没有启动则自动启动port=$(nmap -sT 192.168.4.210 | grep tcp | grep http | awk &#x27;&#123;print $2&#125;’)#使用nmap命令扫描服务器，并截取 apache服务的状态，赋予变量port#只要状态是open，就证明正常启动if [ &quot;$port&quot; == &quot;open&quot;]#如果变量port的值是“open”\tthen\techo &quot;$(date) httpd is ok!” &gt;&gt; /tmp/autostart-acc.log\t#则证明apache 正常启动，在正常日志中写入一句话即可else\t/etc/rc.d/init.d/httpd start &amp;&gt;/dev/null\t#否则证明apache没有启动，自动启动apache\techo &quot;$(date) restart httpd !!&quot; &gt;&gt; /tmp/autostart-err.log\t#并在错误日志中记录自动启动apche 的时间fi\nnmap端口扫描命令，格式如下：\n123456789101112[root@localhost ~]$ nmap -sT 域名或IP选项:-s      扫描-T      扫描所有开启的TCP端口#知道了nmap命令的用法，我们在脚本中使用的命令就是为了截取http的状态，只要状态是“or.#就证明apache启动正常，否则证明apache启动错误。来看看脚本中命令的结果:[root@localhost ~]$ nmap -sT 192.168.4.210 | grep tcp | grep http | awk &#x27; fprint $2&#125;’#扫描指定计算机，提取包含tcp 的行，在提取包含httpd 的行，截取第二列open#把截取的值赋予变量port\n多分支if条件语句\n语法：\n12345678910if [ 条件判断式1 ]\tthen\t\t当条件判断式1成立时，执行程序1elif [ 条件判断式2 ]\tthen\t\t当条件判断式2成立时，执行程序2…省略更多条件…else\t当所有条件都不成立时，最后执行此程序fi\n案例：判断用户输入的是什么文件\n12345678910111213141516171819202122232425262728293031323334[root@localhost ~]$ vi sh/if-elif.sh#!/bin/bash#判断用户输入的是什么文件read -p &quot;Please input a filename: &quot; file#接收键盘的输入，并赋予变量fileif [ -z &quot;$file” ]#判断file变量是否为空\tthen\t\techo &quot;Error, please input a filename&quot;\t\t#如果为空，执行程序1，也就是输出报错信息\t\texit 1\t\t#退出程序，并返回值为Ⅰ(把返回值赋予变量$P）elif [ ! -e &quot;$file” ]\t\t#判断file的值是否存在\t\tthen\t\techo &quot;Your input is not a file!&quot;\t\t#如1果不存在，则执行程序2\t\texit 2\t\t#退出程序，把并定义返回值为2elif [ -f &quot;$file” ]\t\t#判断file的值是否为普通文件\t\tthen\t\techo &quot;$file is a regulare file!”\t\t#如果是普通文件，则执行程序3elif [ -d &quot;$file” ]\t\t#到断file的值是否为目录文件\t\tthen\t\techo &quot;$file is a directory!&quot;\t\t#如果是目录文件，网执行程序4else\techo &quot;$file is an other file!”\t#如果以上判断都不是，则执行程序5fi\n多分支case条件语句case语句语法如下:\n123456789101112case $变量名 in\t&quot;值1&quot;)\t如果变量的值等于值1，则执行程序1\t;;\t&quot;值2&quot;)\t如果变量的值等于值2，则执行程序2\t::\t…省略其他分支…\t*)\t如果变量的值都不是以上的值，则执行此程序\t;;esac\n这个语句需要注意以下内容:\n\ncase语句，会取出变量中的值，然后与语句体中的值逐一比较。如果数值符合，则执行对应的程序，如果数值不符，则依次比较下一个值。如果所有的值都不符合，则执行 “)” (代表所有其他值）中的程序。\ncase语句以“case”开头，以“esac”结尾。\n\n每一个分支程序之后要通过“;;”双分号结尾，代表该程序段结束(千万不要忘记，每次写case语句，都不要忘记双分号）。\n\n\n1234567891011121314151617181920案例：[root@localhost ~]$ vi sh/if-case.sh#!/bin/bashread -p &quot;请输入一个字符，并按Enter确认：&quot; KEYcase &quot;$KEY&quot; in\t[a-z]|[A-Z])\techo &quot;您输入的是字母&quot;\t;;\t\t[0-9])\techo &quot;您输入的是数字&quot;\t;;\t\t*)\techo &quot;您输入的是其他字符&quot;\t;;esac\nfor循环语法一:\n1234for 变量 in 值1 值2 值3 …(可以是一个文件等)\tdo\t\t程序\tdone\n这种语法中for循环的次数，取决于in后面值的个数（空格分隔），有几个值就循环几次，并且每次循环都把值赋予变量。也就是说，假设in后面有三个值，for会循环三次，第一次循环会把值1赋予变量，第二次循环会把值2赋予变量，以此类推。\n语法一举例：打印时间\n12345678[root@localhost ~]$ vi sh/for.sh#!/bin/bash#打印时间for time in morning noon afternoon evening\tdo\t\techo &quot;This time is $time!&quot;\tdone\n语法一举例：批量解压缩脚本\n123456789101112131415161718[root@localhost ~]$ vi sh/auto-tar. sh#!/bin/bash#批量解压缩脚本cd/lamp#进入压缩包目录ls *.tar.gz &gt; ls.log#把所有.tar.gz结尾的文件的文件覆盖到ls.log 临时文件中for i in $(cat ls.log) `#或者这样写for i in `cat ls.log`#读取ls.log文件的内容，文件中有多少个值，就会循环多少次，每次循环把文件名赋予变量i\tdo\t\ttar -zxf $i &amp;&gt;/dev/null\t\t#加压缩，并把所有输出都丢弃\tdonerm -rf /lamp/ls.log#删除临时文件ls.log\n语法二:\n1234for (( 初始值;循环控制条件;变量变化 ))\tdo\t\t程序\tdone\n语法二中需要注意:初始值:在循环开始时，需要给某个变量赋予初始值，如i=1;\n循环控制条件:用于指定变量循环的次数，如i&lt;=100，则只要i的值小于等于100，循环就会继续;\n变量变化:每次循环之后，变量该如何变化，如i=i+1。代表每次循环之后，变量i的值都加1。\n语法二举例：从1加到100\n12345678910111213[root@localhost ~]$ vi sh/add. sh#!/bin/bash#从1加到100s=0for (( i=1;i&lt;=100;i=i+1 ))#定义循环100 次do\ts=$(( $s+$i ))\t#每次循环给变量s赋值\tdoneecho &quot;The sum of 1+2+...+100 is : $s&quot;#输出1加到100的和\n语法二举例：批量添加指定数量的用户\n123456789101112131415161718192021222324252627282930313233[root@localhost ~]$ vi useradd.sh#!/bin/bash#批量添加指定数量的用户read -p &quot;Please input user name: &quot; -t 30 name#让用户输入用户名，把输入保存入变量nameread -p &quot;Please input the number of users: &quot; -t 30 num#让用户输入添加用户的数量，把输入保存入变量numread -p &quot;Please input the password of users: &quot; -t 30 pass#让用户输入初始密码，把输入保存如变量passif [ ! -z &quot;$name&quot; -a ! -z &quot;$num&quot;-a ! -z &quot;$pass&quot;]#判断三个变量不为空theny=$(echo $num | sed &#x27;s/[0-9]//g&#x27;)#定义变量的值为后续命令的结果#后续命令作用是，把变量num 的值替换为空。如果能替换为空，证明num 的值为数字#如果不能替换为空，证明num的值为非数字。我们使用这种方法判断变量num 的值为数字\tif [ -z &quot;$y&quot;]\t#如果变量y的值为空，证明num变量是数字\t\tthen\t\tfor (( i=1 ; i&lt;=$num; i=i+1 ))\t\t#循环num变量指定的次数\t\t\tdo\t\t\t/usr/sbin/useradd $name$i &amp;&gt;/dev/null\t\t\t#添加用户，用户名为变量name 的值加变量i的数字\t\t\techo $pass | /usr/bin/passwd --stdin $name$i &amp;&gt;/dev/null\t\t\t#给用户设定初始密码为变量pass 的值\t\t\tdone\tfifi\n语法二举例：批量删除用户\n12345678910111213[root@localhost ~]$ vi sh/userdel.sh#!/bin/bash#批量删除用户user=$(cat /etc/passwd | grep &quot; /bin/bash&quot;|grep -v &quot;root&quot;Icut -d &quot;:&quot; -f 1)#读取用户信息文件，提取可以登录用户，取消root用户，截取第一列用户名for i in $user#循环，有多少个普通用户，循环多少次\tdo\t\tuserdel -r $i\t\t#每次循环，删除指定普通用户\tdone\nwhile循环语法：\n1234while [ 条件判断式 ]\tdo\t\t程序\tdone\n案例：1加到100\n123456789101112131415[root@localhost ~]$ vi sh/addnum.sh#!/bin/bash#从1加到100i=1s=0#给变量i和变量s赋值while [ $i -le 100 ]#如果变量i的值小于等于100，则执行循环\tdo\t\ts=$(( $s+$i ))\t\ti=$(( $i+1 ))\tdoneecho &quot;The sum is: $s&quot;\n案例：输入的数值进行比较判断\n12345678910111213141516171819202122[root@localhost ~]$ vi sh/addnum.sh#!/bin/bashPRICE=$(expr $RANDOM % 1000)TIMES=0echo &quot;商品的价格为0-999之间，猜猜看是多少？&quot;while truedo  read -p &quot;请输入您猜的价格：&quot; INTlet TIMES++\tif [ $INT -eq $PRICE ] ; then\t  echo &quot;恭喜您猜对了，实际价格是 $PRICE&quot;\t  echo &quot;您总共猜了 $TIMES 次&quot;\texit 0\telif [ $INT -gt $PRICE ] ; then\t  echo &quot;太高了&quot;\telse\t  echo &quot;太低了&quot;\tfidone\nuntil循环和while循环相反，until循环时只要条件判断式不成立则进行循环，并执行循环程序。一旦循环条件成立，则终止循环。\n语法:\n1234until [ 条件判断式 ]\tdo\t\t程序\tdone\n案例一：1加到100\n123456789101112131415[root@localhost ~]$ vi sh/until.sh#!/bin/bash#从1加到100i=1s=0#t给变量i和变量s赋值until [ $i -gt 100 ]#循环直到变量i的值大于100，就停止循环\tdo\t\ts=$(( $s+$i ))\t\ti=$(( $i+1 ))\tdoneecho &quot;The sum is: $s&quot;\n函数语法：\n123function 函数名 () &#123;\t程序&#125;\n案例：接收用户输入的数字，然后从1加到这个数字\n12345678910111213141516171819202122232425262728293031[root@localhost ~]$ vi sh/function.sh#!/bin/bash#接收用户输入的数字，然后从1加到这个数字function sum () &#123;\t#定义函数sum\ts=0\tfor (( i=0; i&lt;=$num;i=i+1 ))\t\t#循环直到i大于$1为止。$1是函数sum 的第一个参数\t\t#在函数中也可以使用位置参数变量，不过这里的$1指的是函数的第一个参数\t\tdo\t\t\ts=$(( $i+$s ))\t\tdone\techo &quot;The sum of 1+2+3...+$1 is :$s&quot;\t#输出1加到$1的和&#125;read -p &quot;Please input a number: &quot; -t 30 num#接收用户输入的数字，并把值赋予变量numy=$(echo $num | sed &#x27;s/[0-9]//g&#x27;)#把变量num的值替换为空，并赋予变量yif [ -z &quot;$y&quot;]#判断变量y是否为空，以确定变量num中是否为数字\tthen\t\tsum $num\t\t#调用sum函数，并把变量num的值作为第一个参数传递给sum函数else\t\techo &quot;Error!! Please input a number!&quot;\t\t#如果变量num 的值不是数字，则输出报错信息fi\n特殊流程控制语句exit语句系统是有exit命令的，用于退出当前用户的登录状态。可是在Shell脚本中，exit语句是用来退出当前脚本的。也就是说，在Shell脚本中，只要碰到了exit语句，后续的程序就不再执行，而直接退出脚本。\nexit的语法如下:\nexit [返回值]\n如果exit命令之后定义了返回值，那么这个脚本执行之后的返回值就是我们自己定义的返回值。可以通过查询$?这个变量，来查看返回值。如果exit之后没有定义返回值，脚本执行之后的返回值是执行exit 语句之前，最后执行的一条命令的返回值。写一个exit 的例子:\n12345678910111213[root@localhost ~]$ vi sh/exit.sh#!/bin/bash#演示exit的作用read -p &quot;Please input a number: &quot; -t 30 num#接收用户的输入，并把输入赋予变量numy=$ (echo $num | sed &#x27;s/[0-9]//g&#x27;)#如果变量num 的值是数字，则把num的值替换为空，否则不替换#把替换之后的值赋予变量y[ -n &quot;$y&quot; ] &amp;&amp; echo &quot;Error! Please input a number!&quot; &amp;&amp; exit 18#判断变量y的值如果不为空，输出报错信息，退出脚本，退出返回值为18echo &quot;The number is: $num&quot;#如果没有退出加班，则打印变量num中的数字\nbreak语句当程序执行到break语句时，会结束整个当前循环。而continue 语句也是结束循环的语句，不过continue 语句单次当前循环，而下次循环会继续。\n案例：\n12345678910111213141516[root@localhost ~]$ vi sh/break.sh#!/bin/bash#演示break 跳出循环for (( i=1;i&lt;=10; i=i+1 ))#循环十次\tdo\t\tif [&quot;$i&quot; -eq 4 ]\t\t#如果变量i的值等于4\t\t\tthen\t\t\tbreak\t\t\t#退出整个循环\t\tfi\techo $i\t#输出变量i的值\tdone\n1234执行下这个脚本，因为一旦变量i的值等于4，整个循环都会跳出，所以应该只能循环三次:[root@localhost ~]$ chmod 755 sh/break.sh[root@localhost ~]#sh/break.sh\ncontinue语句\ncontinue也是结束流程控制的语句。如果在循环中，continue语句只会结束单次当前循环。\n案例：\n12345678910111213141516[root@localhost ~]$ vi sh/break.sh#!/bin/bash#演示continuefor (( i=1;i&lt;=10;i=i+1 ))#循环十次\tdo\t\tif [&quot;$i&quot; -eq 4 ]\t\t#如果变量i的值等于4\t\t\tthen\t\t\tcontinue\t\t\t#退出换成continue\t\tfi\techo $i\t#输出变量i的值\tdone\n1234567891011121314执行下这个脚本:[root@localhost ~]$ chmod 755 sh/continue.sh[root@localhost ~]#sh/break.sh1235678910#少了4这个输出\n17.5字符截取、替换和处理命令正则表达式\n\n| 匹配之前的项1次或者多次 | sa-6+匹配sa-6、sa-666，不能匹配sa-\n| 匹配之前的项0次或者多次| co*l匹配cl、col、cool、coool等\n() | 匹配表达式，创建一个用于匹配的子串 | ma(tri)?匹配max或maxtrix\n{n} | 匹配之前的项n次，n是可以为0的正整数 |[0-9]{3}匹配任意一个三位数，可以扩展为[0-9][0-9][0-9]\n{n,}| 之前的项至少需要匹配n次 | [0-9]{2,}匹配任意一个两位数或更多位数不支持{n,}{n,}{n,}\n{n,m}| 指定之前的项至少匹配n次，最多匹配m次，n&lt;=m | [0-9]{2,5}匹配从两位数到五位数之间的任意一个数字\n|| 交替匹配|两边的任意一项 | ab(c|d)匹配abc或abd\n\n字符截取、替换命令cut 列提取命令\n12345678[root@localhost ~]$ cut [选项] 文件名选项:-f 列号: 提取第几列-d 分隔符: 按照指定分隔符分割列-n\t取消分割多字节字符-c 字符范围: 不依赖分隔符来区分列，而是通过字符范围（行首为0）来进行字段提取。“n-”表示从第n个字符到行尾;“n-m”从第n个字符到第m个字符;“一m”表示从第1个字符到第m个字符。--complement\t补足被选择的字节、字符或字段--out-delimiter\t指定输出内容是的字段分割符\ncut命令的默认分隔符是制表符，也就是“tab”键，对空格符不怎么支持。\n12345[root@localhost ~]$ vi student.txtid\tname\tgender\tmark1\tliming\tm\t\t862\tsc\t\tm\t\t673\ttg\t\tn\t\t90\n12[root@localhost ~]$ cut -f 2 student.txt#提取第二列内容\n那如果想要提取多列呢?只要列号直接用“，”分开，命令如下:\n1[root@localhost ~]$ cut -f 2,3 student.txt\ncut可以按照字符进行提取，需要注意“8-”代表的是提取所有行的第十个字符开始到行尾，而“10-20”代表提取所有行的第十个字符到第二十个字符，而“-8”代表提取所有行从行首到第八个字符:\n12[root@localhost ~]$ cut -c 8- student.txt#提取第八个字符开始到行尾，很乱，那是因为每行的字符个数不相等啊\n12[root@localhost ~]$ cut -d &quot;:&quot; -f 1,3 /etc/passwd#以“:”作为分隔符，提取/etc/passwd_文件的第一列和第三列\n如果我想用cut命令截取df命令的第一列和第三列，就会出现这样的情况:\n123456[root@localhost~]$ df -h | cut -d &quot; &quot; -f 1,3Filesystem /dev/sda2 tmpfs /dev/sda1\nawk 编程AWK 是一种处理文本文件的语言，是一个强大的文本分析工具。\nprintf 格式化输出1234567891011121314151617[root@localhost ~]$ printf ‘输出类型输出格式’ 输出内容输出类型:%c:     ASCII字符.显示相对应参数的第一个字符%-ns:   输出字符串，减号“-”表示左对齐(默认右对齐)，n是数字指代输出几个字符,几个参数就写几个%-ns%-ni:   输出整数，n是数字指代输出几个数字%f：    输出小数点右边的位数%m.nf:  输出浮点数，m和n是数字，指代输出的整数位数和小数位数。如%8.2f代表共输出8位数，其中2位是小数，6位是整数。输出格式:\\a: 输出警告声音\\b: 输出退格键，也就是Backspace键\\f: 清除屏幕\\n: 换行\\r: 回车，也就是Enter键\\t: 水平输出退格键，也就是Tab 键\\v: 垂直输出退格键，也就是Tab 键\n为了演示printf命令，需要修改下刚刚cut命令使用的student.txt文件，文件内容如下:\n12345678910111213[root@localhost ~]$ vi student.txtID      Name    php  \t Linux  \tMySQL \t  Average1       AAA      66         66       66           662       BBB      77         77       77           773       CCC      88         88       88           88#printf格式输出文件[root@localhost ~]$ printf &#x27;%s\\t %s\\t %s\\t %s\\t %s\\t %s\\t \\n’ $(cat student.txt)#%s分别对应后面的参数,6列就写6个ID      Name    php   Linux  MySQL   Average1       AAA      66         66       66           662       BBB      77         77       77           773       CCC      88         88       88           88\n如果不想把成绩当成字符串输出，而是按照整型和浮点型输出，则要这样:\n123[root@localhost ~]$ printf &#x27;%i\\t %s\\t %i\\t %i\\t %i\\t %8.2f\\t \\n’ \\ $(cat student.txt | grep -v Name)\nawk 基本使用123456789101112131415161718[root@localhost ~]$ awk‘条件1&#123;动作1&#125; 条件2&#123;动作2&#125;…’ 文件名条件（Pattern）:\t一般使用关系表达式作为条件。这些关系表达式非常多，例如:\tx &gt; 10  判断变量x是否大于10\tx == y  判断变量x是否等于变量y\tA ~ B   判断字符串A中是否包含能匹配B表达式的子字符串\tA !~ B  判断字符串A中是否不包含能匹配B表达式的子字符串\t动作（Action） :\t格式化输出\t流程控制语句常用参数：   -F\t指定输入时用到的字段分隔符   -v\t自定义变量   -f\t从脚本中读取awk命令   -m\t对val值设置内在限制\n我们这里先来学习awk基本用法，也就是只看看格式化输出动作是干什么的。\n12[root@localhost ~]$ awk &#x27;&#123;printf $2 &quot;\\t&quot; $6 &quot;\\n&quot;&#125;’ student.txt#输出第二列和第六列\n比如刚刚截取df命令的结果时，cut命令已经力不从心了，我们来看看awk命令:\n12[root@localhost ~]$ df -h | awk &#x27;&#123;print $1 &quot;\\t&quot; $3&#125;&#x27;#截取df命令的第一列和第三列\nawk 的条件\nBEGIN\nBEGIN是awk的保留字，是一种特殊的条件类型。BEGIN的执行时机是“在 awk程序一开始时，尚未读取任何数据之前执行”。一旦BEGIN后的动作执行一次，当awk开始从文件中读入数据，BEGIN的条件就不再成立，所以BEGIN定义的动作只能被执行一次。\n例如:\n12345[root@localhost ~]$ awk &#x27;BEGIN&#123;printf &quot;This is a transcript \\n&quot; &#125; &#123;printf $2 &quot;\\t&quot; $6 &quot;\\n&quot;&#125;’ student.txt#awk命令只要检测不到完整的单引号不会执行，所以这个命令的换行不用加入“|”,就是一行命令#这里定义了两个动作#第一个动作使用BEGIN条件，所以会在读入文件数据前打印“这是一张成绩单”(只会执行一次)#第二个动作会打印文件的第二字段和第六字段\nEND\nEND也是awk保留字，不过刚好和BEGIN相反。END是在awk程序处理完所有数据，即将结束时执行。END后的动作只在程序结束时执行一次。例如:\n12[root@localhost ~]$ awk &#x27;END&#123;printf &quot;The End \\n&quot;&#125; &#123;printf $2 &quot;\\t&quot; $6 &quot;\\n&quot;&#125;’ student.txt#在输出结尾输入“The End”，这并不是文档本身的内容，而且只会执行一次\n关系运算符举几个例子看看关系运算符。假设我想看看平均成绩大于等于87分的学员是谁，就可以这样输入命令:例子1:\n123[root@localhost ~]$ cat student.txt | grep -v Name | awk &#x27;$6 &gt;= 87 &#123;printf $2 &quot;\\n&quot;&#125;&#x27;#使用cat输出文件内容，用grep取反包含“Name”的行#判断第六字段（平均成绩）大于等于87分的行，如果判断式成立，则打第六列（学员名$2）\n加入了条件之后，只有条件成立动作才会执行，如果条件不满足，则动作则不运行。通过这个实验，大家可以发现，虽然awk是列提取命令，但是也要按行来读入的。这个命令的执行过程是这样的:\n1）如果有BEGIN条件，则先执行BEGIN定义的动作。2）如果没有BEGIN条件，则读入第一行，把第一行的数据依次赋予$0、$1、$2等变量。其中$0代表此行的整体数据，$1代表第一字段，$2代表第二字段。3）依据条件类型判断动作是否执行。如果条件符合，则执行动作，否则读入下一行数据。如果没有条件，则每行都执行动作。4）读入下一行数据，重复执行以上步骤。\n再举个例子，如果我想看看Sc用户的平均成绩呢:\n例子2:\n123[root@localhost ~]$ awk &#x27;$2 ~ /AAA/ &#123;printf $6 &quot;\\n&quot;&#125;&#x27; student.txt#如果第二字段中输入包含有“Sc”字符，则打印第六字段数据85.66\n这里要注意在awk中，使用“//”包含的字符串，awk命令才会查找。也就是说字符串必须用“//”包含，awk命令才能正确识别。\n正则表达式\n如果要想让awk 识别字符串，必须使用“//”包含，例如:例子1:\n12[root@localhost ~]$ awk &#x27;/Liming/ &#123;print&#125;’student.txt#打印Liming的成绩\n当使用df命令查看分区使用情况是，如果我只想查看真正的系统分区的使用状况，而不想查看光盘和临时分区的使用状况，则可以:\n例子2:\n12[root@localhost ~]$ df -h | awk &#x27;/sda[O-9]/ &#123;printf $1 &quot;\\t&quot; $5 &quot;\\n&quot;&#125;’#查询包含有sda数字的行，并打印第一字段和第五字段\nawk 内置变量\nawk常用统计实例1、打印文件的第一列(域) ： awk ‘{print $1}’ filename\n2、打印文件的前两列(域) ： awk ‘{print $1,$2}’ filename\n3、打印完第一列，然后打印第二列 ：awk ‘{print $1 $2}’ filename\n4、打印文本文件的总行数 ：awk ‘END{print NR}’ filename\n5、打印文本第一行 ：awk ‘NR==1{print}’ filename\n6、打印文本第二行第一列 ：sed -n “2, 1p” filename | awk ‘print $1’\n\n获取第一列ps -aux | grep watchdog | awk ‘{print $1}’\n\n获取第一列，第二列，第三列ps -aux | grep watchdog | awk ‘{print $1, $2, $3}’\n\n获取第一行的第一列，第二列，第三列ps -aux | grep watchdog | awk ‘NR==1{print $1, $2, $3}’\n\n获取行数NRdf -h | awk ‘END{print NR}’\n\n获取列数NF（这里是获取最后一行的列数，注意每行的列数可能是不同的）ps -aux | grep watchdog | awk ‘END{print NF}’\n\n获取最后一列ps -aux | grep watchdog | awk ‘{print $NF}’\n\n对文件进行操作awk ‘{print $1}’ fileName\n\n指定分隔符（这里以:分割）ps -aux | grep watchdog |awk  -F’:’ ‘{print $1}’\n\n超出范围不报错ps -aux | grep watchdog | awk ‘{print $100}’\n\n\n12[root@localhost ~]$ cat /etc/passwd | grep &quot;/bin/bash&quot; | awk &#x27;&#123;FS=&quot;:&quot;&#125; &#123;printf $1 &quot;\\t&quot; $3 &quot;\\n&quot;&#125;’#查询可以登录的用户的用户名和UID\n这里“:”分隔符生效了，可是第一行却没有起作用，忘记了“BEGIN”条件，那么再来试试;\n1[root@localhost ~]$ cat /etc/passwd | grep &quot;/bin/bash&quot; | awk &#x27;BEGIN &#123;FS=&quot;:&quot;&#125; &#123;printf $1 &quot;\\t&quot; $3 &quot;\\n&quot;&#125;’\n12345[root@localhost ~]$ cat /etc/passwd | grep &quot;/bin/bash&quot; | awk &#x27;BEGIN &#123;FS=&quot;:&quot;&#125; &#123;printf $1 &quot;\\t&quot; $3 &quot;\\t 行号:” NR &quot;\\t 字段数:&quot; NF &quot;\\n&quot;&#125;’#解释下awk命令#开始执行&#123;分隔符是“:”&#125;&#123;输出第一字段和第三字段输出行号(NR值）字段数(NF值）&#125;root     0      行号:1       字段数:7user1   501     行号:2       字段数:7\n如果我只想看看sshd这个伪用户的相关信息，则可以这样使用:\n12[root@localhost ~]$ cat /etc/passwd | awk &#x27;BEGIN &#123;FS=&quot;:&quot;&#125; $1==&quot;sshd&quot; &#123;printf $1 &quot;\\t&quot; $3 &quot;\\t 行号:&quot; NR &quot;\\t 字段数:&quot; NF &quot;\\n&quot;&#125;’#可以看到sshd 伪用户的UID是74，是/etc/passwd_文件的第28行，此行有7个字段\nawk 流程控制我们再来利用下student.txt文件做个练习，后面的使用比较复杂，我们再看看这个文件的内容:\n12345[root@localhost ~]$ cat student.txtID      Name    php   Linux  MySQL   Average1       AAA      66         66       66           662       BBB      77         77       77           773       CCC      88         88       88           88\n我们先来看看该如何在awk中定义变量与调用变量的值。假设我想统计PHP成绩的总分，那么就应该这样:\n1234[root@localhost ~]$ awk &#x27;NR==2 &#123;php1=$3&#125;NR==3 &#123;php2=$3&#125;NR==4 &#123;php3=$3;totle=phpl+php2+php3;print &quot;totle php is &quot; totle&#125;’ student.txt#统计PHIP成绩的总分\n我们解释下这个命令。“NR==2 {iphp1=$3}” (条件是NR==2，动作是php1=$3） 这句话是指如果输入数据是第二行（第一行是标题行），就把第二行的第三字段的值赋予变量“php1”。“NR==3 {php2=$3}” 这句话是指如果输入数据是第三行,就把第三行的第三字段的值赋予变量“php2”。“NR==4 {php3=$3;totle=phpl+php2+php3;print “totle php is “ totle}”（“NR==4”是条件，后面(中的都是动作)这句话是指如果输入数据是第四行﹐就把第四行的第三字段的值赋予变量”php3”;然后定义变量totle的值是“php1+php2+php3”;然后输出“totle php is”关键字，后面加变量totle的值。\n在awk编程中，因为命令语句非常长，在输入格式时需要注意以下内容:\n\n多个条件 {动作} 可以用空格分割，也可以用回车分割。\n\n在一个动作中，如果需要执行多个命令，需要用 “;” 分割，或用回车分割。\n\n在awk中，变量的赋值与调用都不需要加入“$”符。\n\n条件中判断两个值是否相同，请使用 “==”，以便和变量赋值进行区分。\n\n\n在看看该如何实现流程控制，假设如果Linux成绩大于90，就是一个好男人(学PHP的表示压力很大!) :\n1234[root@localhost ~]$ awk &#x27;&#123;if (NR&gt;=2) &#123;if ($4&gt;60) printf $2 &quot;is a good man!\\n&quot;&#125;&#125;’ student.txt#程序中有两个if判断，第一个判断行号大于2，第二个判断Linux成绩大于90分Liming is a good man !Sc is a good man !\n其实在 awk中 if判断语句，完全可以直接利用awk自带的条件来取代，刚刚的脚本可以改写成这样:\n123456[root@localhost ~]$  awk ’NR&gt;=2 &#123;test=$4&#125;test&gt;90 &#123;printf $2 &quot;is a good man! \\n&quot;&#125;’ student.txt#先判断行号如果大于2，就把第四字段赋予变量test#在判断如果test的值大于90分，就打印好男人Liming is a good man!Sc is a good man!\nawk 函数awk编程也允许在编程时使用函数，我们讲讲awk的自定义函数。awk函数的定义方法如下:\n123function 函数名（参数列表）&#123;\t函数体&#125;\n我们定义一个简单的函数，使用函数来打印student.txt的学员姓名和平均成绩，应该这样来写函数：\n12345678[root@localhost ~]$ awk &#x27;function test(a,b) &#123; printf a &quot;\\t&quot; b &quot;\\n&quot;&#125;#定义函数test，包含两个参数，函数体的内容是输出这两个参数的值&#123; test($2,$6) &#125; &#x27; student.txt#调用函数test，并向两个参数传递值。Name    AverageAAA      87.66BBB      85.66CCC      91.66\nawk 中调用脚本对于小的单行程序来说，将脚本作为命令行自变量传递给awk是非常简单的，而对于多行程序就比较难处理。当程序是多行的时候，使用外部脚本是很适合的。首先在外部文件中写好脚本，然后可以使用awk的-f选项，使其读入脚本并且执行。例如，我们可以先编写一个awk脚本:\n123[root@localhost ~]$ vi pass.awkBEGIN &#123;FS=&quot;:&quot;&#125;&#123; print $1 &quot;\\t&quot;  $3&#125;\n然后可以使用“一f”选项来调用这个脚本:\n12345[root@localhost ~]$ awk -f pass.awk /etc/passwdrootobin1daemon2…省略部分输出…\n参考原文链接：\n【Linux】一步一步学Linux系列教程汇总（更新中……）_一步一步学linux 极客学院-CSDN博客\n","slug":"Linux操作系统","date":"2024-07-01T00:00:00.000Z","categories_index":"技术栈","tags_index":"","author_index":"Gueason"},{"id":"f54b8359f18b0e0cc3703fc322a76973","title":"git使用","content":"英文差的小白程序员，怎么从github下载代码？_哔哩哔哩_bilibili\nIDEA版本控制工具VCS中使用Git，以及快捷键总结（不使用命令）_idea的vcs-CSDN博客\ngit工具手动克隆：git clone git@github.com:XXX(具体地址).git\n","slug":"git使用","date":"2024-05-31T16:00:00.000Z","categories_index":"理论","tags_index":"","author_index":"Gueason"}]